read
Begin
step: 0 epoch: 0 loss: 8732.244140625 loss_input: 74.26129150390625
step: 1000 epoch: 0 loss: 104.65911314466021 loss_input: 82.90865177303165
step: 2000 epoch: 0 loss: 70.17248471959242 loss_input: 81.82647668475333
step: 3000 epoch: 0 loss: 57.96353633083609 loss_input: 81.78666065169033
step: 4000 epoch: 0 loss: 51.52162770621927 loss_input: 81.72312746093262
step: 5000 epoch: 0 loss: 47.40657450747094 loss_input: 81.7118155958652
step: 6000 epoch: 0 loss: 44.62509016060189 loss_input: 81.82137953204088
step: 7000 epoch: 0 loss: 42.546330376363386 loss_input: 82.02888663784638
step: 8000 epoch: 0 loss: 41.10319180638175 loss_input: 81.97418814011773
step: 9000 epoch: 0 loss: 39.86446428420795 loss_input: 82.01544371182065
step: 10000 epoch: 0 loss: 38.773049956261545 loss_input: 81.93579984652902
step: 11000 epoch: 0 loss: 37.89053740240034 loss_input: 82.04943393817892
step: 12000 epoch: 0 loss: 37.0887445062908 loss_input: 82.03426129996166
step: 13000 epoch: 0 loss: 36.36900971007525 loss_input: 81.9440671030113
step: 14000 epoch: 0 loss: 35.80048548791266 loss_input: 82.0846173428321
step: 15000 epoch: 0 loss: 35.29398118132838 loss_input: 82.17203032004898
Save loss: 34.827977101624015 Name: 0_train_model.pth
step: 0 epoch: 1 loss: 33.60812759399414 loss_input: 107.8380126953125
step: 1000 epoch: 1 loss: 26.599059328332647 loss_input: 81.29384860744725
step: 2000 epoch: 1 loss: 26.678376101303673 loss_input: 81.89972454568614
step: 3000 epoch: 1 loss: 26.615261800366216 loss_input: 82.09094518949412
step: 4000 epoch: 1 loss: 26.403553869389974 loss_input: 81.82864568281758
step: 5000 epoch: 1 loss: 26.361620581214414 loss_input: 81.85039249474323
step: 6000 epoch: 1 loss: 26.306406675944864 loss_input: 81.95261966226657
step: 7000 epoch: 1 loss: 26.281372540066233 loss_input: 82.06759466142114
step: 8000 epoch: 1 loss: 26.180829121759274 loss_input: 81.99979034755665
step: 9000 epoch: 1 loss: 26.13262966952977 loss_input: 82.16597769247427
step: 10000 epoch: 1 loss: 26.070401465531624 loss_input: 82.30982495012218
step: 11000 epoch: 1 loss: 26.02689784497047 loss_input: 82.44024300347695
step: 12000 epoch: 1 loss: 25.957435546085424 loss_input: 82.3910833256571
step: 13000 epoch: 1 loss: 25.889718761426487 loss_input: 82.38509376701342
step: 14000 epoch: 1 loss: 25.85923579396303 loss_input: 82.37808229936701
step: 15000 epoch: 1 loss: 25.782575944242268 loss_input: 82.34083384512647
Save loss: 25.684462902337312 Name: 1_train_model.pth
step: 0 epoch: 2 loss: 35.48899459838867 loss_input: 76.7945556640625
step: 1000 epoch: 2 loss: 25.09691257838841 loss_input: 81.41752205314216
step: 2000 epoch: 2 loss: 24.912225667027936 loss_input: 82.71549942802037
step: 3000 epoch: 2 loss: 24.944031825505746 loss_input: 82.61646004066353
step: 4000 epoch: 2 loss: 24.89095854800929 loss_input: 82.5587043180611
step: 5000 epoch: 2 loss: 24.88106028896836 loss_input: 82.5546534125792
step: 6000 epoch: 2 loss: 24.78781378680399 loss_input: 82.52010535272116
step: 7000 epoch: 2 loss: 24.711409652016876 loss_input: 82.58476184310445
step: 8000 epoch: 2 loss: 24.597934526706425 loss_input: 82.62918570267828
step: 9000 epoch: 2 loss: 24.53035734664227 loss_input: 82.49657691842833
step: 10000 epoch: 2 loss: 24.485308817369607 loss_input: 82.46621493856473
step: 11000 epoch: 2 loss: 24.430329627356155 loss_input: 82.47642550517425
step: 12000 epoch: 2 loss: 24.349307145389773 loss_input: 82.43332232384769
step: 13000 epoch: 2 loss: 24.312466171849426 loss_input: 82.3794174082838
step: 14000 epoch: 2 loss: 24.23604671046288 loss_input: 82.37142969838092
step: 15000 epoch: 2 loss: 24.183075228322945 loss_input: 82.38383635345534
Save loss: 24.115492206662893 Name: 2_train_model.pth
step: 0 epoch: 3 loss: 19.502803802490234 loss_input: 80.98583984375
step: 1000 epoch: 3 loss: 23.708493528547105 loss_input: 81.39774373575644
step: 2000 epoch: 3 loss: 23.713821282927718 loss_input: 82.53589080703789
step: 3000 epoch: 3 loss: 23.69547396951896 loss_input: 82.84761189222733
step: 4000 epoch: 3 loss: 23.605001465435834 loss_input: 83.03476920076622
step: 5000 epoch: 3 loss: 23.582371515122635 loss_input: 83.12381747283357
step: 6000 epoch: 3 loss: 23.51740965742287 loss_input: 82.96153547366129
step: 7000 epoch: 3 loss: 23.471941986827744 loss_input: 82.75699002053837
step: 8000 epoch: 3 loss: 23.426790401795824 loss_input: 82.63846035358861
step: 9000 epoch: 3 loss: 23.44262604543387 loss_input: 82.71753791543779
step: 10000 epoch: 3 loss: 23.39735096047585 loss_input: 82.67371951159376
step: 11000 epoch: 3 loss: 23.371837821074827 loss_input: 82.57491328358466
step: 12000 epoch: 3 loss: 23.31655336747457 loss_input: 82.46694678519628
step: 13000 epoch: 3 loss: 23.285879854440378 loss_input: 82.47381395091003
step: 14000 epoch: 3 loss: 23.296971962951044 loss_input: 82.40857710864542
step: 15000 epoch: 3 loss: 23.289638309302024 loss_input: 82.35073379141261
Save loss: 23.248050446748735 Name: 3_train_model.pth
step: 0 epoch: 4 loss: 29.765478134155273 loss_input: 97.75347900390625
step: 1000 epoch: 4 loss: 22.583006441533627 loss_input: 81.91761961183347
step: 2000 epoch: 4 loss: 22.823396592185475 loss_input: 82.01899752028282
step: 3000 epoch: 4 loss: 22.710831352648277 loss_input: 82.10301827288039
step: 4000 epoch: 4 loss: 22.68460811647407 loss_input: 81.99718574689824
step: 5000 epoch: 4 loss: 22.673081796185013 loss_input: 82.26071028470088
step: 6000 epoch: 4 loss: 22.685976438056706 loss_input: 82.30977215263927
step: 7000 epoch: 4 loss: 22.6933499705943 loss_input: 82.31925329026657
step: 8000 epoch: 4 loss: 22.721626726631342 loss_input: 82.29775740465303
step: 9000 epoch: 4 loss: 22.705458507552674 loss_input: 82.26023635720163
step: 10000 epoch: 4 loss: 22.713928030271695 loss_input: 82.30745927482447
step: 11000 epoch: 4 loss: 22.709974313343604 loss_input: 82.22562749866312
step: 12000 epoch: 4 loss: 22.71161110462144 loss_input: 82.24082950973161
step: 13000 epoch: 4 loss: 22.686064130534998 loss_input: 82.23602102910654
step: 14000 epoch: 4 loss: 22.68334098870682 loss_input: 82.16508376755874
step: 15000 epoch: 4 loss: 22.67832382144423 loss_input: 82.23103266911112
Save loss: 22.679384616553783 Name: 4_train_model.pth
step: 0 epoch: 5 loss: 22.450159072875977 loss_input: 74.42230224609375
step: 1000 epoch: 5 loss: 22.643987299321772 loss_input: 82.76057188660948
step: 2000 epoch: 5 loss: 22.568030477225452 loss_input: 83.0176328766769
step: 3000 epoch: 5 loss: 22.494183700031456 loss_input: 82.97784983130623
step: 4000 epoch: 5 loss: 22.41834427922465 loss_input: 82.85243459642902
step: 5000 epoch: 5 loss: 22.391021062125922 loss_input: 82.53524872418095
step: 6000 epoch: 5 loss: 22.38934786044405 loss_input: 82.50434894558589
step: 7000 epoch: 5 loss: 22.351088274855083 loss_input: 82.4472338563799
step: 8000 epoch: 5 loss: 22.36482419742374 loss_input: 82.45288812165082
step: 9000 epoch: 5 loss: 22.30609508143996 loss_input: 82.19604842761612
step: 10000 epoch: 5 loss: 22.341686816301813 loss_input: 82.28538468784957
step: 11000 epoch: 5 loss: 22.33640082451812 loss_input: 82.22377238721373
step: 12000 epoch: 5 loss: 22.327704350756225 loss_input: 82.27294520093703
step: 13000 epoch: 5 loss: 22.32438662503098 loss_input: 82.2863246218515
step: 14000 epoch: 5 loss: 22.337060464381253 loss_input: 82.33550216732566
step: 15000 epoch: 5 loss: 22.29006346355525 loss_input: 82.2383984174818
Save loss: 22.25587961539626 Name: 5_train_model.pth
step: 0 epoch: 6 loss: 25.067489624023438 loss_input: 93.3343505859375
step: 1000 epoch: 6 loss: 22.118874549865723 loss_input: 82.48509912938624
step: 2000 epoch: 6 loss: 22.174183441840785 loss_input: 82.09274876111749
step: 3000 epoch: 6 loss: 21.98933117598623 loss_input: 82.40504261844359
step: 4000 epoch: 6 loss: 22.04223581636825 loss_input: 82.25418422616026
step: 5000 epoch: 6 loss: 22.035668056456 loss_input: 82.25525771310582
step: 6000 epoch: 6 loss: 21.907839000513107 loss_input: 81.9736992280258
step: 7000 epoch: 6 loss: 21.877401440675865 loss_input: 81.99825053766035
step: 8000 epoch: 6 loss: 21.871681715485277 loss_input: 82.07521252989129
step: 9000 epoch: 6 loss: 21.93801241728375 loss_input: 82.27370057976414
step: 10000 epoch: 6 loss: 21.95722624984053 loss_input: 82.21209481224729
step: 11000 epoch: 6 loss: 21.95921112526418 loss_input: 82.20985618849991
step: 12000 epoch: 6 loss: 21.946789282916455 loss_input: 82.14058554914531
step: 13000 epoch: 6 loss: 21.93539502446298 loss_input: 82.18555096873558
step: 14000 epoch: 6 loss: 21.940837594153464 loss_input: 82.26072586612116
step: 15000 epoch: 6 loss: 21.933900791552453 loss_input: 82.22163055340582
Save loss: 21.916805622935296 Name: 6_train_model.pth
step: 0 epoch: 7 loss: 26.88184356689453 loss_input: 76.47705078125
step: 1000 epoch: 7 loss: 21.719387891885642 loss_input: 82.35157993861607
step: 2000 epoch: 7 loss: 21.66963177094276 loss_input: 81.65265420268382
step: 3000 epoch: 7 loss: 21.761643157725093 loss_input: 81.75955268328526
step: 4000 epoch: 7 loss: 21.710780204757693 loss_input: 81.8350171215741
step: 5000 epoch: 7 loss: 21.70368313174371 loss_input: 82.06911266672446
step: 6000 epoch: 7 loss: 21.671173756330376 loss_input: 81.95417930833142
step: 7000 epoch: 7 loss: 21.666280075305362 loss_input: 81.96108051981692
step: 8000 epoch: 7 loss: 21.670987403358524 loss_input: 82.12347469322683
step: 9000 epoch: 7 loss: 21.65902395018497 loss_input: 82.2066215258309
step: 10000 epoch: 7 loss: 21.64671993179329 loss_input: 82.303494259949
step: 11000 epoch: 7 loss: 21.665723610201724 loss_input: 82.38230747999295
step: 12000 epoch: 7 loss: 21.638399160898643 loss_input: 82.26510553238403
step: 13000 epoch: 7 loss: 21.640758757645898 loss_input: 82.28617339306231
step: 14000 epoch: 7 loss: 21.640517533962136 loss_input: 82.17699241488332
step: 15000 epoch: 7 loss: 21.646202077135136 loss_input: 82.24664342852404
Save loss: 21.6288453733325 Name: 7_train_model.pth
step: 0 epoch: 8 loss: 23.165616989135742 loss_input: 68.51788330078125
step: 1000 epoch: 8 loss: 21.32212315834724 loss_input: 81.08710954572771
step: 2000 epoch: 8 loss: 21.44435062937472 loss_input: 81.71011323561852
step: 3000 epoch: 8 loss: 21.38177817744122 loss_input: 81.87946101706332
step: 4000 epoch: 8 loss: 21.496132003280767 loss_input: 82.39425754523283
step: 5000 epoch: 8 loss: 21.474864565737366 loss_input: 82.56941767164145
step: 6000 epoch: 8 loss: 21.455778887621108 loss_input: 82.48143987110547
step: 7000 epoch: 8 loss: 21.44287497655713 loss_input: 82.32969357997686
step: 8000 epoch: 8 loss: 21.36859126222713 loss_input: 82.30254231374154
step: 9000 epoch: 8 loss: 21.34210263685496 loss_input: 82.30244411048513
step: 10000 epoch: 8 loss: 21.344146550768507 loss_input: 82.30236646135191
step: 11000 epoch: 8 loss: 21.369201984246008 loss_input: 82.2736448619639
step: 12000 epoch: 8 loss: 21.370298997629344 loss_input: 82.2687950900332
step: 13000 epoch: 8 loss: 21.372559638786623 loss_input: 82.26423251020955
step: 14000 epoch: 8 loss: 21.378450893230177 loss_input: 82.27011080280677
step: 15000 epoch: 8 loss: 21.386871862726192 loss_input: 82.24644459330966
Save loss: 21.37500160267949 Name: 8_train_model.pth
step: 0 epoch: 9 loss: 25.34743881225586 loss_input: 64.2576904296875
step: 1000 epoch: 9 loss: 20.91533056958453 loss_input: 81.59391908140688
step: 2000 epoch: 9 loss: 21.100956177604253 loss_input: 82.00948558313617
step: 3000 epoch: 9 loss: 21.094307226882066 loss_input: 82.28852548824871
step: 4000 epoch: 9 loss: 21.143406697911818 loss_input: 82.7016996513811
step: 5000 epoch: 9 loss: 21.159922821954165 loss_input: 82.51054205662774
step: 6000 epoch: 9 loss: 21.155851198224063 loss_input: 82.45650050553098
step: 7000 epoch: 9 loss: 21.090950464592748 loss_input: 82.09852089001237
step: 8000 epoch: 9 loss: 21.154096585097214 loss_input: 82.17965526983687
step: 9000 epoch: 9 loss: 21.14583409345093 loss_input: 82.3149671873481
step: 10000 epoch: 9 loss: 21.143145294931337 loss_input: 82.30215761382846
step: 11000 epoch: 9 loss: 21.140567437593162 loss_input: 82.31231118548882
step: 12000 epoch: 9 loss: 21.141362217145904 loss_input: 82.26933092387972
step: 13000 epoch: 9 loss: 21.14169673205944 loss_input: 82.18167751087206
step: 14000 epoch: 9 loss: 21.13685764134692 loss_input: 82.1536410642125
step: 15000 epoch: 9 loss: 21.147185117481122 loss_input: 82.20988217891593
Save loss: 21.157277643591165 Name: 9_train_model.pth
step: 0 epoch: 10 loss: 40.42790985107422 loss_input: 74.583251953125
step: 1000 epoch: 10 loss: 20.937460343439977 loss_input: 80.1667654245169
step: 2000 epoch: 10 loss: 20.877144217789024 loss_input: 81.27080258698776
step: 3000 epoch: 10 loss: 20.91000119514046 loss_input: 81.74783712639406
step: 4000 epoch: 10 loss: 21.009095447714763 loss_input: 82.14263606339627
step: 5000 epoch: 10 loss: 20.926371380463287 loss_input: 82.03576552755855
step: 6000 epoch: 10 loss: 20.888529565687517 loss_input: 82.0103583708542
step: 7000 epoch: 10 loss: 20.910260311179016 loss_input: 82.30014658376092
step: 8000 epoch: 10 loss: 20.9287624207754 loss_input: 82.11729799796754
step: 9000 epoch: 10 loss: 20.901833147780653 loss_input: 82.06153112707528
step: 10000 epoch: 10 loss: 20.9045033110653 loss_input: 82.03446033570471
step: 11000 epoch: 10 loss: 20.922831314713854 loss_input: 82.14266407688079
step: 12000 epoch: 10 loss: 20.954274666546763 loss_input: 82.1229545009026
step: 13000 epoch: 10 loss: 20.9602913639012 loss_input: 82.18875853262189
step: 14000 epoch: 10 loss: 20.98225554612081 loss_input: 82.25188623960594
step: 15000 epoch: 10 loss: 20.96675032846499 loss_input: 82.1980422903447
Save loss: 20.963989174574614 Name: 10_train_model.pth
step: 0 epoch: 11 loss: 18.099246978759766 loss_input: 68.42327880859375
step: 1000 epoch: 11 loss: 20.51586838130589 loss_input: 81.51306634039788
step: 2000 epoch: 11 loss: 20.690540571560685 loss_input: 82.0083065461898
step: 3000 epoch: 11 loss: 20.770093233177796 loss_input: 81.90643554606147
step: 4000 epoch: 11 loss: 20.84931015831266 loss_input: 81.83759698198008
step: 5000 epoch: 11 loss: 20.856887972037857 loss_input: 82.15127774005745
step: 6000 epoch: 11 loss: 20.876210585055283 loss_input: 82.07399265242903
step: 7000 epoch: 11 loss: 20.83115518639691 loss_input: 81.92476107975088
step: 8000 epoch: 11 loss: 20.797193931588648 loss_input: 82.05971648868598
step: 9000 epoch: 11 loss: 20.782851043349837 loss_input: 81.98257791099383
step: 10000 epoch: 11 loss: 20.76467403003352 loss_input: 81.94743593267626
step: 11000 epoch: 11 loss: 20.767590727700764 loss_input: 81.98038846832462
step: 12000 epoch: 11 loss: 20.78181135383271 loss_input: 82.03463630940098
step: 13000 epoch: 11 loss: 20.7931632701823 loss_input: 82.10391244506131
step: 14000 epoch: 11 loss: 20.77476928823463 loss_input: 82.1217809480545
step: 15000 epoch: 11 loss: 20.770305954339577 loss_input: 82.14541176584322
Save loss: 20.786398529708386 Name: 11_train_model.pth
step: 0 epoch: 12 loss: 23.043180465698242 loss_input: 69.19482421875
step: 1000 epoch: 12 loss: 20.713216669671425 loss_input: 82.59951968197818
step: 2000 epoch: 12 loss: 20.557738158537234 loss_input: 82.10450829272864
step: 3000 epoch: 12 loss: 20.592502310529465 loss_input: 82.23793680101424
step: 4000 epoch: 12 loss: 20.58257133583521 loss_input: 82.35495812021115
step: 5000 epoch: 12 loss: 20.631041047192173 loss_input: 82.3240367185352
step: 6000 epoch: 12 loss: 20.630917252113253 loss_input: 82.21164083329862
step: 7000 epoch: 12 loss: 20.612739300697196 loss_input: 82.13680396034384
step: 8000 epoch: 12 loss: 20.623276659733445 loss_input: 82.06648002465268
step: 9000 epoch: 12 loss: 20.603564616164 loss_input: 81.97179652971501
step: 10000 epoch: 12 loss: 20.60288892318196 loss_input: 82.07899963787801
step: 11000 epoch: 12 loss: 20.626904319908476 loss_input: 82.1003992479288
step: 12000 epoch: 12 loss: 20.634583426797285 loss_input: 82.14622033348621
step: 13000 epoch: 12 loss: 20.654692711018477 loss_input: 82.25286977905996
step: 14000 epoch: 12 loss: 20.65622919178479 loss_input: 82.27357649328061
step: 15000 epoch: 12 loss: 20.65471919590088 loss_input: 82.32305157113875
Save loss: 20.648055327132344 Name: 12_train_model.pth
step: 0 epoch: 13 loss: 23.618125915527344 loss_input: 82.488525390625
step: 1000 epoch: 13 loss: 20.58335451408104 loss_input: 82.37714610828624
step: 2000 epoch: 13 loss: 20.324120449340683 loss_input: 81.83245477480986
step: 3000 epoch: 13 loss: 20.502707053327196 loss_input: 82.26437771475264
step: 4000 epoch: 13 loss: 20.40915193083405 loss_input: 81.83494106026716
step: 5000 epoch: 13 loss: 20.45617560590894 loss_input: 82.13475791169891
step: 6000 epoch: 13 loss: 20.423757263708662 loss_input: 82.07542348492048
step: 7000 epoch: 13 loss: 20.43521934542515 loss_input: 82.03626700963757
step: 8000 epoch: 13 loss: 20.45970069043503 loss_input: 82.10428191715293
step: 9000 epoch: 13 loss: 20.495683904833665 loss_input: 82.15653417523922
step: 10000 epoch: 13 loss: 20.4824017861428 loss_input: 82.13774142605271
step: 11000 epoch: 13 loss: 20.48493171533599 loss_input: 82.0756508263986
step: 12000 epoch: 13 loss: 20.499214357916944 loss_input: 82.1570456032554
step: 13000 epoch: 13 loss: 20.484367820888508 loss_input: 82.09367227535982
step: 14000 epoch: 13 loss: 20.488210771519935 loss_input: 82.10118921820875
step: 15000 epoch: 13 loss: 20.502000066202708 loss_input: 82.1655238853217
Save loss: 20.50581438946724 Name: 13_train_model.pth
step: 0 epoch: 14 loss: 26.01840591430664 loss_input: 83.60986328125
step: 1000 epoch: 14 loss: 20.125357215816564 loss_input: 82.74498511742163
step: 2000 epoch: 14 loss: 20.128091245934346 loss_input: 82.3039544070738
step: 3000 epoch: 14 loss: 20.259047121335886 loss_input: 82.71295021613889
step: 4000 epoch: 14 loss: 20.3449046419311 loss_input: 82.53648009201314
step: 5000 epoch: 14 loss: 20.324058035473136 loss_input: 82.22304386778895
step: 6000 epoch: 14 loss: 20.302142037170924 loss_input: 81.99491506257746
step: 7000 epoch: 14 loss: 20.286234370232854 loss_input: 81.97606344322463
step: 8000 epoch: 14 loss: 20.305632350415294 loss_input: 82.06502968256898
step: 9000 epoch: 14 loss: 20.329083932981057 loss_input: 81.98981107540786
step: 10000 epoch: 14 loss: 20.355905339212708 loss_input: 82.17439413285234
step: 11000 epoch: 14 loss: 20.395057942366428 loss_input: 82.20433471779555
step: 12000 epoch: 14 loss: 20.414488663843063 loss_input: 82.2573702099939
step: 13000 epoch: 14 loss: 20.407462445380787 loss_input: 82.25157888620947
step: 14000 epoch: 14 loss: 20.42224856405869 loss_input: 82.27312518797486
step: 15000 epoch: 14 loss: 20.41556279839345 loss_input: 82.26059369072661
Save loss: 20.395687653779984 Name: 14_train_model.pth
step: 0 epoch: 15 loss: 20.085102081298828 loss_input: 78.02593994140625
step: 1000 epoch: 15 loss: 20.228282050533846 loss_input: 82.28183736429585
step: 2000 epoch: 15 loss: 20.243686148192154 loss_input: 82.70789776010432
step: 3000 epoch: 15 loss: 20.17370419182884 loss_input: 82.29985606197674
step: 4000 epoch: 15 loss: 20.105566763216423 loss_input: 82.32411216348744
step: 5000 epoch: 15 loss: 20.121496166236113 loss_input: 82.19219608226793
step: 6000 epoch: 15 loss: 20.120926485640112 loss_input: 82.06431604254108
step: 7000 epoch: 15 loss: 20.16155669543083 loss_input: 82.19679594809558
step: 8000 epoch: 15 loss: 20.15935696808789 loss_input: 81.99205258339289
step: 9000 epoch: 15 loss: 20.188732579739515 loss_input: 82.22926037460681
step: 10000 epoch: 15 loss: 20.221298776356154 loss_input: 82.29036718520531
step: 11000 epoch: 15 loss: 20.20659930430134 loss_input: 82.29043047866564
step: 12000 epoch: 15 loss: 20.21805374092504 loss_input: 82.36068173009508
step: 13000 epoch: 15 loss: 20.23789550991996 loss_input: 82.36272865203277
step: 14000 epoch: 15 loss: 20.24839687318463 loss_input: 82.32730798508115
step: 15000 epoch: 15 loss: 20.263819622124092 loss_input: 82.261680719495
Save loss: 20.274457624897362 Name: 15_train_model.pth
step: 0 epoch: 16 loss: 26.992124557495117 loss_input: 111.9390869140625
step: 1000 epoch: 16 loss: 20.377987340018226 loss_input: 83.72651652546672
step: 2000 epoch: 16 loss: 20.192685492809627 loss_input: 82.91506663660357
step: 3000 epoch: 16 loss: 20.24761782388455 loss_input: 82.41098712945295
step: 4000 epoch: 16 loss: 20.242871290682196 loss_input: 82.3384158690552
step: 5000 epoch: 16 loss: 20.18360475434515 loss_input: 82.24285282103736
step: 6000 epoch: 16 loss: 20.20593955528337 loss_input: 82.1665920771831
step: 7000 epoch: 16 loss: 20.20861291401796 loss_input: 82.24095066845919
step: 8000 epoch: 16 loss: 20.19900414502974 loss_input: 82.2836818172997
step: 9000 epoch: 16 loss: 20.201972441572096 loss_input: 82.34981278257494
step: 10000 epoch: 16 loss: 20.191023830461116 loss_input: 82.43829883979376
step: 11000 epoch: 16 loss: 20.176063132886398 loss_input: 82.52984363371085
step: 12000 epoch: 16 loss: 20.195408538841882 loss_input: 82.48990629259899
step: 13000 epoch: 16 loss: 20.18675552934016 loss_input: 82.40974387656027
step: 14000 epoch: 16 loss: 20.186635688597555 loss_input: 82.40711900879847
step: 15000 epoch: 16 loss: 20.179787546227388 loss_input: 82.32437346380684
Save loss: 20.172558823168277 Name: 16_train_model.pth
step: 0 epoch: 17 loss: 21.758481979370117 loss_input: 80.6263427734375
step: 1000 epoch: 17 loss: 19.984838620527878 loss_input: 82.10143799072021
step: 2000 epoch: 17 loss: 20.09601418868355 loss_input: 82.57094328812157
step: 3000 epoch: 17 loss: 20.226820019394985 loss_input: 82.81997486433559
step: 4000 epoch: 17 loss: 20.23056569048179 loss_input: 82.56895827007604
step: 5000 epoch: 17 loss: 20.174477649388184 loss_input: 82.28580337399318
step: 6000 epoch: 17 loss: 20.109284642894472 loss_input: 82.19511715902168
step: 7000 epoch: 17 loss: 20.154258271146375 loss_input: 82.47060689502504
step: 8000 epoch: 17 loss: 20.144766826627254 loss_input: 82.49847510516264
step: 9000 epoch: 17 loss: 20.132235078702514 loss_input: 82.42897898090533
step: 10000 epoch: 17 loss: 20.10742155626623 loss_input: 82.28322123232013
step: 11000 epoch: 17 loss: 20.101285620847946 loss_input: 82.2949436514738
step: 12000 epoch: 17 loss: 20.104801760346838 loss_input: 82.37881425734054
step: 13000 epoch: 17 loss: 20.123287592821054 loss_input: 82.33654826938643
step: 14000 epoch: 17 loss: 20.099320792307232 loss_input: 82.28555888760457
step: 15000 epoch: 17 loss: 20.09080249021454 loss_input: 82.25434007117624
Save loss: 20.077778635561465 Name: 17_train_model.pth
step: 0 epoch: 18 loss: 15.62985610961914 loss_input: 61.01324462890625
step: 1000 epoch: 18 loss: 19.78396589391596 loss_input: 81.29465541782436
step: 2000 epoch: 18 loss: 19.782231413799785 loss_input: 81.5819493512521
step: 3000 epoch: 18 loss: 19.867531938657727 loss_input: 81.84324371937869
step: 4000 epoch: 18 loss: 19.865860586135394 loss_input: 82.03194735068674
step: 5000 epoch: 18 loss: 19.966833632937718 loss_input: 82.21044775419917
step: 6000 epoch: 18 loss: 20.02909825774277 loss_input: 82.2927589359293
step: 7000 epoch: 18 loss: 20.044752578601855 loss_input: 82.38165133252448
step: 8000 epoch: 18 loss: 20.042438715133887 loss_input: 82.51786184960523
step: 9000 epoch: 18 loss: 20.055404507283463 loss_input: 82.42006512893967
step: 10000 epoch: 18 loss: 20.04754784226167 loss_input: 82.36948124035253
step: 11000 epoch: 18 loss: 20.042398374954708 loss_input: 82.29409538430633
step: 12000 epoch: 18 loss: 20.039214094324258 loss_input: 82.45415535542122
step: 13000 epoch: 18 loss: 20.01738999728175 loss_input: 82.31221327419675
step: 14000 epoch: 18 loss: 20.02172436195479 loss_input: 82.34826218118907
step: 15000 epoch: 18 loss: 20.007923180576515 loss_input: 82.28078723381331
Save loss: 20.004509253561498 Name: 18_train_model.pth
step: 0 epoch: 19 loss: 20.65306854248047 loss_input: 88.1015625
step: 1000 epoch: 19 loss: 20.084584802538007 loss_input: 82.57932692307692
step: 2000 epoch: 19 loss: 20.104916437216726 loss_input: 82.4002641318501
step: 3000 epoch: 19 loss: 20.005688833658077 loss_input: 82.0578271801652
step: 4000 epoch: 19 loss: 19.839986400466 loss_input: 81.9474499356118
step: 5000 epoch: 19 loss: 19.824588058996476 loss_input: 81.86653467646315
step: 6000 epoch: 19 loss: 19.802392064799985 loss_input: 82.06095261995722
step: 7000 epoch: 19 loss: 19.812614188570922 loss_input: 82.08993957894408
step: 8000 epoch: 19 loss: 19.86085167540236 loss_input: 82.19998819269905
step: 9000 epoch: 19 loss: 19.831921686160406 loss_input: 82.14056830463933
step: 10000 epoch: 19 loss: 19.86747198545412 loss_input: 82.07697928937576
step: 11000 epoch: 19 loss: 19.88188046609778 loss_input: 82.0734680580796
step: 12000 epoch: 19 loss: 19.87787073229305 loss_input: 82.16117257867671
step: 13000 epoch: 19 loss: 19.893355969272847 loss_input: 82.26303924731901
step: 14000 epoch: 19 loss: 19.90228661604672 loss_input: 82.26850052702709
step: 15000 epoch: 19 loss: 19.88274671312857 loss_input: 82.14908730205747
Save loss: 19.907506893992423 Name: 19_train_model.pth
step: 0 epoch: 20 loss: 15.248661041259766 loss_input: 76.3060302734375
step: 1000 epoch: 20 loss: 19.857391891422328 loss_input: 82.60955572747565
step: 2000 epoch: 20 loss: 19.852886016460612 loss_input: 83.1146809407796
step: 3000 epoch: 20 loss: 19.812800414718733 loss_input: 82.8516005122316
step: 4000 epoch: 20 loss: 19.84407807898146 loss_input: 82.7211799039986
step: 5000 epoch: 20 loss: 19.768043074219783 loss_input: 82.5458961674462
step: 6000 epoch: 20 loss: 19.769340932061326 loss_input: 82.44910630129354
step: 7000 epoch: 20 loss: 19.776948431086122 loss_input: 82.45632454640966
step: 8000 epoch: 20 loss: 19.76589239497376 loss_input: 82.27095256586578
step: 9000 epoch: 20 loss: 19.814085078231493 loss_input: 82.37001682294633
step: 10000 epoch: 20 loss: 19.8274317717936 loss_input: 82.32373156117005
step: 11000 epoch: 20 loss: 19.834715569347654 loss_input: 82.32969889256339
step: 12000 epoch: 20 loss: 19.83244461552738 loss_input: 82.2716930317249
step: 13000 epoch: 20 loss: 19.83936837064093 loss_input: 82.36289125596107
step: 14000 epoch: 20 loss: 19.84297979597483 loss_input: 82.28447608356518
step: 15000 epoch: 20 loss: 19.825167761159875 loss_input: 82.22011378440656
Save loss: 19.829966906160116 Name: 20_train_model.pth
step: 0 epoch: 21 loss: 15.998072624206543 loss_input: 74.6903076171875
step: 1000 epoch: 21 loss: 19.64447875742193 loss_input: 81.68935684676651
step: 2000 epoch: 21 loss: 19.816703251395925 loss_input: 82.32702035822909
step: 3000 epoch: 21 loss: 19.750928616611134 loss_input: 82.08947837293168
step: 4000 epoch: 21 loss: 19.732843898648294 loss_input: 82.23904810235429
step: 5000 epoch: 21 loss: 19.688400318135837 loss_input: 82.11373283204688
step: 6000 epoch: 21 loss: 19.69494291020123 loss_input: 82.05916064160205
step: 7000 epoch: 21 loss: 19.70532469530818 loss_input: 82.05812111767236
step: 8000 epoch: 21 loss: 19.71599524182955 loss_input: 82.12399974356232
step: 9000 epoch: 21 loss: 19.70405121170644 loss_input: 82.09108129695659
step: 10000 epoch: 21 loss: 19.727647073244334 loss_input: 82.12653236019172
step: 11000 epoch: 21 loss: 19.719435112442323 loss_input: 82.08373378188098
step: 12000 epoch: 21 loss: 19.73344281784406 loss_input: 82.13864723919491
step: 13000 epoch: 21 loss: 19.750355057253874 loss_input: 82.25772934364875
step: 14000 epoch: 21 loss: 19.77358051266264 loss_input: 82.31215706795217
step: 15000 epoch: 21 loss: 19.76019040464314 loss_input: 82.23481825251363
Save loss: 19.767116904318332 Name: 21_train_model.pth
step: 0 epoch: 22 loss: 18.974388122558594 loss_input: 104.1466064453125
step: 1000 epoch: 22 loss: 19.758222609490424 loss_input: 82.32156161709385
step: 2000 epoch: 22 loss: 19.784861164769787 loss_input: 82.11991480456061
step: 3000 epoch: 22 loss: 19.699614572350242 loss_input: 82.17798280700053
step: 4000 epoch: 22 loss: 19.750494006275385 loss_input: 81.92695399148259
step: 5000 epoch: 22 loss: 19.70368871400891 loss_input: 82.05477872931273
step: 6000 epoch: 22 loss: 19.67586083190478 loss_input: 82.01891297536022
step: 7000 epoch: 22 loss: 19.67252697322798 loss_input: 81.95786080852847
step: 8000 epoch: 22 loss: 19.666083256463203 loss_input: 82.00478999725864
step: 9000 epoch: 22 loss: 19.63353206149579 loss_input: 81.98784259160007
step: 10000 epoch: 22 loss: 19.627901871005793 loss_input: 82.05323322037913
step: 11000 epoch: 22 loss: 19.637958756751033 loss_input: 82.11356896576518
step: 12000 epoch: 22 loss: 19.66849333685087 loss_input: 82.12589726918102
step: 13000 epoch: 22 loss: 19.684195390912848 loss_input: 82.17766910996329
step: 14000 epoch: 22 loss: 19.659984784538036 loss_input: 82.12838368252017
step: 15000 epoch: 22 loss: 19.69126831162064 loss_input: 82.17553602302164
Save loss: 19.699878420814873 Name: 22_train_model.pth
step: 0 epoch: 23 loss: 22.8159236907959 loss_input: 114.84442138671875
step: 1000 epoch: 23 loss: 19.77077639996112 loss_input: 83.55074222652348
step: 2000 epoch: 23 loss: 19.674034376015726 loss_input: 82.82967543327945
step: 3000 epoch: 23 loss: 19.717796629804326 loss_input: 83.32097571073314
step: 4000 epoch: 23 loss: 19.666515725042128 loss_input: 82.90537087102736
step: 5000 epoch: 23 loss: 19.694595479936606 loss_input: 82.61068494308951
step: 6000 epoch: 23 loss: 19.63015918663354 loss_input: 82.26437830023121
step: 7000 epoch: 23 loss: 19.59740311771508 loss_input: 82.23219266841623
step: 8000 epoch: 23 loss: 19.617567068814427 loss_input: 82.13574825973902
step: 9000 epoch: 23 loss: 19.61715430517274 loss_input: 82.13507012946913
step: 10000 epoch: 23 loss: 19.610761162567442 loss_input: 82.07088101614643
step: 11000 epoch: 23 loss: 19.61171965178181 loss_input: 82.07285206960056
step: 12000 epoch: 23 loss: 19.6041793118774 loss_input: 81.96346482133163
step: 13000 epoch: 23 loss: 19.629374505152988 loss_input: 82.14810939084914
step: 14000 epoch: 23 loss: 19.6338953563515 loss_input: 82.08117525819863
step: 15000 epoch: 23 loss: 19.66071636506251 loss_input: 82.25370889987542
Save loss: 19.645041290044784 Name: 23_train_model.pth
step: 0 epoch: 24 loss: 21.201936721801758 loss_input: 79.6612548828125
step: 1000 epoch: 24 loss: 19.505222364858195 loss_input: 82.06576199679228
step: 2000 epoch: 24 loss: 19.51492784095966 loss_input: 81.92344892007121
step: 3000 epoch: 24 loss: 19.50516452077467 loss_input: 81.69452286656401
step: 4000 epoch: 24 loss: 19.47942615169133 loss_input: 81.6157064707212
step: 5000 epoch: 24 loss: 19.474803427032793 loss_input: 81.68104353982719
step: 6000 epoch: 24 loss: 19.4880327341696 loss_input: 81.59642909856821
step: 7000 epoch: 24 loss: 19.485765899935547 loss_input: 81.7146589793813
step: 8000 epoch: 24 loss: 19.510558806722006 loss_input: 81.9183330995979
step: 9000 epoch: 24 loss: 19.532991961391353 loss_input: 81.95007383212847
step: 10000 epoch: 24 loss: 19.52476971719923 loss_input: 81.9528099253063
step: 11000 epoch: 24 loss: 19.553810548114836 loss_input: 82.07063443118275
step: 12000 epoch: 24 loss: 19.56734770666052 loss_input: 82.14420874167875
step: 13000 epoch: 24 loss: 19.560721127274093 loss_input: 82.09233998038312
step: 14000 epoch: 24 loss: 19.562009527533235 loss_input: 82.12930415771781
step: 15000 epoch: 24 loss: 19.553251576085113 loss_input: 82.15171062570438
Save loss: 19.57834560996294 Name: 24_train_model.pth
step: 0 epoch: 25 loss: 23.226856231689453 loss_input: 70.17962646484375
step: 1000 epoch: 25 loss: 19.417693504444014 loss_input: 83.76526214264251
step: 2000 epoch: 25 loss: 19.4940178033294 loss_input: 83.00696505385002
step: 3000 epoch: 25 loss: 19.598188740616838 loss_input: 82.84168279762865
step: 4000 epoch: 25 loss: 19.530912521331796 loss_input: 82.62321559502702
step: 5000 epoch: 25 loss: 19.513298998353864 loss_input: 82.58254186565031
step: 6000 epoch: 25 loss: 19.48831392887969 loss_input: 82.53329588297784
step: 7000 epoch: 25 loss: 19.494086615784887 loss_input: 82.54791067968247
step: 8000 epoch: 25 loss: 19.501477066419554 loss_input: 82.61814218413129
step: 9000 epoch: 25 loss: 19.484382095767078 loss_input: 82.55643135870103
step: 10000 epoch: 25 loss: 19.512291401389266 loss_input: 82.37777814015473
step: 11000 epoch: 25 loss: 19.532283398403102 loss_input: 82.42711956827611
step: 12000 epoch: 25 loss: 19.531826505302618 loss_input: 82.35769432849183
step: 13000 epoch: 25 loss: 19.539849425983306 loss_input: 82.27158337579802
step: 14000 epoch: 25 loss: 19.55380122386237 loss_input: 82.31367941143155
step: 15000 epoch: 25 loss: 19.535811620524356 loss_input: 82.1879930498123
Save loss: 19.540242795303463 Name: 25_train_model.pth
step: 0 epoch: 26 loss: 16.424442291259766 loss_input: 88.72772216796875
step: 1000 epoch: 26 loss: 19.104647069067866 loss_input: 82.54804747254698
step: 2000 epoch: 26 loss: 19.24029307934953 loss_input: 82.26611120709177
step: 3000 epoch: 26 loss: 19.351228388735787 loss_input: 82.35127341179242
step: 4000 epoch: 26 loss: 19.39125367457555 loss_input: 82.11480868491373
step: 5000 epoch: 26 loss: 19.3545359199797 loss_input: 82.04248272659922
step: 6000 epoch: 26 loss: 19.37359428795114 loss_input: 82.2429159043968
step: 7000 epoch: 26 loss: 19.388990535035916 loss_input: 82.28755100589704
step: 8000 epoch: 26 loss: 19.38492247933463 loss_input: 82.20239606432253
step: 9000 epoch: 26 loss: 19.424660326944245 loss_input: 82.1917029699608
step: 10000 epoch: 26 loss: 19.44408549898661 loss_input: 82.20995668377498
step: 11000 epoch: 26 loss: 19.478436958637033 loss_input: 82.19883261566953
step: 12000 epoch: 26 loss: 19.46786765507505 loss_input: 82.24212238091488
step: 13000 epoch: 26 loss: 19.476239213961087 loss_input: 82.23048606856715
step: 14000 epoch: 26 loss: 19.469518375297962 loss_input: 82.24069240241211
step: 15000 epoch: 26 loss: 19.492501934785285 loss_input: 82.26491703124739
Save loss: 19.47438513816893 Name: 26_train_model.pth
step: 0 epoch: 27 loss: 18.377010345458984 loss_input: 63.99151611328125
step: 1000 epoch: 27 loss: 19.336114862939336 loss_input: 82.36684799575424
step: 2000 epoch: 27 loss: 19.463771303673497 loss_input: 82.14395429848553
step: 3000 epoch: 27 loss: 19.49382979231888 loss_input: 82.61432999302967
step: 4000 epoch: 27 loss: 19.422193133929348 loss_input: 82.26340909279516
step: 5000 epoch: 27 loss: 19.524889900216674 loss_input: 82.28713760421743
step: 6000 epoch: 27 loss: 19.50966022682476 loss_input: 82.2959256546435
step: 7000 epoch: 27 loss: 19.47818538594665 loss_input: 82.34511529219319
step: 8000 epoch: 27 loss: 19.4297722486716 loss_input: 82.2487981925233
step: 9000 epoch: 27 loss: 19.448305085796395 loss_input: 82.26800048882373
step: 10000 epoch: 27 loss: 19.46987340276497 loss_input: 82.350151551007
step: 11000 epoch: 27 loss: 19.474905620931594 loss_input: 82.45698338059466
step: 12000 epoch: 27 loss: 19.459669516351003 loss_input: 82.37801145762268
step: 13000 epoch: 27 loss: 19.456089883572815 loss_input: 82.35552917954628
step: 14000 epoch: 27 loss: 19.452101256011105 loss_input: 82.32167524794546
step: 15000 epoch: 27 loss: 19.450234339337502 loss_input: 82.28101282912519
Save loss: 19.44365719819069 Name: 27_train_model.pth
step: 0 epoch: 28 loss: 17.326324462890625 loss_input: 89.49102783203125
step: 1000 epoch: 28 loss: 19.486215179378576 loss_input: 82.55844948508523
step: 2000 epoch: 28 loss: 19.401659088811535 loss_input: 82.42117585616371
step: 3000 epoch: 28 loss: 19.310755721730654 loss_input: 82.07649712465637
step: 4000 epoch: 28 loss: 19.31178526412365 loss_input: 82.06065259197896
step: 5000 epoch: 28 loss: 19.299657925203594 loss_input: 82.15852873126546
step: 6000 epoch: 28 loss: 19.271085889552797 loss_input: 82.06756648753527
step: 7000 epoch: 28 loss: 19.280580560746593 loss_input: 82.03318993457128
step: 8000 epoch: 28 loss: 19.255358257229336 loss_input: 81.88266006849241
step: 9000 epoch: 28 loss: 19.30611401485769 loss_input: 81.98311494713478
step: 10000 epoch: 28 loss: 19.319563448661544 loss_input: 82.12088111867524
step: 11000 epoch: 28 loss: 19.33289623685278 loss_input: 82.06023541316911
step: 12000 epoch: 28 loss: 19.305610674896318 loss_input: 82.05781843415718
step: 13000 epoch: 28 loss: 19.329436171651757 loss_input: 82.1304369315341
step: 14000 epoch: 28 loss: 19.359612515190484 loss_input: 82.1930877094856
step: 15000 epoch: 28 loss: 19.371945653849544 loss_input: 82.18738326380455
Save loss: 19.38958236993849 Name: 28_train_model.pth
step: 0 epoch: 29 loss: 18.186697006225586 loss_input: 83.385498046875
step: 1000 epoch: 29 loss: 19.218056848832777 loss_input: 81.96650974757664
step: 2000 epoch: 29 loss: 19.247578024208874 loss_input: 82.21236022003647
step: 3000 epoch: 29 loss: 19.235514585354533 loss_input: 82.00198412052754
step: 4000 epoch: 29 loss: 19.282066492639878 loss_input: 82.05941716780278
step: 5000 epoch: 29 loss: 19.331010518515498 loss_input: 81.87935557419766
step: 6000 epoch: 29 loss: 19.31430462999476 loss_input: 82.10959019444024
step: 7000 epoch: 29 loss: 19.319037107582893 loss_input: 82.07561743237294
step: 8000 epoch: 29 loss: 19.382762629126717 loss_input: 82.22297862028721
step: 9000 epoch: 29 loss: 19.35242570541843 loss_input: 82.17417071117532
step: 10000 epoch: 29 loss: 19.33278820612659 loss_input: 82.15906417170783
step: 11000 epoch: 29 loss: 19.351011281186434 loss_input: 82.17651865411028
step: 12000 epoch: 29 loss: 19.324484071516213 loss_input: 82.12533657066663
step: 13000 epoch: 29 loss: 19.337342611909527 loss_input: 82.12427670452634
step: 14000 epoch: 29 loss: 19.32663346280303 loss_input: 82.02861102398238
step: 15000 epoch: 29 loss: 19.339980552231626 loss_input: 82.16641310826674
Save loss: 19.333682137578727 Name: 29_train_model.pth
step: 0 epoch: 30 loss: 24.572612762451172 loss_input: 105.561279296875
step: 1000 epoch: 30 loss: 19.172302070792977 loss_input: 81.16899537230348
step: 2000 epoch: 30 loss: 19.11123476619425 loss_input: 81.32945337633917
step: 3000 epoch: 30 loss: 19.29504924470367 loss_input: 81.61161558614576
step: 4000 epoch: 30 loss: 19.365955096666468 loss_input: 81.9257068972503
step: 5000 epoch: 30 loss: 19.303095851891328 loss_input: 81.9536386545933
step: 6000 epoch: 30 loss: 19.2837513618679 loss_input: 81.87122946956757
step: 7000 epoch: 30 loss: 19.263773117792844 loss_input: 81.8276940836019
step: 8000 epoch: 30 loss: 19.27585464494703 loss_input: 81.95412263988241
step: 9000 epoch: 30 loss: 19.28198234092764 loss_input: 81.86137895485571
step: 10000 epoch: 30 loss: 19.268631303945718 loss_input: 81.94533650882958
step: 11000 epoch: 30 loss: 19.285005465256887 loss_input: 82.13639863860921
step: 12000 epoch: 30 loss: 19.2712482645575 loss_input: 82.03352074582978
step: 13000 epoch: 30 loss: 19.276148731750155 loss_input: 82.10061702547821
step: 14000 epoch: 30 loss: 19.290659468205142 loss_input: 82.24220042545092
step: 15000 epoch: 30 loss: 19.2939731997654 loss_input: 82.21653901865527
Save loss: 19.304107328474522 Name: 30_train_model.pth
step: 0 epoch: 31 loss: 19.519330978393555 loss_input: 90.1033935546875
step: 1000 epoch: 31 loss: 19.304117849656752 loss_input: 82.32191447468547
step: 2000 epoch: 31 loss: 19.440054565831936 loss_input: 83.62322714375235
step: 3000 epoch: 31 loss: 19.288409541980776 loss_input: 82.89960808962117
step: 4000 epoch: 31 loss: 19.279899499202426 loss_input: 82.71201311305474
step: 5000 epoch: 31 loss: 19.238990377793428 loss_input: 82.59714735519692
step: 6000 epoch: 31 loss: 19.207214834888347 loss_input: 82.35520806143312
step: 7000 epoch: 31 loss: 19.20916192123948 loss_input: 82.25529933537675
step: 8000 epoch: 31 loss: 19.230901817875555 loss_input: 82.32748006108612
step: 9000 epoch: 31 loss: 19.217863308854955 loss_input: 82.36404784098424
step: 10000 epoch: 31 loss: 19.234596680526842 loss_input: 82.42976077758435
step: 11000 epoch: 31 loss: 19.240622131036268 loss_input: 82.44756835404878
step: 12000 epoch: 31 loss: 19.23968458247179 loss_input: 82.39114791581495
step: 13000 epoch: 31 loss: 19.244223718963745 loss_input: 82.31956698474366
step: 14000 epoch: 31 loss: 19.269999300085743 loss_input: 82.18937947390616
step: 15000 epoch: 31 loss: 19.28049392975471 loss_input: 82.15231954100196
Save loss: 19.26580860427022 Name: 31_train_model.pth
step: 0 epoch: 32 loss: 25.407001495361328 loss_input: 69.953369140625
step: 1000 epoch: 32 loss: 19.165645258767263 loss_input: 83.1810156396338
step: 2000 epoch: 32 loss: 19.16510948593887 loss_input: 82.69814854535623
step: 3000 epoch: 32 loss: 19.245062896705637 loss_input: 82.92657283591016
step: 4000 epoch: 32 loss: 19.218446339466844 loss_input: 82.70425952776942
step: 5000 epoch: 32 loss: 19.24672511281359 loss_input: 82.75591793262441
step: 6000 epoch: 32 loss: 19.248921092322142 loss_input: 82.6529473888141
step: 7000 epoch: 32 loss: 19.257338819665886 loss_input: 82.45078560380045
step: 8000 epoch: 32 loss: 19.2685114015804 loss_input: 82.6040359838443
step: 9000 epoch: 32 loss: 19.256782556478083 loss_input: 82.49377517352673
step: 10000 epoch: 32 loss: 19.26843631983924 loss_input: 82.45223977968413
step: 11000 epoch: 32 loss: 19.24139642758799 loss_input: 82.19773033217683
step: 12000 epoch: 32 loss: 19.26117048795179 loss_input: 82.30432209771094
step: 13000 epoch: 32 loss: 19.236372970359234 loss_input: 82.35097490810503
step: 14000 epoch: 32 loss: 19.226756472117934 loss_input: 82.30704654083023
step: 15000 epoch: 32 loss: 19.215363049124107 loss_input: 82.26699877722996
Save loss: 19.208827104300262 Name: 32_train_model.pth
step: 0 epoch: 33 loss: 20.855159759521484 loss_input: 59.38494873046875
step: 1000 epoch: 33 loss: 19.279576151521056 loss_input: 82.80964159131884
step: 2000 epoch: 33 loss: 19.101072555896582 loss_input: 82.07215172931113
step: 3000 epoch: 33 loss: 19.110822656797353 loss_input: 82.14596164071611
step: 4000 epoch: 33 loss: 19.193202084167336 loss_input: 82.43323893452906
step: 5000 epoch: 33 loss: 19.182642206528786 loss_input: 82.15782962792207
step: 6000 epoch: 33 loss: 19.187590691312675 loss_input: 82.19592075637134
step: 7000 epoch: 33 loss: 19.13875388663218 loss_input: 82.06096236914537
step: 8000 epoch: 33 loss: 19.156047679263196 loss_input: 82.08945511907447
step: 9000 epoch: 33 loss: 19.146782806351137 loss_input: 82.10899211475417
step: 10000 epoch: 33 loss: 19.14927994099966 loss_input: 82.18272684450305
step: 11000 epoch: 33 loss: 19.139658866454422 loss_input: 82.1938585750017
step: 12000 epoch: 33 loss: 19.110679491809304 loss_input: 82.03856869552862
step: 13000 epoch: 33 loss: 19.12757242937178 loss_input: 82.0849630313143
step: 14000 epoch: 33 loss: 19.127565977871566 loss_input: 82.07667292912393
step: 15000 epoch: 33 loss: 19.143840535276215 loss_input: 82.12521516306465
Save loss: 19.1608618029058 Name: 33_train_model.pth
step: 0 epoch: 34 loss: 30.804092407226562 loss_input: 107.24560546875
step: 1000 epoch: 34 loss: 18.881446089539732 loss_input: 81.64388083839988
step: 2000 epoch: 34 loss: 18.947149874864966 loss_input: 81.81163681464932
step: 3000 epoch: 34 loss: 18.989414794410877 loss_input: 82.03671149737197
step: 4000 epoch: 34 loss: 19.038558166106323 loss_input: 82.18365380883783
step: 5000 epoch: 34 loss: 19.00490991117191 loss_input: 81.95964613229698
step: 6000 epoch: 34 loss: 19.03156899114665 loss_input: 82.16362936499516
step: 7000 epoch: 34 loss: 19.063426913916768 loss_input: 82.2045243710999
step: 8000 epoch: 34 loss: 19.04118618069999 loss_input: 82.10328291177959
step: 9000 epoch: 34 loss: 19.064465191957353 loss_input: 82.14389054021125
step: 10000 epoch: 34 loss: 19.054875152395457 loss_input: 82.15016226038529
step: 11000 epoch: 34 loss: 19.05100893822597 loss_input: 82.1454239245296
step: 12000 epoch: 34 loss: 19.06687390469539 loss_input: 82.27966239426337
step: 13000 epoch: 34 loss: 19.091791685558576 loss_input: 82.40039379388935
step: 14000 epoch: 34 loss: 19.102043026711684 loss_input: 82.31370603393702
step: 15000 epoch: 34 loss: 19.111136542344983 loss_input: 82.24053118138478
Save loss: 19.117054642409087 Name: 34_train_model.pth
step: 0 epoch: 35 loss: 12.800299644470215 loss_input: 110.2969970703125
step: 1000 epoch: 35 loss: 18.85174791200773 loss_input: 80.55627002392139
step: 2000 epoch: 35 loss: 18.839032837773846 loss_input: 80.64376161528611
step: 3000 epoch: 35 loss: 18.915475370247577 loss_input: 81.09864308332769
step: 4000 epoch: 35 loss: 18.88569120632592 loss_input: 81.28265962073935
step: 5000 epoch: 35 loss: 18.928430237547918 loss_input: 81.87855195074266
step: 6000 epoch: 35 loss: 18.973762161154447 loss_input: 82.15824022819113
step: 7000 epoch: 35 loss: 18.97146481572688 loss_input: 82.19805236040499
step: 8000 epoch: 35 loss: 19.010009005820358 loss_input: 82.131175375539
step: 9000 epoch: 35 loss: 19.036175044797922 loss_input: 82.14224816491743
step: 10000 epoch: 35 loss: 19.03153584904819 loss_input: 82.08635320549976
step: 11000 epoch: 35 loss: 19.03462649646558 loss_input: 82.16462243555806
step: 12000 epoch: 35 loss: 19.036642625266122 loss_input: 82.18592213364543
step: 13000 epoch: 35 loss: 19.044848532816435 loss_input: 82.19819263545983
step: 14000 epoch: 35 loss: 19.06322031247259 loss_input: 82.1042626681102
step: 15000 epoch: 35 loss: 19.0775778255942 loss_input: 82.15486056626173
Save loss: 19.089863594204186 Name: 35_train_model.pth
step: 0 epoch: 36 loss: 22.351980209350586 loss_input: 86.256591796875
step: 1000 epoch: 36 loss: 18.888568071694998 loss_input: 83.16567801095388
step: 2000 epoch: 36 loss: 18.974333957813194 loss_input: 82.37126117727269
step: 3000 epoch: 36 loss: 19.102829287267454 loss_input: 82.7456915403199
step: 4000 epoch: 36 loss: 19.041161043886483 loss_input: 82.64667879966044
step: 5000 epoch: 36 loss: 19.041258442857174 loss_input: 82.42299953095318
step: 6000 epoch: 36 loss: 19.011607332520438 loss_input: 82.207423294848
step: 7000 epoch: 36 loss: 19.012626173564016 loss_input: 82.20823131747083
step: 8000 epoch: 36 loss: 18.994704948993135 loss_input: 82.15621033210692
step: 9000 epoch: 36 loss: 19.01000318526162 loss_input: 82.02391783399935
step: 10000 epoch: 36 loss: 19.02034988227862 loss_input: 82.05135256737998
step: 11000 epoch: 36 loss: 19.042007254574603 loss_input: 82.07753105985394
step: 12000 epoch: 36 loss: 19.06351837127688 loss_input: 82.19766888119024
step: 13000 epoch: 36 loss: 19.050347727359732 loss_input: 82.17763795625997
step: 14000 epoch: 36 loss: 19.043582639015792 loss_input: 82.1563389436678
step: 15000 epoch: 36 loss: 19.0495995977276 loss_input: 82.170247894932
Save loss: 19.06666058830917 Name: 36_train_model.pth
step: 0 epoch: 37 loss: 15.023008346557617 loss_input: 93.5755615234375
step: 1000 epoch: 37 loss: 19.15406918216061 loss_input: 83.79923171311111
step: 2000 epoch: 37 loss: 19.107093949010526 loss_input: 82.7485756633402
step: 3000 epoch: 37 loss: 19.066086538550618 loss_input: 82.30828540144822
step: 4000 epoch: 37 loss: 19.028867054033267 loss_input: 82.34808630300921
step: 5000 epoch: 37 loss: 18.990350076232616 loss_input: 82.20203635054239
step: 6000 epoch: 37 loss: 18.997017434032294 loss_input: 82.01618389732896
step: 7000 epoch: 37 loss: 19.01209019681519 loss_input: 82.1399389146277
step: 8000 epoch: 37 loss: 19.037495253786656 loss_input: 82.38781478151412
step: 9000 epoch: 37 loss: 19.014349850558293 loss_input: 82.2981036842803
step: 10000 epoch: 37 loss: 19.01605107590933 loss_input: 82.36638075084093
step: 11000 epoch: 37 loss: 19.012872588991 loss_input: 82.30749565535768
step: 12000 epoch: 37 loss: 19.010187705351246 loss_input: 82.24536608847049
step: 13000 epoch: 37 loss: 19.01366209134754 loss_input: 82.29977103154775
step: 14000 epoch: 37 loss: 19.030488198369362 loss_input: 82.26135209005021
step: 15000 epoch: 37 loss: 19.02870848121488 loss_input: 82.24861495236254
Save loss: 19.016606358468533 Name: 37_train_model.pth
step: 0 epoch: 38 loss: 19.811065673828125 loss_input: 99.68621826171875
step: 1000 epoch: 38 loss: 18.496894881203698 loss_input: 81.38111497877122
step: 2000 epoch: 38 loss: 18.92650258475575 loss_input: 82.3759649106111
step: 3000 epoch: 38 loss: 18.887824012771603 loss_input: 82.308376089806
step: 4000 epoch: 38 loss: 18.854558047757273 loss_input: 82.36064663948758
step: 5000 epoch: 38 loss: 18.877422920061907 loss_input: 82.11998427951129
step: 6000 epoch: 38 loss: 18.880525504325036 loss_input: 82.13413965104124
step: 7000 epoch: 38 loss: 18.908199939740044 loss_input: 82.31361423814455
step: 8000 epoch: 38 loss: 18.935987016287854 loss_input: 82.46426079979749
step: 9000 epoch: 38 loss: 18.95348188819097 loss_input: 82.40210300662123
step: 10000 epoch: 38 loss: 18.96172400432019 loss_input: 82.43554151054622
step: 11000 epoch: 38 loss: 18.96331948259182 loss_input: 82.44184708350377
step: 12000 epoch: 38 loss: 18.982811201454133 loss_input: 82.43056998464647
step: 13000 epoch: 38 loss: 19.04001427945628 loss_input: 82.29194210496428
step: 14000 epoch: 38 loss: 19.03146424760785 loss_input: 82.17116932705142
step: 15000 epoch: 38 loss: 19.017370997329653 loss_input: 82.16552372664088
Save loss: 19.045637465551497 Name: 38_train_model.pth
step: 0 epoch: 39 loss: 9.168071746826172 loss_input: 67.392822265625
step: 1000 epoch: 39 loss: 18.882423362293682 loss_input: 81.46068104259022
step: 2000 epoch: 39 loss: 18.76843031640651 loss_input: 81.78783493385143
step: 3000 epoch: 39 loss: 18.83537929402078 loss_input: 82.03213674868793
step: 4000 epoch: 39 loss: 18.893330635651445 loss_input: 82.27013235704746
step: 5000 epoch: 39 loss: 18.866489758469587 loss_input: 82.09906755221222
step: 6000 epoch: 39 loss: 18.883134227179625 loss_input: 82.09823827579844
step: 7000 epoch: 39 loss: 18.87797104629546 loss_input: 82.1233927118252
step: 8000 epoch: 39 loss: 18.86279744500593 loss_input: 82.07322614488862
step: 9000 epoch: 39 loss: 18.886465683736716 loss_input: 82.09549437405175
step: 10000 epoch: 39 loss: 18.90434280172275 loss_input: 82.03096482343953
step: 11000 epoch: 39 loss: 18.900724791427187 loss_input: 82.00666480493074
step: 12000 epoch: 39 loss: 18.94271102182846 loss_input: 82.16274848698318
step: 13000 epoch: 39 loss: 18.98448246241111 loss_input: 82.24871283470232
step: 14000 epoch: 39 loss: 18.994130115186852 loss_input: 82.3023191781293
step: 15000 epoch: 39 loss: 18.985428567346226 loss_input: 82.24458545211569
Save loss: 18.993170244261623 Name: 39_train_model.pth
step: 0 epoch: 40 loss: 20.29490852355957 loss_input: 103.9859619140625
step: 1000 epoch: 40 loss: 18.773845592102447 loss_input: 81.81074601667864
step: 2000 epoch: 40 loss: 18.779407179993072 loss_input: 81.8424519124715
step: 3000 epoch: 40 loss: 18.817313139456903 loss_input: 82.164007973091
step: 4000 epoch: 40 loss: 18.844813721324766 loss_input: 82.38555166811325
step: 5000 epoch: 40 loss: 18.85724297448936 loss_input: 82.06687363699135
step: 6000 epoch: 40 loss: 18.89850880435816 loss_input: 82.22651469709476
step: 7000 epoch: 40 loss: 18.913400354155165 loss_input: 82.19280660624096
step: 8000 epoch: 40 loss: 18.94860063477049 loss_input: 82.33004924372247
step: 9000 epoch: 40 loss: 18.912086754133405 loss_input: 82.2146558910989
step: 10000 epoch: 40 loss: 18.91937984059947 loss_input: 82.19505728870472
step: 11000 epoch: 40 loss: 18.925504495204443 loss_input: 82.21204059352203
step: 12000 epoch: 40 loss: 18.926936968258346 loss_input: 82.1278596402367
step: 13000 epoch: 40 loss: 18.9334945974694 loss_input: 82.1503940661791
step: 14000 epoch: 40 loss: 18.93565767143941 loss_input: 82.21060491267225
step: 15000 epoch: 40 loss: 18.931093336097337 loss_input: 82.21841746601659
Save loss: 18.939661923348904 Name: 40_train_model.pth
step: 0 epoch: 41 loss: 7.523734092712402 loss_input: 43.97589111328125
step: 1000 epoch: 41 loss: 18.5768765774402 loss_input: 82.38885132201783
step: 2000 epoch: 41 loss: 18.651453181661886 loss_input: 82.07338438934829
step: 3000 epoch: 41 loss: 18.883259749579373 loss_input: 82.22269568591068
step: 4000 epoch: 41 loss: 18.856778460781268 loss_input: 82.13534935663057
step: 5000 epoch: 41 loss: 18.910304703681 loss_input: 82.1546170258136
step: 6000 epoch: 41 loss: 18.916922551952705 loss_input: 82.09348672804207
step: 7000 epoch: 41 loss: 18.914860226566187 loss_input: 82.14196224618581
step: 8000 epoch: 41 loss: 18.908354039162997 loss_input: 82.01448039820218
step: 9000 epoch: 41 loss: 18.902045209147428 loss_input: 82.04122695270188
step: 10000 epoch: 41 loss: 18.936803374191772 loss_input: 82.09811390687103
step: 11000 epoch: 41 loss: 18.934482398439282 loss_input: 82.18509431826332
step: 12000 epoch: 41 loss: 18.915903751775232 loss_input: 82.15128630739919
step: 13000 epoch: 41 loss: 18.89876584815627 loss_input: 82.08888661866004
step: 14000 epoch: 41 loss: 18.914730757102397 loss_input: 82.17960245794386
step: 15000 epoch: 41 loss: 18.920408694221692 loss_input: 82.20727029656356
Save loss: 18.916180273279547 Name: 41_train_model.pth
step: 0 epoch: 42 loss: 17.570083618164062 loss_input: 106.180419921875
step: 1000 epoch: 42 loss: 18.745886931290755 loss_input: 81.9952195021775
step: 2000 epoch: 42 loss: 18.886663276275833 loss_input: 82.45976099474677
step: 3000 epoch: 42 loss: 18.830690153357747 loss_input: 82.13659236797369
step: 4000 epoch: 42 loss: 18.842352435815872 loss_input: 82.38594520071155
step: 5000 epoch: 42 loss: 18.84319369045884 loss_input: 82.42984531755758
step: 6000 epoch: 42 loss: 18.84752311994187 loss_input: 82.31161557505715
step: 7000 epoch: 42 loss: 18.856173956944183 loss_input: 82.3406909713233
step: 8000 epoch: 42 loss: 18.896543912359544 loss_input: 82.5720435992254
step: 9000 epoch: 42 loss: 18.895285438158925 loss_input: 82.60723674881393
step: 10000 epoch: 42 loss: 18.890806081282854 loss_input: 82.49165732230487
step: 11000 epoch: 42 loss: 18.897199422616804 loss_input: 82.44257505576553
step: 12000 epoch: 42 loss: 18.904782391440797 loss_input: 82.41224160173338
step: 13000 epoch: 42 loss: 18.914809769790416 loss_input: 82.42257333870953
step: 14000 epoch: 42 loss: 18.888729644651015 loss_input: 82.31862250928087
step: 15000 epoch: 42 loss: 18.889454772683607 loss_input: 82.20950451040895
Save loss: 18.911095961168407 Name: 42_train_model.pth
step: 0 epoch: 43 loss: 20.43075180053711 loss_input: 87.548095703125
step: 1000 epoch: 43 loss: 18.666382752455675 loss_input: 82.18381124490743
step: 2000 epoch: 43 loss: 18.712144511631283 loss_input: 82.62057173317638
step: 3000 epoch: 43 loss: 18.81459611648323 loss_input: 82.940700755998
step: 4000 epoch: 43 loss: 18.786555492886897 loss_input: 82.81680399297149
step: 5000 epoch: 43 loss: 18.85209403083792 loss_input: 82.79946358140481
step: 6000 epoch: 43 loss: 18.845375624205186 loss_input: 82.69540034546651
step: 7000 epoch: 43 loss: 18.84706032965766 loss_input: 82.63293733393834
step: 8000 epoch: 43 loss: 18.85772318342986 loss_input: 82.55463533672061
step: 9000 epoch: 43 loss: 18.855260304379048 loss_input: 82.58726639965352
step: 10000 epoch: 43 loss: 18.853785470418604 loss_input: 82.47814214550225
step: 11000 epoch: 43 loss: 18.84223175614479 loss_input: 82.397048976549
step: 12000 epoch: 43 loss: 18.855589235060393 loss_input: 82.34142419996087
step: 13000 epoch: 43 loss: 18.86416796353879 loss_input: 82.23697858296508
step: 14000 epoch: 43 loss: 18.86664317086904 loss_input: 82.19457786793828
step: 15000 epoch: 43 loss: 18.880926175162312 loss_input: 82.25067845818742
Save loss: 18.86645991243422 Name: 43_train_model.pth
step: 0 epoch: 44 loss: 19.595348358154297 loss_input: 97.21673583984375
step: 1000 epoch: 44 loss: 19.07145075174002 loss_input: 82.67066746729833
step: 2000 epoch: 44 loss: 18.853896161545997 loss_input: 82.34885209372852
step: 3000 epoch: 44 loss: 18.83417438896685 loss_input: 82.3220351720325
step: 4000 epoch: 44 loss: 18.800919245195757 loss_input: 81.90220885025326
step: 5000 epoch: 44 loss: 18.8320428946094 loss_input: 82.02824065313891
step: 6000 epoch: 44 loss: 18.823823628237278 loss_input: 81.99673134921849
step: 7000 epoch: 44 loss: 18.80438759394704 loss_input: 81.80395415401037
step: 8000 epoch: 44 loss: 18.8295854093015 loss_input: 82.00249961125465
step: 9000 epoch: 44 loss: 18.837261555870565 loss_input: 81.89583120186082
step: 10000 epoch: 44 loss: 18.841118061832162 loss_input: 81.90123528540701
step: 11000 epoch: 44 loss: 18.83246095832202 loss_input: 81.9613772571634
step: 12000 epoch: 44 loss: 18.815472648636657 loss_input: 81.86120604960166
step: 13000 epoch: 44 loss: 18.842876844649663 loss_input: 81.88242997515064
step: 14000 epoch: 44 loss: 18.833967741570568 loss_input: 82.01243250952918
step: 15000 epoch: 44 loss: 18.82495426308369 loss_input: 82.13964712736599
Save loss: 18.839103874638678 Name: 44_train_model.pth
step: 0 epoch: 45 loss: 13.002021789550781 loss_input: 59.87847900390625
step: 1000 epoch: 45 loss: 18.70951870319012 loss_input: 81.78156474872783
step: 2000 epoch: 45 loss: 18.874816871177906 loss_input: 81.80474414282116
step: 3000 epoch: 45 loss: 18.906571910048438 loss_input: 82.13079415015959
step: 4000 epoch: 45 loss: 18.853097262128657 loss_input: 81.62241923955047
step: 5000 epoch: 45 loss: 18.81828304425022 loss_input: 82.04902109177773
step: 6000 epoch: 45 loss: 18.819235991923097 loss_input: 82.33381057584153
step: 7000 epoch: 45 loss: 18.783204790558614 loss_input: 82.17976953696905
step: 8000 epoch: 45 loss: 18.792101682208358 loss_input: 82.0638568860533
step: 9000 epoch: 45 loss: 18.78671419121215 loss_input: 82.03633149376631
step: 10000 epoch: 45 loss: 18.790845385838384 loss_input: 82.0807740515011
step: 11000 epoch: 45 loss: 18.79464870968165 loss_input: 82.1639763428378
step: 12000 epoch: 45 loss: 18.7931249101364 loss_input: 82.1075702696162
step: 13000 epoch: 45 loss: 18.801291360313016 loss_input: 82.18593241850988
step: 14000 epoch: 45 loss: 18.815865421642552 loss_input: 82.16156249982562
step: 15000 epoch: 45 loss: 18.82045877117307 loss_input: 82.13610448024168
Save loss: 18.828858934313057 Name: 45_train_model.pth
step: 0 epoch: 46 loss: 24.81743621826172 loss_input: 83.4210205078125
step: 1000 epoch: 46 loss: 18.66554189752508 loss_input: 83.08984009154908
step: 2000 epoch: 46 loss: 18.798815359418718 loss_input: 82.49309210751069
step: 3000 epoch: 46 loss: 18.727609312721032 loss_input: 82.3086730489251
step: 4000 epoch: 46 loss: 18.780579899525947 loss_input: 82.52108567096506
step: 5000 epoch: 46 loss: 18.805509020866953 loss_input: 82.5105923395399
step: 6000 epoch: 46 loss: 18.761351657815624 loss_input: 82.25153667015783
step: 7000 epoch: 46 loss: 18.798250668560705 loss_input: 82.34349556334307
step: 8000 epoch: 46 loss: 18.797918890077103 loss_input: 82.24389878816417
step: 9000 epoch: 46 loss: 18.759583567555858 loss_input: 82.16046654538384
step: 10000 epoch: 46 loss: 18.763440402719144 loss_input: 82.11115499778147
step: 11000 epoch: 46 loss: 18.767325600345895 loss_input: 82.17556628707582
step: 12000 epoch: 46 loss: 18.765411569221687 loss_input: 82.24180635195276
step: 13000 epoch: 46 loss: 18.761148303080116 loss_input: 82.32614861769947
step: 14000 epoch: 46 loss: 18.76023378001648 loss_input: 82.30654142815831
step: 15000 epoch: 46 loss: 18.77188778023713 loss_input: 82.2871585009567
Save loss: 18.773159926995636 Name: 46_train_model.pth
step: 0 epoch: 47 loss: 17.866779327392578 loss_input: 67.50933837890625
step: 1000 epoch: 47 loss: 19.047497401585233 loss_input: 82.53924572741711
step: 2000 epoch: 47 loss: 18.90376475285078 loss_input: 82.21463108777643
step: 3000 epoch: 47 loss: 18.96436812805359 loss_input: 82.37951471010314
step: 4000 epoch: 47 loss: 18.895470773181568 loss_input: 82.3776746700776
step: 5000 epoch: 47 loss: 18.79993344707218 loss_input: 82.05571654223843
step: 6000 epoch: 47 loss: 18.80104733268294 loss_input: 82.01343983766378
step: 7000 epoch: 47 loss: 18.78001296610479 loss_input: 82.10625721158138
step: 8000 epoch: 47 loss: 18.77735293494092 loss_input: 82.31069488964026
step: 9000 epoch: 47 loss: 18.780285194680076 loss_input: 82.21981524351027
step: 10000 epoch: 47 loss: 18.74415965533688 loss_input: 82.11563474389865
step: 11000 epoch: 47 loss: 18.7570453409433 loss_input: 82.15442316190361
step: 12000 epoch: 47 loss: 18.751895456927567 loss_input: 82.10961932880105
step: 13000 epoch: 47 loss: 18.73350190369957 loss_input: 82.18150629225937
step: 14000 epoch: 47 loss: 18.741380089666578 loss_input: 82.21312046698115
step: 15000 epoch: 47 loss: 18.745860457905103 loss_input: 82.20275064751543
Save loss: 18.764911718308927 Name: 47_train_model.pth
step: 0 epoch: 48 loss: 24.735694885253906 loss_input: 75.040771484375
step: 1000 epoch: 48 loss: 18.791491088810023 loss_input: 82.93543017041553
step: 2000 epoch: 48 loss: 18.623808702547986 loss_input: 81.92536449670672
step: 3000 epoch: 48 loss: 18.546824320679068 loss_input: 81.72599329276309
step: 4000 epoch: 48 loss: 18.691289796021188 loss_input: 81.93018009840802
step: 5000 epoch: 48 loss: 18.709019502003986 loss_input: 81.96834956739121
step: 6000 epoch: 48 loss: 18.656656298671557 loss_input: 81.84580604363791
step: 7000 epoch: 48 loss: 18.6807251998551 loss_input: 82.00045036639575
step: 8000 epoch: 48 loss: 18.666604316826092 loss_input: 82.02543922579746
step: 9000 epoch: 48 loss: 18.65613561330299 loss_input: 82.01646906989718
step: 10000 epoch: 48 loss: 18.685411669113506 loss_input: 82.11956943954043
step: 11000 epoch: 48 loss: 18.67978859647774 loss_input: 82.13185401890844
step: 12000 epoch: 48 loss: 18.689805331940196 loss_input: 82.11974775214044
step: 13000 epoch: 48 loss: 18.697949991108096 loss_input: 82.20111255339133
step: 14000 epoch: 48 loss: 18.710186278902626 loss_input: 82.22629298800086
step: 15000 epoch: 48 loss: 18.723981133699972 loss_input: 82.20881885869068
Save loss: 18.713147618725895 Name: 48_train_model.pth
step: 0 epoch: 49 loss: 15.903165817260742 loss_input: 63.1826171875
step: 1000 epoch: 49 loss: 18.456301559577813 loss_input: 81.97857842840753
step: 2000 epoch: 49 loss: 18.412013545505765 loss_input: 81.70839381944769
step: 3000 epoch: 49 loss: 18.527292065841284 loss_input: 82.0077186185334
step: 4000 epoch: 49 loss: 18.59381073506228 loss_input: 82.17445601978412
step: 5000 epoch: 49 loss: 18.646194188457994 loss_input: 82.21418893711493
step: 6000 epoch: 49 loss: 18.659806645883954 loss_input: 82.29858568290376
step: 7000 epoch: 49 loss: 18.673829926097245 loss_input: 82.40949282287241
step: 8000 epoch: 49 loss: 18.665320549528534 loss_input: 82.40288076173096
step: 9000 epoch: 49 loss: 18.70294407613674 loss_input: 82.49630199915939
step: 10000 epoch: 49 loss: 18.717061197837584 loss_input: 82.50264228948198
step: 11000 epoch: 49 loss: 18.712992043681993 loss_input: 82.30945324106722
step: 12000 epoch: 49 loss: 18.726327704485332 loss_input: 82.27207367118821
step: 13000 epoch: 49 loss: 18.724886382509126 loss_input: 82.17062098363228
step: 14000 epoch: 49 loss: 18.727040273054577 loss_input: 82.07822346447213
step: 15000 epoch: 49 loss: 18.71591639234244 loss_input: 82.07292279147536
Save loss: 18.73063112848997 Name: 49_train_model.pth
step: 0 epoch: 50 loss: 18.815370559692383 loss_input: 109.45831298828125
step: 1000 epoch: 50 loss: 18.729242356268916 loss_input: 82.46716881751061
step: 2000 epoch: 50 loss: 18.738796282743944 loss_input: 82.69266605710817
step: 3000 epoch: 50 loss: 18.687741230027513 loss_input: 82.55487058513047
step: 4000 epoch: 50 loss: 18.662703966742843 loss_input: 82.72038730124598
step: 5000 epoch: 50 loss: 18.656020611578214 loss_input: 82.54626084060531
step: 6000 epoch: 50 loss: 18.672522973068713 loss_input: 82.50935573034377
step: 7000 epoch: 50 loss: 18.66193451593304 loss_input: 82.46633665719351
step: 8000 epoch: 50 loss: 18.687873132436547 loss_input: 82.59357900086708
step: 9000 epoch: 50 loss: 18.70821435275892 loss_input: 82.50959383012348
step: 10000 epoch: 50 loss: 18.72946019869258 loss_input: 82.60966394815597
step: 11000 epoch: 50 loss: 18.721808943788783 loss_input: 82.46666633821641
step: 12000 epoch: 50 loss: 18.728688295598168 loss_input: 82.4259242078541
step: 13000 epoch: 50 loss: 18.72018739280586 loss_input: 82.3097941393706
step: 14000 epoch: 50 loss: 18.697013442307384 loss_input: 82.2306139379065
step: 15000 epoch: 50 loss: 18.705031053947295 loss_input: 82.23343503174007
Save loss: 18.704666102319955 Name: 50_train_model.pth
step: 0 epoch: 51 loss: 14.566652297973633 loss_input: 64.09783935546875
step: 1000 epoch: 51 loss: 18.37982929384077 loss_input: 81.19156850229848
step: 2000 epoch: 51 loss: 18.348517188663664 loss_input: 81.11554189946042
step: 3000 epoch: 51 loss: 18.419456640031886 loss_input: 81.13346940586861
step: 4000 epoch: 51 loss: 18.46985476245227 loss_input: 81.51043026901966
step: 5000 epoch: 51 loss: 18.4916051208818 loss_input: 81.80525926064787
step: 6000 epoch: 51 loss: 18.539729563638858 loss_input: 81.68634456309691
step: 7000 epoch: 51 loss: 18.550701531831272 loss_input: 81.80106360189336
step: 8000 epoch: 51 loss: 18.557778735948105 loss_input: 81.7656773692473
step: 9000 epoch: 51 loss: 18.626121790538086 loss_input: 81.99977917221437
step: 10000 epoch: 51 loss: 18.646031045660997 loss_input: 82.11601180580185
step: 11000 epoch: 51 loss: 18.63623908393741 loss_input: 82.17270584044029
step: 12000 epoch: 51 loss: 18.637993007289918 loss_input: 82.13988851453948
step: 13000 epoch: 51 loss: 18.631591881672133 loss_input: 82.12428733321596
step: 14000 epoch: 51 loss: 18.63944337804116 loss_input: 82.13716324601256
step: 15000 epoch: 51 loss: 18.666877749633585 loss_input: 82.27385412261579
Save loss: 18.66461094737053 Name: 51_train_model.pth
step: 0 epoch: 52 loss: 24.055702209472656 loss_input: 139.40972900390625
step: 1000 epoch: 52 loss: 18.873964678872 loss_input: 81.91685280051979
step: 2000 epoch: 52 loss: 18.87472551480226 loss_input: 81.90109608281797
step: 3000 epoch: 52 loss: 18.817310795788764 loss_input: 81.87588554872985
step: 4000 epoch: 52 loss: 18.749545134415897 loss_input: 82.0569115228517
step: 5000 epoch: 52 loss: 18.74312616305169 loss_input: 82.34418527671419
step: 6000 epoch: 52 loss: 18.705432240833066 loss_input: 82.28416730959879
step: 7000 epoch: 52 loss: 18.684711356790316 loss_input: 82.2708912939195
step: 8000 epoch: 52 loss: 18.716596346470048 loss_input: 82.34747962870608
step: 9000 epoch: 52 loss: 18.702012819709307 loss_input: 82.31471853062334
step: 10000 epoch: 52 loss: 18.67597678057874 loss_input: 82.26034552306488
step: 11000 epoch: 52 loss: 18.677409683896265 loss_input: 82.24939086902967
step: 12000 epoch: 52 loss: 18.693721815681727 loss_input: 82.26090808757473
step: 13000 epoch: 52 loss: 18.66625996047135 loss_input: 82.2877231395737
step: 14000 epoch: 52 loss: 18.69038147022448 loss_input: 82.39499370650422
step: 15000 epoch: 52 loss: 18.685612649998976 loss_input: 82.31530437440239
Save loss: 18.673245688185094 Name: 52_train_model.pth
step: 0 epoch: 53 loss: 17.491384506225586 loss_input: 83.7685546875
step: 1000 epoch: 53 loss: 18.273195739273543 loss_input: 82.72930335045814
step: 2000 epoch: 53 loss: 18.444904144616917 loss_input: 82.75921755918915
step: 3000 epoch: 53 loss: 18.57025935593465 loss_input: 82.89170974177227
step: 4000 epoch: 53 loss: 18.655032955327947 loss_input: 82.86033507187167
step: 5000 epoch: 53 loss: 18.627277447781164 loss_input: 82.81981301493607
step: 6000 epoch: 53 loss: 18.56712558141968 loss_input: 82.79049577349087
step: 7000 epoch: 53 loss: 18.595508351460165 loss_input: 82.64730439596799
step: 8000 epoch: 53 loss: 18.59029754214459 loss_input: 82.48147433108947
step: 9000 epoch: 53 loss: 18.587271445169144 loss_input: 82.384786239029
step: 10000 epoch: 53 loss: 18.575861547961377 loss_input: 82.3517323987816
step: 11000 epoch: 53 loss: 18.596322709392954 loss_input: 82.37674104156369
step: 12000 epoch: 53 loss: 18.617253015959385 loss_input: 82.40776494435197
step: 13000 epoch: 53 loss: 18.605588099340522 loss_input: 82.31441738462276
step: 14000 epoch: 53 loss: 18.606283654809705 loss_input: 82.30747876529327
step: 15000 epoch: 53 loss: 18.63130299705369 loss_input: 82.31264408626014
Save loss: 18.631678739503027 Name: 53_train_model.pth
step: 0 epoch: 54 loss: 19.417329788208008 loss_input: 68.1712646484375
step: 1000 epoch: 54 loss: 18.561607324636423 loss_input: 81.55867905288072
step: 2000 epoch: 54 loss: 18.50193032582124 loss_input: 81.63825301704617
step: 3000 epoch: 54 loss: 18.508423038738165 loss_input: 81.59075724351648
step: 4000 epoch: 54 loss: 18.570772378869545 loss_input: 81.60634392793403
step: 5000 epoch: 54 loss: 18.59292971737455 loss_input: 81.72483245050209
step: 6000 epoch: 54 loss: 18.619173222195208 loss_input: 81.93471254139935
step: 7000 epoch: 54 loss: 18.611172203335858 loss_input: 82.11446964398502
step: 8000 epoch: 54 loss: 18.601070750670022 loss_input: 82.04923703995009
step: 9000 epoch: 54 loss: 18.586366787418736 loss_input: 81.9806698452776
step: 10000 epoch: 54 loss: 18.56887313320975 loss_input: 81.91536910151758
step: 11000 epoch: 54 loss: 18.592007711057 loss_input: 82.0028166495695
step: 12000 epoch: 54 loss: 18.5799763059032 loss_input: 82.04694661878763
step: 13000 epoch: 54 loss: 18.595305326800833 loss_input: 82.12630686874748
step: 14000 epoch: 54 loss: 18.60185857364411 loss_input: 82.23854863899928
step: 15000 epoch: 54 loss: 18.648539719665205 loss_input: 82.29340574886153
Save loss: 18.64084163171053 Name: 54_train_model.pth
step: 0 epoch: 55 loss: 14.291421890258789 loss_input: 81.2415771484375
step: 1000 epoch: 55 loss: 18.62450559560831 loss_input: 83.8417421201845
step: 2000 epoch: 55 loss: 18.45454559345236 loss_input: 82.68209965451844
step: 3000 epoch: 55 loss: 18.50781759759737 loss_input: 82.68013457590959
step: 4000 epoch: 55 loss: 18.562016321819623 loss_input: 82.98516572639066
step: 5000 epoch: 55 loss: 18.545747710046612 loss_input: 82.78002320678442
step: 6000 epoch: 55 loss: 18.572672501541618 loss_input: 82.63747770757283
step: 7000 epoch: 55 loss: 18.581162382340263 loss_input: 82.77104975342529
step: 8000 epoch: 55 loss: 18.571858607267025 loss_input: 82.70401298104339
step: 9000 epoch: 55 loss: 18.584164510739008 loss_input: 82.76731623867964
step: 10000 epoch: 55 loss: 18.586382024133936 loss_input: 82.57935341719734
step: 11000 epoch: 55 loss: 18.579742764702775 loss_input: 82.55241232175804
step: 12000 epoch: 55 loss: 18.58537543601726 loss_input: 82.46285519195828
step: 13000 epoch: 55 loss: 18.59630285297098 loss_input: 82.36965876422967
step: 14000 epoch: 55 loss: 18.57232287706422 loss_input: 82.28370714783625
step: 15000 epoch: 55 loss: 18.580804800766323 loss_input: 82.3043668711474
Save loss: 18.590089395955204 Name: 55_train_model.pth
step: 0 epoch: 56 loss: 24.49609375 loss_input: 108.70465087890625
step: 1000 epoch: 56 loss: 18.44909172220068 loss_input: 81.27047372793223
step: 2000 epoch: 56 loss: 18.548760544711623 loss_input: 81.94733163203554
step: 3000 epoch: 56 loss: 18.54139954437617 loss_input: 82.02615219161972
step: 4000 epoch: 56 loss: 18.5856241427848 loss_input: 82.50821243968792
step: 5000 epoch: 56 loss: 18.55921552915903 loss_input: 82.60199428620135
step: 6000 epoch: 56 loss: 18.5513603868057 loss_input: 82.49277752007153
step: 7000 epoch: 56 loss: 18.548610730062908 loss_input: 82.41590052913757
step: 8000 epoch: 56 loss: 18.573433119063825 loss_input: 82.32516810185103
step: 9000 epoch: 56 loss: 18.576225723137977 loss_input: 82.43577501262881
step: 10000 epoch: 56 loss: 18.568331787460103 loss_input: 82.48617942771152
step: 11000 epoch: 56 loss: 18.56944720880106 loss_input: 82.47274362220622
step: 12000 epoch: 56 loss: 18.54495041663026 loss_input: 82.42648747980311
step: 13000 epoch: 56 loss: 18.550478956165904 loss_input: 82.39163426543811
step: 14000 epoch: 56 loss: 18.56057678557372 loss_input: 82.37605187886273
step: 15000 epoch: 56 loss: 18.566516759085516 loss_input: 82.25717490822878
Save loss: 18.578181147038936 Name: 56_train_model.pth
step: 0 epoch: 57 loss: 17.24602699279785 loss_input: 75.9659423828125
step: 1000 epoch: 57 loss: 18.6090881817348 loss_input: 83.10356117271401
step: 2000 epoch: 57 loss: 18.561822651267825 loss_input: 82.32444571781492
step: 3000 epoch: 57 loss: 18.442238745551155 loss_input: 82.10115728025553
step: 4000 epoch: 57 loss: 18.54578752751292 loss_input: 82.21665524780914
step: 5000 epoch: 57 loss: 18.488307272713318 loss_input: 82.08956198262301
step: 6000 epoch: 57 loss: 18.51167569198602 loss_input: 82.21665405619245
step: 7000 epoch: 57 loss: 18.51809123314682 loss_input: 82.1706912876146
step: 8000 epoch: 57 loss: 18.557698475720063 loss_input: 82.36996719709353
step: 9000 epoch: 57 loss: 18.57453804635403 loss_input: 82.31300380280013
step: 10000 epoch: 57 loss: 18.553628668548132 loss_input: 82.21721604084709
step: 11000 epoch: 57 loss: 18.559765104293476 loss_input: 82.2093632689997
step: 12000 epoch: 57 loss: 18.54948356965911 loss_input: 82.1895560318704
step: 13000 epoch: 57 loss: 18.532769123835433 loss_input: 82.0763689235819
step: 14000 epoch: 57 loss: 18.54512468246671 loss_input: 82.180487404499
step: 15000 epoch: 57 loss: 18.54988770781815 loss_input: 82.22909513551961
Save loss: 18.555805795922875 Name: 57_train_model.pth
step: 0 epoch: 58 loss: 22.87601661682129 loss_input: 114.20513916015625
step: 1000 epoch: 58 loss: 18.57880508411419 loss_input: 83.16972529423701
step: 2000 epoch: 58 loss: 18.538703294350825 loss_input: 82.58112206714026
step: 3000 epoch: 58 loss: 18.52249646290109 loss_input: 82.10585513650398
step: 4000 epoch: 58 loss: 18.516187009439562 loss_input: 82.30427688182935
step: 5000 epoch: 58 loss: 18.594390862609263 loss_input: 82.57309552493798
step: 6000 epoch: 58 loss: 18.592021991204984 loss_input: 82.42433230325871
step: 7000 epoch: 58 loss: 18.585114249432536 loss_input: 82.31763370090545
step: 8000 epoch: 58 loss: 18.577520025952012 loss_input: 82.24750497343838
step: 9000 epoch: 58 loss: 18.589034677862976 loss_input: 82.2972577496272
step: 10000 epoch: 58 loss: 18.5632966706877 loss_input: 82.26974668866121
step: 11000 epoch: 58 loss: 18.591155920209868 loss_input: 82.52479239613693
step: 12000 epoch: 58 loss: 18.58268033769784 loss_input: 82.35556173948395
step: 13000 epoch: 58 loss: 18.564410804629407 loss_input: 82.28349845609833
step: 14000 epoch: 58 loss: 18.547658317745196 loss_input: 82.23702963041703
step: 15000 epoch: 58 loss: 18.551958283379303 loss_input: 82.25144093583866
Save loss: 18.536083797872067 Name: 58_train_model.pth
step: 0 epoch: 59 loss: 18.356552124023438 loss_input: 61.050048828125
step: 1000 epoch: 59 loss: 18.369742842701886 loss_input: 82.26969783146541
step: 2000 epoch: 59 loss: 18.40354109287024 loss_input: 81.51267527247118
step: 3000 epoch: 59 loss: 18.53719402066631 loss_input: 81.72100899228252
step: 4000 epoch: 59 loss: 18.54509527306055 loss_input: 81.93954015463419
step: 5000 epoch: 59 loss: 18.49604917864541 loss_input: 81.64626303254974
step: 6000 epoch: 59 loss: 18.484484482161147 loss_input: 81.73267557620605
step: 7000 epoch: 59 loss: 18.5291781611416 loss_input: 82.00977466563091
step: 8000 epoch: 59 loss: 18.56787753519364 loss_input: 82.16777311863116
step: 9000 epoch: 59 loss: 18.541779677850144 loss_input: 82.08503633466724
step: 10000 epoch: 59 loss: 18.568163838723624 loss_input: 82.15355478612295
step: 11000 epoch: 59 loss: 18.554820368392026 loss_input: 82.257658000773
step: 12000 epoch: 59 loss: 18.54838153682404 loss_input: 82.20024899353277
step: 13000 epoch: 59 loss: 18.564813581542523 loss_input: 82.25733960781562
step: 14000 epoch: 59 loss: 18.553308648847185 loss_input: 82.24431470227015
step: 15000 epoch: 59 loss: 18.555885924457606 loss_input: 82.23080269176717
Save loss: 18.531541007757188 Name: 59_train_model.pth
step: 0 epoch: 60 loss: 26.145366668701172 loss_input: 115.041259765625
step: 1000 epoch: 60 loss: 18.53973324791892 loss_input: 82.08740874603912
step: 2000 epoch: 60 loss: 18.491278894658926 loss_input: 82.05035119769217
step: 3000 epoch: 60 loss: 18.517919652027434 loss_input: 82.66697569316088
step: 4000 epoch: 60 loss: 18.492003318280823 loss_input: 82.61416826579487
step: 5000 epoch: 60 loss: 18.50740646724819 loss_input: 82.72717507279793
step: 6000 epoch: 60 loss: 18.4697754290914 loss_input: 82.45725745945012
step: 7000 epoch: 60 loss: 18.471840276460686 loss_input: 82.37763042256519
step: 8000 epoch: 60 loss: 18.49887439868194 loss_input: 82.37724783456024
step: 9000 epoch: 60 loss: 18.466065801258765 loss_input: 82.18305185193182
step: 10000 epoch: 60 loss: 18.483322467676174 loss_input: 82.34606596713375
step: 11000 epoch: 60 loss: 18.490261941787903 loss_input: 82.35513970609384
step: 12000 epoch: 60 loss: 18.520774281508682 loss_input: 82.39971643089801
step: 13000 epoch: 60 loss: 18.526691429102094 loss_input: 82.41382403520792
step: 14000 epoch: 60 loss: 18.52261265280689 loss_input: 82.29438003486636
step: 15000 epoch: 60 loss: 18.5180235978691 loss_input: 82.27916130461094
Save loss: 18.50456010013819 Name: 60_train_model.pth
step: 0 epoch: 61 loss: 20.441734313964844 loss_input: 81.01409912109375
step: 1000 epoch: 61 loss: 18.30373631562148 loss_input: 81.93045388449441
step: 2000 epoch: 61 loss: 18.312971707524685 loss_input: 82.0468795143444
step: 3000 epoch: 61 loss: 18.446973950971724 loss_input: 82.22963227752207
step: 4000 epoch: 61 loss: 18.469793959934393 loss_input: 82.30982537568732
step: 5000 epoch: 61 loss: 18.417684937400644 loss_input: 82.1938067586678
step: 6000 epoch: 61 loss: 18.478294133782445 loss_input: 82.35085475256078
step: 7000 epoch: 61 loss: 18.451601580472833 loss_input: 82.46385412784223
step: 8000 epoch: 61 loss: 18.4527495144278 loss_input: 82.29053666570488
step: 9000 epoch: 61 loss: 18.439986169238896 loss_input: 82.17807359105282
step: 10000 epoch: 61 loss: 18.456321796075475 loss_input: 82.15092940779164
step: 11000 epoch: 61 loss: 18.454447810774056 loss_input: 82.15440546331726
step: 12000 epoch: 61 loss: 18.495764165885767 loss_input: 82.17794786835798
step: 13000 epoch: 61 loss: 18.491027813986186 loss_input: 82.21091036961249
step: 14000 epoch: 61 loss: 18.473683267122983 loss_input: 82.21247284303911
step: 15000 epoch: 61 loss: 18.478387538611624 loss_input: 82.17774237007247
Save loss: 18.493135017365216 Name: 61_train_model.pth
step: 0 epoch: 62 loss: 16.30118179321289 loss_input: 56.501708984375
step: 1000 epoch: 62 loss: 18.0614471264057 loss_input: 81.94238836115056
step: 2000 epoch: 62 loss: 18.295578054640664 loss_input: 81.89035218718766
step: 3000 epoch: 62 loss: 18.328059139111883 loss_input: 81.5779498874724
step: 4000 epoch: 62 loss: 18.365822846279894 loss_input: 81.70795104873685
step: 5000 epoch: 62 loss: 18.404236945026614 loss_input: 82.13733974396527
step: 6000 epoch: 62 loss: 18.42033274947276 loss_input: 82.01960345272857
step: 7000 epoch: 62 loss: 18.43070326864915 loss_input: 82.20864811932559
step: 8000 epoch: 62 loss: 18.39519820614407 loss_input: 82.15806764388826
step: 9000 epoch: 62 loss: 18.406149193361433 loss_input: 82.09940190564764
step: 10000 epoch: 62 loss: 18.412494074450816 loss_input: 82.15531791547407
step: 11000 epoch: 62 loss: 18.433339534895104 loss_input: 82.13286268853045
step: 12000 epoch: 62 loss: 18.479031900258555 loss_input: 82.27649235000274
step: 13000 epoch: 62 loss: 18.477217398297924 loss_input: 82.16071958463381
step: 14000 epoch: 62 loss: 18.4726561447968 loss_input: 82.20286790776802
step: 15000 epoch: 62 loss: 18.4791192128558 loss_input: 82.30778626802484
Save loss: 18.47531778559089 Name: 62_train_model.pth
step: 0 epoch: 63 loss: 19.5327205657959 loss_input: 77.81298828125
step: 1000 epoch: 63 loss: 18.36381157009037 loss_input: 81.85643439812141
step: 2000 epoch: 63 loss: 18.548118426405388 loss_input: 82.07100063249625
step: 3000 epoch: 63 loss: 18.510683270860856 loss_input: 81.88966267429086
step: 4000 epoch: 63 loss: 18.522673865074935 loss_input: 81.98380258756112
step: 5000 epoch: 63 loss: 18.46074933015068 loss_input: 81.94797498019928
step: 6000 epoch: 63 loss: 18.516263809269258 loss_input: 82.0175337293231
step: 7000 epoch: 63 loss: 18.489254661976073 loss_input: 81.87998928619851
step: 8000 epoch: 63 loss: 18.462579394113927 loss_input: 81.91295269152147
step: 9000 epoch: 63 loss: 18.448148587321906 loss_input: 81.97908990879269
step: 10000 epoch: 63 loss: 18.4701903759867 loss_input: 82.17437794184448
step: 11000 epoch: 63 loss: 18.460028929446416 loss_input: 82.18855916884864
step: 12000 epoch: 63 loss: 18.450285906672885 loss_input: 82.07366449524389
step: 13000 epoch: 63 loss: 18.455836739249253 loss_input: 82.06281887477256
step: 14000 epoch: 63 loss: 18.438810677675168 loss_input: 82.07055383756769
step: 15000 epoch: 63 loss: 18.457163143170675 loss_input: 82.23775413670339
Save loss: 18.46990330055356 Name: 63_train_model.pth
step: 0 epoch: 64 loss: 17.07563018798828 loss_input: 51.27178955078125
step: 1000 epoch: 64 loss: 18.325924870493886 loss_input: 82.21576708203905
step: 2000 epoch: 64 loss: 18.241080900360977 loss_input: 81.65365782074782
step: 3000 epoch: 64 loss: 18.276982269458713 loss_input: 81.81302289699006
step: 4000 epoch: 64 loss: 18.367697259063455 loss_input: 82.12807836248946
step: 5000 epoch: 64 loss: 18.359230170796284 loss_input: 82.00294923925371
step: 6000 epoch: 64 loss: 18.37573938031253 loss_input: 81.84539986133495
step: 7000 epoch: 64 loss: 18.416120624256173 loss_input: 81.95304541280655
step: 8000 epoch: 64 loss: 18.43879425017599 loss_input: 82.13013975364896
step: 9000 epoch: 64 loss: 18.4293886187765 loss_input: 82.30161294469885
step: 10000 epoch: 64 loss: 18.41103628761422 loss_input: 82.20285027161346
step: 11000 epoch: 64 loss: 18.401822778747555 loss_input: 82.13489719928693
step: 12000 epoch: 64 loss: 18.398509229464228 loss_input: 82.1568308231801
step: 13000 epoch: 64 loss: 18.428488511560918 loss_input: 82.19120748787053
step: 14000 epoch: 64 loss: 18.42857036303813 loss_input: 82.14371345039947
step: 15000 epoch: 64 loss: 18.44285054347029 loss_input: 82.14289649977111
Save loss: 18.449220652222632 Name: 64_train_model.pth
step: 0 epoch: 65 loss: 11.113054275512695 loss_input: 57.1435546875
step: 1000 epoch: 65 loss: 18.48809275522337 loss_input: 83.51129083319024
step: 2000 epoch: 65 loss: 18.32944726765245 loss_input: 82.91779799797366
step: 3000 epoch: 65 loss: 18.432591921883557 loss_input: 82.56389949687994
step: 4000 epoch: 65 loss: 18.432707659812667 loss_input: 82.4692239415732
step: 5000 epoch: 65 loss: 18.374872524436533 loss_input: 82.1619102937225
step: 6000 epoch: 65 loss: 18.44481715324064 loss_input: 82.27833649429196
step: 7000 epoch: 65 loss: 18.41633091274627 loss_input: 82.26063812870483
step: 8000 epoch: 65 loss: 18.432831612874114 loss_input: 82.18210856882635
step: 9000 epoch: 65 loss: 18.402440479259706 loss_input: 81.9942401283755
step: 10000 epoch: 65 loss: 18.408516825419547 loss_input: 82.08194737166909
step: 11000 epoch: 65 loss: 18.382983413699236 loss_input: 81.99362289419001
step: 12000 epoch: 65 loss: 18.352452638059663 loss_input: 81.96521580456674
step: 13000 epoch: 65 loss: 18.39646788542385 loss_input: 82.10814802951606
step: 14000 epoch: 65 loss: 18.40829228236074 loss_input: 82.07522380952281
step: 15000 epoch: 65 loss: 18.41209158470818 loss_input: 82.13896311941834
Save loss: 18.42771377392113 Name: 65_train_model.pth
step: 0 epoch: 66 loss: 21.094079971313477 loss_input: 107.89935302734375
step: 1000 epoch: 66 loss: 18.235237758952778 loss_input: 81.56860382049591
step: 2000 epoch: 66 loss: 18.235877974279994 loss_input: 81.94118898192505
step: 3000 epoch: 66 loss: 18.284729222463234 loss_input: 81.9681251472491
step: 4000 epoch: 66 loss: 18.272437734384592 loss_input: 81.91825824366663
step: 5000 epoch: 66 loss: 18.259577855709146 loss_input: 81.86064009122003
step: 6000 epoch: 66 loss: 18.281552966366885 loss_input: 81.9148815773543
step: 7000 epoch: 66 loss: 18.314855793819582 loss_input: 82.05575162672073
step: 8000 epoch: 66 loss: 18.329554025925127 loss_input: 82.00610823762996
step: 9000 epoch: 66 loss: 18.336712624547005 loss_input: 82.02493214593994
step: 10000 epoch: 66 loss: 18.35337318106778 loss_input: 82.06635094547185
step: 11000 epoch: 66 loss: 18.395148449253576 loss_input: 82.0878809545812
step: 12000 epoch: 66 loss: 18.394597585415227 loss_input: 82.05638652121601
step: 13000 epoch: 66 loss: 18.387982214994498 loss_input: 82.00429499995643
step: 14000 epoch: 66 loss: 18.38995765778058 loss_input: 82.0979107484231
step: 15000 epoch: 66 loss: 18.387760146253388 loss_input: 82.18454351578384
Save loss: 18.416565236106514 Name: 66_train_model.pth
step: 0 epoch: 67 loss: 17.920209884643555 loss_input: 57.5687255859375
step: 1000 epoch: 67 loss: 18.436850092389605 loss_input: 83.38947722199676
step: 2000 epoch: 67 loss: 18.383462094831682 loss_input: 82.48977985982594
step: 3000 epoch: 67 loss: 18.428518088489167 loss_input: 82.60423118644697
step: 4000 epoch: 67 loss: 18.360954439184184 loss_input: 82.28260168234308
step: 5000 epoch: 67 loss: 18.362992129309657 loss_input: 82.25149562372681
step: 6000 epoch: 67 loss: 18.387674401073014 loss_input: 82.45597116463662
step: 7000 epoch: 67 loss: 18.39711634071703 loss_input: 82.44772679307532
step: 8000 epoch: 67 loss: 18.412213826981088 loss_input: 82.53422005720965
step: 9000 epoch: 67 loss: 18.38492737983468 loss_input: 82.47965914079208
step: 10000 epoch: 67 loss: 18.38206222028497 loss_input: 82.48765655050705
step: 11000 epoch: 67 loss: 18.397140448293452 loss_input: 82.43049920488407
step: 12000 epoch: 67 loss: 18.396807369833418 loss_input: 82.3429085072175
step: 13000 epoch: 67 loss: 18.408953835822814 loss_input: 82.32130121695116
step: 14000 epoch: 67 loss: 18.412665764013962 loss_input: 82.26418496962829
step: 15000 epoch: 67 loss: 18.40955626251745 loss_input: 82.18147508427823
Save loss: 18.41004494212568 Name: 67_train_model.pth
step: 0 epoch: 68 loss: 23.085002899169922 loss_input: 75.00238037109375
step: 1000 epoch: 68 loss: 18.18786595989536 loss_input: 81.4830016175231
step: 2000 epoch: 68 loss: 18.29853755625887 loss_input: 82.64648108074869
step: 3000 epoch: 68 loss: 18.39223162836331 loss_input: 82.66479307109219
step: 4000 epoch: 68 loss: 18.384273394618265 loss_input: 82.50943061048703
step: 5000 epoch: 68 loss: 18.38414881806735 loss_input: 82.5123013239149
step: 6000 epoch: 68 loss: 18.37234293947695 loss_input: 82.3181076672768
step: 7000 epoch: 68 loss: 18.32894609015663 loss_input: 82.22805294869985
step: 8000 epoch: 68 loss: 18.35079263725991 loss_input: 82.30592386431432
step: 9000 epoch: 68 loss: 18.362500807534666 loss_input: 82.22825480328575
step: 10000 epoch: 68 loss: 18.353163314001545 loss_input: 82.12499636877132
step: 11000 epoch: 68 loss: 18.350078829829556 loss_input: 82.07659284614041
step: 12000 epoch: 68 loss: 18.367279156914613 loss_input: 82.11784630435257
step: 13000 epoch: 68 loss: 18.39061453804384 loss_input: 82.1812133742116
step: 14000 epoch: 68 loss: 18.392993484426505 loss_input: 82.22189546188247
step: 15000 epoch: 68 loss: 18.394055156308518 loss_input: 82.27414968803512
Save loss: 18.39136895343661 Name: 68_train_model.pth
step: 0 epoch: 69 loss: 21.194711685180664 loss_input: 108.695068359375
step: 1000 epoch: 69 loss: 18.050118747886483 loss_input: 81.68459958010739
step: 2000 epoch: 69 loss: 18.235465525865912 loss_input: 82.66464466741239
step: 3000 epoch: 69 loss: 18.275348622176853 loss_input: 82.29209455002629
step: 4000 epoch: 69 loss: 18.37544939917822 loss_input: 82.06682297016079
step: 5000 epoch: 69 loss: 18.366367589614555 loss_input: 82.13001327957065
step: 6000 epoch: 69 loss: 18.310513918726787 loss_input: 82.07236793338805
step: 7000 epoch: 69 loss: 18.287380822844682 loss_input: 82.01041509450938
step: 8000 epoch: 69 loss: 18.31899741768405 loss_input: 82.04299984352542
step: 9000 epoch: 69 loss: 18.310356820242866 loss_input: 82.0377275517564
step: 10000 epoch: 69 loss: 18.31515032676992 loss_input: 81.94943046588895
step: 11000 epoch: 69 loss: 18.33092300463412 loss_input: 81.96543774562753
step: 12000 epoch: 69 loss: 18.337211501051033 loss_input: 82.13360530604383
step: 13000 epoch: 69 loss: 18.35121142322325 loss_input: 82.13949926027325
step: 14000 epoch: 69 loss: 18.354363532945367 loss_input: 82.12892260189423
step: 15000 epoch: 69 loss: 18.362773943071357 loss_input: 82.16608540387672
Save loss: 18.38523136217892 Name: 69_train_model.pth
step: 0 epoch: 70 loss: 19.991531372070312 loss_input: 90.6571044921875
step: 1000 epoch: 70 loss: 18.585142339978898 loss_input: 84.00994842559784
step: 2000 epoch: 70 loss: 18.48132301175195 loss_input: 83.17774965666581
step: 3000 epoch: 70 loss: 18.412711701207222 loss_input: 82.745228702249
step: 4000 epoch: 70 loss: 18.332342258664077 loss_input: 82.69774035983191
step: 5000 epoch: 70 loss: 18.358166020528195 loss_input: 82.31322304874962
step: 6000 epoch: 70 loss: 18.35167726805321 loss_input: 82.34838314911381
step: 7000 epoch: 70 loss: 18.36822536335283 loss_input: 82.41435119864293
step: 8000 epoch: 70 loss: 18.35272664359891 loss_input: 82.38395233402221
step: 9000 epoch: 70 loss: 18.35091143467071 loss_input: 82.45658560724155
step: 10000 epoch: 70 loss: 18.357401271829985 loss_input: 82.34536640940398
step: 11000 epoch: 70 loss: 18.346892181325746 loss_input: 82.35624714381434
step: 12000 epoch: 70 loss: 18.341519071801326 loss_input: 82.27249486513331
step: 13000 epoch: 70 loss: 18.351986297689283 loss_input: 82.23843558195342
step: 14000 epoch: 70 loss: 18.358146408951015 loss_input: 82.24546222075098
step: 15000 epoch: 70 loss: 18.357521202713478 loss_input: 82.24598837779304
Save loss: 18.358933177769185 Name: 70_train_model.pth
step: 0 epoch: 71 loss: 12.611574172973633 loss_input: 78.81890869140625
step: 1000 epoch: 71 loss: 18.27520675401945 loss_input: 81.78118426983173
step: 2000 epoch: 71 loss: 18.398160007463463 loss_input: 82.48842525148558
step: 3000 epoch: 71 loss: 18.29107196630537 loss_input: 82.15532552381627
step: 4000 epoch: 71 loss: 18.270820268599042 loss_input: 82.2489242954154
step: 5000 epoch: 71 loss: 18.276209377284243 loss_input: 82.08864918910749
step: 6000 epoch: 71 loss: 18.31834815283891 loss_input: 82.22772311385603
step: 7000 epoch: 71 loss: 18.29482304064006 loss_input: 82.00360314911765
step: 8000 epoch: 71 loss: 18.267478298685013 loss_input: 82.00951038499338
step: 9000 epoch: 71 loss: 18.26620439237627 loss_input: 82.12029849602214
step: 10000 epoch: 71 loss: 18.263314371001254 loss_input: 82.03677734643527
step: 11000 epoch: 71 loss: 18.300164379149003 loss_input: 82.00526226131431
step: 12000 epoch: 71 loss: 18.319143919932447 loss_input: 81.95691473850975
step: 13000 epoch: 71 loss: 18.34242825628785 loss_input: 82.20664333569142
step: 14000 epoch: 71 loss: 18.332029711731025 loss_input: 82.28081852534389
step: 15000 epoch: 71 loss: 18.34328910934314 loss_input: 82.28438341333884
Save loss: 18.34811177405715 Name: 71_train_model.pth
step: 0 epoch: 72 loss: 28.839588165283203 loss_input: 77.16229248046875
step: 1000 epoch: 72 loss: 18.329670746009665 loss_input: 83.34788843968532
step: 2000 epoch: 72 loss: 18.350405121373868 loss_input: 82.8441547963811
step: 3000 epoch: 72 loss: 18.323593201298827 loss_input: 82.95237655156615
step: 4000 epoch: 72 loss: 18.383793103757483 loss_input: 82.80756684608144
step: 5000 epoch: 72 loss: 18.37658756939179 loss_input: 82.85330516904432
step: 6000 epoch: 72 loss: 18.385594244977472 loss_input: 82.7434538193016
step: 7000 epoch: 72 loss: 18.33421147630787 loss_input: 82.55211605512828
step: 8000 epoch: 72 loss: 18.337236277774906 loss_input: 82.30080878867193
step: 9000 epoch: 72 loss: 18.348007572238917 loss_input: 82.32477846157285
step: 10000 epoch: 72 loss: 18.36267777192999 loss_input: 82.38616361752497
step: 11000 epoch: 72 loss: 18.360396683925867 loss_input: 82.42102921285388
step: 12000 epoch: 72 loss: 18.375020771163292 loss_input: 82.50186846612065
step: 13000 epoch: 72 loss: 18.364716763735167 loss_input: 82.3688099009286
step: 14000 epoch: 72 loss: 18.355385875013944 loss_input: 82.3145741926637
step: 15000 epoch: 72 loss: 18.348260824906493 loss_input: 82.33981880049143
Save loss: 18.343533745080233 Name: 72_train_model.pth
step: 0 epoch: 73 loss: 20.291149139404297 loss_input: 59.90802001953125
step: 1000 epoch: 73 loss: 18.306307194830772 loss_input: 82.26188075840176
step: 2000 epoch: 73 loss: 18.264462534634248 loss_input: 81.86596655285638
step: 3000 epoch: 73 loss: 18.35490500263594 loss_input: 82.45025018515963
step: 4000 epoch: 73 loss: 18.350757791590198 loss_input: 82.64798137623796
step: 5000 epoch: 73 loss: 18.398548452026056 loss_input: 82.75414500840067
step: 6000 epoch: 73 loss: 18.388987685417934 loss_input: 82.78281801665868
step: 7000 epoch: 73 loss: 18.37596023513528 loss_input: 82.40293246333113
step: 8000 epoch: 73 loss: 18.36589185364052 loss_input: 82.288296345159
step: 9000 epoch: 73 loss: 18.339679743340117 loss_input: 82.11782137461071
step: 10000 epoch: 73 loss: 18.341554982818828 loss_input: 82.26257954648871
step: 11000 epoch: 73 loss: 18.296144003131673 loss_input: 82.12182105093606
step: 12000 epoch: 73 loss: 18.300799741694135 loss_input: 82.22249269958297
step: 13000 epoch: 73 loss: 18.30588529643788 loss_input: 82.14565367060857
step: 14000 epoch: 73 loss: 18.294261104114973 loss_input: 82.19611151519311
step: 15000 epoch: 73 loss: 18.305792797756915 loss_input: 82.23064175685813
Save loss: 18.31363930428028 Name: 73_train_model.pth
step: 0 epoch: 74 loss: 18.541805267333984 loss_input: 132.9453125
step: 1000 epoch: 74 loss: 18.512736273335886 loss_input: 83.07547012313859
step: 2000 epoch: 74 loss: 18.409226772965102 loss_input: 82.33031158981056
step: 3000 epoch: 74 loss: 18.278342723608095 loss_input: 82.22907222250787
step: 4000 epoch: 74 loss: 18.321344980625057 loss_input: 82.58537791699536
step: 5000 epoch: 74 loss: 18.327103089103936 loss_input: 82.4759318277946
step: 6000 epoch: 74 loss: 18.338327944626354 loss_input: 82.5435502367444
step: 7000 epoch: 74 loss: 18.326212575581735 loss_input: 82.38253550971513
step: 8000 epoch: 74 loss: 18.314305673285048 loss_input: 82.34195802581561
step: 9000 epoch: 74 loss: 18.304410940672923 loss_input: 82.26630474760617
step: 10000 epoch: 74 loss: 18.321770940014343 loss_input: 82.3930877908303
step: 11000 epoch: 74 loss: 18.312542492839903 loss_input: 82.38901576574537
step: 12000 epoch: 74 loss: 18.304316980502435 loss_input: 82.37232872907563
step: 13000 epoch: 74 loss: 18.301933349586342 loss_input: 82.29750594699597
step: 14000 epoch: 74 loss: 18.293242588868083 loss_input: 82.21553926568023
step: 15000 epoch: 74 loss: 18.304091009185534 loss_input: 82.17047670454782
Save loss: 18.30684738814831 Name: 74_train_model.pth
step: 0 epoch: 75 loss: 23.237693786621094 loss_input: 76.23065185546875
step: 1000 epoch: 75 loss: 18.29610526668918 loss_input: 82.64083469021213
step: 2000 epoch: 75 loss: 18.309481225449822 loss_input: 82.63981854897746
step: 3000 epoch: 75 loss: 18.282735174713594 loss_input: 82.205813678969
step: 4000 epoch: 75 loss: 18.24711916643928 loss_input: 81.83782295392949
step: 5000 epoch: 75 loss: 18.294543240743025 loss_input: 82.02792016059679
step: 6000 epoch: 75 loss: 18.270568745391724 loss_input: 82.11023224848267
step: 7000 epoch: 75 loss: 18.250443544647997 loss_input: 81.9965708198215
step: 8000 epoch: 75 loss: 18.245940298605614 loss_input: 81.9958360383785
step: 9000 epoch: 75 loss: 18.2647284550768 loss_input: 82.11557757145907
step: 10000 epoch: 75 loss: 18.266478848950815 loss_input: 82.16246164690172
step: 11000 epoch: 75 loss: 18.28330204081789 loss_input: 82.16855496157842
step: 12000 epoch: 75 loss: 18.2881311932044 loss_input: 82.18371748846776
step: 13000 epoch: 75 loss: 18.28630331085055 loss_input: 82.17369621469923
step: 14000 epoch: 75 loss: 18.2842846834594 loss_input: 82.17343472484248
step: 15000 epoch: 75 loss: 18.279210413943037 loss_input: 82.14023201269003
Save loss: 18.285625997185708 Name: 75_train_model.pth
step: 0 epoch: 76 loss: 12.062097549438477 loss_input: 64.91217041015625
step: 1000 epoch: 76 loss: 18.309223185528765 loss_input: 82.95349102801495
step: 2000 epoch: 76 loss: 18.287198496603597 loss_input: 82.68399021924584
step: 3000 epoch: 76 loss: 18.287562498765084 loss_input: 82.60537240760242
step: 4000 epoch: 76 loss: 18.31616914066724 loss_input: 82.52029854474858
step: 5000 epoch: 76 loss: 18.31062709839433 loss_input: 82.55055911425137
step: 6000 epoch: 76 loss: 18.28702662110706 loss_input: 82.74310167462323
step: 7000 epoch: 76 loss: 18.29209222621261 loss_input: 82.68034014288416
step: 8000 epoch: 76 loss: 18.275668787547996 loss_input: 82.39734884518829
step: 9000 epoch: 76 loss: 18.25919899197767 loss_input: 82.36065888783625
step: 10000 epoch: 76 loss: 18.239522913481853 loss_input: 82.29549637199366
step: 11000 epoch: 76 loss: 18.259085995014683 loss_input: 82.30205510991365
step: 12000 epoch: 76 loss: 18.252492021574653 loss_input: 82.2192517994106
step: 13000 epoch: 76 loss: 18.248172532704306 loss_input: 82.23638016051049
step: 14000 epoch: 76 loss: 18.263918250486277 loss_input: 82.27419135676274
step: 15000 epoch: 76 loss: 18.262747550422322 loss_input: 82.2243003908789
Save loss: 18.280666580066086 Name: 76_train_model.pth
step: 0 epoch: 77 loss: 13.30960750579834 loss_input: 48.17462158203125
step: 1000 epoch: 77 loss: 18.49633251441704 loss_input: 84.7674536766944
step: 2000 epoch: 77 loss: 18.1390281467066 loss_input: 82.98790424123875
step: 3000 epoch: 77 loss: 18.132863554148944 loss_input: 82.33319805670246
step: 4000 epoch: 77 loss: 18.131797775629906 loss_input: 82.04436392427772
step: 5000 epoch: 77 loss: 18.125768679615213 loss_input: 81.96527823341582
step: 6000 epoch: 77 loss: 18.168165682554445 loss_input: 81.96296745545665
step: 7000 epoch: 77 loss: 18.233042832289982 loss_input: 81.93410394845937
step: 8000 epoch: 77 loss: 18.238560163979233 loss_input: 81.9784756901115
step: 9000 epoch: 77 loss: 18.24108920326737 loss_input: 82.0713610827638
step: 10000 epoch: 77 loss: 18.25285289678773 loss_input: 82.14821957135341
step: 11000 epoch: 77 loss: 18.2757121891382 loss_input: 82.24886097668323
step: 12000 epoch: 77 loss: 18.264068537161318 loss_input: 82.15226513818347
step: 13000 epoch: 77 loss: 18.270569389063198 loss_input: 82.26985346736399
step: 14000 epoch: 77 loss: 18.292280012739955 loss_input: 82.2772380490191
step: 15000 epoch: 77 loss: 18.29270886252796 loss_input: 82.26103990121362
Save loss: 18.287646300166845 Name: 77_train_model.pth
step: 0 epoch: 78 loss: 12.794515609741211 loss_input: 54.24493408203125
step: 1000 epoch: 78 loss: 18.13454973447573 loss_input: 81.41968529517358
step: 2000 epoch: 78 loss: 18.134381474643156 loss_input: 81.66350323983517
step: 3000 epoch: 78 loss: 18.144422085751536 loss_input: 81.66225997990904
step: 4000 epoch: 78 loss: 18.218818812214174 loss_input: 82.13223288721277
step: 5000 epoch: 78 loss: 18.21354660199323 loss_input: 82.16660965902523
step: 6000 epoch: 78 loss: 18.18630217782777 loss_input: 82.09382339271579
step: 7000 epoch: 78 loss: 18.18110091780581 loss_input: 82.08873135136666
step: 8000 epoch: 78 loss: 18.200979880907582 loss_input: 82.10165108145661
step: 9000 epoch: 78 loss: 18.21879054551495 loss_input: 82.13915507733299
step: 10000 epoch: 78 loss: 18.22757027499879 loss_input: 82.12902393232356
step: 11000 epoch: 78 loss: 18.23450903259248 loss_input: 82.211741004724
step: 12000 epoch: 78 loss: 18.21898331633012 loss_input: 82.19004852419525
step: 13000 epoch: 78 loss: 18.240920595751938 loss_input: 82.2424737328649
step: 14000 epoch: 78 loss: 18.226953222942168 loss_input: 82.21885588843263
step: 15000 epoch: 78 loss: 18.24883066245011 loss_input: 82.20124563717236
Save loss: 18.256401824146508 Name: 78_train_model.pth
step: 0 epoch: 79 loss: 8.996766090393066 loss_input: 56.40167236328125
step: 1000 epoch: 79 loss: 17.975612006344637 loss_input: 81.95525897442401
step: 2000 epoch: 79 loss: 17.99221711728288 loss_input: 81.62572717547476
step: 3000 epoch: 79 loss: 17.997023262766273 loss_input: 81.8569977813385
step: 4000 epoch: 79 loss: 18.064696248785552 loss_input: 82.01299305910827
step: 5000 epoch: 79 loss: 18.127023531946747 loss_input: 81.93371272034656
step: 6000 epoch: 79 loss: 18.114250632846897 loss_input: 81.97878472484403
step: 7000 epoch: 79 loss: 18.1269379757316 loss_input: 82.08033230011505
step: 8000 epoch: 79 loss: 18.150530695632135 loss_input: 82.0803158043355
step: 9000 epoch: 79 loss: 18.21700695061363 loss_input: 82.28633555554374
step: 10000 epoch: 79 loss: 18.2433217716341 loss_input: 82.34178888019402
step: 11000 epoch: 79 loss: 18.218456881792477 loss_input: 82.17485938112516
step: 12000 epoch: 79 loss: 18.221311107177296 loss_input: 82.22567614502991
step: 13000 epoch: 79 loss: 18.229715643511508 loss_input: 82.20519780104054
step: 14000 epoch: 79 loss: 18.238689328387725 loss_input: 82.18824343793383
step: 15000 epoch: 79 loss: 18.240146990736584 loss_input: 82.14178212500677
Save loss: 18.260048995420338 Name: 79_train_model.pth
step: 0 epoch: 80 loss: 20.865009307861328 loss_input: 110.4832763671875
step: 1000 epoch: 80 loss: 18.249217855584966 loss_input: 82.72093659514314
step: 2000 epoch: 80 loss: 18.181602325754007 loss_input: 83.10946104706436
step: 3000 epoch: 80 loss: 18.103685346296412 loss_input: 82.25747960315312
step: 4000 epoch: 80 loss: 18.215768417457557 loss_input: 82.44728023926635
step: 5000 epoch: 80 loss: 18.160136787397008 loss_input: 82.23040585974601
step: 6000 epoch: 80 loss: 18.22693458204964 loss_input: 82.15624629781755
step: 7000 epoch: 80 loss: 18.281149799015637 loss_input: 82.25514506054101
step: 8000 epoch: 80 loss: 18.23733035112497 loss_input: 82.05564259606709
step: 9000 epoch: 80 loss: 18.21300839440132 loss_input: 82.05463165225565
step: 10000 epoch: 80 loss: 18.190257772757118 loss_input: 82.00715934876239
step: 11000 epoch: 80 loss: 18.20385494308898 loss_input: 81.88623834156945
step: 12000 epoch: 80 loss: 18.199658962023594 loss_input: 81.96891399797087
step: 13000 epoch: 80 loss: 18.221929240012184 loss_input: 82.04478595077272
step: 14000 epoch: 80 loss: 18.254390798443666 loss_input: 82.13257217815915
step: 15000 epoch: 80 loss: 18.260825726550607 loss_input: 82.18377951642417
Save loss: 18.254682493701576 Name: 80_train_model.pth
step: 0 epoch: 81 loss: 18.144718170166016 loss_input: 58.41461181640625
step: 1000 epoch: 81 loss: 17.940526279655252 loss_input: 81.55697951950394
step: 2000 epoch: 81 loss: 17.977281608919927 loss_input: 81.79451479094438
step: 3000 epoch: 81 loss: 18.022542283996586 loss_input: 81.56307593920516
step: 4000 epoch: 81 loss: 18.053022249196058 loss_input: 81.48742367630749
step: 5000 epoch: 81 loss: 18.071544724830364 loss_input: 81.45703426453382
step: 6000 epoch: 81 loss: 18.106237003156053 loss_input: 81.84553019036215
step: 7000 epoch: 81 loss: 18.139200537089977 loss_input: 82.08578853446438
step: 8000 epoch: 81 loss: 18.135764549738347 loss_input: 82.18219793601239
step: 9000 epoch: 81 loss: 18.155209300226932 loss_input: 82.1039998715963
step: 10000 epoch: 81 loss: 18.159873716951118 loss_input: 82.28757162125585
step: 11000 epoch: 81 loss: 18.170549679600036 loss_input: 82.29237827744531
step: 12000 epoch: 81 loss: 18.176841591290756 loss_input: 82.22317336287243
step: 13000 epoch: 81 loss: 18.20047145949429 loss_input: 82.21730601927857
step: 14000 epoch: 81 loss: 18.22592808319734 loss_input: 82.25879155168465
step: 15000 epoch: 81 loss: 18.217458189809236 loss_input: 82.21772206153054
Save loss: 18.225159719944 Name: 81_train_model.pth
step: 0 epoch: 82 loss: 20.982776641845703 loss_input: 107.91314697265625
step: 1000 epoch: 82 loss: 18.087896978700314 loss_input: 82.39624627081902
step: 2000 epoch: 82 loss: 18.1003711973054 loss_input: 81.77751001818427
step: 3000 epoch: 82 loss: 18.198944903103282 loss_input: 82.37301447613166
step: 4000 epoch: 82 loss: 18.138826331267563 loss_input: 81.98027106077484
step: 5000 epoch: 82 loss: 18.148648165436036 loss_input: 82.20675918527232
step: 6000 epoch: 82 loss: 18.119858892654065 loss_input: 82.05656763911743
step: 7000 epoch: 82 loss: 18.106156581777313 loss_input: 81.95716554303627
step: 8000 epoch: 82 loss: 18.12737507618095 loss_input: 82.07256538458816
step: 9000 epoch: 82 loss: 18.169926085851944 loss_input: 82.10667299580645
step: 10000 epoch: 82 loss: 18.190808980539554 loss_input: 82.11581015970668
step: 11000 epoch: 82 loss: 18.1967385946301 loss_input: 82.01156301353642
step: 12000 epoch: 82 loss: 18.20124729263138 loss_input: 82.06686654896272
step: 13000 epoch: 82 loss: 18.202151133660013 loss_input: 82.05805157415776
step: 14000 epoch: 82 loss: 18.20447293711291 loss_input: 82.05781323149769
step: 15000 epoch: 82 loss: 18.20707724474405 loss_input: 82.13383568618212
Save loss: 18.207565660715105 Name: 82_train_model.pth
step: 0 epoch: 83 loss: 12.52161979675293 loss_input: 43.55657958984375
step: 1000 epoch: 83 loss: 17.790985984878464 loss_input: 79.84696340036916
step: 2000 epoch: 83 loss: 17.96258275095431 loss_input: 80.94189989965955
step: 3000 epoch: 83 loss: 18.18854228054353 loss_input: 81.78288843090715
step: 4000 epoch: 83 loss: 18.220012985864482 loss_input: 82.18785995639762
step: 5000 epoch: 83 loss: 18.2265685130491 loss_input: 82.35889942372853
step: 6000 epoch: 83 loss: 18.216895191495844 loss_input: 82.0522300645205
step: 7000 epoch: 83 loss: 18.205993767926188 loss_input: 82.0055399451864
step: 8000 epoch: 83 loss: 18.164753760237826 loss_input: 81.7975149880557
step: 9000 epoch: 83 loss: 18.16230501899215 loss_input: 81.78024859217031
step: 10000 epoch: 83 loss: 18.167461720386417 loss_input: 81.83284853277368
step: 11000 epoch: 83 loss: 18.178677439440403 loss_input: 82.0002946232072
step: 12000 epoch: 83 loss: 18.210707004165045 loss_input: 82.23062449873969
step: 13000 epoch: 83 loss: 18.201043583614883 loss_input: 82.23870502674674
step: 14000 epoch: 83 loss: 18.211519903067597 loss_input: 82.18572406920013
step: 15000 epoch: 83 loss: 18.21083810435765 loss_input: 82.22883718152514
Save loss: 18.20190517421067 Name: 83_train_model.pth
step: 0 epoch: 84 loss: 12.108634948730469 loss_input: 60.70538330078125
step: 1000 epoch: 84 loss: 18.03125443206086 loss_input: 81.82202843543176
step: 2000 epoch: 84 loss: 18.28795213248955 loss_input: 82.66338428076001
step: 3000 epoch: 84 loss: 18.16754343699551 loss_input: 81.88332604138465
step: 4000 epoch: 84 loss: 18.1463367714342 loss_input: 81.97448855559547
step: 5000 epoch: 84 loss: 18.166475347413275 loss_input: 82.10097627691259
step: 6000 epoch: 84 loss: 18.168281683264684 loss_input: 82.13789657290191
step: 7000 epoch: 84 loss: 18.19218269400453 loss_input: 82.09907847986003
step: 8000 epoch: 84 loss: 18.159548791404546 loss_input: 81.95793635689337
step: 9000 epoch: 84 loss: 18.131852022185218 loss_input: 81.93886598456291
step: 10000 epoch: 84 loss: 18.142750686317097 loss_input: 81.99779112664905
step: 11000 epoch: 84 loss: 18.156587924451006 loss_input: 81.86971488587153
step: 12000 epoch: 84 loss: 18.16316791769167 loss_input: 81.94897569774459
step: 13000 epoch: 84 loss: 18.171490596336692 loss_input: 82.011918446365
step: 14000 epoch: 84 loss: 18.196624114899777 loss_input: 82.16091207718561
step: 15000 epoch: 84 loss: 18.19327880042321 loss_input: 82.18488885815816
Save loss: 18.202145699709654 Name: 84_train_model.pth
step: 0 epoch: 85 loss: 23.486658096313477 loss_input: 141.5272216796875
step: 1000 epoch: 85 loss: 17.94029935566219 loss_input: 81.7758734795478
step: 2000 epoch: 85 loss: 18.149169778657043 loss_input: 82.90419987760026
step: 3000 epoch: 85 loss: 18.214785958480455 loss_input: 82.55704377024065
step: 4000 epoch: 85 loss: 18.26532498004287 loss_input: 82.57568129024872
step: 5000 epoch: 85 loss: 18.255646598074488 loss_input: 82.72207839926155
step: 6000 epoch: 85 loss: 18.209122218840957 loss_input: 82.68048970394582
step: 7000 epoch: 85 loss: 18.19215424819225 loss_input: 82.57939110679501
step: 8000 epoch: 85 loss: 18.177007599303668 loss_input: 82.41003232597635
step: 9000 epoch: 85 loss: 18.169679703758554 loss_input: 82.31907877748827
step: 10000 epoch: 85 loss: 18.182123666858093 loss_input: 82.28555623441562
step: 11000 epoch: 85 loss: 18.16875007596626 loss_input: 82.32611036270318
step: 12000 epoch: 85 loss: 18.158971173814333 loss_input: 82.27315997084065
step: 13000 epoch: 85 loss: 18.165287823688434 loss_input: 82.1609483455825
step: 14000 epoch: 85 loss: 18.191923200262504 loss_input: 82.22390832731395
step: 15000 epoch: 85 loss: 18.18008075034759 loss_input: 82.21297670731457
Save loss: 18.189113116294145 Name: 85_train_model.pth
step: 0 epoch: 86 loss: 17.08324432373047 loss_input: 84.49188232421875
step: 1000 epoch: 86 loss: 18.15544426143467 loss_input: 82.37978421724759
step: 2000 epoch: 86 loss: 18.06352006525233 loss_input: 81.94452672955515
step: 3000 epoch: 86 loss: 18.053498039957447 loss_input: 81.98453994339127
step: 4000 epoch: 86 loss: 18.078433606720544 loss_input: 81.86906164999277
step: 5000 epoch: 86 loss: 18.04560591406499 loss_input: 81.85194811623613
step: 6000 epoch: 86 loss: 18.108257374670522 loss_input: 82.01254104133686
step: 7000 epoch: 86 loss: 18.130844433330463 loss_input: 82.13060724862966
step: 8000 epoch: 86 loss: 18.16124734498906 loss_input: 82.19358647368786
step: 9000 epoch: 86 loss: 18.200430682441578 loss_input: 82.15742308064947
step: 10000 epoch: 86 loss: 18.18586023313238 loss_input: 82.13114471633891
step: 11000 epoch: 86 loss: 18.171451825920467 loss_input: 82.08646356967371
step: 12000 epoch: 86 loss: 18.161832418613418 loss_input: 82.04974905859082
step: 13000 epoch: 86 loss: 18.176434897943675 loss_input: 82.08887841241028
step: 14000 epoch: 86 loss: 18.181293363137275 loss_input: 82.24234007699226
step: 15000 epoch: 86 loss: 18.18663948534743 loss_input: 82.25279342135654
Save loss: 18.180351336807014 Name: 86_train_model.pth
step: 0 epoch: 87 loss: 21.25572967529297 loss_input: 75.851806640625
step: 1000 epoch: 87 loss: 17.96826394287856 loss_input: 82.09990778264704
step: 2000 epoch: 87 loss: 18.132248714767297 loss_input: 82.27819421588035
step: 3000 epoch: 87 loss: 18.16177808916676 loss_input: 82.59560186074043
step: 4000 epoch: 87 loss: 18.1672552192667 loss_input: 82.31957232859754
step: 5000 epoch: 87 loss: 18.16091221214604 loss_input: 82.3843462630716
step: 6000 epoch: 87 loss: 18.19863257577391 loss_input: 82.26807555999781
step: 7000 epoch: 87 loss: 18.15636506194371 loss_input: 82.22099575548917
step: 8000 epoch: 87 loss: 18.118553236594366 loss_input: 82.24317809120295
step: 9000 epoch: 87 loss: 18.127388461432634 loss_input: 82.24877343137004
step: 10000 epoch: 87 loss: 18.126251588212455 loss_input: 82.1476495905097
step: 11000 epoch: 87 loss: 18.153610885907234 loss_input: 82.28929851259083
step: 12000 epoch: 87 loss: 18.156412518746595 loss_input: 82.17810519879366
step: 13000 epoch: 87 loss: 18.16699148920736 loss_input: 82.18123255186049
step: 14000 epoch: 87 loss: 18.164187367971724 loss_input: 82.1852843205117
step: 15000 epoch: 87 loss: 18.17012229417771 loss_input: 82.18345716649489
Save loss: 18.169509476199746 Name: 87_train_model.pth
step: 0 epoch: 88 loss: 21.069082260131836 loss_input: 83.6107177734375
step: 1000 epoch: 88 loss: 17.849758879883545 loss_input: 81.04419292865337
step: 2000 epoch: 88 loss: 18.025085565032274 loss_input: 81.55339868517889
step: 3000 epoch: 88 loss: 18.12612738413876 loss_input: 81.92709119746543
step: 4000 epoch: 88 loss: 18.091473035590703 loss_input: 81.73934832026856
step: 5000 epoch: 88 loss: 18.110592351820774 loss_input: 81.85343707625663
step: 6000 epoch: 88 loss: 18.10087165234983 loss_input: 81.94210729486166
step: 7000 epoch: 88 loss: 18.07418263385916 loss_input: 81.93499720987124
step: 8000 epoch: 88 loss: 18.131859178081807 loss_input: 82.0238706199933
step: 9000 epoch: 88 loss: 18.14332519723447 loss_input: 82.07730675188651
step: 10000 epoch: 88 loss: 18.13666748025515 loss_input: 81.98580029082052
step: 11000 epoch: 88 loss: 18.144710939284423 loss_input: 81.9119581047941
step: 12000 epoch: 88 loss: 18.148320061993015 loss_input: 82.03039929676271
step: 13000 epoch: 88 loss: 18.15391097068053 loss_input: 82.1000216357681
step: 14000 epoch: 88 loss: 18.157512916461815 loss_input: 82.0920704934999
step: 15000 epoch: 88 loss: 18.157589178054813 loss_input: 82.11701662516118
Save loss: 18.162017346546055 Name: 88_train_model.pth
step: 0 epoch: 89 loss: 11.076813697814941 loss_input: 50.3232421875
step: 1000 epoch: 89 loss: 17.971943085962955 loss_input: 80.63649119435252
step: 2000 epoch: 89 loss: 17.94899821769947 loss_input: 81.12906159835121
step: 3000 epoch: 89 loss: 18.018310186188447 loss_input: 81.37702292595176
step: 4000 epoch: 89 loss: 17.95964930135588 loss_input: 81.53106991501636
step: 5000 epoch: 89 loss: 18.005350017566677 loss_input: 81.92630796350495
step: 6000 epoch: 89 loss: 17.986417241581997 loss_input: 81.94398468785477
step: 7000 epoch: 89 loss: 18.029824559577207 loss_input: 82.07458270295929
step: 8000 epoch: 89 loss: 18.06855959809433 loss_input: 82.1655747621391
step: 9000 epoch: 89 loss: 18.08224646487033 loss_input: 82.15541649479268
step: 10000 epoch: 89 loss: 18.08173141826595 loss_input: 82.09244319708654
step: 11000 epoch: 89 loss: 18.07891510170315 loss_input: 82.12265374445718
step: 12000 epoch: 89 loss: 18.113431888941417 loss_input: 82.24923536950534
step: 13000 epoch: 89 loss: 18.106388567631157 loss_input: 82.2431606494888
step: 14000 epoch: 89 loss: 18.12997596306968 loss_input: 82.20678145966694
step: 15000 epoch: 89 loss: 18.12906084824511 loss_input: 82.16936341613685
Save loss: 18.150845354661346 Name: 89_train_model.pth
step: 0 epoch: 90 loss: 18.632482528686523 loss_input: 54.547607421875
step: 1000 epoch: 90 loss: 18.212572499350472 loss_input: 81.83879981102882
step: 2000 epoch: 90 loss: 18.099182278558292 loss_input: 82.12131647799147
step: 3000 epoch: 90 loss: 18.13952945097809 loss_input: 82.34897305146332
step: 4000 epoch: 90 loss: 18.111681860168407 loss_input: 82.2593314226375
step: 5000 epoch: 90 loss: 18.08454162443764 loss_input: 82.18451588002712
step: 6000 epoch: 90 loss: 18.092133471060347 loss_input: 82.074344278395
step: 7000 epoch: 90 loss: 18.087510175797586 loss_input: 82.19620092288713
step: 8000 epoch: 90 loss: 18.106393587587775 loss_input: 82.14774907134411
step: 9000 epoch: 90 loss: 18.12905571498072 loss_input: 82.24035479754636
step: 10000 epoch: 90 loss: 18.163779482556848 loss_input: 82.30934542654133
step: 11000 epoch: 90 loss: 18.139215180271595 loss_input: 82.10780812584154
step: 12000 epoch: 90 loss: 18.166783829528423 loss_input: 82.16108271698387
step: 13000 epoch: 90 loss: 18.15094868963145 loss_input: 82.16827316055316
step: 14000 epoch: 90 loss: 18.146966436285435 loss_input: 82.13808013347122
step: 15000 epoch: 90 loss: 18.14946449145453 loss_input: 82.18856671762246
Save loss: 18.152102795526385 Name: 90_train_model.pth
step: 0 epoch: 91 loss: 18.133665084838867 loss_input: 75.879638671875
step: 1000 epoch: 91 loss: 18.097583090508735 loss_input: 82.69615260228053
step: 2000 epoch: 91 loss: 17.961043786788093 loss_input: 81.74657315507285
step: 3000 epoch: 91 loss: 18.025804526485707 loss_input: 81.88234990598836
step: 4000 epoch: 91 loss: 18.021458953537067 loss_input: 81.67755627316315
step: 5000 epoch: 91 loss: 18.021314557183626 loss_input: 81.60800432686901
step: 6000 epoch: 91 loss: 18.08484094259322 loss_input: 81.86296604577947
step: 7000 epoch: 91 loss: 18.14482381489937 loss_input: 82.14559711161519
step: 8000 epoch: 91 loss: 18.139415250809666 loss_input: 82.22800452708304
step: 9000 epoch: 91 loss: 18.156724325591572 loss_input: 82.29807008476922
step: 10000 epoch: 91 loss: 18.137244945716265 loss_input: 82.27031532076285
step: 11000 epoch: 91 loss: 18.13207762174829 loss_input: 82.23796367966015
step: 12000 epoch: 91 loss: 18.1367034426372 loss_input: 82.19618187843884
step: 13000 epoch: 91 loss: 18.13849577202484 loss_input: 82.2340882413342
step: 14000 epoch: 91 loss: 18.12403641793653 loss_input: 82.19104241926358
step: 15000 epoch: 91 loss: 18.133404798201262 loss_input: 82.22067950190039
Save loss: 18.13397948296368 Name: 91_train_model.pth
step: 0 epoch: 92 loss: 5.106802940368652 loss_input: 38.54278564453125
step: 1000 epoch: 92 loss: 17.962918370634643 loss_input: 82.81736427634866
step: 2000 epoch: 92 loss: 18.08877594979747 loss_input: 82.76526736021637
step: 3000 epoch: 92 loss: 18.031491543999596 loss_input: 82.40522887634461
step: 4000 epoch: 92 loss: 18.054269877769624 loss_input: 82.37240032338107
step: 5000 epoch: 92 loss: 18.069574383205712 loss_input: 82.27629871974824
step: 6000 epoch: 92 loss: 18.090945750191857 loss_input: 82.2520530432428
step: 7000 epoch: 92 loss: 18.07631770520019 loss_input: 82.1640644354099
step: 8000 epoch: 92 loss: 18.139093140604615 loss_input: 82.21736346219662
step: 9000 epoch: 92 loss: 18.089642912159363 loss_input: 82.18375065395294
step: 10000 epoch: 92 loss: 18.10132992068549 loss_input: 82.17584239500854
step: 11000 epoch: 92 loss: 18.10613618021867 loss_input: 82.1982242780841
step: 12000 epoch: 92 loss: 18.103821269095654 loss_input: 82.14214481658593
step: 13000 epoch: 92 loss: 18.130135361849916 loss_input: 82.21703541490649
step: 14000 epoch: 92 loss: 18.12886550940579 loss_input: 82.24199275944575
step: 15000 epoch: 92 loss: 18.1189134792601 loss_input: 82.16306219233822
Save loss: 18.125070035740734 Name: 92_train_model.pth
step: 0 epoch: 93 loss: 18.71516227722168 loss_input: 75.23468017578125
step: 1000 epoch: 93 loss: 17.851127644995234 loss_input: 81.56827254776474
step: 2000 epoch: 93 loss: 17.918837728886412 loss_input: 81.38285980910912
step: 3000 epoch: 93 loss: 18.035704242750153 loss_input: 81.72926846650076
step: 4000 epoch: 93 loss: 18.031430338061295 loss_input: 81.98259228832929
step: 5000 epoch: 93 loss: 18.049888605261966 loss_input: 82.06437701717469
step: 6000 epoch: 93 loss: 18.050692268459624 loss_input: 82.2834809717586
step: 7000 epoch: 93 loss: 18.045258829856223 loss_input: 82.22503812931858
step: 8000 epoch: 93 loss: 18.086147772879947 loss_input: 82.30316011829088
step: 9000 epoch: 93 loss: 18.09478464612377 loss_input: 82.39448671284788
step: 10000 epoch: 93 loss: 18.08172309273494 loss_input: 82.32971400735903
step: 11000 epoch: 93 loss: 18.07871478911411 loss_input: 82.32541331026015
step: 12000 epoch: 93 loss: 18.092786124185487 loss_input: 82.33205421764983
step: 13000 epoch: 93 loss: 18.08429676816515 loss_input: 82.2246046427919
step: 14000 epoch: 93 loss: 18.09641889532705 loss_input: 82.28578756564124
step: 15000 epoch: 93 loss: 18.118546290680868 loss_input: 82.3158567830504
Save loss: 18.109238894954323 Name: 93_train_model.pth
step: 0 epoch: 94 loss: 19.24112319946289 loss_input: 125.911865234375
step: 1000 epoch: 94 loss: 18.005109757452935 loss_input: 82.19439105816059
step: 2000 epoch: 94 loss: 18.020538966575902 loss_input: 82.26484227978784
step: 3000 epoch: 94 loss: 18.00115657273788 loss_input: 81.97591826487525
step: 4000 epoch: 94 loss: 17.966331479073286 loss_input: 82.09229347021154
step: 5000 epoch: 94 loss: 18.0100724335266 loss_input: 82.06815115248435
step: 6000 epoch: 94 loss: 18.09690254423106 loss_input: 82.2263380094203
step: 7000 epoch: 94 loss: 18.106225456310806 loss_input: 82.42413227204986
step: 8000 epoch: 94 loss: 18.145189582251145 loss_input: 82.65190551132281
step: 9000 epoch: 94 loss: 18.157321118921747 loss_input: 82.48457469162497
step: 10000 epoch: 94 loss: 18.13271343168074 loss_input: 82.27819929188722
step: 11000 epoch: 94 loss: 18.105148669384075 loss_input: 82.2098573036773
step: 12000 epoch: 94 loss: 18.109999672371828 loss_input: 82.2896484090193
step: 13000 epoch: 94 loss: 18.119283249117615 loss_input: 82.33720146433298
step: 14000 epoch: 94 loss: 18.110958780699836 loss_input: 82.29402218950739
step: 15000 epoch: 94 loss: 18.103317597795904 loss_input: 82.25796552121135
Save loss: 18.11340883333981 Name: 94_train_model.pth
step: 0 epoch: 95 loss: 19.585969924926758 loss_input: 85.1544189453125
step: 1000 epoch: 95 loss: 17.98781127434272 loss_input: 81.24488853431724
step: 2000 epoch: 95 loss: 18.11878186949845 loss_input: 82.12724359865966
step: 3000 epoch: 95 loss: 18.007362709884365 loss_input: 81.83159003151293
step: 4000 epoch: 95 loss: 18.001642264237674 loss_input: 81.74925088918201
step: 5000 epoch: 95 loss: 17.990261971676404 loss_input: 81.7914618736409
step: 6000 epoch: 95 loss: 18.042893550451986 loss_input: 82.11918704467224
step: 7000 epoch: 95 loss: 18.071082813980546 loss_input: 82.2751495618511
step: 8000 epoch: 95 loss: 18.053465560411993 loss_input: 82.2720295491628
step: 9000 epoch: 95 loss: 18.034959349469627 loss_input: 82.19998713793086
step: 10000 epoch: 95 loss: 18.019462081339704 loss_input: 82.08239043039056
step: 11000 epoch: 95 loss: 18.054182662691662 loss_input: 82.1037662514079
step: 12000 epoch: 95 loss: 18.05146494822426 loss_input: 82.26619394265973
step: 13000 epoch: 95 loss: 18.05462805178246 loss_input: 82.24636135918482
step: 14000 epoch: 95 loss: 18.09089050646484 loss_input: 82.28162962463479
step: 15000 epoch: 95 loss: 18.101403711795903 loss_input: 82.2912724071976
Save loss: 18.0918621314466 Name: 95_train_model.pth
step: 0 epoch: 96 loss: 17.813045501708984 loss_input: 77.3037109375
step: 1000 epoch: 96 loss: 17.93382795040424 loss_input: 81.58871371548373
step: 2000 epoch: 96 loss: 17.975326289063034 loss_input: 81.31189898214954
step: 3000 epoch: 96 loss: 17.933593014485435 loss_input: 80.87848703752395
step: 4000 epoch: 96 loss: 17.967405173814885 loss_input: 81.42674666111304
step: 5000 epoch: 96 loss: 17.974188976539562 loss_input: 81.59237448164664
step: 6000 epoch: 96 loss: 18.026927147045114 loss_input: 81.76429453326789
step: 7000 epoch: 96 loss: 18.007190455506315 loss_input: 81.88330616029462
step: 8000 epoch: 96 loss: 17.994194729628227 loss_input: 81.6952473607425
step: 9000 epoch: 96 loss: 18.027179652194874 loss_input: 81.8931833094934
step: 10000 epoch: 96 loss: 18.00967962941388 loss_input: 81.83336615068961
step: 11000 epoch: 96 loss: 18.02730831890992 loss_input: 81.88197435265118
step: 12000 epoch: 96 loss: 18.026467882328337 loss_input: 81.92976551677334
step: 13000 epoch: 96 loss: 18.044657204428248 loss_input: 81.95558973397029
step: 14000 epoch: 96 loss: 18.053301933262826 loss_input: 82.00630018300869
step: 15000 epoch: 96 loss: 18.074637808344235 loss_input: 82.20856023335423
Save loss: 18.081146730437876 Name: 96_train_model.pth
step: 0 epoch: 97 loss: 13.552120208740234 loss_input: 57.50531005859375
step: 1000 epoch: 97 loss: 17.994875953152224 loss_input: 82.08163363735873
step: 2000 epoch: 97 loss: 17.953625272953886 loss_input: 81.87007829703312
step: 3000 epoch: 97 loss: 17.89855845115138 loss_input: 81.68505324478429
step: 4000 epoch: 97 loss: 17.976784233628617 loss_input: 82.0160965312871
step: 5000 epoch: 97 loss: 18.004078364233997 loss_input: 82.44340221311207
step: 6000 epoch: 97 loss: 17.984195100567852 loss_input: 82.39119445465144
step: 7000 epoch: 97 loss: 17.983096083374196 loss_input: 82.20057925597682
step: 8000 epoch: 97 loss: 18.016357226932573 loss_input: 82.36388838236041
step: 9000 epoch: 97 loss: 18.020160470137583 loss_input: 82.4551528237314
step: 10000 epoch: 97 loss: 18.034659787900758 loss_input: 82.4421179525114
step: 11000 epoch: 97 loss: 18.076110039740385 loss_input: 82.38508533137613
step: 12000 epoch: 97 loss: 18.071522141504204 loss_input: 82.43460364000428
step: 13000 epoch: 97 loss: 18.064223017640117 loss_input: 82.31871425305025
step: 14000 epoch: 97 loss: 18.068570336531966 loss_input: 82.26780991124797
step: 15000 epoch: 97 loss: 18.053986550839646 loss_input: 82.18586955449715
Save loss: 18.070884434342386 Name: 97_train_model.pth
step: 0 epoch: 98 loss: 30.728195190429688 loss_input: 70.526123046875
step: 1000 epoch: 98 loss: 18.117451151410542 loss_input: 82.18806266975213
step: 2000 epoch: 98 loss: 18.120925280643903 loss_input: 83.31996016762126
step: 3000 epoch: 98 loss: 18.105526308900554 loss_input: 83.31613916780344
step: 4000 epoch: 98 loss: 18.077231052487353 loss_input: 83.04961062895897
step: 5000 epoch: 98 loss: 18.061123789417532 loss_input: 82.77459409241199
step: 6000 epoch: 98 loss: 18.04242245377431 loss_input: 82.47835999094552
step: 7000 epoch: 98 loss: 18.052422143139 loss_input: 82.46969907444863
step: 8000 epoch: 98 loss: 18.04035047027055 loss_input: 82.4766439107221
step: 9000 epoch: 98 loss: 18.049900504830493 loss_input: 82.57062865410788
step: 10000 epoch: 98 loss: 18.03214489034552 loss_input: 82.47364497559033
step: 11000 epoch: 98 loss: 18.034728199273346 loss_input: 82.39102270776343
step: 12000 epoch: 98 loss: 18.03401882111475 loss_input: 82.33037927788244
step: 13000 epoch: 98 loss: 18.042025021980034 loss_input: 82.35020022028804
step: 14000 epoch: 98 loss: 18.04974843149449 loss_input: 82.32900314598065
step: 15000 epoch: 98 loss: 18.05548272472359 loss_input: 82.29362098923222
Save loss: 18.06892393167317 Name: 98_train_model.pth
step: 0 epoch: 99 loss: 17.688148498535156 loss_input: 89.88641357421875
step: 1000 epoch: 99 loss: 17.95951832019604 loss_input: 81.52477423699348
step: 2000 epoch: 99 loss: 17.950049174421732 loss_input: 81.74485624009284
step: 3000 epoch: 99 loss: 18.02797518615443 loss_input: 81.95807153230824
step: 4000 epoch: 99 loss: 18.01397395175685 loss_input: 81.80802080363698
step: 5000 epoch: 99 loss: 18.021521435859466 loss_input: 81.83036293131998
step: 6000 epoch: 99 loss: 18.00100298608031 loss_input: 81.7986545801123
step: 7000 epoch: 99 loss: 17.960495034961458 loss_input: 81.67417893112089
step: 8000 epoch: 99 loss: 17.97504222412405 loss_input: 81.75175938548453
step: 9000 epoch: 99 loss: 17.98582521765355 loss_input: 81.84158745952055
step: 10000 epoch: 99 loss: 18.006919867944006 loss_input: 82.0768528066627
step: 11000 epoch: 99 loss: 18.029034484224898 loss_input: 82.034755673845
step: 12000 epoch: 99 loss: 18.021595849602257 loss_input: 82.07554599637042
step: 13000 epoch: 99 loss: 18.034104863823988 loss_input: 82.14433322493072
step: 14000 epoch: 99 loss: 18.057589343322803 loss_input: 82.20226615718855
step: 15000 epoch: 99 loss: 18.067401198639597 loss_input: 82.26294470586727
Save loss: 18.058066804900765 Name: 99_train_model.pth
read
Begin
step: 0 epoch: 100 loss: 14.493624687194824 loss_input: 70.00079345703125
step: 1000 epoch: 100 loss: 18.003718684365104 loss_input: 82.14133455655673
step: 2000 epoch: 100 loss: 18.088218128246286 loss_input: 81.78530622994167
step: 3000 epoch: 100 loss: 18.159836927520082 loss_input: 82.66223689596957
step: 4000 epoch: 100 loss: 18.056938278231137 loss_input: 82.2804125628749
step: 5000 epoch: 100 loss: 18.040497985178888 loss_input: 82.24317742652642
step: 6000 epoch: 100 loss: 17.98150373943407 loss_input: 82.00179180545105
step: 7000 epoch: 100 loss: 17.97704707154409 loss_input: 82.00903193028786
step: 8000 epoch: 100 loss: 18.041799974060105 loss_input: 82.16936010897808
step: 9000 epoch: 100 loss: 18.04069677232014 loss_input: 82.15033359737373
step: 10000 epoch: 100 loss: 18.028776736178884 loss_input: 82.16632374848453
step: 11000 epoch: 100 loss: 18.04195339388657 loss_input: 82.17856071589372
step: 12000 epoch: 100 loss: 18.06832534535191 loss_input: 82.35056097111536
step: 13000 epoch: 100 loss: 18.060285613478335 loss_input: 82.24277621391654
step: 14000 epoch: 100 loss: 18.079039315872215 loss_input: 82.34467845279467
step: 15000 epoch: 100 loss: 18.060920031481874 loss_input: 82.24529019842687
Save loss: 18.06085257510841 Name: 100_train_model.pth
step: 0 epoch: 101 loss: 12.271210670471191 loss_input: 51.26556396484375
step: 1000 epoch: 101 loss: 18.20953569140706 loss_input: 82.07257220318743
step: 2000 epoch: 101 loss: 17.982602656691864 loss_input: 81.79748294187867
step: 3000 epoch: 101 loss: 17.945466542792136 loss_input: 81.74882694944785
step: 4000 epoch: 101 loss: 17.946359834680553 loss_input: 81.83056907587068
step: 5000 epoch: 101 loss: 18.002442888774006 loss_input: 82.12281126538365
step: 6000 epoch: 101 loss: 17.95893409800041 loss_input: 82.08085045721546
step: 7000 epoch: 101 loss: 17.989970666308896 loss_input: 82.25902130296956
step: 8000 epoch: 101 loss: 17.995175650947886 loss_input: 82.23664063934147
step: 9000 epoch: 101 loss: 18.02765490589983 loss_input: 82.20981294368985
step: 10000 epoch: 101 loss: 18.03786080373 loss_input: 82.1866672463613
step: 11000 epoch: 101 loss: 18.067882945955628 loss_input: 82.31393326370188
step: 12000 epoch: 101 loss: 18.07380842606273 loss_input: 82.28622880771492
step: 13000 epoch: 101 loss: 18.070946614363223 loss_input: 82.29307861422018
step: 14000 epoch: 101 loss: 18.052770639264526 loss_input: 82.22051593918654
step: 15000 epoch: 101 loss: 18.050723940171796 loss_input: 82.22682830259232
Save loss: 18.040630964949727 Name: 101_train_model.pth
step: 0 epoch: 102 loss: 23.211807250976562 loss_input: 118.93280029296875
step: 1000 epoch: 102 loss: 18.07404920699951 loss_input: 80.80777590329592
step: 2000 epoch: 102 loss: 17.98096258124371 loss_input: 81.54490305506427
step: 3000 epoch: 102 loss: 17.926359626620023 loss_input: 81.87643927854127
step: 4000 epoch: 102 loss: 17.981703660155024 loss_input: 82.01257594231814
step: 5000 epoch: 102 loss: 18.0195416343901 loss_input: 81.887586752669
step: 6000 epoch: 102 loss: 17.973427044910267 loss_input: 81.91328274307797
step: 7000 epoch: 102 loss: 17.968613199158543 loss_input: 82.01796269617734
step: 8000 epoch: 102 loss: 17.96327833607262 loss_input: 81.97983604710544
step: 9000 epoch: 102 loss: 18.006600206314943 loss_input: 82.17735656867119
step: 10000 epoch: 102 loss: 18.02016761197816 loss_input: 82.26694458239723
step: 11000 epoch: 102 loss: 18.011898205763817 loss_input: 82.29417365755279
step: 12000 epoch: 102 loss: 18.01911403991592 loss_input: 82.28313541213689
step: 13000 epoch: 102 loss: 18.021626859066643 loss_input: 82.23682632604734
step: 14000 epoch: 102 loss: 18.022082120246115 loss_input: 82.3629358462049
step: 15000 epoch: 102 loss: 18.020986730039507 loss_input: 82.24230950114394
Save loss: 18.027697375670076 Name: 102_train_model.pth
step: 0 epoch: 103 loss: 20.998275756835938 loss_input: 77.68768310546875
step: 1000 epoch: 103 loss: 17.952782824799254 loss_input: 82.48557826450893
step: 2000 epoch: 103 loss: 18.06645246865093 loss_input: 82.65633796871096
step: 3000 epoch: 103 loss: 18.061824686882378 loss_input: 82.69399946111713
step: 4000 epoch: 103 loss: 18.05861961981619 loss_input: 82.72060088615541
step: 5000 epoch: 103 loss: 18.031629527671125 loss_input: 82.72395540423166
step: 6000 epoch: 103 loss: 18.018664482811495 loss_input: 82.54025625042311
step: 7000 epoch: 103 loss: 18.01069135159156 loss_input: 82.451449161699
step: 8000 epoch: 103 loss: 18.002575550894637 loss_input: 82.22617880467578
step: 9000 epoch: 103 loss: 17.98994566098728 loss_input: 82.13325535579597
step: 10000 epoch: 103 loss: 18.03161576604524 loss_input: 82.21003242083019
step: 11000 epoch: 103 loss: 18.020645274713726 loss_input: 82.24437748090905
step: 12000 epoch: 103 loss: 18.027166747935624 loss_input: 82.28868763296418
step: 13000 epoch: 103 loss: 18.03274161928792 loss_input: 82.21098227758021
step: 14000 epoch: 103 loss: 18.031352920401446 loss_input: 82.23429853713016
step: 15000 epoch: 103 loss: 18.026602703923743 loss_input: 82.16966995087698
Save loss: 18.027161855816843 Name: 103_train_model.pth
step: 0 epoch: 104 loss: 13.668561935424805 loss_input: 72.24090576171875
step: 1000 epoch: 104 loss: 18.276907737676677 loss_input: 83.0897760686579
step: 2000 epoch: 104 loss: 18.020235950859828 loss_input: 82.37998228058822
step: 3000 epoch: 104 loss: 18.006144315232756 loss_input: 82.55637685794188
step: 4000 epoch: 104 loss: 17.894922563535694 loss_input: 82.18063585605064
step: 5000 epoch: 104 loss: 17.917266165726282 loss_input: 82.32879221274885
step: 6000 epoch: 104 loss: 17.946516355819014 loss_input: 82.46376235674587
step: 7000 epoch: 104 loss: 17.959949991734025 loss_input: 82.24583300146575
step: 8000 epoch: 104 loss: 17.97677004332841 loss_input: 82.21509660993304
step: 9000 epoch: 104 loss: 18.000120755341726 loss_input: 82.29739907778259
step: 10000 epoch: 104 loss: 18.005417212976504 loss_input: 82.26436611709923
step: 11000 epoch: 104 loss: 18.028396740857822 loss_input: 82.39663005822268
step: 12000 epoch: 104 loss: 18.007428616248074 loss_input: 82.27392511500507
step: 13000 epoch: 104 loss: 18.020632805709482 loss_input: 82.29440722389594
step: 14000 epoch: 104 loss: 18.01530185970832 loss_input: 82.27773838821084
step: 15000 epoch: 104 loss: 18.021262042181196 loss_input: 82.23606274555655
Save loss: 18.018206801533697 Name: 104_train_model.pth
step: 0 epoch: 105 loss: 16.631256103515625 loss_input: 65.57769775390625
step: 1000 epoch: 105 loss: 18.116359443454954 loss_input: 81.65912639606488
step: 2000 epoch: 105 loss: 18.073206274346195 loss_input: 81.98149802052099
step: 3000 epoch: 105 loss: 18.04949347236402 loss_input: 82.21387115346317
step: 4000 epoch: 105 loss: 18.063136245929666 loss_input: 82.14518408666876
step: 5000 epoch: 105 loss: 18.08013459726039 loss_input: 82.42851698215044
step: 6000 epoch: 105 loss: 18.063408491274174 loss_input: 82.19443866289213
step: 7000 epoch: 105 loss: 18.038266103823787 loss_input: 82.20154957634401
step: 8000 epoch: 105 loss: 17.995834245456486 loss_input: 82.16315709559743
step: 9000 epoch: 105 loss: 18.030921109927096 loss_input: 82.21990325320841
step: 10000 epoch: 105 loss: 18.02520014622035 loss_input: 82.18598633300732
step: 11000 epoch: 105 loss: 18.03232748548981 loss_input: 82.1212372139642
step: 12000 epoch: 105 loss: 18.016630105620255 loss_input: 82.17423727059203
step: 13000 epoch: 105 loss: 18.002160107426512 loss_input: 82.13581594985482
step: 14000 epoch: 105 loss: 18.006909509802128 loss_input: 82.19454853828103
step: 15000 epoch: 105 loss: 17.999488272847163 loss_input: 82.176628044133
Save loss: 18.01262025050819 Name: 105_train_model.pth
step: 0 epoch: 106 loss: 22.39327049255371 loss_input: 76.5191650390625
step: 1000 epoch: 106 loss: 17.750300840421634 loss_input: 81.39563049255432
step: 2000 epoch: 106 loss: 17.803804582503364 loss_input: 81.21044040357751
step: 3000 epoch: 106 loss: 17.91210421511667 loss_input: 81.83232871757592
step: 4000 epoch: 106 loss: 17.95293422497323 loss_input: 81.51277055003827
step: 5000 epoch: 106 loss: 17.925387737107883 loss_input: 81.49769734773749
step: 6000 epoch: 106 loss: 17.943916721833464 loss_input: 81.57464822350572
step: 7000 epoch: 106 loss: 17.911702877350354 loss_input: 81.62075827895121
step: 8000 epoch: 106 loss: 17.929217815697157 loss_input: 81.58990230591293
step: 9000 epoch: 106 loss: 17.95419651555321 loss_input: 81.80541747395905
step: 10000 epoch: 106 loss: 17.961733858247552 loss_input: 81.89951938644026
step: 11000 epoch: 106 loss: 17.97375647842467 loss_input: 81.98025084655833
step: 12000 epoch: 106 loss: 17.96834779296276 loss_input: 82.041253850794
step: 13000 epoch: 106 loss: 17.967097256295673 loss_input: 82.02342154288162
step: 14000 epoch: 106 loss: 17.979432240135967 loss_input: 82.01230792823532
step: 15000 epoch: 106 loss: 17.980268972046048 loss_input: 82.14769074395691
Save loss: 17.993192066311835 Name: 106_train_model.pth
step: 0 epoch: 107 loss: 16.210952758789062 loss_input: 71.787109375
step: 1000 epoch: 107 loss: 17.880115851536615 loss_input: 82.3257118247963
step: 2000 epoch: 107 loss: 17.848373229118778 loss_input: 81.55743740678489
step: 3000 epoch: 107 loss: 17.86495334567407 loss_input: 81.49283509093974
step: 4000 epoch: 107 loss: 17.90409861454753 loss_input: 81.76791953134763
step: 5000 epoch: 107 loss: 17.928940511565045 loss_input: 82.01939792705521
step: 6000 epoch: 107 loss: 17.96093434926729 loss_input: 82.1359076058938
step: 7000 epoch: 107 loss: 17.942078014592276 loss_input: 82.1764926003177
step: 8000 epoch: 107 loss: 17.960592461621637 loss_input: 82.2955407704164
step: 9000 epoch: 107 loss: 17.967747899190993 loss_input: 82.22400229184605
step: 10000 epoch: 107 loss: 17.975416437314397 loss_input: 82.20394994191987
step: 11000 epoch: 107 loss: 17.988777397957815 loss_input: 82.29082375900842
step: 12000 epoch: 107 loss: 17.976850402483333 loss_input: 82.2327353864717
step: 13000 epoch: 107 loss: 17.996450625184078 loss_input: 82.21005995238197
step: 14000 epoch: 107 loss: 17.992216267233943 loss_input: 82.23118810692512
step: 15000 epoch: 107 loss: 17.999068148207183 loss_input: 82.18828490047278
Save loss: 17.99023841831088 Name: 107_train_model.pth
step: 0 epoch: 108 loss: 19.359172821044922 loss_input: 89.1529541015625
step: 1000 epoch: 108 loss: 17.813880167760097 loss_input: 81.95023153116415
step: 2000 epoch: 108 loss: 17.879739345520036 loss_input: 82.13519058341923
step: 3000 epoch: 108 loss: 17.921220040090954 loss_input: 81.93369224825449
step: 4000 epoch: 108 loss: 17.97111245364852 loss_input: 81.95966952170649
step: 5000 epoch: 108 loss: 17.95911233555291 loss_input: 82.17712454823489
step: 6000 epoch: 108 loss: 18.002804657952463 loss_input: 82.37064563336124
step: 7000 epoch: 108 loss: 17.958717216918885 loss_input: 82.27397183877471
step: 8000 epoch: 108 loss: 17.981099446793376 loss_input: 82.15884371570253
step: 9000 epoch: 108 loss: 17.958861482022883 loss_input: 82.05451226041073
step: 10000 epoch: 108 loss: 17.97541400511591 loss_input: 82.18117052430499
step: 11000 epoch: 108 loss: 17.962482843016748 loss_input: 82.10291239725721
step: 12000 epoch: 108 loss: 17.974859702111324 loss_input: 82.13478454432183
step: 13000 epoch: 108 loss: 17.960952921011256 loss_input: 82.20779333737399
step: 14000 epoch: 108 loss: 17.9700860295685 loss_input: 82.31537603707018
step: 15000 epoch: 108 loss: 17.980074048288646 loss_input: 82.2796589927859
Save loss: 17.970878229767084 Name: 108_train_model.pth
step: 0 epoch: 109 loss: 18.66482925415039 loss_input: 68.723876953125
step: 1000 epoch: 109 loss: 18.16341807434966 loss_input: 81.8835153493967
step: 2000 epoch: 109 loss: 17.987036607076025 loss_input: 81.70728960470936
step: 3000 epoch: 109 loss: 17.95496629755007 loss_input: 81.58670834087205
step: 4000 epoch: 109 loss: 17.967143997911272 loss_input: 81.8135053638934
step: 5000 epoch: 109 loss: 17.955721260952583 loss_input: 81.94598034738755
step: 6000 epoch: 109 loss: 17.975489001416342 loss_input: 82.10343314973221
step: 7000 epoch: 109 loss: 17.95733864817206 loss_input: 82.24815435999378
step: 8000 epoch: 109 loss: 17.962095915086717 loss_input: 82.13801302943821
step: 9000 epoch: 109 loss: 17.960684284662197 loss_input: 82.10094920551363
step: 10000 epoch: 109 loss: 17.953119857062603 loss_input: 82.2081155898082
step: 11000 epoch: 109 loss: 17.95777490069439 loss_input: 82.15857950568947
step: 12000 epoch: 109 loss: 17.96618704841133 loss_input: 82.11080538210358
step: 13000 epoch: 109 loss: 17.967944987947487 loss_input: 82.119726024493
step: 14000 epoch: 109 loss: 17.966499886626167 loss_input: 82.12029379318888
step: 15000 epoch: 109 loss: 17.958536077329267 loss_input: 82.16875810574217
Save loss: 17.973123458907008 Name: 109_train_model.pth
step: 0 epoch: 110 loss: 15.959297180175781 loss_input: 48.9522705078125
step: 1000 epoch: 110 loss: 17.881575562975385 loss_input: 82.11220645809269
step: 2000 epoch: 110 loss: 17.947819567036476 loss_input: 82.8242629783741
step: 3000 epoch: 110 loss: 17.87255160914227 loss_input: 82.72712202621912
step: 4000 epoch: 110 loss: 17.924978672281203 loss_input: 82.47008186613014
step: 5000 epoch: 110 loss: 17.959936235599866 loss_input: 82.55804910795185
step: 6000 epoch: 110 loss: 17.98108286167101 loss_input: 82.56230598122968
step: 7000 epoch: 110 loss: 17.992695557493906 loss_input: 82.68886285985373
step: 8000 epoch: 110 loss: 17.98601960247866 loss_input: 82.61752917787132
step: 9000 epoch: 110 loss: 17.99316039481119 loss_input: 82.61361118973977
step: 10000 epoch: 110 loss: 17.970829798476526 loss_input: 82.54657764814338
step: 11000 epoch: 110 loss: 17.96055751721996 loss_input: 82.4394293179047
step: 12000 epoch: 110 loss: 17.99458836690653 loss_input: 82.44327343448919
step: 13000 epoch: 110 loss: 17.993914093276956 loss_input: 82.33855291817997
step: 14000 epoch: 110 loss: 17.97907954706021 loss_input: 82.28103362402825
step: 15000 epoch: 110 loss: 17.981468232995105 loss_input: 82.2500995905286
Save loss: 17.97380551624298 Name: 110_train_model.pth
step: 0 epoch: 111 loss: 15.770733833312988 loss_input: 69.61346435546875
step: 1000 epoch: 111 loss: 17.878239992257956 loss_input: 83.84373384184175
step: 2000 epoch: 111 loss: 17.960242737298724 loss_input: 82.83232864768787
step: 3000 epoch: 111 loss: 18.00218407101172 loss_input: 82.73382838858402
step: 4000 epoch: 111 loss: 17.94377553608977 loss_input: 82.32922528034983
step: 5000 epoch: 111 loss: 17.94476518943724 loss_input: 82.28983832139822
step: 6000 epoch: 111 loss: 17.976661559284338 loss_input: 82.29615777334637
step: 7000 epoch: 111 loss: 17.95200493601012 loss_input: 82.30174539797886
step: 8000 epoch: 111 loss: 17.938273895145908 loss_input: 82.23527565188385
step: 9000 epoch: 111 loss: 17.91064091738483 loss_input: 82.12423813532438
step: 10000 epoch: 111 loss: 17.902652426631843 loss_input: 82.13909871219909
step: 11000 epoch: 111 loss: 17.94441116110822 loss_input: 82.22288215941661
step: 12000 epoch: 111 loss: 17.94497660227333 loss_input: 82.24335184694081
step: 13000 epoch: 111 loss: 17.996489275162904 loss_input: 82.17087980443061
step: 14000 epoch: 111 loss: 18.014772996060568 loss_input: 82.1626723361487
step: 15000 epoch: 111 loss: 18.003113887189397 loss_input: 82.16327201314834
Save loss: 18.001185317531228 Name: 111_train_model.pth
step: 0 epoch: 112 loss: 21.940637588500977 loss_input: 127.9102783203125
step: 1000 epoch: 112 loss: 17.907752937846606 loss_input: 82.92233791599025
step: 2000 epoch: 112 loss: 17.875103742941686 loss_input: 82.90473818266648
step: 3000 epoch: 112 loss: 17.930147516930035 loss_input: 82.80209213302597
step: 4000 epoch: 112 loss: 17.961016329489063 loss_input: 82.63119248264105
step: 5000 epoch: 112 loss: 17.97029810937684 loss_input: 82.55864914663552
step: 6000 epoch: 112 loss: 17.967899159379172 loss_input: 82.53781593245202
step: 7000 epoch: 112 loss: 17.955682803147045 loss_input: 82.50575440952632
step: 8000 epoch: 112 loss: 17.919031351421076 loss_input: 82.32046376435612
step: 9000 epoch: 112 loss: 17.940646405035146 loss_input: 82.35957690220835
step: 10000 epoch: 112 loss: 17.92124985346924 loss_input: 82.37560269241631
step: 11000 epoch: 112 loss: 17.952298961783654 loss_input: 82.37003499171418
step: 12000 epoch: 112 loss: 17.952873194538526 loss_input: 82.27946286661984
step: 13000 epoch: 112 loss: 17.95147502360899 loss_input: 82.28389112139361
step: 14000 epoch: 112 loss: 17.94568486285545 loss_input: 82.3465672992875
step: 15000 epoch: 112 loss: 17.940135387490333 loss_input: 82.26224746637985
Save loss: 17.942753615006804 Name: 112_train_model.pth
step: 0 epoch: 113 loss: 19.65188980102539 loss_input: 82.74627685546875
step: 1000 epoch: 113 loss: 17.917297469986067 loss_input: 82.47032112174935
step: 2000 epoch: 113 loss: 17.73456512910613 loss_input: 81.38205494420758
step: 3000 epoch: 113 loss: 17.70241468916413 loss_input: 81.23075397393578
step: 4000 epoch: 113 loss: 17.730893489629796 loss_input: 81.64858692248325
step: 5000 epoch: 113 loss: 17.8151765682058 loss_input: 81.89803778746204
step: 6000 epoch: 113 loss: 17.856995849882715 loss_input: 82.20600233823335
step: 7000 epoch: 113 loss: 17.855905221915386 loss_input: 82.15623959935132
step: 8000 epoch: 113 loss: 17.87157288701396 loss_input: 82.05917245116089
step: 9000 epoch: 113 loss: 17.881100986549793 loss_input: 82.19268457747316
step: 10000 epoch: 113 loss: 17.885600272112757 loss_input: 82.19133789746026
step: 11000 epoch: 113 loss: 17.90448769100492 loss_input: 82.15903792681667
step: 12000 epoch: 113 loss: 17.904834347837678 loss_input: 82.19020305233353
step: 13000 epoch: 113 loss: 17.92088219463142 loss_input: 82.15735544475095
step: 14000 epoch: 113 loss: 17.927552521854594 loss_input: 82.20845978478518
step: 15000 epoch: 113 loss: 17.944496147617183 loss_input: 82.2111264685219
Save loss: 17.94359351362288 Name: 113_train_model.pth
step: 0 epoch: 114 loss: 12.002168655395508 loss_input: 57.67694091796875
step: 1000 epoch: 114 loss: 18.2158737196908 loss_input: 84.34591202254776
step: 2000 epoch: 114 loss: 17.993088589019624 loss_input: 83.44456522349176
step: 3000 epoch: 114 loss: 17.90098046128331 loss_input: 82.72372461946279
step: 4000 epoch: 114 loss: 17.934118993101762 loss_input: 82.41283801364112
step: 5000 epoch: 114 loss: 17.918363894016547 loss_input: 82.43937453964286
step: 6000 epoch: 114 loss: 17.950059566631296 loss_input: 82.45010637875458
step: 7000 epoch: 114 loss: 17.972333984091662 loss_input: 82.67890191537927
step: 8000 epoch: 114 loss: 18.002840466327093 loss_input: 82.57680380742202
step: 9000 epoch: 114 loss: 17.990362864734728 loss_input: 82.42905283880133
step: 10000 epoch: 114 loss: 17.983658066392838 loss_input: 82.3231856684937
step: 11000 epoch: 114 loss: 17.98041070878381 loss_input: 82.33390152453292
step: 12000 epoch: 114 loss: 17.96282822594485 loss_input: 82.19029078305975
step: 13000 epoch: 114 loss: 17.948469754705318 loss_input: 82.2110450967038
step: 14000 epoch: 114 loss: 17.951851951472293 loss_input: 82.2187380161671
step: 15000 epoch: 114 loss: 17.962028814612943 loss_input: 82.25005952158529
Save loss: 17.958478656351566 Name: 114_train_model.pth
step: 0 epoch: 115 loss: 22.875776290893555 loss_input: 114.9598388671875
step: 1000 epoch: 115 loss: 17.965901092334942 loss_input: 83.14160894037603
step: 2000 epoch: 115 loss: 18.059351272430497 loss_input: 83.13087648680542
step: 3000 epoch: 115 loss: 18.024710638528664 loss_input: 83.02546215470494
step: 4000 epoch: 115 loss: 17.997153422439794 loss_input: 82.78263916381745
step: 5000 epoch: 115 loss: 18.007205022141783 loss_input: 82.86223377883017
step: 6000 epoch: 115 loss: 17.96554687336631 loss_input: 82.57437609275408
step: 7000 epoch: 115 loss: 17.921258116769646 loss_input: 82.50785033639167
step: 8000 epoch: 115 loss: 17.92194967159047 loss_input: 82.56962198115129
step: 9000 epoch: 115 loss: 17.919334968981804 loss_input: 82.44813458769337
step: 10000 epoch: 115 loss: 17.911932801690153 loss_input: 82.43264889209321
step: 11000 epoch: 115 loss: 17.924631681391112 loss_input: 82.51065396642741
step: 12000 epoch: 115 loss: 17.93289531654998 loss_input: 82.44052128854985
step: 13000 epoch: 115 loss: 17.929009978326135 loss_input: 82.4982067654201
step: 14000 epoch: 115 loss: 17.931865287809643 loss_input: 82.40111492541966
step: 15000 epoch: 115 loss: 17.919074414007014 loss_input: 82.32115739810054
Save loss: 17.921961962908508 Name: 115_train_model.pth
step: 0 epoch: 116 loss: 16.211776733398438 loss_input: 67.443603515625
step: 1000 epoch: 116 loss: 17.793892469320383 loss_input: 81.64080341045673
step: 2000 epoch: 116 loss: 17.79950269420763 loss_input: 82.27358403317872
step: 3000 epoch: 116 loss: 17.84975396883722 loss_input: 82.82144369438703
step: 4000 epoch: 116 loss: 17.8037558524378 loss_input: 82.42801836162053
step: 5000 epoch: 116 loss: 17.906457633358123 loss_input: 82.47856600554888
step: 6000 epoch: 116 loss: 17.882973695671414 loss_input: 82.42184651150264
step: 7000 epoch: 116 loss: 17.92331870190332 loss_input: 82.45119361784812
step: 8000 epoch: 116 loss: 17.934381101537 loss_input: 82.4856393225505
step: 9000 epoch: 116 loss: 17.913045298588326 loss_input: 82.39070600499684
step: 10000 epoch: 116 loss: 17.898997434174202 loss_input: 82.34882584739573
step: 11000 epoch: 116 loss: 17.889995358551797 loss_input: 82.34289708903417
step: 12000 epoch: 116 loss: 17.879930974066173 loss_input: 82.3755409044282
step: 13000 epoch: 116 loss: 17.893211503293312 loss_input: 82.32968207486142
step: 14000 epoch: 116 loss: 17.910378097644934 loss_input: 82.34256948129543
step: 15000 epoch: 116 loss: 17.922810855257328 loss_input: 82.30215706159954
Save loss: 17.917812251165508 Name: 116_train_model.pth
step: 0 epoch: 117 loss: 18.231361389160156 loss_input: 119.7552490234375
step: 1000 epoch: 117 loss: 17.852097687544997 loss_input: 82.5592765388908
step: 2000 epoch: 117 loss: 17.78654839455158 loss_input: 82.32793673963799
step: 3000 epoch: 117 loss: 17.780663453273398 loss_input: 82.27032820521414
step: 4000 epoch: 117 loss: 17.805822608173564 loss_input: 82.10431587049527
step: 5000 epoch: 117 loss: 17.8173546655682 loss_input: 81.8702911517306
step: 6000 epoch: 117 loss: 17.890702928350162 loss_input: 81.94773530380027
step: 7000 epoch: 117 loss: 17.862950015180434 loss_input: 81.96844730886659
step: 8000 epoch: 117 loss: 17.868909720822643 loss_input: 82.06930547713519
step: 9000 epoch: 117 loss: 17.889189100201932 loss_input: 82.11700133633155
step: 10000 epoch: 117 loss: 17.87974981586524 loss_input: 82.04568853417392
step: 11000 epoch: 117 loss: 17.881091392600485 loss_input: 82.05387629999723
step: 12000 epoch: 117 loss: 17.8780035707774 loss_input: 82.15696807472261
step: 13000 epoch: 117 loss: 17.899835191848744 loss_input: 82.18821046029812
step: 14000 epoch: 117 loss: 17.911936506340567 loss_input: 82.23882856982732
step: 15000 epoch: 117 loss: 17.91004219654997 loss_input: 82.23823918330575
Save loss: 17.90923250040412 Name: 117_train_model.pth
step: 0 epoch: 118 loss: 15.211396217346191 loss_input: 46.3079833984375
step: 1000 epoch: 118 loss: 17.63830497048118 loss_input: 80.63790774655033
step: 2000 epoch: 118 loss: 17.828426777393087 loss_input: 81.1528923343504
step: 3000 epoch: 118 loss: 17.7658895810657 loss_input: 81.08675601378317
step: 4000 epoch: 118 loss: 17.849906022892508 loss_input: 81.84704518145367
step: 5000 epoch: 118 loss: 17.861776313026578 loss_input: 81.95483485090091
step: 6000 epoch: 118 loss: 17.876969460784704 loss_input: 81.91327137208903
step: 7000 epoch: 118 loss: 17.868042946202365 loss_input: 81.87299994566904
step: 8000 epoch: 118 loss: 17.91044710248936 loss_input: 82.15631050879382
step: 9000 epoch: 118 loss: 17.883741210717755 loss_input: 82.06014111089436
step: 10000 epoch: 118 loss: 17.888904430904145 loss_input: 81.96075388799011
step: 11000 epoch: 118 loss: 17.87801356365893 loss_input: 81.91496754134745
step: 12000 epoch: 118 loss: 17.896245648548827 loss_input: 81.95418044895185
step: 13000 epoch: 118 loss: 17.908981029881524 loss_input: 82.1195055261299
step: 14000 epoch: 118 loss: 17.90124799060733 loss_input: 82.1337775189607
step: 15000 epoch: 118 loss: 17.896277717189562 loss_input: 82.1913982915462
Save loss: 17.89953792336583 Name: 118_train_model.pth
step: 0 epoch: 119 loss: 20.567153930664062 loss_input: 133.1715087890625
step: 1000 epoch: 119 loss: 17.755941555812047 loss_input: 80.90274319186673
step: 2000 epoch: 119 loss: 17.79693479456942 loss_input: 81.02363515507871
step: 3000 epoch: 119 loss: 17.80044764131357 loss_input: 81.69857547077406
step: 4000 epoch: 119 loss: 17.8556885883767 loss_input: 81.86496158553135
step: 5000 epoch: 119 loss: 17.8787358544679 loss_input: 81.841636299205
step: 6000 epoch: 119 loss: 17.907526983417803 loss_input: 81.95082247644598
step: 7000 epoch: 119 loss: 17.911451040310716 loss_input: 82.08060772986671
step: 8000 epoch: 119 loss: 17.949169653413474 loss_input: 82.1652477003607
step: 9000 epoch: 119 loss: 17.92523536596308 loss_input: 82.11955781497369
step: 10000 epoch: 119 loss: 17.93383055622012 loss_input: 82.10624289743878
step: 11000 epoch: 119 loss: 17.930617793453095 loss_input: 82.30561980493783
step: 12000 epoch: 119 loss: 17.922782015456786 loss_input: 82.34361420300749
step: 13000 epoch: 119 loss: 17.921177923216305 loss_input: 82.32776102841098
step: 14000 epoch: 119 loss: 17.887690310392728 loss_input: 82.23060112143915
step: 15000 epoch: 119 loss: 17.89582277947955 loss_input: 82.25077732854919
Save loss: 17.896376445025204 Name: 119_train_model.pth
step: 0 epoch: 120 loss: 21.764028549194336 loss_input: 79.75909423828125
step: 1000 epoch: 120 loss: 17.95151266208538 loss_input: 82.59325434087397
step: 2000 epoch: 120 loss: 17.805514127120322 loss_input: 82.5707972734824
step: 3000 epoch: 120 loss: 17.758254984226756 loss_input: 82.13665694198939
step: 4000 epoch: 120 loss: 17.735838104444454 loss_input: 82.17364805527134
step: 5000 epoch: 120 loss: 17.742010209255756 loss_input: 81.97663414094525
step: 6000 epoch: 120 loss: 17.764208786488773 loss_input: 81.93505486105506
step: 7000 epoch: 120 loss: 17.799557569963117 loss_input: 81.91690269658743
step: 8000 epoch: 120 loss: 17.808579254978792 loss_input: 82.09680703669366
step: 9000 epoch: 120 loss: 17.828931262686233 loss_input: 82.08378686683467
step: 10000 epoch: 120 loss: 17.841239758890016 loss_input: 82.16634092206014
step: 11000 epoch: 120 loss: 17.883169957263156 loss_input: 82.24279726345901
step: 12000 epoch: 120 loss: 17.87608883534061 loss_input: 82.27465552288774
step: 13000 epoch: 120 loss: 17.884315557254663 loss_input: 82.23401014112196
step: 14000 epoch: 120 loss: 17.8979567733205 loss_input: 82.14446174900036
step: 15000 epoch: 120 loss: 17.889509721753058 loss_input: 82.11319477708648
Save loss: 17.891832503795623 Name: 120_train_model.pth
step: 0 epoch: 121 loss: 21.962560653686523 loss_input: 100.28253173828125
step: 1000 epoch: 121 loss: 17.771671821544697 loss_input: 82.25162032791427
step: 2000 epoch: 121 loss: 17.902514333786932 loss_input: 82.57261435810415
step: 3000 epoch: 121 loss: 17.815753343303136 loss_input: 82.09636320363916
step: 4000 epoch: 121 loss: 17.857959397045917 loss_input: 82.1781622770577
step: 5000 epoch: 121 loss: 17.861072752528656 loss_input: 82.32514439494913
step: 6000 epoch: 121 loss: 17.858774627135052 loss_input: 82.2802996375604
step: 7000 epoch: 121 loss: 17.868453650930203 loss_input: 82.29712181707022
step: 8000 epoch: 121 loss: 17.864120578277173 loss_input: 82.28256081789826
step: 9000 epoch: 121 loss: 17.893532439717028 loss_input: 82.35131496212877
step: 10000 epoch: 121 loss: 17.873708970975116 loss_input: 82.33760654085958
step: 11000 epoch: 121 loss: 17.873839707843132 loss_input: 82.34260861317296
step: 12000 epoch: 121 loss: 17.86940286265007 loss_input: 82.26600518173778
step: 13000 epoch: 121 loss: 17.878825850655836 loss_input: 82.2763242643069
step: 14000 epoch: 121 loss: 17.8706986136321 loss_input: 82.25321802749998
step: 15000 epoch: 121 loss: 17.888835705961277 loss_input: 82.2228452510718
Save loss: 17.888025984197856 Name: 121_train_model.pth
step: 0 epoch: 122 loss: 9.045456886291504 loss_input: 45.31292724609375
step: 1000 epoch: 122 loss: 17.823051987589896 loss_input: 82.26613937819992
step: 2000 epoch: 122 loss: 17.906534661059972 loss_input: 82.5061621105951
step: 3000 epoch: 122 loss: 17.797445990649194 loss_input: 81.93822514077338
step: 4000 epoch: 122 loss: 17.85184510020547 loss_input: 82.19201989663716
step: 5000 epoch: 122 loss: 17.846682714524448 loss_input: 82.06067983180708
step: 6000 epoch: 122 loss: 17.820319800034422 loss_input: 81.86693155104032
step: 7000 epoch: 122 loss: 17.835304598283162 loss_input: 82.1390962179789
step: 8000 epoch: 122 loss: 17.818640364153566 loss_input: 82.17312971834332
step: 9000 epoch: 122 loss: 17.824531873243807 loss_input: 82.27782650479156
step: 10000 epoch: 122 loss: 17.85789599636533 loss_input: 82.35811084418901
step: 11000 epoch: 122 loss: 17.882910752424316 loss_input: 82.40364293165287
step: 12000 epoch: 122 loss: 17.86567170710755 loss_input: 82.29575669007563
step: 13000 epoch: 122 loss: 17.874631093415964 loss_input: 82.22864644614177
step: 14000 epoch: 122 loss: 17.88074331993325 loss_input: 82.24714967869204
step: 15000 epoch: 122 loss: 17.881701089725375 loss_input: 82.24202698823984
Save loss: 17.873028486162426 Name: 122_train_model.pth
step: 0 epoch: 123 loss: 21.298921585083008 loss_input: 59.5487060546875
step: 1000 epoch: 123 loss: 17.477536267691203 loss_input: 82.11344764854286
step: 2000 epoch: 123 loss: 17.567683020929646 loss_input: 81.7123930710426
step: 3000 epoch: 123 loss: 17.6587468203367 loss_input: 82.32328809098139
step: 4000 epoch: 123 loss: 17.73045949803624 loss_input: 82.17907444830686
step: 5000 epoch: 123 loss: 17.74835500944092 loss_input: 82.12783244132423
step: 6000 epoch: 123 loss: 17.775432122069546 loss_input: 81.97376072086944
step: 7000 epoch: 123 loss: 17.773900207562576 loss_input: 82.16568410323357
step: 8000 epoch: 123 loss: 17.784276800175903 loss_input: 82.28903664721354
step: 9000 epoch: 123 loss: 17.79481231285564 loss_input: 82.15724125677659
step: 10000 epoch: 123 loss: 17.817095215279345 loss_input: 82.33757649015372
step: 11000 epoch: 123 loss: 17.846679308665642 loss_input: 82.35176829183852
step: 12000 epoch: 123 loss: 17.84226443062086 loss_input: 82.2969637784091
step: 13000 epoch: 123 loss: 17.8461564235454 loss_input: 82.32374947269682
step: 14000 epoch: 123 loss: 17.869883601933356 loss_input: 82.30905084015343
step: 15000 epoch: 123 loss: 17.867659619582287 loss_input: 82.26139467492531
Save loss: 17.872353906780482 Name: 123_train_model.pth
step: 0 epoch: 124 loss: 15.239667892456055 loss_input: 80.72784423828125
step: 1000 epoch: 124 loss: 17.56204322525314 loss_input: 82.33252264093329
step: 2000 epoch: 124 loss: 17.673179008077824 loss_input: 82.40897632848615
step: 3000 epoch: 124 loss: 17.801887982847372 loss_input: 82.53986773289112
step: 4000 epoch: 124 loss: 17.8469718321953 loss_input: 82.3106156596837
step: 5000 epoch: 124 loss: 17.82043284727225 loss_input: 82.10066713463948
step: 6000 epoch: 124 loss: 17.845409376782946 loss_input: 82.33671701282208
step: 7000 epoch: 124 loss: 17.837690990425113 loss_input: 82.15229974993807
step: 8000 epoch: 124 loss: 17.850386606098905 loss_input: 82.06736993569163
step: 9000 epoch: 124 loss: 17.85892552961814 loss_input: 82.31824543502327
step: 10000 epoch: 124 loss: 17.840584256460446 loss_input: 82.23260381481383
step: 11000 epoch: 124 loss: 17.833359375247934 loss_input: 82.15009333868264
step: 12000 epoch: 124 loss: 17.842530736922424 loss_input: 82.25238575699747
step: 13000 epoch: 124 loss: 17.83809896828844 loss_input: 82.20662615796381
step: 14000 epoch: 124 loss: 17.838095211946968 loss_input: 82.21895662848162
step: 15000 epoch: 124 loss: 17.859076937233063 loss_input: 82.28704769698297
Save loss: 17.860315940767528 Name: 124_train_model.pth
step: 0 epoch: 125 loss: 15.903995513916016 loss_input: 46.3079833984375
step: 1000 epoch: 125 loss: 17.689685784377062 loss_input: 81.43789962478927
step: 2000 epoch: 125 loss: 17.784346499960165 loss_input: 82.24436661674046
step: 3000 epoch: 125 loss: 17.811273705915625 loss_input: 82.11145143594713
step: 4000 epoch: 125 loss: 17.823814467411285 loss_input: 82.40233561909815
step: 5000 epoch: 125 loss: 17.863159647847958 loss_input: 82.44768768414286
step: 6000 epoch: 125 loss: 17.868871527380833 loss_input: 82.44182613241217
step: 7000 epoch: 125 loss: 17.863662133028193 loss_input: 82.29152938042891
step: 8000 epoch: 125 loss: 17.840725162538526 loss_input: 82.25068958817891
step: 9000 epoch: 125 loss: 17.829693589710075 loss_input: 82.22146309779599
step: 10000 epoch: 125 loss: 17.857836086098498 loss_input: 82.16361521196514
step: 11000 epoch: 125 loss: 17.843442352909726 loss_input: 82.12717831315761
step: 12000 epoch: 125 loss: 17.863369590510388 loss_input: 82.14201712642111
step: 13000 epoch: 125 loss: 17.881867175010175 loss_input: 82.20107097286838
step: 14000 epoch: 125 loss: 17.88138847628301 loss_input: 82.24716046806506
step: 15000 epoch: 125 loss: 17.868523246391195 loss_input: 82.2500482837277
Save loss: 17.870551785752177 Name: 125_train_model.pth
step: 0 epoch: 126 loss: 20.736400604248047 loss_input: 103.43408203125
step: 1000 epoch: 126 loss: 18.05294963291713 loss_input: 83.15876719715831
step: 2000 epoch: 126 loss: 17.834394430172914 loss_input: 83.208210439458
step: 3000 epoch: 126 loss: 17.832562339500523 loss_input: 82.75488230404318
step: 4000 epoch: 126 loss: 17.78124981145804 loss_input: 82.3965102779481
step: 5000 epoch: 126 loss: 17.826438529089533 loss_input: 82.4023403449193
step: 6000 epoch: 126 loss: 17.810259514303134 loss_input: 82.25781337469145
step: 7000 epoch: 126 loss: 17.81084534754465 loss_input: 82.30875016947914
step: 8000 epoch: 126 loss: 17.835156453846245 loss_input: 82.32876306568856
step: 9000 epoch: 126 loss: 17.858580082896655 loss_input: 82.32124140611981
step: 10000 epoch: 126 loss: 17.857553213861582 loss_input: 82.29256196694783
step: 11000 epoch: 126 loss: 17.861266256169767 loss_input: 82.2761917273767
step: 12000 epoch: 126 loss: 17.877145860247012 loss_input: 82.28329119647744
step: 13000 epoch: 126 loss: 17.870034130781708 loss_input: 82.24269403874732
step: 14000 epoch: 126 loss: 17.85858837060184 loss_input: 82.27918601691675
step: 15000 epoch: 126 loss: 17.842695831068436 loss_input: 82.22110184109471
Save loss: 17.849843405365945 Name: 126_train_model.pth
step: 0 epoch: 127 loss: 19.9528751373291 loss_input: 85.0706787109375
step: 1000 epoch: 127 loss: 17.825641034247276 loss_input: 82.14556464043768
step: 2000 epoch: 127 loss: 17.752205565355826 loss_input: 81.69331227940717
step: 3000 epoch: 127 loss: 17.733842860059475 loss_input: 82.48363133106022
step: 4000 epoch: 127 loss: 17.777130328008933 loss_input: 82.67526387751207
step: 5000 epoch: 127 loss: 17.77791891739717 loss_input: 82.58575388193846
step: 6000 epoch: 127 loss: 17.80510149203108 loss_input: 82.71453488220713
step: 7000 epoch: 127 loss: 17.82878702614175 loss_input: 82.71983619213717
step: 8000 epoch: 127 loss: 17.826868535965446 loss_input: 82.6257259605855
step: 9000 epoch: 127 loss: 17.821773692934052 loss_input: 82.49605519362866
step: 10000 epoch: 127 loss: 17.80949768613856 loss_input: 82.31846184278056
step: 11000 epoch: 127 loss: 17.838102354633627 loss_input: 82.35425128699238
step: 12000 epoch: 127 loss: 17.83691822523952 loss_input: 82.28062781877551
step: 13000 epoch: 127 loss: 17.83829693861736 loss_input: 82.24424878506191
step: 14000 epoch: 127 loss: 17.842934432978904 loss_input: 82.28195762157475
step: 15000 epoch: 127 loss: 17.85638011189256 loss_input: 82.21587093575272
Save loss: 17.84863344050944 Name: 127_train_model.pth
step: 0 epoch: 128 loss: 10.653767585754395 loss_input: 47.11163330078125
step: 1000 epoch: 128 loss: 17.571223064616962 loss_input: 81.39516836922843
step: 2000 epoch: 128 loss: 17.70187142537511 loss_input: 81.74629756654876
step: 3000 epoch: 128 loss: 17.784545211385225 loss_input: 81.70793142194789
step: 4000 epoch: 128 loss: 17.801049351066506 loss_input: 82.00327576187009
step: 5000 epoch: 128 loss: 17.804849403711636 loss_input: 82.00062784074045
step: 6000 epoch: 128 loss: 17.822923180501157 loss_input: 82.1060351330605
step: 7000 epoch: 128 loss: 17.807134982705577 loss_input: 82.20184848383825
step: 8000 epoch: 128 loss: 17.8043949719057 loss_input: 82.15516535774825
step: 9000 epoch: 128 loss: 17.78076626178808 loss_input: 82.2007491178823
step: 10000 epoch: 128 loss: 17.803150824553583 loss_input: 82.28528750518502
step: 11000 epoch: 128 loss: 17.79929733408569 loss_input: 82.23190940938075
step: 12000 epoch: 128 loss: 17.794375485871676 loss_input: 82.2237685586341
step: 13000 epoch: 128 loss: 17.79977945619047 loss_input: 82.19848906980113
step: 14000 epoch: 128 loss: 17.82260052552096 loss_input: 82.18170274563256
step: 15000 epoch: 128 loss: 17.83272645109678 loss_input: 82.23291033120579
Save loss: 17.840955023184417 Name: 128_train_model.pth
step: 0 epoch: 129 loss: 10.00999641418457 loss_input: 66.175048828125
step: 1000 epoch: 129 loss: 17.687215125286855 loss_input: 83.15175662960087
step: 2000 epoch: 129 loss: 17.69722343337113 loss_input: 82.42747074338807
step: 3000 epoch: 129 loss: 17.693282960455086 loss_input: 82.19728736661347
step: 4000 epoch: 129 loss: 17.762837879063635 loss_input: 82.66189344082436
step: 5000 epoch: 129 loss: 17.780497990424955 loss_input: 82.65137737833292
step: 6000 epoch: 129 loss: 17.767013270265757 loss_input: 82.46410094370665
step: 7000 epoch: 129 loss: 17.781465510916085 loss_input: 82.31544850979715
step: 8000 epoch: 129 loss: 17.796164821824526 loss_input: 82.28128221490624
step: 9000 epoch: 129 loss: 17.794686163336603 loss_input: 82.1166636866737
step: 10000 epoch: 129 loss: 17.79850892674481 loss_input: 82.21455815844197
step: 11000 epoch: 129 loss: 17.78401028673255 loss_input: 82.24403797874731
step: 12000 epoch: 129 loss: 17.81825832612971 loss_input: 82.31921596255324
step: 13000 epoch: 129 loss: 17.80906791355452 loss_input: 82.15334729265868
step: 14000 epoch: 129 loss: 17.822549131340438 loss_input: 82.18018739889813
step: 15000 epoch: 129 loss: 17.832096125601005 loss_input: 82.21712318788123
Save loss: 17.835190583616495 Name: 129_train_model.pth
step: 0 epoch: 130 loss: 24.614116668701172 loss_input: 81.63897705078125
step: 1000 epoch: 130 loss: 17.867301040119703 loss_input: 82.45836646263892
step: 2000 epoch: 130 loss: 17.875322942195208 loss_input: 82.74486798348872
step: 3000 epoch: 130 loss: 17.86192457312864 loss_input: 82.7288673347491
step: 4000 epoch: 130 loss: 17.85358469809809 loss_input: 82.61661217386083
step: 5000 epoch: 130 loss: 17.85580289232757 loss_input: 82.55236719989986
step: 6000 epoch: 130 loss: 17.821446327304187 loss_input: 82.32256520682425
step: 7000 epoch: 130 loss: 17.81607495919413 loss_input: 82.29861006881829
step: 8000 epoch: 130 loss: 17.834227241258176 loss_input: 82.29864141890711
step: 9000 epoch: 130 loss: 17.79242698646442 loss_input: 82.25002251947049
step: 10000 epoch: 130 loss: 17.798084076828104 loss_input: 82.06717858487589
step: 11000 epoch: 130 loss: 17.804251243521783 loss_input: 82.09806077088695
step: 12000 epoch: 130 loss: 17.83493563495331 loss_input: 82.24052533586068
step: 13000 epoch: 130 loss: 17.823929216520447 loss_input: 82.19986013789955
step: 14000 epoch: 130 loss: 17.829470198406984 loss_input: 82.18424328333411
step: 15000 epoch: 130 loss: 17.828208647857593 loss_input: 82.2737008694863
Save loss: 17.82458382202685 Name: 130_train_model.pth
step: 0 epoch: 131 loss: 9.548786163330078 loss_input: 50.9671630859375
step: 1000 epoch: 131 loss: 17.727038470657913 loss_input: 82.20326981368241
step: 2000 epoch: 131 loss: 17.689869881867768 loss_input: 81.95421539718421
step: 3000 epoch: 131 loss: 17.716975155293007 loss_input: 82.36728363901486
step: 4000 epoch: 131 loss: 17.752019220755955 loss_input: 82.00547144097764
step: 5000 epoch: 131 loss: 17.751129202355482 loss_input: 82.02478568007102
step: 6000 epoch: 131 loss: 17.743782601343 loss_input: 82.03941898762315
step: 7000 epoch: 131 loss: 17.726928633564693 loss_input: 81.97706531426036
step: 8000 epoch: 131 loss: 17.713007361721118 loss_input: 82.04215037848053
step: 9000 epoch: 131 loss: 17.73839470095508 loss_input: 82.05370318689633
step: 10000 epoch: 131 loss: 17.744498118223305 loss_input: 82.04851011480883
step: 11000 epoch: 131 loss: 17.75187515057322 loss_input: 82.03675833824234
step: 12000 epoch: 131 loss: 17.747211738324506 loss_input: 82.04687416592238
step: 13000 epoch: 131 loss: 17.773258131400006 loss_input: 82.11906187223128
step: 14000 epoch: 131 loss: 17.792529730547855 loss_input: 82.13881137832371
step: 15000 epoch: 131 loss: 17.804019911147286 loss_input: 82.18566596292669
Save loss: 17.81673141695559 Name: 131_train_model.pth
step: 0 epoch: 132 loss: 17.41046142578125 loss_input: 79.81939697265625
step: 1000 epoch: 132 loss: 17.67606507529031 loss_input: 81.68504048441793
step: 2000 epoch: 132 loss: 17.868254625815144 loss_input: 82.91987609100723
step: 3000 epoch: 132 loss: 17.863156734804676 loss_input: 82.19886651145582
step: 4000 epoch: 132 loss: 17.740287112283934 loss_input: 82.07711516639883
step: 5000 epoch: 132 loss: 17.762385255073315 loss_input: 81.9564476631041
step: 6000 epoch: 132 loss: 17.784016941015253 loss_input: 81.88923650867758
step: 7000 epoch: 132 loss: 17.744477764876667 loss_input: 82.01949995215527
step: 8000 epoch: 132 loss: 17.78068985937238 loss_input: 82.07820046053814
step: 9000 epoch: 132 loss: 17.767023495416776 loss_input: 82.154401701406
step: 10000 epoch: 132 loss: 17.753068335210546 loss_input: 82.03615549699913
step: 11000 epoch: 132 loss: 17.75194524275304 loss_input: 82.07210422387915
step: 12000 epoch: 132 loss: 17.766668218680692 loss_input: 82.24192752715757
step: 13000 epoch: 132 loss: 17.76038833491922 loss_input: 82.13053513424147
step: 14000 epoch: 132 loss: 17.77279419775018 loss_input: 82.13068897265569
step: 15000 epoch: 132 loss: 17.801211984672037 loss_input: 82.21681778831932
Save loss: 17.823606274202465 Name: 132_train_model.pth
step: 0 epoch: 133 loss: 15.25185775756836 loss_input: 66.20599365234375
step: 1000 epoch: 133 loss: 17.45759261547626 loss_input: 81.88692459883866
step: 2000 epoch: 133 loss: 17.589404682229006 loss_input: 81.96296151240786
step: 3000 epoch: 133 loss: 17.741948004366357 loss_input: 82.84823088755572
step: 4000 epoch: 133 loss: 17.7389251508882 loss_input: 82.58235369441331
step: 5000 epoch: 133 loss: 17.706446212664815 loss_input: 82.69544566428512
step: 6000 epoch: 133 loss: 17.689741354269934 loss_input: 82.43389442010316
step: 7000 epoch: 133 loss: 17.715542774679932 loss_input: 82.29741679271822
step: 8000 epoch: 133 loss: 17.74930814724686 loss_input: 82.34698057609742
step: 9000 epoch: 133 loss: 17.765105897513326 loss_input: 82.30479370971585
step: 10000 epoch: 133 loss: 17.775474047663213 loss_input: 82.32685211703439
step: 11000 epoch: 133 loss: 17.758596377419988 loss_input: 82.3076667102529
step: 12000 epoch: 133 loss: 17.776693844395908 loss_input: 82.29138546722669
step: 13000 epoch: 133 loss: 17.79700775643824 loss_input: 82.35264233543033
step: 14000 epoch: 133 loss: 17.796293776917906 loss_input: 82.3675388237824
step: 15000 epoch: 133 loss: 17.810457699982756 loss_input: 82.32650525528116
Save loss: 17.804348026230933 Name: 133_train_model.pth
step: 0 epoch: 134 loss: 19.46387481689453 loss_input: 101.2064208984375
step: 1000 epoch: 134 loss: 17.5087673542621 loss_input: 81.50531207074175
step: 2000 epoch: 134 loss: 17.61389180125742 loss_input: 81.40570544195675
step: 3000 epoch: 134 loss: 17.65264881217293 loss_input: 81.49206420939115
step: 4000 epoch: 134 loss: 17.7164024752517 loss_input: 81.4375763358965
step: 5000 epoch: 134 loss: 17.726324805782404 loss_input: 81.46676020020605
step: 6000 epoch: 134 loss: 17.752894399166504 loss_input: 81.74114207080396
step: 7000 epoch: 134 loss: 17.781331232659664 loss_input: 81.92776485334684
step: 8000 epoch: 134 loss: 17.80871898367202 loss_input: 82.09063224091618
step: 9000 epoch: 134 loss: 17.8113999820235 loss_input: 82.03255820425335
step: 10000 epoch: 134 loss: 17.80203255476111 loss_input: 82.05311543430618
step: 11000 epoch: 134 loss: 17.802853812630442 loss_input: 82.17658758982671
step: 12000 epoch: 134 loss: 17.8145464441099 loss_input: 82.22668519078488
step: 13000 epoch: 134 loss: 17.799901349701685 loss_input: 82.22042301395986
step: 14000 epoch: 134 loss: 17.79721540533196 loss_input: 82.26391058388407
step: 15000 epoch: 134 loss: 17.7902051318082 loss_input: 82.14992956768377
Save loss: 17.80539067450166 Name: 134_train_model.pth
step: 0 epoch: 135 loss: 26.085224151611328 loss_input: 133.88818359375
step: 1000 epoch: 135 loss: 17.707085733289844 loss_input: 83.4694304108977
step: 2000 epoch: 135 loss: 17.677535240320132 loss_input: 82.60969204118643
step: 3000 epoch: 135 loss: 17.6779554915881 loss_input: 82.27003405277668
step: 4000 epoch: 135 loss: 17.674751259272945 loss_input: 82.05148853167567
step: 5000 epoch: 135 loss: 17.717123895138652 loss_input: 82.07112732726893
step: 6000 epoch: 135 loss: 17.76296948929704 loss_input: 81.92577364221849
step: 7000 epoch: 135 loss: 17.721070333849173 loss_input: 81.8447733697781
step: 8000 epoch: 135 loss: 17.74575541925615 loss_input: 81.95527097961632
step: 9000 epoch: 135 loss: 17.783757081284495 loss_input: 82.1350238428369
step: 10000 epoch: 135 loss: 17.79500658449418 loss_input: 82.11231106503608
step: 11000 epoch: 135 loss: 17.80917739337189 loss_input: 82.1665688861382
step: 12000 epoch: 135 loss: 17.815243890450187 loss_input: 82.25176042255268
step: 13000 epoch: 135 loss: 17.817719030248213 loss_input: 82.24844806108665
step: 14000 epoch: 135 loss: 17.81084386017244 loss_input: 82.16758342334504
step: 15000 epoch: 135 loss: 17.803819844853233 loss_input: 82.17644219633405
Save loss: 17.80241873051226 Name: 135_train_model.pth
step: 0 epoch: 136 loss: 18.990079879760742 loss_input: 107.08441162109375
step: 1000 epoch: 136 loss: 17.718248411134763 loss_input: 82.64494306962568
step: 2000 epoch: 136 loss: 17.666013602076145 loss_input: 82.12938397744487
step: 3000 epoch: 136 loss: 17.740152385861666 loss_input: 82.12334590862291
step: 4000 epoch: 136 loss: 17.745188985994773 loss_input: 81.98950488208025
step: 5000 epoch: 136 loss: 17.78591349858614 loss_input: 82.25330110979758
step: 6000 epoch: 136 loss: 17.79470240539401 loss_input: 82.23126013218175
step: 7000 epoch: 136 loss: 17.774072717452217 loss_input: 82.1597775460788
step: 8000 epoch: 136 loss: 17.770579921053493 loss_input: 82.05716918441836
step: 9000 epoch: 136 loss: 17.745340507383574 loss_input: 81.9582633247456
step: 10000 epoch: 136 loss: 17.759299677713503 loss_input: 81.99675892396112
step: 11000 epoch: 136 loss: 17.747928544224724 loss_input: 82.09958516289002
step: 12000 epoch: 136 loss: 17.753382786821597 loss_input: 82.010611593321
step: 13000 epoch: 136 loss: 17.783950998456504 loss_input: 82.10378640307417
step: 14000 epoch: 136 loss: 17.78818135736295 loss_input: 82.10116934396567
step: 15000 epoch: 136 loss: 17.788947241646902 loss_input: 82.20376889831694
Save loss: 17.802437524825333 Name: 136_train_model.pth
step: 0 epoch: 137 loss: 17.723569869995117 loss_input: 71.5753173828125
step: 1000 epoch: 137 loss: 18.041653278705244 loss_input: 83.17385897793612
step: 2000 epoch: 137 loss: 17.873197025802362 loss_input: 82.85253793391391
step: 3000 epoch: 137 loss: 17.813485860586244 loss_input: 82.27614387326621
step: 4000 epoch: 137 loss: 17.86961318665342 loss_input: 82.3998079001978
step: 5000 epoch: 137 loss: 17.846083436386987 loss_input: 82.25480830347603
step: 6000 epoch: 137 loss: 17.811277480706277 loss_input: 81.99253525087941
step: 7000 epoch: 137 loss: 17.772268422891916 loss_input: 82.0408900587444
step: 8000 epoch: 137 loss: 17.774600908497305 loss_input: 82.07013773241724
step: 9000 epoch: 137 loss: 17.76444599358536 loss_input: 82.0266256697932
step: 10000 epoch: 137 loss: 17.75335882075035 loss_input: 82.03765423628536
step: 11000 epoch: 137 loss: 17.758433673980182 loss_input: 82.19891396813713
step: 12000 epoch: 137 loss: 17.76065197772517 loss_input: 82.27965846290967
step: 13000 epoch: 137 loss: 17.770166148901737 loss_input: 82.26433043123787
step: 14000 epoch: 137 loss: 17.7823277907613 loss_input: 82.27892375886377
step: 15000 epoch: 137 loss: 17.777205114833166 loss_input: 82.22287579916548
Save loss: 17.790762475579978 Name: 137_train_model.pth
step: 0 epoch: 138 loss: 15.546305656433105 loss_input: 60.140869140625
step: 1000 epoch: 138 loss: 17.79637072064898 loss_input: 82.80453524746737
step: 2000 epoch: 138 loss: 17.763546908872833 loss_input: 82.98907122976598
step: 3000 epoch: 138 loss: 17.72105882422839 loss_input: 82.54727893843328
step: 4000 epoch: 138 loss: 17.702376792205033 loss_input: 82.20265035669793
step: 5000 epoch: 138 loss: 17.673018602055805 loss_input: 81.97921301970075
step: 6000 epoch: 138 loss: 17.68023673813853 loss_input: 81.908997619456
step: 7000 epoch: 138 loss: 17.718941329871054 loss_input: 81.9798737335096
step: 8000 epoch: 138 loss: 17.73518233835034 loss_input: 82.08685340113736
step: 9000 epoch: 138 loss: 17.765032432381542 loss_input: 82.21354832780082
step: 10000 epoch: 138 loss: 17.763880083387154 loss_input: 82.21152826733928
step: 11000 epoch: 138 loss: 17.76516396929357 loss_input: 82.35226478434488
step: 12000 epoch: 138 loss: 17.766040611799514 loss_input: 82.33923867245275
step: 13000 epoch: 138 loss: 17.784116234800265 loss_input: 82.38492959507171
step: 14000 epoch: 138 loss: 17.788975734283614 loss_input: 82.31062441898682
step: 15000 epoch: 138 loss: 17.792508515856454 loss_input: 82.30605652087387
Save loss: 17.784488980457187 Name: 138_train_model.pth
step: 0 epoch: 139 loss: 20.023256301879883 loss_input: 90.61077880859375
step: 1000 epoch: 139 loss: 17.580512979052045 loss_input: 82.15844287548389
step: 2000 epoch: 139 loss: 17.56576172415463 loss_input: 82.06452666611031
step: 3000 epoch: 139 loss: 17.585114102806898 loss_input: 82.00011633491961
step: 4000 epoch: 139 loss: 17.64410247090041 loss_input: 81.92299654578542
step: 5000 epoch: 139 loss: 17.697633398506454 loss_input: 81.90845125652604
step: 6000 epoch: 139 loss: 17.732918196848047 loss_input: 82.23975545091443
step: 7000 epoch: 139 loss: 17.72572135945726 loss_input: 82.26980514537264
step: 8000 epoch: 139 loss: 17.73453983666375 loss_input: 82.38449784383641
step: 9000 epoch: 139 loss: 17.728867755123222 loss_input: 82.37908587007043
step: 10000 epoch: 139 loss: 17.75458387407872 loss_input: 82.4889661912715
step: 11000 epoch: 139 loss: 17.76701104559689 loss_input: 82.48568862347742
step: 12000 epoch: 139 loss: 17.772563810617108 loss_input: 82.43914104773495
step: 13000 epoch: 139 loss: 17.785327119026245 loss_input: 82.39430046565678
step: 14000 epoch: 139 loss: 17.780411377505537 loss_input: 82.32547506023565
step: 15000 epoch: 139 loss: 17.78116457664635 loss_input: 82.28802330739799
Save loss: 17.765039235159755 Name: 139_train_model.pth
step: 0 epoch: 140 loss: 17.55897331237793 loss_input: 87.94610595703125
step: 1000 epoch: 140 loss: 18.03989958501124 loss_input: 84.21942535004058
step: 2000 epoch: 140 loss: 17.92339047213187 loss_input: 82.84426637389313
step: 3000 epoch: 140 loss: 17.865095229754246 loss_input: 82.52476129782912
step: 4000 epoch: 140 loss: 17.852179141379512 loss_input: 82.57914680875679
step: 5000 epoch: 140 loss: 17.75289118096867 loss_input: 82.33884906368336
step: 6000 epoch: 140 loss: 17.72366244092502 loss_input: 82.27408840266531
step: 7000 epoch: 140 loss: 17.7056468395723 loss_input: 82.1898721063704
step: 8000 epoch: 140 loss: 17.70487271641332 loss_input: 82.16362892992868
step: 9000 epoch: 140 loss: 17.747533419227324 loss_input: 82.27232239630709
step: 10000 epoch: 140 loss: 17.739559362964766 loss_input: 82.19088846730561
step: 11000 epoch: 140 loss: 17.741874093717254 loss_input: 82.13547595970502
step: 12000 epoch: 140 loss: 17.751167783180126 loss_input: 82.2176213200495
step: 13000 epoch: 140 loss: 17.75857036717625 loss_input: 82.25700614676275
step: 14000 epoch: 140 loss: 17.77732784531302 loss_input: 82.23865993301678
step: 15000 epoch: 140 loss: 17.784926934398325 loss_input: 82.24993928627414
Save loss: 17.784427255690098 Name: 140_train_model.pth
step: 0 epoch: 141 loss: 14.460161209106445 loss_input: 60.617431640625
step: 1000 epoch: 141 loss: 17.674887794119257 loss_input: 80.57141277911542
step: 2000 epoch: 141 loss: 17.733868058474883 loss_input: 82.36671099264821
step: 3000 epoch: 141 loss: 17.754935310984088 loss_input: 82.92229124308824
step: 4000 epoch: 141 loss: 17.79189865281063 loss_input: 82.72240873802545
step: 5000 epoch: 141 loss: 17.772948278329107 loss_input: 82.51248164673706
step: 6000 epoch: 141 loss: 17.857992117016778 loss_input: 82.588251272493
step: 7000 epoch: 141 loss: 17.79210828062705 loss_input: 82.527099051419
step: 8000 epoch: 141 loss: 17.78646569871229 loss_input: 82.53437563926336
step: 9000 epoch: 141 loss: 17.79240310135159 loss_input: 82.34986408031486
step: 10000 epoch: 141 loss: 17.792120148689552 loss_input: 82.26812634288329
step: 11000 epoch: 141 loss: 17.79549963636167 loss_input: 82.37082378830263
step: 12000 epoch: 141 loss: 17.79902392134608 loss_input: 82.30195926259074
step: 13000 epoch: 141 loss: 17.79174074493237 loss_input: 82.34511701192005
step: 14000 epoch: 141 loss: 17.803879266602255 loss_input: 82.38772220964407
step: 15000 epoch: 141 loss: 17.785219637777526 loss_input: 82.26691327674509
Save loss: 17.784025335371496 Name: 141_train_model.pth
step: 0 epoch: 142 loss: 21.115985870361328 loss_input: 96.5887451171875
step: 1000 epoch: 142 loss: 17.540620474667698 loss_input: 82.12096722857221
step: 2000 epoch: 142 loss: 17.50736065997534 loss_input: 81.49766150860117
step: 3000 epoch: 142 loss: 17.647809027036562 loss_input: 82.03205498883185
step: 4000 epoch: 142 loss: 17.735471228723732 loss_input: 82.2831501597257
step: 5000 epoch: 142 loss: 17.712185083163117 loss_input: 82.28948523039533
step: 6000 epoch: 142 loss: 17.74549706660396 loss_input: 82.5185220696453
step: 7000 epoch: 142 loss: 17.748863104632406 loss_input: 82.32555385154241
step: 8000 epoch: 142 loss: 17.786819243159925 loss_input: 82.32126013545182
step: 9000 epoch: 142 loss: 17.814316934459065 loss_input: 82.3647724037607
step: 10000 epoch: 142 loss: 17.80133248739106 loss_input: 82.38449390534579
step: 11000 epoch: 142 loss: 17.778918329449894 loss_input: 82.34657426717422
step: 12000 epoch: 142 loss: 17.77172627352484 loss_input: 82.33974600017056
step: 13000 epoch: 142 loss: 17.75788752565603 loss_input: 82.21773834028993
step: 14000 epoch: 142 loss: 17.765607682749778 loss_input: 82.29965078962691
step: 15000 epoch: 142 loss: 17.760764694334657 loss_input: 82.22589668338517
Save loss: 17.76213698244095 Name: 142_train_model.pth
step: 0 epoch: 143 loss: 17.094532012939453 loss_input: 101.43963623046875
step: 1000 epoch: 143 loss: 17.92648193290779 loss_input: 82.1397813612169
step: 2000 epoch: 143 loss: 17.801502390303416 loss_input: 82.56280508427427
step: 3000 epoch: 143 loss: 17.718542994041915 loss_input: 82.69041463717824
step: 4000 epoch: 143 loss: 17.68217568444002 loss_input: 82.18058629263582
step: 5000 epoch: 143 loss: 17.66721125093371 loss_input: 82.09416573716507
step: 6000 epoch: 143 loss: 17.69874661001756 loss_input: 82.11742839612994
step: 7000 epoch: 143 loss: 17.725213648574453 loss_input: 82.20733815195764
step: 8000 epoch: 143 loss: 17.732609469210292 loss_input: 82.20971457699868
step: 9000 epoch: 143 loss: 17.700148575571614 loss_input: 82.08308584066832
step: 10000 epoch: 143 loss: 17.706975153619894 loss_input: 82.1154128422607
step: 11000 epoch: 143 loss: 17.72127184984023 loss_input: 82.09607926711918
step: 12000 epoch: 143 loss: 17.741897385275948 loss_input: 82.11889610293706
step: 13000 epoch: 143 loss: 17.737168556726417 loss_input: 82.09776092663094
step: 14000 epoch: 143 loss: 17.73790427596473 loss_input: 82.09224774873084
step: 15000 epoch: 143 loss: 17.737728147679636 loss_input: 82.17024569374412
Save loss: 17.752182336762548 Name: 143_train_model.pth
step: 0 epoch: 144 loss: 15.423314094543457 loss_input: 85.98382568359375
step: 1000 epoch: 144 loss: 17.508006898554175 loss_input: 82.66017155451969
step: 2000 epoch: 144 loss: 17.54714112041117 loss_input: 82.55556489943505
step: 3000 epoch: 144 loss: 17.60078423518175 loss_input: 82.38669607282598
step: 4000 epoch: 144 loss: 17.635877739873656 loss_input: 82.07287283403758
step: 5000 epoch: 144 loss: 17.675165196510108 loss_input: 82.34965433671078
step: 6000 epoch: 144 loss: 17.652240192824614 loss_input: 82.20902792649792
step: 7000 epoch: 144 loss: 17.697281236666267 loss_input: 82.24632943410428
step: 8000 epoch: 144 loss: 17.683253141600346 loss_input: 82.16030717107627
step: 9000 epoch: 144 loss: 17.714170341186556 loss_input: 82.1552836563624
step: 10000 epoch: 144 loss: 17.706996782078 loss_input: 82.11950859967715
step: 11000 epoch: 144 loss: 17.705840832948056 loss_input: 82.08718706458149
step: 12000 epoch: 144 loss: 17.7104549092478 loss_input: 82.14416734803349
step: 13000 epoch: 144 loss: 17.718874794090265 loss_input: 82.12676834823553
step: 14000 epoch: 144 loss: 17.72853297044972 loss_input: 82.17657071387339
step: 15000 epoch: 144 loss: 17.74718475373584 loss_input: 82.27264537344645
Save loss: 17.75233858293295 Name: 144_train_model.pth
step: 0 epoch: 145 loss: 23.0260009765625 loss_input: 139.67315673828125
step: 1000 epoch: 145 loss: 17.472536835875307 loss_input: 82.61056588674997
step: 2000 epoch: 145 loss: 17.65262289228349 loss_input: 81.96792906037216
step: 3000 epoch: 145 loss: 17.655480211156878 loss_input: 81.96817576920974
step: 4000 epoch: 145 loss: 17.696939315059367 loss_input: 82.28315885506163
step: 5000 epoch: 145 loss: 17.698965766338844 loss_input: 82.14531022018252
step: 6000 epoch: 145 loss: 17.726249983302356 loss_input: 82.36111405952198
step: 7000 epoch: 145 loss: 17.703816736209326 loss_input: 82.345226682879
step: 8000 epoch: 145 loss: 17.717974030126737 loss_input: 82.24907146616438
step: 9000 epoch: 145 loss: 17.721259313482083 loss_input: 82.16801374147447
step: 10000 epoch: 145 loss: 17.73342681803616 loss_input: 82.08205819432705
step: 11000 epoch: 145 loss: 17.711524128187854 loss_input: 81.96569340420402
step: 12000 epoch: 145 loss: 17.713302861808806 loss_input: 82.0901175105525
step: 13000 epoch: 145 loss: 17.73343661783035 loss_input: 82.191619542074
step: 14000 epoch: 145 loss: 17.723788068359404 loss_input: 82.17087408185269
step: 15000 epoch: 145 loss: 17.736535671543862 loss_input: 82.20418585863807
Save loss: 17.74363970667124 Name: 145_train_model.pth
step: 0 epoch: 146 loss: 18.920454025268555 loss_input: 105.87384033203125
step: 1000 epoch: 146 loss: 17.75729033520648 loss_input: 83.4276619376717
step: 2000 epoch: 146 loss: 17.741497918404917 loss_input: 82.81012020845046
step: 3000 epoch: 146 loss: 17.789469864161084 loss_input: 82.83656009520264
step: 4000 epoch: 146 loss: 17.789583263859633 loss_input: 82.7897465330933
step: 5000 epoch: 146 loss: 17.737228906147482 loss_input: 82.38182009594175
step: 6000 epoch: 146 loss: 17.714060104880566 loss_input: 82.39587334199183
step: 7000 epoch: 146 loss: 17.724166805004156 loss_input: 82.21772695279023
step: 8000 epoch: 146 loss: 17.739502528833906 loss_input: 82.25633253667999
step: 9000 epoch: 146 loss: 17.735705935151138 loss_input: 82.17012698458852
step: 10000 epoch: 146 loss: 17.752508337766383 loss_input: 82.307729877647
step: 11000 epoch: 146 loss: 17.754000639224117 loss_input: 82.34618841025106
step: 12000 epoch: 146 loss: 17.7692222137092 loss_input: 82.36730532872261
step: 13000 epoch: 146 loss: 17.769516362968606 loss_input: 82.3444884357228
step: 14000 epoch: 146 loss: 17.757037575283558 loss_input: 82.25728700740126
step: 15000 epoch: 146 loss: 17.76752679906331 loss_input: 82.31014643375988
Save loss: 17.756831754714252 Name: 146_train_model.pth
step: 0 epoch: 147 loss: 20.002731323242188 loss_input: 47.685546875
step: 1000 epoch: 147 loss: 17.843627832986257 loss_input: 81.49884228320508
step: 2000 epoch: 147 loss: 17.920905213782575 loss_input: 81.92305083420204
step: 3000 epoch: 147 loss: 17.834608783804548 loss_input: 82.08077757853502
step: 4000 epoch: 147 loss: 17.778657182876064 loss_input: 82.02969592989936
step: 5000 epoch: 147 loss: 17.73439635746099 loss_input: 81.81636684137781
step: 6000 epoch: 147 loss: 17.763516460969196 loss_input: 82.03328742084652
step: 7000 epoch: 147 loss: 17.735721165581168 loss_input: 82.0742204761764
step: 8000 epoch: 147 loss: 17.696743605658646 loss_input: 82.01679467648331
step: 9000 epoch: 147 loss: 17.704588942813313 loss_input: 82.05083646033687
step: 10000 epoch: 147 loss: 17.696102280102306 loss_input: 81.95949506001548
step: 11000 epoch: 147 loss: 17.678786205883494 loss_input: 81.96758914472797
step: 12000 epoch: 147 loss: 17.68688275563539 loss_input: 82.03903484829226
step: 13000 epoch: 147 loss: 17.71365607262538 loss_input: 82.20812422106353
step: 14000 epoch: 147 loss: 17.72664706339692 loss_input: 82.28018692198604
step: 15000 epoch: 147 loss: 17.729327080400935 loss_input: 82.27800284912647
Save loss: 17.72393166847527 Name: 147_train_model.pth
step: 0 epoch: 148 loss: 18.786083221435547 loss_input: 142.600830078125
step: 1000 epoch: 148 loss: 17.76418996547962 loss_input: 83.08657132662259
step: 2000 epoch: 148 loss: 17.615646492177877 loss_input: 82.01102927540136
step: 3000 epoch: 148 loss: 17.679333549068595 loss_input: 82.52519688261465
step: 4000 epoch: 148 loss: 17.66124551458914 loss_input: 82.23720237238054
step: 5000 epoch: 148 loss: 17.648647129047013 loss_input: 82.05238288279844
step: 6000 epoch: 148 loss: 17.671022481231805 loss_input: 82.17518243215974
step: 7000 epoch: 148 loss: 17.689370191534866 loss_input: 82.16771151484498
step: 8000 epoch: 148 loss: 17.677340571842496 loss_input: 82.12083352862142
step: 9000 epoch: 148 loss: 17.667774601309 loss_input: 81.9843352773086
step: 10000 epoch: 148 loss: 17.668233446735893 loss_input: 82.0159596406082
step: 11000 epoch: 148 loss: 17.690554011270095 loss_input: 82.04221450726602
step: 12000 epoch: 148 loss: 17.682074758233888 loss_input: 82.00077825037204
step: 13000 epoch: 148 loss: 17.703220040896664 loss_input: 82.09611261592407
step: 14000 epoch: 148 loss: 17.71335420577188 loss_input: 82.04694687684139
step: 15000 epoch: 148 loss: 17.724698156064754 loss_input: 82.18075106434962
Save loss: 17.74314126187563 Name: 148_train_model.pth
step: 0 epoch: 149 loss: 16.828357696533203 loss_input: 111.38714599609375
step: 1000 epoch: 149 loss: 17.745095014810325 loss_input: 82.67963695240307
step: 2000 epoch: 149 loss: 17.640460050803075 loss_input: 82.0980497300178
step: 3000 epoch: 149 loss: 17.604692838542345 loss_input: 81.88349407619336
step: 4000 epoch: 149 loss: 17.61908137956222 loss_input: 81.85714952792564
step: 5000 epoch: 149 loss: 17.59074127671719 loss_input: 81.85256487520856
step: 6000 epoch: 149 loss: 17.627109308358808 loss_input: 81.80380569913153
step: 7000 epoch: 149 loss: 17.644828829147563 loss_input: 81.79513379366564
step: 8000 epoch: 149 loss: 17.632457413623936 loss_input: 81.8843239611528
step: 9000 epoch: 149 loss: 17.649617693660232 loss_input: 81.93815846904597
step: 10000 epoch: 149 loss: 17.62948120702399 loss_input: 82.00906211869048
step: 11000 epoch: 149 loss: 17.641498486244313 loss_input: 81.9213727263253
step: 12000 epoch: 149 loss: 17.65765838785555 loss_input: 81.98038783938415
step: 13000 epoch: 149 loss: 17.66663256834749 loss_input: 82.0129318618745
step: 14000 epoch: 149 loss: 17.688367176001417 loss_input: 82.13184155233331
step: 15000 epoch: 149 loss: 17.69231396727177 loss_input: 82.16762829011269
Save loss: 17.71791826221347 Name: 149_train_model.pth
step: 0 epoch: 150 loss: 25.306428909301758 loss_input: 126.6708984375
step: 1000 epoch: 150 loss: 17.788871906139516 loss_input: 81.56875698764127
step: 2000 epoch: 150 loss: 17.724591354678 loss_input: 82.3186875800381
step: 3000 epoch: 150 loss: 17.704058773396056 loss_input: 82.33046333188416
step: 4000 epoch: 150 loss: 17.685081993749932 loss_input: 82.19479743101334
step: 5000 epoch: 150 loss: 17.672888792650483 loss_input: 82.33468883615855
step: 6000 epoch: 150 loss: 17.640536991482357 loss_input: 82.06120385982219
step: 7000 epoch: 150 loss: 17.6558988468253 loss_input: 82.2332795841526
step: 8000 epoch: 150 loss: 17.654877858465277 loss_input: 82.21523843027923
step: 9000 epoch: 150 loss: 17.660808689606718 loss_input: 82.16515227691715
step: 10000 epoch: 150 loss: 17.680303965648548 loss_input: 82.24772991450855
step: 11000 epoch: 150 loss: 17.694611213606233 loss_input: 82.27883114696427
step: 12000 epoch: 150 loss: 17.686221936575304 loss_input: 82.22527898676087
step: 13000 epoch: 150 loss: 17.679783253621693 loss_input: 82.12292352774246
step: 14000 epoch: 150 loss: 17.698151407918882 loss_input: 82.11291320377924
step: 15000 epoch: 150 loss: 17.7066366353692 loss_input: 82.08478326786678
Save loss: 17.719115160956978 Name: 150_train_model.pth
step: 0 epoch: 151 loss: 21.476539611816406 loss_input: 98.6671142578125
step: 1000 epoch: 151 loss: 17.58657153336318 loss_input: 81.00090882018372
step: 2000 epoch: 151 loss: 17.434062530492795 loss_input: 80.84490439106618
step: 3000 epoch: 151 loss: 17.50556117969527 loss_input: 81.08997104724857
step: 4000 epoch: 151 loss: 17.55069401704559 loss_input: 81.42389263829062
step: 5000 epoch: 151 loss: 17.591386877806322 loss_input: 81.53170022733734
step: 6000 epoch: 151 loss: 17.588982146891965 loss_input: 81.60788115921109
step: 7000 epoch: 151 loss: 17.600980912254737 loss_input: 81.84636886241418
step: 8000 epoch: 151 loss: 17.653758994937913 loss_input: 82.08709477263828
step: 9000 epoch: 151 loss: 17.659162340608123 loss_input: 82.08389261544724
step: 10000 epoch: 151 loss: 17.65663098263843 loss_input: 82.07831417785955
step: 11000 epoch: 151 loss: 17.65226668516578 loss_input: 82.02841233405273
step: 12000 epoch: 151 loss: 17.672771317294693 loss_input: 82.17706108617739
step: 13000 epoch: 151 loss: 17.676768712709155 loss_input: 82.15493969946272
step: 14000 epoch: 151 loss: 17.705642030559414 loss_input: 82.24344645635664
step: 15000 epoch: 151 loss: 17.715031861003325 loss_input: 82.26447853915859
Save loss: 17.71606700283289 Name: 151_train_model.pth
step: 0 epoch: 152 loss: 16.156951904296875 loss_input: 78.61541748046875
step: 1000 epoch: 152 loss: 17.647965520292846 loss_input: 82.12640813776068
step: 2000 epoch: 152 loss: 17.567805823059693 loss_input: 82.02008312860171
step: 3000 epoch: 152 loss: 17.58691465755337 loss_input: 82.06479928240066
step: 4000 epoch: 152 loss: 17.602862936948068 loss_input: 82.13727828723971
step: 5000 epoch: 152 loss: 17.618716147536638 loss_input: 82.04327825841082
step: 6000 epoch: 152 loss: 17.624892692966394 loss_input: 82.01915639385624
step: 7000 epoch: 152 loss: 17.60690664097405 loss_input: 82.11275433901463
step: 8000 epoch: 152 loss: 17.64984185891425 loss_input: 82.29244488970129
step: 9000 epoch: 152 loss: 17.656836478077906 loss_input: 82.19790103210103
step: 10000 epoch: 152 loss: 17.673265572965484 loss_input: 82.22931650316414
step: 11000 epoch: 152 loss: 17.68594854379305 loss_input: 82.12379885963847
step: 12000 epoch: 152 loss: 17.68269896052319 loss_input: 82.06126989315831
step: 13000 epoch: 152 loss: 17.686993326208114 loss_input: 82.09564336209414
step: 14000 epoch: 152 loss: 17.704201600354242 loss_input: 82.18232858158011
step: 15000 epoch: 152 loss: 17.713851841741636 loss_input: 82.30597863706873
Save loss: 17.699327338606118 Name: 152_train_model.pth
step: 0 epoch: 153 loss: 19.00880241394043 loss_input: 78.5980224609375
step: 1000 epoch: 153 loss: 17.483492618316895 loss_input: 82.04599453281094
step: 2000 epoch: 153 loss: 17.533849741446264 loss_input: 81.89391848875367
step: 3000 epoch: 153 loss: 17.5483824923133 loss_input: 81.7009902949017
step: 4000 epoch: 153 loss: 17.53930334161741 loss_input: 81.57379763640633
step: 5000 epoch: 153 loss: 17.50846551890565 loss_input: 81.27956552244238
step: 6000 epoch: 153 loss: 17.502980794177176 loss_input: 81.5441290282186
step: 7000 epoch: 153 loss: 17.535367904739914 loss_input: 81.6438567073426
step: 8000 epoch: 153 loss: 17.57184645814041 loss_input: 81.75678127209197
step: 9000 epoch: 153 loss: 17.615930022246147 loss_input: 81.86182943241766
step: 10000 epoch: 153 loss: 17.657342501681228 loss_input: 81.99782558365257
step: 11000 epoch: 153 loss: 17.67638252028746 loss_input: 82.10134169487787
step: 12000 epoch: 153 loss: 17.668116255468473 loss_input: 82.11632767276718
step: 13000 epoch: 153 loss: 17.689631802496034 loss_input: 82.21328996608885
step: 14000 epoch: 153 loss: 17.695176132678405 loss_input: 82.13433051465213
step: 15000 epoch: 153 loss: 17.71514798110266 loss_input: 82.31975976178411
Save loss: 17.70683067509532 Name: 153_train_model.pth
step: 0 epoch: 154 loss: 10.165389060974121 loss_input: 69.24755859375
step: 1000 epoch: 154 loss: 17.519079250770133 loss_input: 80.97873775394527
step: 2000 epoch: 154 loss: 17.447194912265623 loss_input: 80.92709962157593
step: 3000 epoch: 154 loss: 17.53558653626828 loss_input: 81.48478638224108
step: 4000 epoch: 154 loss: 17.626163886267374 loss_input: 81.52602893893226
step: 5000 epoch: 154 loss: 17.604753889720026 loss_input: 81.49927396112574
step: 6000 epoch: 154 loss: 17.639456717218284 loss_input: 81.64734030127465
step: 7000 epoch: 154 loss: 17.666973741475793 loss_input: 82.05973591209906
step: 8000 epoch: 154 loss: 17.681872033608016 loss_input: 82.1400434305453
step: 9000 epoch: 154 loss: 17.696832834992218 loss_input: 82.27133762210015
step: 10000 epoch: 154 loss: 17.67980382435561 loss_input: 82.17544757365084
step: 11000 epoch: 154 loss: 17.688432992756255 loss_input: 82.0974024389562
step: 12000 epoch: 154 loss: 17.70135959572876 loss_input: 82.21229936059203
step: 13000 epoch: 154 loss: 17.69091296427049 loss_input: 82.21598608531765
step: 14000 epoch: 154 loss: 17.70137344104172 loss_input: 82.17595874935412
step: 15000 epoch: 154 loss: 17.69819102636314 loss_input: 82.25884789646837
Save loss: 17.69384341670573 Name: 154_train_model.pth
step: 0 epoch: 155 loss: 13.817473411560059 loss_input: 47.16949462890625
step: 1000 epoch: 155 loss: 17.4045030453822 loss_input: 80.99266279374922
step: 2000 epoch: 155 loss: 17.545073077179442 loss_input: 82.2265776596565
step: 3000 epoch: 155 loss: 17.628516286343427 loss_input: 82.15860047383295
step: 4000 epoch: 155 loss: 17.639817144655638 loss_input: 82.20614051199114
step: 5000 epoch: 155 loss: 17.6280639731772 loss_input: 82.24268918189019
step: 6000 epoch: 155 loss: 17.650485505225003 loss_input: 82.46586096531112
step: 7000 epoch: 155 loss: 17.639600223514833 loss_input: 82.41719313012483
step: 8000 epoch: 155 loss: 17.643326624499725 loss_input: 82.28081703257551
step: 9000 epoch: 155 loss: 17.677935448583927 loss_input: 82.29090910336134
step: 10000 epoch: 155 loss: 17.675599060181604 loss_input: 82.32615662994247
step: 11000 epoch: 155 loss: 17.677916400292364 loss_input: 82.33561575190608
step: 12000 epoch: 155 loss: 17.687257730467163 loss_input: 82.2440606400069
step: 13000 epoch: 155 loss: 17.690606023007085 loss_input: 82.21367599314776
step: 14000 epoch: 155 loss: 17.69238108073411 loss_input: 82.16748713854423
step: 15000 epoch: 155 loss: 17.678895039539658 loss_input: 82.14904687913383
Save loss: 17.68824840219319 Name: 155_train_model.pth
step: 0 epoch: 156 loss: 28.658498764038086 loss_input: 140.15106201171875
step: 1000 epoch: 156 loss: 17.638842425979934 loss_input: 81.95405955128855
step: 2000 epoch: 156 loss: 17.76717667529608 loss_input: 82.40833083824299
step: 3000 epoch: 156 loss: 17.795758696485862 loss_input: 82.27939443769395
step: 4000 epoch: 156 loss: 17.776794475246984 loss_input: 82.50060929896921
step: 5000 epoch: 156 loss: 17.73630658547131 loss_input: 82.15564819165073
step: 6000 epoch: 156 loss: 17.72312215458769 loss_input: 82.0214239500936
step: 7000 epoch: 156 loss: 17.69357701656291 loss_input: 82.03287480276255
step: 8000 epoch: 156 loss: 17.700481450642634 loss_input: 82.10775989884809
step: 9000 epoch: 156 loss: 17.686164598255182 loss_input: 82.11778811414617
step: 10000 epoch: 156 loss: 17.70666005086713 loss_input: 82.27332643322582
step: 11000 epoch: 156 loss: 17.708116743003504 loss_input: 82.20890459820592
step: 12000 epoch: 156 loss: 17.710099993303572 loss_input: 82.29576568692518
step: 13000 epoch: 156 loss: 17.68155087983459 loss_input: 82.19641371017585
step: 14000 epoch: 156 loss: 17.66207960605383 loss_input: 82.11008957036691
step: 15000 epoch: 156 loss: 17.675643220741538 loss_input: 82.20174532309825
Save loss: 17.68130646006763 Name: 156_train_model.pth
step: 0 epoch: 157 loss: 21.870563507080078 loss_input: 164.0164794921875
step: 1000 epoch: 157 loss: 17.56545391330471 loss_input: 82.30355478261973
step: 2000 epoch: 157 loss: 17.61606070376944 loss_input: 82.2025197423261
step: 3000 epoch: 157 loss: 17.66494338236742 loss_input: 82.13935640024289
step: 4000 epoch: 157 loss: 17.5740865154643 loss_input: 81.81234081433254
step: 5000 epoch: 157 loss: 17.597765881260546 loss_input: 81.98441652001631
step: 6000 epoch: 157 loss: 17.639362468935612 loss_input: 82.21135523636686
step: 7000 epoch: 157 loss: 17.68311057869255 loss_input: 82.40104868180104
step: 8000 epoch: 157 loss: 17.62361906182988 loss_input: 82.24384976780246
step: 9000 epoch: 157 loss: 17.633072778074336 loss_input: 82.27234972345738
step: 10000 epoch: 157 loss: 17.646010322626108 loss_input: 82.3704480443069
step: 11000 epoch: 157 loss: 17.650967095464093 loss_input: 82.28982655740458
step: 12000 epoch: 157 loss: 17.65526198279946 loss_input: 82.27065970617357
step: 13000 epoch: 157 loss: 17.64411331846039 loss_input: 82.13428714148641
step: 14000 epoch: 157 loss: 17.639486527393547 loss_input: 82.09522029728764
step: 15000 epoch: 157 loss: 17.666137652400018 loss_input: 82.16789523602073
Save loss: 17.692526674702762 Name: 157_train_model.pth
step: 0 epoch: 158 loss: 9.545866012573242 loss_input: 59.61163330078125
step: 1000 epoch: 158 loss: 17.535971599144418 loss_input: 82.41345038018622
step: 2000 epoch: 158 loss: 17.580848469369595 loss_input: 82.8080297924768
step: 3000 epoch: 158 loss: 17.52602350008722 loss_input: 82.68358309274512
step: 4000 epoch: 158 loss: 17.518241099910835 loss_input: 82.34211306487343
step: 5000 epoch: 158 loss: 17.51387456197115 loss_input: 82.14512658991639
step: 6000 epoch: 158 loss: 17.610688591972824 loss_input: 82.56074976273486
step: 7000 epoch: 158 loss: 17.62280022548005 loss_input: 82.46868901715213
step: 8000 epoch: 158 loss: 17.647385318314132 loss_input: 82.41336658021507
step: 9000 epoch: 158 loss: 17.657232926297407 loss_input: 82.45223023586571
step: 10000 epoch: 158 loss: 17.636534854920193 loss_input: 82.34812639341534
step: 11000 epoch: 158 loss: 17.65591714300033 loss_input: 82.276743018923
step: 12000 epoch: 158 loss: 17.661455142061946 loss_input: 82.25018952379781
step: 13000 epoch: 158 loss: 17.668795166506364 loss_input: 82.25146689061779
step: 14000 epoch: 158 loss: 17.696122324847295 loss_input: 82.34176887058291
step: 15000 epoch: 158 loss: 17.682999793406083 loss_input: 82.30235782539627
Save loss: 17.673865280166268 Name: 158_train_model.pth
step: 0 epoch: 159 loss: 21.691802978515625 loss_input: 99.1558837890625
step: 1000 epoch: 159 loss: 17.570769953084636 loss_input: 81.72846093115868
step: 2000 epoch: 159 loss: 17.689375271146623 loss_input: 82.64431068767374
step: 3000 epoch: 159 loss: 17.672396934894433 loss_input: 82.31265542507965
step: 4000 epoch: 159 loss: 17.69538758236657 loss_input: 82.35821951189449
step: 5000 epoch: 159 loss: 17.628607758806364 loss_input: 82.16209284998469
step: 6000 epoch: 159 loss: 17.61975239992261 loss_input: 81.97963920686071
step: 7000 epoch: 159 loss: 17.60914847568484 loss_input: 81.89323105558023
step: 8000 epoch: 159 loss: 17.647060932539176 loss_input: 82.0190409014425
step: 9000 epoch: 159 loss: 17.634197414351785 loss_input: 82.07280059295712
step: 10000 epoch: 159 loss: 17.634605850771468 loss_input: 82.16719357727314
step: 11000 epoch: 159 loss: 17.639281171526154 loss_input: 82.17699452415725
step: 12000 epoch: 159 loss: 17.668092636294826 loss_input: 82.29371709563753
step: 13000 epoch: 159 loss: 17.671266013168847 loss_input: 82.26555175875143
step: 14000 epoch: 159 loss: 17.65721174933452 loss_input: 82.23993442607058
step: 15000 epoch: 159 loss: 17.668849139219603 loss_input: 82.20366562151065
Save loss: 17.672337897226214 Name: 159_train_model.pth
step: 0 epoch: 160 loss: 11.842217445373535 loss_input: 62.088134765625
step: 1000 epoch: 160 loss: 17.693441852108464 loss_input: 81.45773489206107
step: 2000 epoch: 160 loss: 17.660396133405694 loss_input: 81.96102821392039
step: 3000 epoch: 160 loss: 17.67700748084506 loss_input: 82.42386802869096
step: 4000 epoch: 160 loss: 17.623962266240767 loss_input: 82.58785407407913
step: 5000 epoch: 160 loss: 17.60183095703171 loss_input: 82.40048722433248
step: 6000 epoch: 160 loss: 17.58429913591532 loss_input: 82.30603525714464
step: 7000 epoch: 160 loss: 17.635030373354127 loss_input: 82.44024095016282
step: 8000 epoch: 160 loss: 17.6384195880344 loss_input: 82.4619609011276
step: 9000 epoch: 160 loss: 17.649652293861845 loss_input: 82.4806994576015
step: 10000 epoch: 160 loss: 17.656934388386418 loss_input: 82.52269632045585
step: 11000 epoch: 160 loss: 17.651660870469794 loss_input: 82.52923458596271
step: 12000 epoch: 160 loss: 17.652025487043293 loss_input: 82.38326190509515
step: 13000 epoch: 160 loss: 17.665430785857588 loss_input: 82.34423915445507
step: 14000 epoch: 160 loss: 17.653802613004363 loss_input: 82.2855517339233
step: 15000 epoch: 160 loss: 17.661487539309945 loss_input: 82.23577750659722
Save loss: 17.672633625105025 Name: 160_train_model.pth
step: 0 epoch: 161 loss: 16.964641571044922 loss_input: 73.265869140625
step: 1000 epoch: 161 loss: 17.533343145063707 loss_input: 80.26525836224323
step: 2000 epoch: 161 loss: 17.667371174623106 loss_input: 81.38352637646021
step: 3000 epoch: 161 loss: 17.529234667533004 loss_input: 80.9272076340764
step: 4000 epoch: 161 loss: 17.60928290404549 loss_input: 81.34239018675507
step: 5000 epoch: 161 loss: 17.61211501472212 loss_input: 81.33172807577157
step: 6000 epoch: 161 loss: 17.605833186683725 loss_input: 81.53951953645746
step: 7000 epoch: 161 loss: 17.614649460496537 loss_input: 81.81181954649615
step: 8000 epoch: 161 loss: 17.61253470621203 loss_input: 81.8557964983381
step: 9000 epoch: 161 loss: 17.604060452589128 loss_input: 81.89246050297797
step: 10000 epoch: 161 loss: 17.59064943501263 loss_input: 81.9306329437857
step: 11000 epoch: 161 loss: 17.619741238872415 loss_input: 82.04153253468435
step: 12000 epoch: 161 loss: 17.614855382553927 loss_input: 82.00263578702426
step: 13000 epoch: 161 loss: 17.628886448622723 loss_input: 82.07861505113338
step: 14000 epoch: 161 loss: 17.639709499970255 loss_input: 82.20123597913347
step: 15000 epoch: 161 loss: 17.662906598157942 loss_input: 82.24278247579718
Save loss: 17.667892427444457 Name: 161_train_model.pth
step: 0 epoch: 162 loss: 17.452730178833008 loss_input: 104.14019775390625
step: 1000 epoch: 162 loss: 17.557179424788927 loss_input: 82.45791915603927
step: 2000 epoch: 162 loss: 17.525307183263305 loss_input: 81.76905025880615
step: 3000 epoch: 162 loss: 17.531341514917898 loss_input: 81.95355358841975
step: 4000 epoch: 162 loss: 17.58858958234551 loss_input: 81.78373131326543
step: 5000 epoch: 162 loss: 17.60378616110274 loss_input: 81.73830500013278
step: 6000 epoch: 162 loss: 17.61063244477488 loss_input: 81.82603691824474
step: 7000 epoch: 162 loss: 17.62821041517744 loss_input: 82.19539732674907
step: 8000 epoch: 162 loss: 17.611806988731026 loss_input: 82.06890650967213
step: 9000 epoch: 162 loss: 17.62239206254436 loss_input: 82.10965859886011
step: 10000 epoch: 162 loss: 17.632654333410233 loss_input: 82.1547591883878
step: 11000 epoch: 162 loss: 17.647951255938256 loss_input: 82.16830097299393
step: 12000 epoch: 162 loss: 17.636680783912368 loss_input: 82.0391064823374
step: 13000 epoch: 162 loss: 17.650278179327 loss_input: 82.15146763894514
step: 14000 epoch: 162 loss: 17.656978466299446 loss_input: 82.19500837917967
step: 15000 epoch: 162 loss: 17.65606318582782 loss_input: 82.17100282914082
Save loss: 17.655388196364044 Name: 162_train_model.pth
step: 0 epoch: 163 loss: 17.179340362548828 loss_input: 108.583984375
step: 1000 epoch: 163 loss: 17.349176197976142 loss_input: 80.73545905998299
step: 2000 epoch: 163 loss: 17.494506024885393 loss_input: 81.63125517319465
step: 3000 epoch: 163 loss: 17.47738931886914 loss_input: 81.76659653894666
step: 4000 epoch: 163 loss: 17.508875793172432 loss_input: 82.01032900649349
step: 5000 epoch: 163 loss: 17.51610945820975 loss_input: 82.1138994247049
step: 6000 epoch: 163 loss: 17.511201634960877 loss_input: 82.11782588237247
step: 7000 epoch: 163 loss: 17.516330147313997 loss_input: 82.00910839641492
step: 8000 epoch: 163 loss: 17.535146906023964 loss_input: 82.00529171982164
step: 9000 epoch: 163 loss: 17.582462571724403 loss_input: 82.09986706392432
step: 10000 epoch: 163 loss: 17.587109586499047 loss_input: 82.10902803152302
step: 11000 epoch: 163 loss: 17.622061458157145 loss_input: 82.12818342086976
step: 12000 epoch: 163 loss: 17.642371748697855 loss_input: 82.27421685399915
step: 13000 epoch: 163 loss: 17.651954132578446 loss_input: 82.1687495643364
step: 14000 epoch: 163 loss: 17.657576635372433 loss_input: 82.18941951010893
step: 15000 epoch: 163 loss: 17.642595500630083 loss_input: 82.1484722266888
Save loss: 17.654134106978773 Name: 163_train_model.pth
step: 0 epoch: 164 loss: 18.72967529296875 loss_input: 106.8458251953125
step: 1000 epoch: 164 loss: 17.61844752242158 loss_input: 81.63794805048467
step: 2000 epoch: 164 loss: 17.541866893472818 loss_input: 82.27357930531804
step: 3000 epoch: 164 loss: 17.49939472394878 loss_input: 82.54561935572256
step: 4000 epoch: 164 loss: 17.49119662148033 loss_input: 82.2734939891736
step: 5000 epoch: 164 loss: 17.49730064796939 loss_input: 82.28166511473096
step: 6000 epoch: 164 loss: 17.549881961619413 loss_input: 82.27459435064858
step: 7000 epoch: 164 loss: 17.571492295591444 loss_input: 82.29897035147323
step: 8000 epoch: 164 loss: 17.59103927116456 loss_input: 82.31124242099132
step: 9000 epoch: 164 loss: 17.61728513316305 loss_input: 82.3335476084797
step: 10000 epoch: 164 loss: 17.622963391307735 loss_input: 82.254287321512
step: 11000 epoch: 164 loss: 17.61607393348513 loss_input: 82.29258207749935
step: 12000 epoch: 164 loss: 17.617523031804513 loss_input: 82.1121091725836
step: 13000 epoch: 164 loss: 17.6309614765049 loss_input: 82.13436948096842
step: 14000 epoch: 164 loss: 17.63287362677056 loss_input: 82.17738329098145
step: 15000 epoch: 164 loss: 17.64782630525488 loss_input: 82.19339071242906
Save loss: 17.646713916271924 Name: 164_train_model.pth
step: 0 epoch: 165 loss: 6.937969207763672 loss_input: 77.323974609375
step: 1000 epoch: 165 loss: 17.5379950873978 loss_input: 82.08192692317448
step: 2000 epoch: 165 loss: 17.571569656027012 loss_input: 82.2082882813964
step: 3000 epoch: 165 loss: 17.679234442254856 loss_input: 82.29599549173713
step: 4000 epoch: 165 loss: 17.608251387522955 loss_input: 81.95484978852943
step: 5000 epoch: 165 loss: 17.638160033741848 loss_input: 82.09136622382555
step: 6000 epoch: 165 loss: 17.658444054185143 loss_input: 82.12422928494665
step: 7000 epoch: 165 loss: 17.624667138308222 loss_input: 82.1473678634637
step: 8000 epoch: 165 loss: 17.642373801618646 loss_input: 82.3709623501042
step: 9000 epoch: 165 loss: 17.64811321799324 loss_input: 82.41901976711789
step: 10000 epoch: 165 loss: 17.642826256591338 loss_input: 82.38974123827852
step: 11000 epoch: 165 loss: 17.64869492596447 loss_input: 82.4133331407017
step: 12000 epoch: 165 loss: 17.66336075723812 loss_input: 82.4797810224799
step: 13000 epoch: 165 loss: 17.677228254975194 loss_input: 82.41579489190435
step: 14000 epoch: 165 loss: 17.670424505145487 loss_input: 82.3596075055395
step: 15000 epoch: 165 loss: 17.668808600432268 loss_input: 82.3085692032966
Save loss: 17.666592830345035 Name: 165_train_model.pth
step: 0 epoch: 166 loss: 19.592206954956055 loss_input: 107.04315185546875
step: 1000 epoch: 166 loss: 17.804863298570478 loss_input: 84.52976954685938
step: 2000 epoch: 166 loss: 17.65649106251127 loss_input: 83.22587674668524
step: 3000 epoch: 166 loss: 17.593855142990616 loss_input: 82.58437891715131
step: 4000 epoch: 166 loss: 17.519998150567833 loss_input: 82.29700048742697
step: 5000 epoch: 166 loss: 17.54500257976054 loss_input: 82.33952851763631
step: 6000 epoch: 166 loss: 17.63921662772264 loss_input: 82.50958770665501
step: 7000 epoch: 166 loss: 17.619997122273787 loss_input: 82.44159765506434
step: 8000 epoch: 166 loss: 17.630480420215473 loss_input: 82.38337634095787
step: 9000 epoch: 166 loss: 17.648079080192822 loss_input: 82.51274645427851
step: 10000 epoch: 166 loss: 17.64317170029556 loss_input: 82.44504471061683
step: 11000 epoch: 166 loss: 17.651898728382715 loss_input: 82.49765195240596
step: 12000 epoch: 166 loss: 17.66004941312683 loss_input: 82.51156720724933
step: 13000 epoch: 166 loss: 17.641317353102988 loss_input: 82.3982925338751
step: 14000 epoch: 166 loss: 17.644242272871868 loss_input: 82.37493900843718
step: 15000 epoch: 166 loss: 17.64010834709802 loss_input: 82.31639811252167
Save loss: 17.638905748918653 Name: 166_train_model.pth
step: 0 epoch: 167 loss: 21.325817108154297 loss_input: 84.873779296875
step: 1000 epoch: 167 loss: 17.71116759655597 loss_input: 82.70241824825565
step: 2000 epoch: 167 loss: 17.675922989070802 loss_input: 82.51817722120386
step: 3000 epoch: 167 loss: 17.608747514396143 loss_input: 82.23590175345197
step: 4000 epoch: 167 loss: 17.631836089215497 loss_input: 82.20026017665505
step: 5000 epoch: 167 loss: 17.63461002519764 loss_input: 82.25120914537796
step: 6000 epoch: 167 loss: 17.632357533346354 loss_input: 82.18728392069747
step: 7000 epoch: 167 loss: 17.637816692144423 loss_input: 82.13852256074188
step: 8000 epoch: 167 loss: 17.62536157001452 loss_input: 82.10646965485516
step: 9000 epoch: 167 loss: 17.632341246302428 loss_input: 82.13767052141776
step: 10000 epoch: 167 loss: 17.644106985795332 loss_input: 82.13075983661399
step: 11000 epoch: 167 loss: 17.65891031520647 loss_input: 82.2315168392007
step: 12000 epoch: 167 loss: 17.64122859514432 loss_input: 82.29509494610801
step: 13000 epoch: 167 loss: 17.642359365638278 loss_input: 82.28562310955797
step: 14000 epoch: 167 loss: 17.651939171738697 loss_input: 82.28600314385328
step: 15000 epoch: 167 loss: 17.64882768900281 loss_input: 82.32801288996099
Save loss: 17.63912949949503 Name: 167_train_model.pth
step: 0 epoch: 168 loss: 17.79310417175293 loss_input: 56.4207763671875
step: 1000 epoch: 168 loss: 17.76124293511207 loss_input: 82.829609599385
step: 2000 epoch: 168 loss: 17.71554082396744 loss_input: 82.84419954329476
step: 3000 epoch: 168 loss: 17.67972771289626 loss_input: 82.74648054326943
step: 4000 epoch: 168 loss: 17.68163201869115 loss_input: 82.52889768280586
step: 5000 epoch: 168 loss: 17.645585902141015 loss_input: 82.3064953293521
step: 6000 epoch: 168 loss: 17.631920181618316 loss_input: 82.26148308167714
step: 7000 epoch: 168 loss: 17.593041724434002 loss_input: 82.24352522567925
step: 8000 epoch: 168 loss: 17.574621990283955 loss_input: 82.00760530391464
step: 9000 epoch: 168 loss: 17.580469645946348 loss_input: 81.96238275364152
step: 10000 epoch: 168 loss: 17.5936749453259 loss_input: 81.92467397547355
step: 11000 epoch: 168 loss: 17.61707628628263 loss_input: 81.98656578539025
step: 12000 epoch: 168 loss: 17.617258688239552 loss_input: 82.01540102472744
step: 13000 epoch: 168 loss: 17.609608204472423 loss_input: 82.04654623829266
step: 14000 epoch: 168 loss: 17.614030871593258 loss_input: 82.20969377518654
step: 15000 epoch: 168 loss: 17.616162225283208 loss_input: 82.14819022644583
Save loss: 17.63314549705386 Name: 168_train_model.pth
step: 0 epoch: 169 loss: 24.381153106689453 loss_input: 151.69915771484375
step: 1000 epoch: 169 loss: 17.52796453862757 loss_input: 82.28583909986497
step: 2000 epoch: 169 loss: 17.654664754033504 loss_input: 82.49986331907289
step: 3000 epoch: 169 loss: 17.489158417295908 loss_input: 82.07938149848488
step: 4000 epoch: 169 loss: 17.559378024251185 loss_input: 82.28133852462177
step: 5000 epoch: 169 loss: 17.59135250605671 loss_input: 82.0672365160757
step: 6000 epoch: 169 loss: 17.601459825581223 loss_input: 82.18323019364936
step: 7000 epoch: 169 loss: 17.56974117181383 loss_input: 81.98203523764184
step: 8000 epoch: 169 loss: 17.591146158167845 loss_input: 81.98283087386294
step: 9000 epoch: 169 loss: 17.610466262073388 loss_input: 82.20418456787137
step: 10000 epoch: 169 loss: 17.595324939852322 loss_input: 82.07684179702147
step: 11000 epoch: 169 loss: 17.627455813246915 loss_input: 82.21314097954874
step: 12000 epoch: 169 loss: 17.624637960543307 loss_input: 82.19738313840058
step: 13000 epoch: 169 loss: 17.62940708216076 loss_input: 82.25529659902965
step: 14000 epoch: 169 loss: 17.65220354276166 loss_input: 82.25406734078844
step: 15000 epoch: 169 loss: 17.63584995249114 loss_input: 82.21664711691747
Save loss: 17.63646197156608 Name: 169_train_model.pth
step: 0 epoch: 170 loss: 12.120850563049316 loss_input: 69.8028564453125
step: 1000 epoch: 170 loss: 17.447787178622615 loss_input: 82.0975930197732
step: 2000 epoch: 170 loss: 17.55621662359128 loss_input: 82.00621545916495
step: 3000 epoch: 170 loss: 17.619875787933918 loss_input: 82.48448124713637
step: 4000 epoch: 170 loss: 17.55727310890974 loss_input: 82.2008692010347
step: 5000 epoch: 170 loss: 17.520348194336275 loss_input: 82.28857794115005
step: 6000 epoch: 170 loss: 17.564894471560255 loss_input: 82.23414269822217
step: 7000 epoch: 170 loss: 17.544652108890023 loss_input: 82.25725854558445
step: 8000 epoch: 170 loss: 17.559955105515154 loss_input: 82.1298970166571
step: 9000 epoch: 170 loss: 17.582665159654358 loss_input: 82.17699393806504
step: 10000 epoch: 170 loss: 17.590294117212842 loss_input: 82.18967179820103
step: 11000 epoch: 170 loss: 17.610890306501908 loss_input: 82.22832319309751
step: 12000 epoch: 170 loss: 17.627379423518466 loss_input: 82.12585575856146
step: 13000 epoch: 170 loss: 17.63744173001513 loss_input: 82.1726474109113
step: 14000 epoch: 170 loss: 17.632945109295783 loss_input: 82.14650261013024
step: 15000 epoch: 170 loss: 17.626076499093937 loss_input: 82.18307984789946
Save loss: 17.63981424704194 Name: 170_train_model.pth
step: 0 epoch: 171 loss: 15.11927318572998 loss_input: 43.78216552734375
step: 1000 epoch: 171 loss: 17.26256054145592 loss_input: 81.64465106426776
step: 2000 epoch: 171 loss: 17.235785910393343 loss_input: 81.85350372909248
step: 3000 epoch: 171 loss: 17.37706143400821 loss_input: 82.09177950611515
step: 4000 epoch: 171 loss: 17.475961880932985 loss_input: 82.37178013235949
step: 5000 epoch: 171 loss: 17.515630008983173 loss_input: 82.50498431807779
step: 6000 epoch: 171 loss: 17.51830833467955 loss_input: 82.21409742459916
step: 7000 epoch: 171 loss: 17.563127499480807 loss_input: 82.30431752285946
step: 8000 epoch: 171 loss: 17.59877142264327 loss_input: 82.3388775392944
step: 9000 epoch: 171 loss: 17.613160503452296 loss_input: 82.32912095596149
step: 10000 epoch: 171 loss: 17.634999841585742 loss_input: 82.41273282339735
step: 11000 epoch: 171 loss: 17.63434095589012 loss_input: 82.32392533628868
step: 12000 epoch: 171 loss: 17.642806666521775 loss_input: 82.32856181764312
step: 13000 epoch: 171 loss: 17.639419230816227 loss_input: 82.3139635057745
step: 14000 epoch: 171 loss: 17.650550035602425 loss_input: 82.30136169211268
step: 15000 epoch: 171 loss: 17.64149061605681 loss_input: 82.32883442942996
Save loss: 17.630365656495094 Name: 171_train_model.pth
step: 0 epoch: 172 loss: 15.119900703430176 loss_input: 80.5606689453125
step: 1000 epoch: 172 loss: 17.65948155447915 loss_input: 84.04581685404439
step: 2000 epoch: 172 loss: 17.631158481056485 loss_input: 82.93826993973717
step: 3000 epoch: 172 loss: 17.551249011362923 loss_input: 82.74343608689682
step: 4000 epoch: 172 loss: 17.532641074145562 loss_input: 82.59456995492337
step: 5000 epoch: 172 loss: 17.545660766070664 loss_input: 82.41723494900629
step: 6000 epoch: 172 loss: 17.585902350720833 loss_input: 82.44182517635407
step: 7000 epoch: 172 loss: 17.5857113534562 loss_input: 82.41814921389579
step: 8000 epoch: 172 loss: 17.584194887103088 loss_input: 82.36304142707259
step: 9000 epoch: 172 loss: 17.548880469254712 loss_input: 82.14352590923058
step: 10000 epoch: 172 loss: 17.56029049740614 loss_input: 82.27328518368866
step: 11000 epoch: 172 loss: 17.56179719397766 loss_input: 82.1203689679223
step: 12000 epoch: 172 loss: 17.590501117066594 loss_input: 82.22360277044784
step: 13000 epoch: 172 loss: 17.632767056604596 loss_input: 82.19026918208064
step: 14000 epoch: 172 loss: 17.63135740689044 loss_input: 82.17624312671165
step: 15000 epoch: 172 loss: 17.60989631262551 loss_input: 82.14789049870633
Save loss: 17.619789509311317 Name: 172_train_model.pth
step: 0 epoch: 173 loss: 19.01110076904297 loss_input: 107.2757568359375
step: 1000 epoch: 173 loss: 17.421797861943354 loss_input: 81.99422434255197
step: 2000 epoch: 173 loss: 17.513358998572688 loss_input: 82.7755323693134
step: 3000 epoch: 173 loss: 17.59000295942841 loss_input: 82.47223144449897
step: 4000 epoch: 173 loss: 17.609037238161314 loss_input: 82.29488868017371
step: 5000 epoch: 173 loss: 17.58028485293008 loss_input: 82.48689042709036
step: 6000 epoch: 173 loss: 17.580534208180445 loss_input: 82.38848610477355
step: 7000 epoch: 173 loss: 17.558282200770112 loss_input: 82.24876438898795
step: 8000 epoch: 173 loss: 17.535315166516774 loss_input: 81.99513756306585
step: 9000 epoch: 173 loss: 17.551115447528467 loss_input: 81.99122441120909
step: 10000 epoch: 173 loss: 17.5837665911067 loss_input: 82.1824591832809
step: 11000 epoch: 173 loss: 17.5866204283799 loss_input: 82.12042603060588
step: 12000 epoch: 173 loss: 17.594471862162962 loss_input: 82.17226230191267
step: 13000 epoch: 173 loss: 17.58977148964005 loss_input: 82.15945704747472
step: 14000 epoch: 173 loss: 17.592510065839303 loss_input: 82.18613797134607
step: 15000 epoch: 173 loss: 17.59517899949299 loss_input: 82.20899345642582
Save loss: 17.61917694374919 Name: 173_train_model.pth
step: 0 epoch: 174 loss: 17.79007339477539 loss_input: 91.31475830078125
step: 1000 epoch: 174 loss: 17.29598988686408 loss_input: 81.07866895067822
step: 2000 epoch: 174 loss: 17.387484162524604 loss_input: 82.07775122448541
step: 3000 epoch: 174 loss: 17.441702006142048 loss_input: 82.06751718746746
step: 4000 epoch: 174 loss: 17.440412801314938 loss_input: 81.77231831873574
step: 5000 epoch: 174 loss: 17.462386122800616 loss_input: 81.85387254824425
step: 6000 epoch: 174 loss: 17.53704531958373 loss_input: 82.03791706102368
step: 7000 epoch: 174 loss: 17.570109635791578 loss_input: 82.18348423369764
step: 8000 epoch: 174 loss: 17.567957953682036 loss_input: 82.25065204862088
step: 9000 epoch: 174 loss: 17.579036112693053 loss_input: 82.27345289271244
step: 10000 epoch: 174 loss: 17.595419631887825 loss_input: 82.35547839869334
step: 11000 epoch: 174 loss: 17.605542068578536 loss_input: 82.43118704185628
step: 12000 epoch: 174 loss: 17.600679996122707 loss_input: 82.29108173073793
step: 13000 epoch: 174 loss: 17.607556655309576 loss_input: 82.37772947002875
step: 14000 epoch: 174 loss: 17.598226923472915 loss_input: 82.31630005641338
step: 15000 epoch: 174 loss: 17.595069718944192 loss_input: 82.30094713249585
Save loss: 17.59681981648505 Name: 174_train_model.pth
step: 0 epoch: 175 loss: 10.350654602050781 loss_input: 39.7022705078125
step: 1000 epoch: 175 loss: 17.356942737495505 loss_input: 80.9032485458877
step: 2000 epoch: 175 loss: 17.39719873425485 loss_input: 81.96854191312546
step: 3000 epoch: 175 loss: 17.447380006333187 loss_input: 81.90395470390078
step: 4000 epoch: 175 loss: 17.50147969220406 loss_input: 82.08980192085768
step: 5000 epoch: 175 loss: 17.560843711422816 loss_input: 82.05402280139675
step: 6000 epoch: 175 loss: 17.55267387993076 loss_input: 81.95002157436652
step: 7000 epoch: 175 loss: 17.567621718405178 loss_input: 82.02975977796977
step: 8000 epoch: 175 loss: 17.54054642659309 loss_input: 81.99458821614583
step: 9000 epoch: 175 loss: 17.54159307861286 loss_input: 81.99541848458895
step: 10000 epoch: 175 loss: 17.54969356675325 loss_input: 82.14306234781795
step: 11000 epoch: 175 loss: 17.559189056723824 loss_input: 82.21232493601434
step: 12000 epoch: 175 loss: 17.578331535135288 loss_input: 82.24352190723201
step: 13000 epoch: 175 loss: 17.610296129492667 loss_input: 82.1577513822179
step: 14000 epoch: 175 loss: 17.59623043598954 loss_input: 82.12447295983462
step: 15000 epoch: 175 loss: 17.621715370126346 loss_input: 82.20806859539442
Save loss: 17.62757276341319 Name: 175_train_model.pth
step: 0 epoch: 176 loss: 13.321390151977539 loss_input: 74.62152099609375
step: 1000 epoch: 176 loss: 17.488833567479272 loss_input: 82.91381549358844
step: 2000 epoch: 176 loss: 17.561705108167885 loss_input: 82.47133818344734
step: 3000 epoch: 176 loss: 17.52528811962912 loss_input: 82.39777835692139
step: 4000 epoch: 176 loss: 17.537436059700553 loss_input: 82.2471303865928
step: 5000 epoch: 176 loss: 17.60083192559486 loss_input: 82.92564164169119
step: 6000 epoch: 176 loss: 17.53036333858679 loss_input: 82.56282650401366
step: 7000 epoch: 176 loss: 17.54910580694054 loss_input: 82.47544933417714
step: 8000 epoch: 176 loss: 17.548660809271723 loss_input: 82.48399884920391
step: 9000 epoch: 176 loss: 17.562727485070187 loss_input: 82.35624494956289
step: 10000 epoch: 176 loss: 17.576158228808314 loss_input: 82.35316262685255
step: 11000 epoch: 176 loss: 17.578925318310947 loss_input: 82.3068579569826
step: 12000 epoch: 176 loss: 17.593419172596747 loss_input: 82.36526065353115
step: 13000 epoch: 176 loss: 17.577232491043052 loss_input: 82.27117917105085
step: 14000 epoch: 176 loss: 17.588439048183755 loss_input: 82.26097172868651
step: 15000 epoch: 176 loss: 17.59889721919692 loss_input: 82.21580593357578
Save loss: 17.597562725424766 Name: 176_train_model.pth
step: 0 epoch: 177 loss: 24.03201675415039 loss_input: 58.70184326171875
step: 1000 epoch: 177 loss: 17.598968093330924 loss_input: 82.9714079865447
step: 2000 epoch: 177 loss: 17.554336364003554 loss_input: 82.65368027046047
step: 3000 epoch: 177 loss: 17.620485384676385 loss_input: 82.6444695030042
step: 4000 epoch: 177 loss: 17.589987726397467 loss_input: 82.26846829857477
step: 5000 epoch: 177 loss: 17.58969086981897 loss_input: 82.25344981052618
step: 6000 epoch: 177 loss: 17.586642828489854 loss_input: 82.36924040085115
step: 7000 epoch: 177 loss: 17.620884327050057 loss_input: 82.42447007953942
step: 8000 epoch: 177 loss: 17.62317698655345 loss_input: 82.22623586541428
step: 9000 epoch: 177 loss: 17.584262393498577 loss_input: 82.1265601565104
step: 10000 epoch: 177 loss: 17.583906719272417 loss_input: 82.05374582780979
step: 11000 epoch: 177 loss: 17.587117118300572 loss_input: 82.06797841154177
step: 12000 epoch: 177 loss: 17.600344627601448 loss_input: 82.21237459541132
step: 13000 epoch: 177 loss: 17.585591742721174 loss_input: 82.12426612747788
step: 14000 epoch: 177 loss: 17.586022349649067 loss_input: 82.11905644173436
step: 15000 epoch: 177 loss: 17.578229535111554 loss_input: 82.08571865065767
Save loss: 17.605656620576976 Name: 177_train_model.pth
step: 0 epoch: 178 loss: 17.620594024658203 loss_input: 87.88397216796875
step: 1000 epoch: 178 loss: 17.446226154293093 loss_input: 81.72398073118288
step: 2000 epoch: 178 loss: 17.66279029250443 loss_input: 82.91027154879592
step: 3000 epoch: 178 loss: 17.591415936928595 loss_input: 82.66128372248313
step: 4000 epoch: 178 loss: 17.612512010837968 loss_input: 82.54978252202086
step: 5000 epoch: 178 loss: 17.60421637806075 loss_input: 82.56056200869202
step: 6000 epoch: 178 loss: 17.59830652135866 loss_input: 82.39035429316468
step: 7000 epoch: 178 loss: 17.633405536638808 loss_input: 82.50302025945233
step: 8000 epoch: 178 loss: 17.63325407579234 loss_input: 82.48221174196487
step: 9000 epoch: 178 loss: 17.604633672331957 loss_input: 82.4080620816437
step: 10000 epoch: 178 loss: 17.61054327485323 loss_input: 82.42208879697968
step: 11000 epoch: 178 loss: 17.610959358686056 loss_input: 82.39959356167373
step: 12000 epoch: 178 loss: 17.603719656174007 loss_input: 82.32409894288594
step: 13000 epoch: 178 loss: 17.58948756498462 loss_input: 82.30578963971672
step: 14000 epoch: 178 loss: 17.594467037805924 loss_input: 82.31342168273555
step: 15000 epoch: 178 loss: 17.569243241290984 loss_input: 82.18653468349505
Save loss: 17.577003014832734 Name: 178_train_model.pth
step: 0 epoch: 179 loss: 17.723812103271484 loss_input: 83.35272216796875
step: 1000 epoch: 179 loss: 17.455109473351357 loss_input: 82.26803890689389
step: 2000 epoch: 179 loss: 17.440017537436802 loss_input: 81.95914994163074
step: 3000 epoch: 179 loss: 17.532291281425884 loss_input: 82.37784510062401
step: 4000 epoch: 179 loss: 17.564682583545512 loss_input: 82.58942358221479
step: 5000 epoch: 179 loss: 17.52490757799368 loss_input: 82.2094186265286
step: 6000 epoch: 179 loss: 17.58876673020317 loss_input: 82.39766791390888
step: 7000 epoch: 179 loss: 17.57750413867682 loss_input: 82.37126527788162
step: 8000 epoch: 179 loss: 17.565423944535485 loss_input: 82.42507428423254
step: 9000 epoch: 179 loss: 17.570183950137487 loss_input: 82.39422532831638
step: 10000 epoch: 179 loss: 17.583470955930608 loss_input: 82.29485160614797
step: 11000 epoch: 179 loss: 17.592682530387794 loss_input: 82.29161449189988
step: 12000 epoch: 179 loss: 17.58818461944457 loss_input: 82.18951653524316
step: 13000 epoch: 179 loss: 17.57852773546815 loss_input: 82.23954688093404
step: 14000 epoch: 179 loss: 17.58002920801593 loss_input: 82.25257597479784
step: 15000 epoch: 179 loss: 17.587219145637967 loss_input: 82.18494098277563
Save loss: 17.59882872225344 Name: 179_train_model.pth
step: 0 epoch: 180 loss: 22.334537506103516 loss_input: 83.72772216796875
step: 1000 epoch: 180 loss: 17.745520327831958 loss_input: 82.26088626949223
step: 2000 epoch: 180 loss: 17.64829223099975 loss_input: 82.29423590816896
step: 3000 epoch: 180 loss: 17.597727593005956 loss_input: 82.16645690450189
step: 4000 epoch: 180 loss: 17.572452264975023 loss_input: 81.90736782357145
step: 5000 epoch: 180 loss: 17.600354102057853 loss_input: 82.0167471022397
step: 6000 epoch: 180 loss: 17.619328465467294 loss_input: 82.32981241204266
step: 7000 epoch: 180 loss: 17.634204804292832 loss_input: 82.43053919368941
step: 8000 epoch: 180 loss: 17.6238410810488 loss_input: 82.42336518543092
step: 9000 epoch: 180 loss: 17.610483471466004 loss_input: 82.38613996366941
step: 10000 epoch: 180 loss: 17.58290035931328 loss_input: 82.36859152829834
step: 11000 epoch: 180 loss: 17.575058127173445 loss_input: 82.42827476443902
step: 12000 epoch: 180 loss: 17.59688762410185 loss_input: 82.51988463953687
step: 13000 epoch: 180 loss: 17.593423858164606 loss_input: 82.51360173691567
step: 14000 epoch: 180 loss: 17.60122160992957 loss_input: 82.36996427919769
step: 15000 epoch: 180 loss: 17.581455804341537 loss_input: 82.2412329290352
Save loss: 17.587641000628473 Name: 180_train_model.pth
step: 0 epoch: 181 loss: 14.547473907470703 loss_input: 65.853271484375
step: 1000 epoch: 181 loss: 17.620160465831166 loss_input: 81.77188643875655
step: 2000 epoch: 181 loss: 17.72231950145075 loss_input: 82.67768136195573
step: 3000 epoch: 181 loss: 17.65091354622121 loss_input: 82.70034687330785
step: 4000 epoch: 181 loss: 17.56353740893552 loss_input: 82.55245328682477
step: 5000 epoch: 181 loss: 17.557643719945663 loss_input: 82.55406384543404
step: 6000 epoch: 181 loss: 17.563933888786416 loss_input: 82.52397209921394
step: 7000 epoch: 181 loss: 17.57039990297063 loss_input: 82.55359808217966
step: 8000 epoch: 181 loss: 17.56593450539232 loss_input: 82.55998132527135
step: 9000 epoch: 181 loss: 17.562370872036666 loss_input: 82.39449785391685
step: 10000 epoch: 181 loss: 17.565748545255985 loss_input: 82.2072838492518
step: 11000 epoch: 181 loss: 17.57936483865954 loss_input: 82.33272226605997
step: 12000 epoch: 181 loss: 17.584146728119087 loss_input: 82.37538103615877
step: 13000 epoch: 181 loss: 17.60580005773021 loss_input: 82.4017423839993
step: 14000 epoch: 181 loss: 17.595464106483398 loss_input: 82.32295007230928
step: 15000 epoch: 181 loss: 17.589638231515995 loss_input: 82.2410628760817
Save loss: 17.58917189745605 Name: 181_train_model.pth
step: 0 epoch: 182 loss: 9.722969055175781 loss_input: 49.43768310546875
step: 1000 epoch: 182 loss: 17.346599475487128 loss_input: 82.81488140765485
step: 2000 epoch: 182 loss: 17.551696478992863 loss_input: 83.14508090193185
step: 3000 epoch: 182 loss: 17.500178948993167 loss_input: 82.43700081743586
step: 4000 epoch: 182 loss: 17.532174136989624 loss_input: 82.33290320913424
step: 5000 epoch: 182 loss: 17.56078624658598 loss_input: 82.38878440132287
step: 6000 epoch: 182 loss: 17.528992025638853 loss_input: 82.2846845268627
step: 7000 epoch: 182 loss: 17.575255333227936 loss_input: 82.36270978099668
step: 8000 epoch: 182 loss: 17.577642255031442 loss_input: 82.27753845832747
step: 9000 epoch: 182 loss: 17.56554703574196 loss_input: 82.18171822596969
step: 10000 epoch: 182 loss: 17.558469896113415 loss_input: 82.19128160426145
step: 11000 epoch: 182 loss: 17.552505928953607 loss_input: 82.22238842433892
step: 12000 epoch: 182 loss: 17.567727000462433 loss_input: 82.2193136584728
step: 13000 epoch: 182 loss: 17.568832426930875 loss_input: 82.22261696516134
step: 14000 epoch: 182 loss: 17.591858201874604 loss_input: 82.25337814179568
step: 15000 epoch: 182 loss: 17.57792164888249 loss_input: 82.23013656185942
Save loss: 17.575084498152137 Name: 182_train_model.pth
step: 0 epoch: 183 loss: 15.697357177734375 loss_input: 83.8323974609375
step: 1000 epoch: 183 loss: 17.0918271024744 loss_input: 80.48135818039383
step: 2000 epoch: 183 loss: 17.212253890592773 loss_input: 80.80317055827555
step: 3000 epoch: 183 loss: 17.322778935831256 loss_input: 81.66211069698772
step: 4000 epoch: 183 loss: 17.394711686920683 loss_input: 81.9159816979349
step: 5000 epoch: 183 loss: 17.42495318303893 loss_input: 81.90286676551409
step: 6000 epoch: 183 loss: 17.44434247130534 loss_input: 82.12487192889726
step: 7000 epoch: 183 loss: 17.480188616853564 loss_input: 82.04335489914666
step: 8000 epoch: 183 loss: 17.479107023879685 loss_input: 82.03669161868683
step: 9000 epoch: 183 loss: 17.48186419066052 loss_input: 82.1194866423262
step: 10000 epoch: 183 loss: 17.518695917931478 loss_input: 82.17525827983607
step: 11000 epoch: 183 loss: 17.518697227914597 loss_input: 82.06189980709317
step: 12000 epoch: 183 loss: 17.54704947805774 loss_input: 82.19426396778947
step: 13000 epoch: 183 loss: 17.552930474528882 loss_input: 82.19815730351722
step: 14000 epoch: 183 loss: 17.554436163190143 loss_input: 82.19824995148923
step: 15000 epoch: 183 loss: 17.566025726255294 loss_input: 82.23935789941478
Save loss: 17.574235634893178 Name: 183_train_model.pth
step: 0 epoch: 184 loss: 18.2557315826416 loss_input: 65.749267578125
step: 1000 epoch: 184 loss: 17.422075547419347 loss_input: 80.99818010358781
step: 2000 epoch: 184 loss: 17.45232448644605 loss_input: 81.40960107714697
step: 3000 epoch: 184 loss: 17.510788404397353 loss_input: 81.41006350747668
step: 4000 epoch: 184 loss: 17.524611774548028 loss_input: 81.84744879759064
step: 5000 epoch: 184 loss: 17.487158545301668 loss_input: 81.80814508485022
step: 6000 epoch: 184 loss: 17.482985515154276 loss_input: 81.86433641268123
step: 7000 epoch: 184 loss: 17.49699270526029 loss_input: 82.02955207884644
step: 8000 epoch: 184 loss: 17.505151457346138 loss_input: 81.98187452673152
step: 9000 epoch: 184 loss: 17.485141519413645 loss_input: 81.88650926671126
step: 10000 epoch: 184 loss: 17.482774568860407 loss_input: 81.92980177079949
step: 11000 epoch: 184 loss: 17.494742600422168 loss_input: 81.96623477566493
step: 12000 epoch: 184 loss: 17.52126260257127 loss_input: 82.0406381794437
step: 13000 epoch: 184 loss: 17.54095012875174 loss_input: 82.08061156879158
step: 14000 epoch: 184 loss: 17.55652323924391 loss_input: 82.1225361518541
step: 15000 epoch: 184 loss: 17.567454282700734 loss_input: 82.22186247153836
Save loss: 17.57006666509807 Name: 184_train_model.pth
step: 0 epoch: 185 loss: 20.20514488220215 loss_input: 95.14263916015625
step: 1000 epoch: 185 loss: 17.12329546626393 loss_input: 81.11755297924732
step: 2000 epoch: 185 loss: 17.273884975808908 loss_input: 81.87740660309494
step: 3000 epoch: 185 loss: 17.425892852775576 loss_input: 82.15728430285608
step: 4000 epoch: 185 loss: 17.45013592100775 loss_input: 82.26384980903212
step: 5000 epoch: 185 loss: 17.45465494165228 loss_input: 81.93953174697091
step: 6000 epoch: 185 loss: 17.465187870250347 loss_input: 81.9614716618682
step: 7000 epoch: 185 loss: 17.511654183381353 loss_input: 82.16878726274315
step: 8000 epoch: 185 loss: 17.519434063632165 loss_input: 82.18761239744934
step: 9000 epoch: 185 loss: 17.515829166535895 loss_input: 82.24829700996764
step: 10000 epoch: 185 loss: 17.54747411690048 loss_input: 82.35403690465914
step: 11000 epoch: 185 loss: 17.54688810647677 loss_input: 82.34496477215671
step: 12000 epoch: 185 loss: 17.53953939475136 loss_input: 82.29296996042973
step: 13000 epoch: 185 loss: 17.534196481641626 loss_input: 82.18577629789445
step: 14000 epoch: 185 loss: 17.551654935981468 loss_input: 82.30804305604148
step: 15000 epoch: 185 loss: 17.556305905070577 loss_input: 82.23377766840284
Save loss: 17.555858139142394 Name: 185_train_model.pth
step: 0 epoch: 186 loss: 33.63841247558594 loss_input: 79.25714111328125
step: 1000 epoch: 186 loss: 17.74003613030875 loss_input: 84.06554925787104
step: 2000 epoch: 186 loss: 17.67384944898614 loss_input: 83.35887241315865
step: 3000 epoch: 186 loss: 17.605933355752803 loss_input: 83.23669372578932
step: 4000 epoch: 186 loss: 17.604445602857478 loss_input: 83.06549282784285
step: 5000 epoch: 186 loss: 17.561959240251102 loss_input: 83.21523527325785
step: 6000 epoch: 186 loss: 17.530824675557614 loss_input: 82.81287135737794
step: 7000 epoch: 186 loss: 17.531361156796542 loss_input: 82.73114193263964
step: 8000 epoch: 186 loss: 17.524781571762396 loss_input: 82.59763075575026
step: 9000 epoch: 186 loss: 17.543164745911852 loss_input: 82.46683513301039
step: 10000 epoch: 186 loss: 17.558984908803964 loss_input: 82.51745630490662
step: 11000 epoch: 186 loss: 17.56637890890895 loss_input: 82.50920030219199
step: 12000 epoch: 186 loss: 17.571701448566266 loss_input: 82.5560310515709
step: 13000 epoch: 186 loss: 17.58111187458295 loss_input: 82.48521062438556
step: 14000 epoch: 186 loss: 17.57760712087397 loss_input: 82.45937050813335
step: 15000 epoch: 186 loss: 17.56732444505391 loss_input: 82.34970744788889
Save loss: 17.554546764150263 Name: 186_train_model.pth
step: 0 epoch: 187 loss: 12.845269203186035 loss_input: 96.56951904296875
step: 1000 epoch: 187 loss: 17.330092723552998 loss_input: 81.55286449390454
step: 2000 epoch: 187 loss: 17.412313856642463 loss_input: 81.96360624009284
step: 3000 epoch: 187 loss: 17.45689762707195 loss_input: 82.53256789973004
step: 4000 epoch: 187 loss: 17.410348667201028 loss_input: 82.29221613053768
step: 5000 epoch: 187 loss: 17.42180054055717 loss_input: 82.02000254246026
step: 6000 epoch: 187 loss: 17.484511746424197 loss_input: 82.3658416346299
step: 7000 epoch: 187 loss: 17.494327489247816 loss_input: 82.37092872579308
step: 8000 epoch: 187 loss: 17.516480789111863 loss_input: 82.380722764879
step: 9000 epoch: 187 loss: 17.501171997581213 loss_input: 82.23273718149156
step: 10000 epoch: 187 loss: 17.502157744521224 loss_input: 82.06496526250696
step: 11000 epoch: 187 loss: 17.52179914164051 loss_input: 82.16754505471981
step: 12000 epoch: 187 loss: 17.5335942353321 loss_input: 82.21958512531182
step: 13000 epoch: 187 loss: 17.54266438290904 loss_input: 82.26064162338324
step: 14000 epoch: 187 loss: 17.539256002432822 loss_input: 82.27090422496329
step: 15000 epoch: 187 loss: 17.552751731764484 loss_input: 82.27979314318499
Save loss: 17.559853942632675 Name: 187_train_model.pth
step: 0 epoch: 188 loss: 24.637985229492188 loss_input: 58.70184326171875
step: 1000 epoch: 188 loss: 17.271984217526555 loss_input: 81.74638295054555
step: 2000 epoch: 188 loss: 17.353940120403436 loss_input: 82.28465241256325
step: 3000 epoch: 188 loss: 17.35609383934222 loss_input: 81.76223613531818
step: 4000 epoch: 188 loss: 17.44826611171094 loss_input: 81.72890673205723
step: 5000 epoch: 188 loss: 17.39167937362845 loss_input: 81.5233179682423
step: 6000 epoch: 188 loss: 17.47136912177432 loss_input: 82.0100759472217
step: 7000 epoch: 188 loss: 17.517470063762723 loss_input: 82.02583267436404
step: 8000 epoch: 188 loss: 17.52922855948019 loss_input: 82.18065404635699
step: 9000 epoch: 188 loss: 17.555052597619948 loss_input: 82.10814744135418
step: 10000 epoch: 188 loss: 17.543910479667176 loss_input: 82.1395079547221
step: 11000 epoch: 188 loss: 17.55362813599098 loss_input: 82.16717401689513
step: 12000 epoch: 188 loss: 17.553670677023423 loss_input: 82.20096483048613
step: 13000 epoch: 188 loss: 17.553723949513504 loss_input: 82.23443876274843
step: 14000 epoch: 188 loss: 17.551355734713358 loss_input: 82.23910401925113
step: 15000 epoch: 188 loss: 17.551897050905097 loss_input: 82.24740390842433
Save loss: 17.546751094281674 Name: 188_train_model.pth
step: 0 epoch: 189 loss: 16.55620765686035 loss_input: 106.7156982421875
step: 1000 epoch: 189 loss: 17.538490521443354 loss_input: 82.8815734436462
step: 2000 epoch: 189 loss: 17.5386438568731 loss_input: 83.36625812459981
step: 3000 epoch: 189 loss: 17.48015809329261 loss_input: 82.77772946534098
step: 4000 epoch: 189 loss: 17.464817570257534 loss_input: 82.75543162549206
step: 5000 epoch: 189 loss: 17.524105337232953 loss_input: 82.70975701100015
step: 6000 epoch: 189 loss: 17.510557423589866 loss_input: 82.37004503562557
step: 7000 epoch: 189 loss: 17.495609624064695 loss_input: 82.1518394100787
step: 8000 epoch: 189 loss: 17.510794091591194 loss_input: 82.18476939192415
step: 9000 epoch: 189 loss: 17.509750301421263 loss_input: 82.16748384565344
step: 10000 epoch: 189 loss: 17.504011121514726 loss_input: 82.08076587971085
step: 11000 epoch: 189 loss: 17.5005686328407 loss_input: 82.13622015474623
step: 12000 epoch: 189 loss: 17.503909221322086 loss_input: 82.15158467832758
step: 13000 epoch: 189 loss: 17.51940769289523 loss_input: 82.18505710085101
step: 14000 epoch: 189 loss: 17.526614358191676 loss_input: 82.23054468538938
step: 15000 epoch: 189 loss: 17.53350828563473 loss_input: 82.23242611055747
Save loss: 17.548155451878905 Name: 189_train_model.pth
step: 0 epoch: 190 loss: 12.930696487426758 loss_input: 59.0985107421875
step: 1000 epoch: 190 loss: 17.37780685310478 loss_input: 82.50221269209307
step: 2000 epoch: 190 loss: 17.35224550500743 loss_input: 82.13048746012736
step: 3000 epoch: 190 loss: 17.5191896444001 loss_input: 82.60108237439734
step: 4000 epoch: 190 loss: 17.553050443191164 loss_input: 82.95638088463724
step: 5000 epoch: 190 loss: 17.55696838251521 loss_input: 82.70657067317005
step: 6000 epoch: 190 loss: 17.585724359829214 loss_input: 82.80005394405295
step: 7000 epoch: 190 loss: 17.55074495479833 loss_input: 82.56848944849396
step: 8000 epoch: 190 loss: 17.556526028473755 loss_input: 82.54286065499137
step: 9000 epoch: 190 loss: 17.557136383179547 loss_input: 82.43473096630757
step: 10000 epoch: 190 loss: 17.544599525738498 loss_input: 82.34532946851799
step: 11000 epoch: 190 loss: 17.5394559459636 loss_input: 82.3844125909097
step: 12000 epoch: 190 loss: 17.543288215439812 loss_input: 82.25932020720279
step: 13000 epoch: 190 loss: 17.534261984656787 loss_input: 82.23964473154258
step: 14000 epoch: 190 loss: 17.528348261744163 loss_input: 82.16437487741528
step: 15000 epoch: 190 loss: 17.536920928452844 loss_input: 82.19993728119948
Save loss: 17.542261602431537 Name: 190_train_model.pth
step: 0 epoch: 191 loss: 11.71826171875 loss_input: 57.301025390625
step: 1000 epoch: 191 loss: 17.663311264493487 loss_input: 82.75662252786276
step: 2000 epoch: 191 loss: 17.539952292792623 loss_input: 82.03702625615902
step: 3000 epoch: 191 loss: 17.532566032819613 loss_input: 82.45961982399096
step: 4000 epoch: 191 loss: 17.57005878109778 loss_input: 82.67366164745435
step: 5000 epoch: 191 loss: 17.560346215182697 loss_input: 82.7102747541312
step: 6000 epoch: 191 loss: 17.556040100009614 loss_input: 82.46772649911539
step: 7000 epoch: 191 loss: 17.523414152858088 loss_input: 82.30464149478367
step: 8000 epoch: 191 loss: 17.50242375764321 loss_input: 82.27808273996268
step: 9000 epoch: 191 loss: 17.494911087233415 loss_input: 82.2767528393655
step: 10000 epoch: 191 loss: 17.5129632663517 loss_input: 82.28473734657784
step: 11000 epoch: 191 loss: 17.497181182470705 loss_input: 82.12123271441766
step: 12000 epoch: 191 loss: 17.51408106598712 loss_input: 82.2488001844166
step: 13000 epoch: 191 loss: 17.497440290986532 loss_input: 82.24760567170915
step: 14000 epoch: 191 loss: 17.515534286047423 loss_input: 82.23653501504218
step: 15000 epoch: 191 loss: 17.524261489183726 loss_input: 82.24567774176383
Save loss: 17.53900977270305 Name: 191_train_model.pth
step: 0 epoch: 192 loss: 19.762910842895508 loss_input: 83.86468505859375
step: 1000 epoch: 192 loss: 17.53829934856632 loss_input: 81.09388267982018
step: 2000 epoch: 192 loss: 17.60992624210394 loss_input: 82.07207352134479
step: 3000 epoch: 192 loss: 17.496159569417106 loss_input: 82.19167811169063
step: 4000 epoch: 192 loss: 17.619365576534562 loss_input: 82.58160472089008
step: 5000 epoch: 192 loss: 17.60920138140722 loss_input: 82.42742677050528
step: 6000 epoch: 192 loss: 17.602416850749382 loss_input: 82.65224474713199
step: 7000 epoch: 192 loss: 17.580083480887268 loss_input: 82.53731827721597
step: 8000 epoch: 192 loss: 17.57022607721339 loss_input: 82.63898233729084
step: 9000 epoch: 192 loss: 17.54907547766706 loss_input: 82.52135135348459
step: 10000 epoch: 192 loss: 17.53563754123016 loss_input: 82.47224227772536
step: 11000 epoch: 192 loss: 17.54281832250549 loss_input: 82.38969905878585
step: 12000 epoch: 192 loss: 17.535756575943044 loss_input: 82.3647813945599
step: 13000 epoch: 192 loss: 17.55173641930598 loss_input: 82.37135644388533
step: 14000 epoch: 192 loss: 17.539143867538993 loss_input: 82.2906500897606
step: 15000 epoch: 192 loss: 17.536714542421468 loss_input: 82.22938776737801
Save loss: 17.537572305917738 Name: 192_train_model.pth
step: 0 epoch: 193 loss: 20.26915740966797 loss_input: 129.32275390625
step: 1000 epoch: 193 loss: 17.360331152821637 loss_input: 79.97596568470593
step: 2000 epoch: 193 loss: 17.423854757463378 loss_input: 81.38544353921672
step: 3000 epoch: 193 loss: 17.429500693362858 loss_input: 81.29984280142297
step: 4000 epoch: 193 loss: 17.392188778283504 loss_input: 81.34672732878792
step: 5000 epoch: 193 loss: 17.35070647656548 loss_input: 81.52255970924956
step: 6000 epoch: 193 loss: 17.35255647023466 loss_input: 81.37794444537565
step: 7000 epoch: 193 loss: 17.400200341126183 loss_input: 81.63755897943709
step: 8000 epoch: 193 loss: 17.421629958474 loss_input: 81.70337560954295
step: 9000 epoch: 193 loss: 17.43178461432629 loss_input: 81.93309585339043
step: 10000 epoch: 193 loss: 17.437053782667427 loss_input: 82.01598675581661
step: 11000 epoch: 193 loss: 17.46890685950807 loss_input: 82.04926502486812
step: 12000 epoch: 193 loss: 17.48804257595681 loss_input: 82.1387655459864
step: 13000 epoch: 193 loss: 17.496009508175774 loss_input: 82.13582255992327
step: 14000 epoch: 193 loss: 17.514059785672337 loss_input: 82.14217868715292
step: 15000 epoch: 193 loss: 17.522981365301888 loss_input: 82.2489546880143
Save loss: 17.52494263456762 Name: 193_train_model.pth
step: 0 epoch: 194 loss: 18.173168182373047 loss_input: 73.95697021484375
step: 1000 epoch: 194 loss: 17.4866494370269 loss_input: 83.1187733775014
step: 2000 epoch: 194 loss: 17.49452536062024 loss_input: 83.27987291144467
step: 3000 epoch: 194 loss: 17.580292663745823 loss_input: 83.71083896106897
step: 4000 epoch: 194 loss: 17.582235729596043 loss_input: 83.33143607713794
step: 5000 epoch: 194 loss: 17.551732356871064 loss_input: 82.99566654052002
step: 6000 epoch: 194 loss: 17.56245069626152 loss_input: 82.69057873085606
step: 7000 epoch: 194 loss: 17.5443576613659 loss_input: 82.49913610974102
step: 8000 epoch: 194 loss: 17.534569534953274 loss_input: 82.4855966337948
step: 9000 epoch: 194 loss: 17.515878048145485 loss_input: 82.35930875030813
step: 10000 epoch: 194 loss: 17.508498590644056 loss_input: 82.29343668416361
step: 11000 epoch: 194 loss: 17.50761754956422 loss_input: 82.19927189569236
step: 12000 epoch: 194 loss: 17.526816412902118 loss_input: 82.2844338828291
step: 13000 epoch: 194 loss: 17.525603349883138 loss_input: 82.30663521206752
step: 14000 epoch: 194 loss: 17.518186616536575 loss_input: 82.27970474381573
step: 15000 epoch: 194 loss: 17.52347440929717 loss_input: 82.21931132732428
Save loss: 17.521264078095555 Name: 194_train_model.pth
step: 0 epoch: 195 loss: 19.255474090576172 loss_input: 70.63311767578125
step: 1000 epoch: 195 loss: 17.696959293567456 loss_input: 83.16771674370551
step: 2000 epoch: 195 loss: 17.59554911541498 loss_input: 82.9412159459821
step: 3000 epoch: 195 loss: 17.568782712967543 loss_input: 83.00478933779885
step: 4000 epoch: 195 loss: 17.51535350791933 loss_input: 82.647690860488
step: 5000 epoch: 195 loss: 17.54239004494023 loss_input: 82.63201253792211
step: 6000 epoch: 195 loss: 17.508373104757197 loss_input: 82.44649515404659
step: 7000 epoch: 195 loss: 17.513069731697765 loss_input: 82.43671195688562
step: 8000 epoch: 195 loss: 17.4902004827903 loss_input: 82.15995318852966
step: 9000 epoch: 195 loss: 17.475437886528724 loss_input: 82.1488566225389
step: 10000 epoch: 195 loss: 17.474669841394082 loss_input: 82.21412377194898
step: 11000 epoch: 195 loss: 17.459170421332992 loss_input: 82.21728679850128
step: 12000 epoch: 195 loss: 17.486894674593582 loss_input: 82.2861100431979
step: 13000 epoch: 195 loss: 17.500634264408667 loss_input: 82.28802247990389
step: 14000 epoch: 195 loss: 17.505868202328266 loss_input: 82.3153271252458
step: 15000 epoch: 195 loss: 17.511662835860584 loss_input: 82.23054626761862
Save loss: 17.5362355081141 Name: 195_train_model.pth
step: 0 epoch: 196 loss: 20.053207397460938 loss_input: 84.9686279296875
step: 1000 epoch: 196 loss: 17.78437646142729 loss_input: 84.92771705833229
step: 2000 epoch: 196 loss: 17.699022954252587 loss_input: 82.87801265382933
step: 3000 epoch: 196 loss: 17.657392362561556 loss_input: 82.61261128696033
step: 4000 epoch: 196 loss: 17.56862381493202 loss_input: 82.61043056801718
step: 5000 epoch: 196 loss: 17.541055414491215 loss_input: 82.82919418313602
step: 6000 epoch: 196 loss: 17.522656894489003 loss_input: 82.7774906676524
step: 7000 epoch: 196 loss: 17.515831987204987 loss_input: 82.67338828130579
step: 8000 epoch: 196 loss: 17.554629416663026 loss_input: 82.64550417373366
step: 9000 epoch: 196 loss: 17.56066267552104 loss_input: 82.63797748058376
step: 10000 epoch: 196 loss: 17.580983379699862 loss_input: 82.42446734330473
step: 11000 epoch: 196 loss: 17.570151976071404 loss_input: 82.49797995880878
step: 12000 epoch: 196 loss: 17.55722793308201 loss_input: 82.30447726666495
step: 13000 epoch: 196 loss: 17.545697981903583 loss_input: 82.21815812188876
step: 14000 epoch: 196 loss: 17.551971300492397 loss_input: 82.24393291976075
step: 15000 epoch: 196 loss: 17.53661959624737 loss_input: 82.2227270867489
Save loss: 17.532882779195905 Name: 196_train_model.pth
step: 0 epoch: 197 loss: 19.332138061523438 loss_input: 108.34014892578125
step: 1000 epoch: 197 loss: 17.349130123883455 loss_input: 81.85851453234265
step: 2000 epoch: 197 loss: 17.319670708044836 loss_input: 82.0232680290714
step: 3000 epoch: 197 loss: 17.268487031838767 loss_input: 81.96076682121584
step: 4000 epoch: 197 loss: 17.31642258420523 loss_input: 82.073482621166
step: 5000 epoch: 197 loss: 17.345501083823304 loss_input: 81.92676243803973
step: 6000 epoch: 197 loss: 17.350630514026978 loss_input: 81.86054350642081
step: 7000 epoch: 197 loss: 17.34862218211675 loss_input: 81.82868748939883
step: 8000 epoch: 197 loss: 17.376408971677556 loss_input: 81.97627847025714
step: 9000 epoch: 197 loss: 17.385720369537967 loss_input: 81.9617946299695
step: 10000 epoch: 197 loss: 17.427236953552647 loss_input: 82.04601720003781
step: 11000 epoch: 197 loss: 17.44194755826405 loss_input: 82.11677819661104
step: 12000 epoch: 197 loss: 17.47889634949219 loss_input: 82.16079606891483
step: 13000 epoch: 197 loss: 17.482758600522384 loss_input: 82.23462685863106
step: 14000 epoch: 197 loss: 17.48529913881644 loss_input: 82.18349995438383
step: 15000 epoch: 197 loss: 17.49047719875214 loss_input: 82.2381636675068
Save loss: 17.51117638280988 Name: 197_train_model.pth
step: 0 epoch: 198 loss: 16.886735916137695 loss_input: 73.67852783203125
step: 1000 epoch: 198 loss: 17.24870075164856 loss_input: 80.84330690061891
step: 2000 epoch: 198 loss: 17.391954604296135 loss_input: 81.97567829854604
step: 3000 epoch: 198 loss: 17.381750987554064 loss_input: 81.86276144442738
step: 4000 epoch: 198 loss: 17.398883642598527 loss_input: 81.64832418604334
step: 5000 epoch: 198 loss: 17.42125583691398 loss_input: 81.38707411105669
step: 6000 epoch: 198 loss: 17.502652793621106 loss_input: 81.66526844524935
step: 7000 epoch: 198 loss: 17.49757996185765 loss_input: 81.76485564841184
step: 8000 epoch: 198 loss: 17.493526923777146 loss_input: 81.85629551280455
step: 9000 epoch: 198 loss: 17.488722148384998 loss_input: 82.00242134147638
step: 10000 epoch: 198 loss: 17.492459044863182 loss_input: 82.10076387991084
step: 11000 epoch: 198 loss: 17.501492350808643 loss_input: 82.198395283046
step: 12000 epoch: 198 loss: 17.494079665336674 loss_input: 82.17942528938832
step: 13000 epoch: 198 loss: 17.509419321216132 loss_input: 82.23941217262134
step: 14000 epoch: 198 loss: 17.48888276718436 loss_input: 82.16002563537593
step: 15000 epoch: 198 loss: 17.494085644540863 loss_input: 82.10876219905653
Save loss: 17.51142448928952 Name: 198_train_model.pth
step: 0 epoch: 199 loss: 19.33702850341797 loss_input: 138.46990966796875
step: 1000 epoch: 199 loss: 17.641122545038428 loss_input: 82.67126178265094
step: 2000 epoch: 199 loss: 17.62709062758355 loss_input: 82.9605516150616
step: 3000 epoch: 199 loss: 17.535303138248604 loss_input: 83.06094129512168
step: 4000 epoch: 199 loss: 17.576681947028806 loss_input: 83.52235651719394
step: 5000 epoch: 199 loss: 17.562537563631377 loss_input: 83.09980156068396
step: 6000 epoch: 199 loss: 17.522539507327647 loss_input: 82.80352569615675
step: 7000 epoch: 199 loss: 17.4964406667071 loss_input: 82.72602954739725
step: 8000 epoch: 199 loss: 17.50623754036008 loss_input: 82.72020075401952
step: 9000 epoch: 199 loss: 17.486751454258716 loss_input: 82.5292964071158
step: 10000 epoch: 199 loss: 17.4857265988346 loss_input: 82.39961901026695
step: 11000 epoch: 199 loss: 17.48791709007778 loss_input: 82.30606447775877
step: 12000 epoch: 199 loss: 17.50879885927656 loss_input: 82.34744500966879
step: 13000 epoch: 199 loss: 17.508255031895395 loss_input: 82.33648530942331
step: 14000 epoch: 199 loss: 17.511501191267072 loss_input: 82.26809504277178
step: 15000 epoch: 199 loss: 17.50517521178863 loss_input: 82.25590569759542
Save loss: 17.509307388409972 Name: 199_train_model.pth
step: 0 epoch: 200 loss: 15.364856719970703 loss_input: 78.8470458984375
step: 1000 epoch: 200 loss: 17.340425323177648 loss_input: 82.43435092739291
step: 2000 epoch: 200 loss: 17.368311126848152 loss_input: 82.10441736529197
step: 3000 epoch: 200 loss: 17.44357497769171 loss_input: 82.28764150556586
step: 4000 epoch: 200 loss: 17.39121229247551 loss_input: 82.14127416021971
step: 5000 epoch: 200 loss: 17.39814419060844 loss_input: 82.45385537150382
step: 6000 epoch: 200 loss: 17.412303339022632 loss_input: 82.32182292941411
step: 7000 epoch: 200 loss: 17.440282286584182 loss_input: 82.29841746937937
step: 8000 epoch: 200 loss: 17.440331294318764 loss_input: 82.25071033753838
step: 9000 epoch: 200 loss: 17.470969039987768 loss_input: 82.17671331603346
step: 10000 epoch: 200 loss: 17.487938887833856 loss_input: 82.16128692916841
step: 11000 epoch: 200 loss: 17.45468934179815 loss_input: 82.11527652981476
step: 12000 epoch: 200 loss: 17.46974215574259 loss_input: 82.11865906722943
step: 13000 epoch: 200 loss: 17.492161797244826 loss_input: 82.29335004015617
step: 14000 epoch: 200 loss: 17.50220201323862 loss_input: 82.29780279889327
step: 15000 epoch: 200 loss: 17.505173036348232 loss_input: 82.3461549544158
Save loss: 17.499857148990035 Name: 200_train_model.pth
step: 0 epoch: 201 loss: 20.90252685546875 loss_input: 89.87969970703125
step: 1000 epoch: 201 loss: 17.50172346955413 loss_input: 82.41174407867523
step: 2000 epoch: 201 loss: 17.531183555208404 loss_input: 83.0868429005712
step: 3000 epoch: 201 loss: 17.523002115101544 loss_input: 82.94799743672682
step: 4000 epoch: 201 loss: 17.510849261695043 loss_input: 82.63954413923375
step: 5000 epoch: 201 loss: 17.488147901597202 loss_input: 82.55105664658085
step: 6000 epoch: 201 loss: 17.550483402699715 loss_input: 82.57632789554606
step: 7000 epoch: 201 loss: 17.568388028956026 loss_input: 82.69321535260588
step: 8000 epoch: 201 loss: 17.544171409656997 loss_input: 82.61196906661081
step: 9000 epoch: 201 loss: 17.50961299594701 loss_input: 82.6051646049202
step: 10000 epoch: 201 loss: 17.507387372901732 loss_input: 82.40995011216651
step: 11000 epoch: 201 loss: 17.504579650674145 loss_input: 82.3260956767602
step: 12000 epoch: 201 loss: 17.4958062544434 loss_input: 82.22384618888367
step: 13000 epoch: 201 loss: 17.502908755720473 loss_input: 82.19795568704
step: 14000 epoch: 201 loss: 17.497048944534978 loss_input: 82.24913245594065
step: 15000 epoch: 201 loss: 17.512938524085627 loss_input: 82.25361233648877
Save loss: 17.504863813787697 Name: 201_train_model.pth
step: 0 epoch: 202 loss: 16.367000579833984 loss_input: 135.3406982421875
step: 1000 epoch: 202 loss: 17.43916722754022 loss_input: 82.55122184943963
step: 2000 epoch: 202 loss: 17.35173298942036 loss_input: 82.69763470315624
step: 3000 epoch: 202 loss: 17.362991503738076 loss_input: 82.77056738330062
step: 4000 epoch: 202 loss: 17.502307259658075 loss_input: 82.82944299935758
step: 5000 epoch: 202 loss: 17.495010788405903 loss_input: 82.94336092498297
step: 6000 epoch: 202 loss: 17.47569337035791 loss_input: 82.703941148155
step: 7000 epoch: 202 loss: 17.447133769241166 loss_input: 82.41123980928226
step: 8000 epoch: 202 loss: 17.465035336000625 loss_input: 82.14029076748707
step: 9000 epoch: 202 loss: 17.478806353717257 loss_input: 82.28055435111159
step: 10000 epoch: 202 loss: 17.48868404830793 loss_input: 82.37843105337903
step: 11000 epoch: 202 loss: 17.49985353875817 loss_input: 82.31866933328327
step: 12000 epoch: 202 loss: 17.49869606427556 loss_input: 82.28283068899532
step: 13000 epoch: 202 loss: 17.499448217013168 loss_input: 82.28446738509598
step: 14000 epoch: 202 loss: 17.517075418293558 loss_input: 82.31890684740432
step: 15000 epoch: 202 loss: 17.516211875398415 loss_input: 82.20105422333823
Save loss: 17.53003732661903 Name: 202_train_model.pth
step: 0 epoch: 203 loss: 15.363542556762695 loss_input: 83.8323974609375
step: 1000 epoch: 203 loss: 17.578623702118804 loss_input: 80.6215517880557
step: 2000 epoch: 203 loss: 17.373000183801302 loss_input: 81.69552076988849
step: 3000 epoch: 203 loss: 17.363408843901347 loss_input: 81.70865235269883
step: 4000 epoch: 203 loss: 17.4357680600454 loss_input: 81.53142804081695
step: 5000 epoch: 203 loss: 17.43066427698614 loss_input: 81.68041092709193
step: 6000 epoch: 203 loss: 17.430975746540323 loss_input: 81.85703426463428
step: 7000 epoch: 203 loss: 17.41561515994318 loss_input: 81.87846539501054
step: 8000 epoch: 203 loss: 17.429853646520705 loss_input: 81.86971701474327
step: 9000 epoch: 203 loss: 17.398510696887705 loss_input: 81.8560429252978
step: 10000 epoch: 203 loss: 17.418239534360126 loss_input: 81.94269386537908
step: 11000 epoch: 203 loss: 17.4243817965493 loss_input: 81.99611565962931
step: 12000 epoch: 203 loss: 17.462370733173376 loss_input: 82.10711295604109
step: 13000 epoch: 203 loss: 17.457765608473508 loss_input: 82.01786061622813
step: 14000 epoch: 203 loss: 17.46507753383159 loss_input: 82.1540869770114
step: 15000 epoch: 203 loss: 17.473555829110015 loss_input: 82.1475357209259
Save loss: 17.48912220568955 Name: 203_train_model.pth
step: 0 epoch: 204 loss: 17.440227508544922 loss_input: 78.832275390625
step: 1000 epoch: 204 loss: 17.443241848693148 loss_input: 81.844504860374
step: 2000 epoch: 204 loss: 17.465408006350675 loss_input: 81.86341817494574
step: 3000 epoch: 204 loss: 17.548655500732952 loss_input: 82.36424205033869
step: 4000 epoch: 204 loss: 17.54098694338199 loss_input: 82.034302688366
step: 5000 epoch: 204 loss: 17.481084592913035 loss_input: 81.93543251798859
step: 6000 epoch: 204 loss: 17.484553099632105 loss_input: 82.05141034640386
step: 7000 epoch: 204 loss: 17.497818175800663 loss_input: 81.95619133441316
step: 8000 epoch: 204 loss: 17.471447426771284 loss_input: 82.01877990196294
step: 9000 epoch: 204 loss: 17.472371218270982 loss_input: 82.01206818491522
step: 10000 epoch: 204 loss: 17.465991797870117 loss_input: 82.02264551987184
step: 11000 epoch: 204 loss: 17.477727513152484 loss_input: 82.09115967040994
step: 12000 epoch: 204 loss: 17.481833682577964 loss_input: 82.12331961838467
step: 13000 epoch: 204 loss: 17.501956982499276 loss_input: 82.19413505808298
step: 14000 epoch: 204 loss: 17.494461387099985 loss_input: 82.23086492269944
step: 15000 epoch: 204 loss: 17.489385500406236 loss_input: 82.20507562272542
Save loss: 17.492763611078264 Name: 204_train_model.pth
step: 0 epoch: 205 loss: 19.705978393554688 loss_input: 97.28253173828125
step: 1000 epoch: 205 loss: 17.407306638273685 loss_input: 82.48626532159247
step: 2000 epoch: 205 loss: 17.40070785873238 loss_input: 82.1996519440475
step: 3000 epoch: 205 loss: 17.29546893926034 loss_input: 81.80753896650336
step: 4000 epoch: 205 loss: 17.352247160454148 loss_input: 82.04908001515246
step: 5000 epoch: 205 loss: 17.356882287845256 loss_input: 82.01666904167995
step: 6000 epoch: 205 loss: 17.359992001577528 loss_input: 81.83948984576789
step: 7000 epoch: 205 loss: 17.367743479832633 loss_input: 81.94271028036731
step: 8000 epoch: 205 loss: 17.391659515140326 loss_input: 82.12091696850882
step: 9000 epoch: 205 loss: 17.416820440964095 loss_input: 82.19376660378664
step: 10000 epoch: 205 loss: 17.418462997817382 loss_input: 82.18134573261423
step: 11000 epoch: 205 loss: 17.42336837996809 loss_input: 82.09941741468992
step: 12000 epoch: 205 loss: 17.427747630147216 loss_input: 82.11938200910015
step: 13000 epoch: 205 loss: 17.44373210931996 loss_input: 82.18703606520953
step: 14000 epoch: 205 loss: 17.44769110401037 loss_input: 82.2102609648976
step: 15000 epoch: 205 loss: 17.47270027978143 loss_input: 82.21788047382128
Save loss: 17.47744044430554 Name: 205_train_model.pth
step: 0 epoch: 206 loss: 16.949216842651367 loss_input: 57.3858642578125
step: 1000 epoch: 206 loss: 17.272605640666708 loss_input: 83.27112688337054
step: 2000 epoch: 206 loss: 17.293504947069465 loss_input: 82.20929013032546
step: 3000 epoch: 206 loss: 17.309954107145355 loss_input: 82.46457331198845
step: 4000 epoch: 206 loss: 17.39349182436866 loss_input: 82.41004450730578
step: 5000 epoch: 206 loss: 17.39942856641608 loss_input: 82.30428030604817
step: 6000 epoch: 206 loss: 17.429739028687198 loss_input: 82.44722194128127
step: 7000 epoch: 206 loss: 17.41084286622057 loss_input: 82.19426317654955
step: 8000 epoch: 206 loss: 17.52130564742201 loss_input: 82.25496355954773
step: 9000 epoch: 206 loss: 17.510440224608427 loss_input: 82.1114469608086
step: 10000 epoch: 206 loss: 17.505667389422555 loss_input: 82.15318818459951
step: 11000 epoch: 206 loss: 17.488504505560577 loss_input: 82.11314748974347
step: 12000 epoch: 206 loss: 17.490010439242017 loss_input: 82.1137036645465
step: 13000 epoch: 206 loss: 17.50921327062281 loss_input: 82.2203090372255
step: 14000 epoch: 206 loss: 17.52234263337005 loss_input: 82.25899122701476
step: 15000 epoch: 206 loss: 17.522698745943057 loss_input: 82.21230431968598
Save loss: 17.520987580433488 Name: 206_train_model.pth
step: 0 epoch: 207 loss: 16.259958267211914 loss_input: 115.94244384765625
step: 1000 epoch: 207 loss: 17.608875970144968 loss_input: 83.06232732111638
step: 2000 epoch: 207 loss: 17.468801043976075 loss_input: 82.38183029456951
step: 3000 epoch: 207 loss: 17.457599896186593 loss_input: 82.21780980233152
step: 4000 epoch: 207 loss: 17.462038277924226 loss_input: 82.28937948313215
step: 5000 epoch: 207 loss: 17.46755525937583 loss_input: 82.21846094648258
step: 6000 epoch: 207 loss: 17.451664414888143 loss_input: 82.10981970941042
step: 7000 epoch: 207 loss: 17.429923401647187 loss_input: 82.03312872505379
step: 8000 epoch: 207 loss: 17.433936072414987 loss_input: 82.08382288853298
step: 9000 epoch: 207 loss: 17.435840028032384 loss_input: 81.98088860487941
step: 10000 epoch: 207 loss: 17.47017646505766 loss_input: 82.02702074152937
step: 11000 epoch: 207 loss: 17.478307324445808 loss_input: 82.12213350033697
step: 12000 epoch: 207 loss: 17.489179385720607 loss_input: 82.10198026608948
step: 13000 epoch: 207 loss: 17.483786847937814 loss_input: 82.1466259093351
step: 14000 epoch: 207 loss: 17.501465848340075 loss_input: 82.22296597693496
step: 15000 epoch: 207 loss: 17.483360328544624 loss_input: 82.19648728333476
Save loss: 17.488290742143988 Name: 207_train_model.pth
step: 0 epoch: 208 loss: 16.57412338256836 loss_input: 64.48785400390625
step: 1000 epoch: 208 loss: 17.36899889432467 loss_input: 81.64450594571444
step: 2000 epoch: 208 loss: 17.289689396930182 loss_input: 81.32280759034545
step: 3000 epoch: 208 loss: 17.35471653199442 loss_input: 81.25841725853752
step: 4000 epoch: 208 loss: 17.40535225101901 loss_input: 81.70677704109129
step: 5000 epoch: 208 loss: 17.442044106323085 loss_input: 81.56283848210826
step: 6000 epoch: 208 loss: 17.409385374914983 loss_input: 81.52942244407834
step: 7000 epoch: 208 loss: 17.379872718822476 loss_input: 81.57305380760337
step: 8000 epoch: 208 loss: 17.398316279722295 loss_input: 81.82922255720233
step: 9000 epoch: 208 loss: 17.444699987280753 loss_input: 82.00155014107591
step: 10000 epoch: 208 loss: 17.45419486624374 loss_input: 82.01908355917338
step: 11000 epoch: 208 loss: 17.46763592262223 loss_input: 82.1138034859047
step: 12000 epoch: 208 loss: 17.478072080400803 loss_input: 82.19442232556467
step: 13000 epoch: 208 loss: 17.468239236451986 loss_input: 82.14719223446153
step: 14000 epoch: 208 loss: 17.462607675616667 loss_input: 82.11818633438153
step: 15000 epoch: 208 loss: 17.458032558396788 loss_input: 82.12799178459717
Save loss: 17.475491442099212 Name: 208_train_model.pth
step: 0 epoch: 209 loss: 16.078750610351562 loss_input: 85.3212890625
step: 1000 epoch: 209 loss: 17.505367152340764 loss_input: 81.79996108627701
step: 2000 epoch: 209 loss: 17.347318415281954 loss_input: 81.40544104778665
step: 3000 epoch: 209 loss: 17.381763857549448 loss_input: 82.05642119537629
step: 4000 epoch: 209 loss: 17.327481041965232 loss_input: 81.82469861050362
step: 5000 epoch: 209 loss: 17.33280140703808 loss_input: 81.6826439643712
step: 6000 epoch: 209 loss: 17.35673801375715 loss_input: 81.6525965460021
step: 7000 epoch: 209 loss: 17.37436385271192 loss_input: 81.74893382453455
step: 8000 epoch: 209 loss: 17.451795172980393 loss_input: 81.8981683762308
step: 9000 epoch: 209 loss: 17.473667649795793 loss_input: 81.98651936080452
step: 10000 epoch: 209 loss: 17.46034536165257 loss_input: 82.08867930345637
step: 11000 epoch: 209 loss: 17.459546148641554 loss_input: 82.22258435712165
step: 12000 epoch: 209 loss: 17.46991661810575 loss_input: 82.270509836664
step: 13000 epoch: 209 loss: 17.48353702706031 loss_input: 82.28241161624887
step: 14000 epoch: 209 loss: 17.494469571067608 loss_input: 82.25972509183217
step: 15000 epoch: 209 loss: 17.4885430601897 loss_input: 82.22423556772648
Save loss: 17.49955821289122 Name: 209_train_model.pth
step: 0 epoch: 210 loss: 15.663705825805664 loss_input: 60.07086181640625
step: 1000 epoch: 210 loss: 17.4297121111806 loss_input: 82.48589758630042
step: 2000 epoch: 210 loss: 17.45739513489677 loss_input: 82.37422893167674
step: 3000 epoch: 210 loss: 17.44010051152421 loss_input: 82.27617714668027
step: 4000 epoch: 210 loss: 17.427061045715078 loss_input: 82.37224791092267
step: 5000 epoch: 210 loss: 17.387295924194145 loss_input: 81.90554293926371
step: 6000 epoch: 210 loss: 17.418662856726066 loss_input: 82.22593686167865
step: 7000 epoch: 210 loss: 17.42271931664192 loss_input: 82.41710358690388
step: 8000 epoch: 210 loss: 17.4267750146344 loss_input: 82.46822337820565
step: 9000 epoch: 210 loss: 17.42218591986517 loss_input: 82.32030247778458
step: 10000 epoch: 210 loss: 17.440478865140296 loss_input: 82.28537895111856
step: 11000 epoch: 210 loss: 17.454830808624788 loss_input: 82.32440322586086
step: 12000 epoch: 210 loss: 17.45992161685075 loss_input: 82.29950184611894
step: 13000 epoch: 210 loss: 17.448323740041143 loss_input: 82.34343203598972
step: 14000 epoch: 210 loss: 17.47349990285232 loss_input: 82.48703256425462
step: 15000 epoch: 210 loss: 17.466348173014204 loss_input: 82.39697753466827
Save loss: 17.472161680579184 Name: 210_train_model.pth
step: 0 epoch: 211 loss: 12.413711547851562 loss_input: 80.6065673828125
step: 1000 epoch: 211 loss: 17.410065901982083 loss_input: 82.91816313569244
step: 2000 epoch: 211 loss: 17.43993824902086 loss_input: 82.65487538213316
step: 3000 epoch: 211 loss: 17.440028314549142 loss_input: 82.64737596419683
step: 4000 epoch: 211 loss: 17.481822399043107 loss_input: 82.48625323832616
step: 5000 epoch: 211 loss: 17.487448703000794 loss_input: 82.59708954205253
step: 6000 epoch: 211 loss: 17.484964180739595 loss_input: 82.41220860162629
step: 7000 epoch: 211 loss: 17.495482780136154 loss_input: 82.38043690640455
step: 8000 epoch: 211 loss: 17.49584825991571 loss_input: 82.43911993758348
step: 9000 epoch: 211 loss: 17.48646928606901 loss_input: 82.28147580498869
step: 10000 epoch: 211 loss: 17.506474818626458 loss_input: 82.28560474641013
step: 11000 epoch: 211 loss: 17.493858625169082 loss_input: 82.28726906174367
step: 12000 epoch: 211 loss: 17.47863538296419 loss_input: 82.22876776565006
step: 13000 epoch: 211 loss: 17.487173332360037 loss_input: 82.36976932325818
step: 14000 epoch: 211 loss: 17.483680391659032 loss_input: 82.30031561989433
step: 15000 epoch: 211 loss: 17.47423922358398 loss_input: 82.19062807515306
Save loss: 17.477548592835664 Name: 211_train_model.pth
step: 0 epoch: 212 loss: 20.690786361694336 loss_input: 92.82806396484375
step: 1000 epoch: 212 loss: 17.71557991702359 loss_input: 84.42523888465051
step: 2000 epoch: 212 loss: 17.572871669300312 loss_input: 83.1546567414535
step: 3000 epoch: 212 loss: 17.491721957574406 loss_input: 82.69325520236744
step: 4000 epoch: 212 loss: 17.471457507484587 loss_input: 82.35887747423496
step: 5000 epoch: 212 loss: 17.468393121283427 loss_input: 82.44137703221075
step: 6000 epoch: 212 loss: 17.476387417171424 loss_input: 82.48105853614241
step: 7000 epoch: 212 loss: 17.51055899750691 loss_input: 82.58833553388857
step: 8000 epoch: 212 loss: 17.49459241077522 loss_input: 82.4203558493787
step: 9000 epoch: 212 loss: 17.514575131296276 loss_input: 82.48414107145646
step: 10000 epoch: 212 loss: 17.50355931952314 loss_input: 82.39046132618184
step: 11000 epoch: 212 loss: 17.48993244796176 loss_input: 82.29395951575425
step: 12000 epoch: 212 loss: 17.49033407528851 loss_input: 82.26547074649703
step: 13000 epoch: 212 loss: 17.472283763891365 loss_input: 82.2207570618441
step: 14000 epoch: 212 loss: 17.46314135078668 loss_input: 82.29394258791085
step: 15000 epoch: 212 loss: 17.469021168305677 loss_input: 82.30652062567573
Save loss: 17.465842736274006 Name: 212_train_model.pth
step: 0 epoch: 213 loss: 22.30194091796875 loss_input: 67.3135986328125
step: 1000 epoch: 213 loss: 17.50917595416516 loss_input: 83.89738819530079
step: 2000 epoch: 213 loss: 17.494396670587893 loss_input: 82.74239058449291
step: 3000 epoch: 213 loss: 17.3988772900412 loss_input: 82.05118758834945
step: 4000 epoch: 213 loss: 17.382228917820516 loss_input: 81.70471894660612
step: 5000 epoch: 213 loss: 17.426059749836302 loss_input: 81.7272003851183
step: 6000 epoch: 213 loss: 17.45593926044846 loss_input: 81.90789370798227
step: 7000 epoch: 213 loss: 17.456128709742963 loss_input: 81.81574778345002
step: 8000 epoch: 213 loss: 17.46402984138668 loss_input: 81.75025854312156
step: 9000 epoch: 213 loss: 17.46615959935527 loss_input: 81.8327201383006
step: 10000 epoch: 213 loss: 17.4621282445587 loss_input: 81.914902455067
step: 11000 epoch: 213 loss: 17.447974094682667 loss_input: 81.97790551755594
step: 12000 epoch: 213 loss: 17.4504061966834 loss_input: 81.98374027201021
step: 13000 epoch: 213 loss: 17.46691460087156 loss_input: 82.0828207259673
step: 14000 epoch: 213 loss: 17.46240060844214 loss_input: 82.16045689642425
step: 15000 epoch: 213 loss: 17.47776055212029 loss_input: 82.27598866048903
Save loss: 17.468896984145044 Name: 213_train_model.pth
step: 0 epoch: 214 loss: 15.912506103515625 loss_input: 123.01287841796875
step: 1000 epoch: 214 loss: 17.52834046065629 loss_input: 82.23390513295298
step: 2000 epoch: 214 loss: 17.31379555726516 loss_input: 81.87601838119026
step: 3000 epoch: 214 loss: 17.238108536594115 loss_input: 81.6579581708441
step: 4000 epoch: 214 loss: 17.32744802602736 loss_input: 81.93694662577032
step: 5000 epoch: 214 loss: 17.29855788638415 loss_input: 81.62541861744839
step: 6000 epoch: 214 loss: 17.36456432709633 loss_input: 81.80475636669604
step: 7000 epoch: 214 loss: 17.367851985827325 loss_input: 81.83758132557514
step: 8000 epoch: 214 loss: 17.385193197984364 loss_input: 81.74967981144691
step: 9000 epoch: 214 loss: 17.396019425236933 loss_input: 81.82303911219066
step: 10000 epoch: 214 loss: 17.395879055282947 loss_input: 81.92651285408569
step: 11000 epoch: 214 loss: 17.39556709307669 loss_input: 81.95363757658217
step: 12000 epoch: 214 loss: 17.429541876907894 loss_input: 82.05400704522995
step: 13000 epoch: 214 loss: 17.44822504868664 loss_input: 82.10480631128586
step: 14000 epoch: 214 loss: 17.452580417907967 loss_input: 82.11016153439515
step: 15000 epoch: 214 loss: 17.459121471579476 loss_input: 82.15296520922043
Save loss: 17.457802492529154 Name: 214_train_model.pth
step: 0 epoch: 215 loss: 9.636378288269043 loss_input: 43.16290283203125
step: 1000 epoch: 215 loss: 17.39286767328893 loss_input: 81.83020080505432
step: 2000 epoch: 215 loss: 17.283305146943206 loss_input: 82.45846162612054
step: 3000 epoch: 215 loss: 17.346085599405452 loss_input: 82.70000921730517
step: 4000 epoch: 215 loss: 17.382083601368812 loss_input: 82.81290765870544
step: 5000 epoch: 215 loss: 17.398448717019672 loss_input: 82.5489381982002
step: 6000 epoch: 215 loss: 17.418468153729 loss_input: 82.57713598840297
step: 7000 epoch: 215 loss: 17.444013296749027 loss_input: 82.553688654132
step: 8000 epoch: 215 loss: 17.458542019318767 loss_input: 82.55796953756024
step: 9000 epoch: 215 loss: 17.454256848909633 loss_input: 82.5641304476369
step: 10000 epoch: 215 loss: 17.465626958274516 loss_input: 82.47703237488751
step: 11000 epoch: 215 loss: 17.472744397711356 loss_input: 82.41016182033877
step: 12000 epoch: 215 loss: 17.46025440653367 loss_input: 82.27261549615332
step: 13000 epoch: 215 loss: 17.472335271712094 loss_input: 82.22130988045991
step: 14000 epoch: 215 loss: 17.477695033461885 loss_input: 82.25921816131437
step: 15000 epoch: 215 loss: 17.48104987267804 loss_input: 82.25503856383533
Save loss: 17.47144340746105 Name: 215_train_model.pth
step: 0 epoch: 216 loss: 19.85626983642578 loss_input: 90.34918212890625
step: 1000 epoch: 216 loss: 17.523354410291553 loss_input: 83.71651682058176
step: 2000 epoch: 216 loss: 17.429149806171818 loss_input: 82.40455571774659
step: 3000 epoch: 216 loss: 17.507410649576414 loss_input: 82.55147069710488
step: 4000 epoch: 216 loss: 17.473871258728746 loss_input: 82.54892339232086
step: 5000 epoch: 216 loss: 17.426632178256426 loss_input: 82.450490456108
step: 6000 epoch: 216 loss: 17.439026849863033 loss_input: 82.53812309154529
step: 7000 epoch: 216 loss: 17.462917564119923 loss_input: 82.37836815374031
step: 8000 epoch: 216 loss: 17.46596200235813 loss_input: 82.44358893004585
step: 9000 epoch: 216 loss: 17.459187429198398 loss_input: 82.26604810294516
step: 10000 epoch: 216 loss: 17.46086218526108 loss_input: 82.40894785643506
step: 11000 epoch: 216 loss: 17.441989980148364 loss_input: 82.29478177321238
step: 12000 epoch: 216 loss: 17.45991990914117 loss_input: 82.35035978548825
step: 13000 epoch: 216 loss: 17.46009135411323 loss_input: 82.29195678044297
step: 14000 epoch: 216 loss: 17.46134713293612 loss_input: 82.3084302702918
step: 15000 epoch: 216 loss: 17.460566586887396 loss_input: 82.29509880005395
Save loss: 17.471815407142042 Name: 216_train_model.pth
step: 0 epoch: 217 loss: 21.289466857910156 loss_input: 68.309814453125
step: 1000 epoch: 217 loss: 17.472607830783108 loss_input: 82.34145298061314
step: 2000 epoch: 217 loss: 17.45970094257566 loss_input: 82.44075258513321
step: 3000 epoch: 217 loss: 17.4168064601419 loss_input: 82.14653654266937
step: 4000 epoch: 217 loss: 17.381585532324042 loss_input: 81.8049181094619
step: 5000 epoch: 217 loss: 17.401578295543132 loss_input: 81.85851720036852
step: 6000 epoch: 217 loss: 17.37071870982776 loss_input: 81.8665852139064
step: 7000 epoch: 217 loss: 17.366743621409338 loss_input: 81.85582209307711
step: 8000 epoch: 217 loss: 17.412796535785162 loss_input: 81.916017562624
step: 9000 epoch: 217 loss: 17.44388028349218 loss_input: 82.18880941803992
step: 10000 epoch: 217 loss: 17.440963950661132 loss_input: 82.08400533237692
step: 11000 epoch: 217 loss: 17.441627322473416 loss_input: 82.13275777308597
step: 12000 epoch: 217 loss: 17.440852554527265 loss_input: 82.15267876948568
step: 13000 epoch: 217 loss: 17.43844431523644 loss_input: 82.21841477375838
step: 14000 epoch: 217 loss: 17.45595702630487 loss_input: 82.32222047528832
step: 15000 epoch: 217 loss: 17.44912815767879 loss_input: 82.26273111333593
Save loss: 17.45406969356537 Name: 217_train_model.pth
step: 0 epoch: 218 loss: 22.755260467529297 loss_input: 99.462158203125
step: 1000 epoch: 218 loss: 17.194177013534407 loss_input: 81.80506346680663
step: 2000 epoch: 218 loss: 17.260816818353593 loss_input: 81.46682935997822
step: 3000 epoch: 218 loss: 17.335396785729728 loss_input: 81.80057015906807
step: 4000 epoch: 218 loss: 17.465366359234928 loss_input: 82.37000042591892
step: 5000 epoch: 218 loss: 17.44158436112155 loss_input: 82.31767084083184
step: 6000 epoch: 218 loss: 17.426126878910353 loss_input: 82.39704901884842
step: 7000 epoch: 218 loss: 17.42525226418384 loss_input: 82.35353633911673
step: 8000 epoch: 218 loss: 17.426815241191584 loss_input: 82.40788633873159
step: 9000 epoch: 218 loss: 17.43855465815976 loss_input: 82.35601079046772
step: 10000 epoch: 218 loss: 17.44610265736198 loss_input: 82.37066292760372
step: 11000 epoch: 218 loss: 17.440028164735892 loss_input: 82.24059061591879
step: 12000 epoch: 218 loss: 17.428280661755945 loss_input: 82.16897891972543
step: 13000 epoch: 218 loss: 17.4295313610461 loss_input: 82.23936138119333
step: 14000 epoch: 218 loss: 17.439013688004568 loss_input: 82.1867594087169
step: 15000 epoch: 218 loss: 17.45614361685122 loss_input: 82.2677678625136
Save loss: 17.444578549742698 Name: 218_train_model.pth
step: 0 epoch: 219 loss: 22.117382049560547 loss_input: 87.060791015625
step: 1000 epoch: 219 loss: 17.21126584358863 loss_input: 81.61236535681115
step: 2000 epoch: 219 loss: 17.418384186927227 loss_input: 81.99859747250399
step: 3000 epoch: 219 loss: 17.380423099030974 loss_input: 82.28136424007752
step: 4000 epoch: 219 loss: 17.39500319138374 loss_input: 82.45338032252191
step: 5000 epoch: 219 loss: 17.386007981070563 loss_input: 82.20436619463217
step: 6000 epoch: 219 loss: 17.409714075271893 loss_input: 82.1579462810072
step: 7000 epoch: 219 loss: 17.393378258023905 loss_input: 82.01547311391342
step: 8000 epoch: 219 loss: 17.401742076087096 loss_input: 82.23964323948032
step: 9000 epoch: 219 loss: 17.41144352062002 loss_input: 82.22096092241921
step: 10000 epoch: 219 loss: 17.404398431778908 loss_input: 82.30004406175594
step: 11000 epoch: 219 loss: 17.40780002456851 loss_input: 82.26788390552919
step: 12000 epoch: 219 loss: 17.418143571490955 loss_input: 82.36759561316667
step: 13000 epoch: 219 loss: 17.415284287974465 loss_input: 82.33922299978578
step: 14000 epoch: 219 loss: 17.44184754463734 loss_input: 82.36770712928289
step: 15000 epoch: 219 loss: 17.44299478261012 loss_input: 82.25054951170573
Save loss: 17.43967635460198 Name: 219_train_model.pth
step: 0 epoch: 220 loss: 17.383258819580078 loss_input: 142.8323974609375
step: 1000 epoch: 220 loss: 17.077188095250925 loss_input: 81.09854726572256
step: 2000 epoch: 220 loss: 17.365846562063854 loss_input: 82.08257083616395
step: 3000 epoch: 220 loss: 17.356451463000212 loss_input: 81.90600807624672
step: 4000 epoch: 220 loss: 17.401339437686868 loss_input: 82.09336337790552
step: 5000 epoch: 220 loss: 17.423713690232 loss_input: 82.2560525004374
step: 6000 epoch: 220 loss: 17.420135643338785 loss_input: 82.20249963629108
step: 7000 epoch: 220 loss: 17.449004463324666 loss_input: 82.00682408093486
step: 8000 epoch: 220 loss: 17.42871497112756 loss_input: 82.06607914245095
step: 9000 epoch: 220 loss: 17.434123864426585 loss_input: 82.24327297102623
step: 10000 epoch: 220 loss: 17.432656398785973 loss_input: 82.27380662202334
step: 11000 epoch: 220 loss: 17.42875256079802 loss_input: 82.27346385924255
step: 12000 epoch: 220 loss: 17.443756658130365 loss_input: 82.30155476036991
step: 13000 epoch: 220 loss: 17.466176192583134 loss_input: 82.29956952304751
step: 14000 epoch: 220 loss: 17.478783340217742 loss_input: 82.43001649522941
step: 15000 epoch: 220 loss: 17.462751216089302 loss_input: 82.2815839458359
Save loss: 17.43955941773951 Name: 220_train_model.pth
step: 0 epoch: 221 loss: 19.18239402770996 loss_input: 81.248291015625
step: 1000 epoch: 221 loss: 17.6604488851069 loss_input: 83.15197144164429
step: 2000 epoch: 221 loss: 17.608501603042168 loss_input: 83.02809148404313
step: 3000 epoch: 221 loss: 17.581510225084376 loss_input: 82.59694231561485
step: 4000 epoch: 221 loss: 17.51629016578749 loss_input: 82.80236151289326
step: 5000 epoch: 221 loss: 17.425513014224165 loss_input: 82.35711242761214
step: 6000 epoch: 221 loss: 17.438647052523017 loss_input: 82.55407790107898
step: 7000 epoch: 221 loss: 17.44979281749679 loss_input: 82.57441111405123
step: 8000 epoch: 221 loss: 17.442775387478505 loss_input: 82.45999811089288
step: 9000 epoch: 221 loss: 17.42318936268921 loss_input: 82.38545560503043
step: 10000 epoch: 221 loss: 17.39851392637359 loss_input: 82.23347570028153
step: 11000 epoch: 221 loss: 17.410149821714104 loss_input: 82.22546580349052
step: 12000 epoch: 221 loss: 17.393901819328935 loss_input: 82.10713070053407
step: 13000 epoch: 221 loss: 17.428359495879192 loss_input: 82.18375442912144
step: 14000 epoch: 221 loss: 17.432798036481046 loss_input: 82.22714068833253
step: 15000 epoch: 221 loss: 17.433344598230015 loss_input: 82.22215614906271
Save loss: 17.435023060157896 Name: 221_train_model.pth
step: 0 epoch: 222 loss: 23.454057693481445 loss_input: 84.32464599609375
step: 1000 epoch: 222 loss: 17.450026901332766 loss_input: 82.1353035392342
step: 2000 epoch: 222 loss: 17.30361264947055 loss_input: 81.80709556720663
step: 3000 epoch: 222 loss: 17.327522833956674 loss_input: 82.27823979327854
step: 4000 epoch: 222 loss: 17.325545593429524 loss_input: 81.93027487756967
step: 5000 epoch: 222 loss: 17.3736771806863 loss_input: 82.06354012400645
step: 6000 epoch: 222 loss: 17.393243900995774 loss_input: 82.12728782670337
step: 7000 epoch: 222 loss: 17.40368144250975 loss_input: 82.25295814325632
step: 8000 epoch: 222 loss: 17.400589991265456 loss_input: 82.23042813618024
step: 9000 epoch: 222 loss: 17.388765888735396 loss_input: 82.20499168847776
step: 10000 epoch: 222 loss: 17.41203694748361 loss_input: 82.20295083528757
step: 11000 epoch: 222 loss: 17.410230619172033 loss_input: 82.12553241675295
step: 12000 epoch: 222 loss: 17.428224337358255 loss_input: 82.0758654989607
step: 13000 epoch: 222 loss: 17.4396278143901 loss_input: 82.19856020315144
step: 14000 epoch: 222 loss: 17.42904725468812 loss_input: 82.1983134627521
step: 15000 epoch: 222 loss: 17.41909223708715 loss_input: 82.14992169874222
Save loss: 17.436675734877586 Name: 222_train_model.pth
step: 0 epoch: 223 loss: 19.15378761291504 loss_input: 66.412841796875
step: 1000 epoch: 223 loss: 17.29978733891612 loss_input: 82.92707195148601
step: 2000 epoch: 223 loss: 17.407278594703808 loss_input: 82.97216806025698
step: 3000 epoch: 223 loss: 17.354388857952397 loss_input: 82.61900228406937
step: 4000 epoch: 223 loss: 17.400217560761217 loss_input: 82.77634055231309
step: 5000 epoch: 223 loss: 17.436840714704655 loss_input: 82.82752432667763
step: 6000 epoch: 223 loss: 17.43861020729753 loss_input: 82.65967773274767
step: 7000 epoch: 223 loss: 17.495655545574003 loss_input: 82.52338832140889
step: 8000 epoch: 223 loss: 17.492814916384965 loss_input: 82.53817534282824
step: 9000 epoch: 223 loss: 17.46754929521669 loss_input: 82.48930015941154
step: 10000 epoch: 223 loss: 17.45694870057672 loss_input: 82.3616552163143
step: 11000 epoch: 223 loss: 17.436789553530183 loss_input: 82.26828849299389
step: 12000 epoch: 223 loss: 17.44129970121181 loss_input: 82.29416287452933
step: 13000 epoch: 223 loss: 17.451023430267156 loss_input: 82.2547131243435
step: 14000 epoch: 223 loss: 17.443886279634096 loss_input: 82.23328119733914
step: 15000 epoch: 223 loss: 17.440457413652865 loss_input: 82.21031532061274
Save loss: 17.43783641158044 Name: 223_train_model.pth
step: 0 epoch: 224 loss: 35.830963134765625 loss_input: 85.217041015625
step: 1000 epoch: 224 loss: 17.552515817807986 loss_input: 83.63887498000047
step: 2000 epoch: 224 loss: 17.550544879366196 loss_input: 83.62477306268741
step: 3000 epoch: 224 loss: 17.431676583383847 loss_input: 82.79934199179701
step: 4000 epoch: 224 loss: 17.39349735346296 loss_input: 82.55988886689461
step: 5000 epoch: 224 loss: 17.367313064829965 loss_input: 82.86426855253949
step: 6000 epoch: 224 loss: 17.39697802565412 loss_input: 82.75769351144926
step: 7000 epoch: 224 loss: 17.41654269182074 loss_input: 82.7178066533185
step: 8000 epoch: 224 loss: 17.431885732053114 loss_input: 82.52679206877704
step: 9000 epoch: 224 loss: 17.416612797929847 loss_input: 82.50143019997562
step: 10000 epoch: 224 loss: 17.412332020477226 loss_input: 82.52069845505684
step: 11000 epoch: 224 loss: 17.415846564208213 loss_input: 82.38239474123624
step: 12000 epoch: 224 loss: 17.419163869923427 loss_input: 82.37842936102982
step: 13000 epoch: 224 loss: 17.419615602650996 loss_input: 82.3050000322992
step: 14000 epoch: 224 loss: 17.431937691348168 loss_input: 82.32269013342658
step: 15000 epoch: 224 loss: 17.42971324529674 loss_input: 82.24896716683732
Save loss: 17.42981115299463 Name: 224_train_model.pth
step: 0 epoch: 225 loss: 11.219974517822266 loss_input: 58.07635498046875
step: 1000 epoch: 225 loss: 17.323103649871094 loss_input: 81.6867737365174
step: 2000 epoch: 225 loss: 17.343119615795967 loss_input: 82.41365331128381
step: 3000 epoch: 225 loss: 17.30970571280241 loss_input: 82.65117551961846
step: 4000 epoch: 225 loss: 17.387397845725182 loss_input: 82.64934025892941
step: 5000 epoch: 225 loss: 17.34085598871055 loss_input: 82.58276315928221
step: 6000 epoch: 225 loss: 17.302308342095674 loss_input: 82.38594484130574
step: 7000 epoch: 225 loss: 17.332223640682187 loss_input: 82.43057640238058
step: 8000 epoch: 225 loss: 17.36264830025386 loss_input: 82.39467204050665
step: 9000 epoch: 225 loss: 17.360496056396396 loss_input: 82.31950853930726
step: 10000 epoch: 225 loss: 17.371994313258266 loss_input: 82.35825526944376
step: 11000 epoch: 225 loss: 17.38365097898059 loss_input: 82.3871087680328
step: 12000 epoch: 225 loss: 17.39734872159695 loss_input: 82.40161960278537
step: 13000 epoch: 225 loss: 17.397480934218766 loss_input: 82.32218631356783
step: 14000 epoch: 225 loss: 17.411898351483632 loss_input: 82.28204253285061
step: 15000 epoch: 225 loss: 17.410290070068328 loss_input: 82.201292346298
Save loss: 17.416490144342184 Name: 225_train_model.pth
step: 0 epoch: 226 loss: 18.774578094482422 loss_input: 80.6170654296875
step: 1000 epoch: 226 loss: 17.239929280676446 loss_input: 81.05798010583167
step: 2000 epoch: 226 loss: 17.40980358698081 loss_input: 81.89617364779524
step: 3000 epoch: 226 loss: 17.420085379855706 loss_input: 82.26867663578287
step: 4000 epoch: 226 loss: 17.41144337501564 loss_input: 82.63613729136075
step: 5000 epoch: 226 loss: 17.392341166919433 loss_input: 82.34068403604435
step: 6000 epoch: 226 loss: 17.426073692576367 loss_input: 82.34211709344015
step: 7000 epoch: 226 loss: 17.406189632286363 loss_input: 82.32698867028482
step: 8000 epoch: 226 loss: 17.430605246355913 loss_input: 82.39826059645377
step: 9000 epoch: 226 loss: 17.42831371877077 loss_input: 82.32274510460209
step: 10000 epoch: 226 loss: 17.41042670561378 loss_input: 82.32455343332855
step: 11000 epoch: 226 loss: 17.420280852390196 loss_input: 82.38355872566272
step: 12000 epoch: 226 loss: 17.44272164717961 loss_input: 82.47607496636836
step: 13000 epoch: 226 loss: 17.442699648382003 loss_input: 82.47344515321988
step: 14000 epoch: 226 loss: 17.43160929987068 loss_input: 82.27485013626259
step: 15000 epoch: 226 loss: 17.42460668906316 loss_input: 82.22203713437334
Save loss: 17.428888787671923 Name: 226_train_model.pth
step: 0 epoch: 227 loss: 27.833797454833984 loss_input: 69.28961181640625
step: 1000 epoch: 227 loss: 17.22112461284443 loss_input: 82.9441006381314
step: 2000 epoch: 227 loss: 17.369299181099834 loss_input: 83.18895242930293
step: 3000 epoch: 227 loss: 17.35261344862 loss_input: 82.7016219202974
step: 4000 epoch: 227 loss: 17.338385141125265 loss_input: 82.37453153496294
step: 5000 epoch: 227 loss: 17.357273376409925 loss_input: 82.4654315230704
step: 6000 epoch: 227 loss: 17.424227312393455 loss_input: 82.4671274066627
step: 7000 epoch: 227 loss: 17.3872805548947 loss_input: 82.38375688413095
step: 8000 epoch: 227 loss: 17.38072558761075 loss_input: 82.36437131607329
step: 9000 epoch: 227 loss: 17.392972749440542 loss_input: 82.43003009295626
step: 10000 epoch: 227 loss: 17.387220340756794 loss_input: 82.38702332769653
step: 11000 epoch: 227 loss: 17.381333987199874 loss_input: 82.32418224319807
step: 12000 epoch: 227 loss: 17.376827992515636 loss_input: 82.28280806718256
step: 13000 epoch: 227 loss: 17.383184922529125 loss_input: 82.18979601291187
step: 14000 epoch: 227 loss: 17.393557839219582 loss_input: 82.20879285164062
step: 15000 epoch: 227 loss: 17.414491739918983 loss_input: 82.30680491661884
Save loss: 17.407896796867252 Name: 227_train_model.pth
step: 0 epoch: 228 loss: 17.25505256652832 loss_input: 83.385498046875
step: 1000 epoch: 228 loss: 17.168693278576587 loss_input: 81.83375773396526
step: 2000 epoch: 228 loss: 17.253027437687635 loss_input: 82.16036195769303
step: 3000 epoch: 228 loss: 17.304327800964284 loss_input: 82.46425666542142
step: 4000 epoch: 228 loss: 17.31179823407052 loss_input: 82.10205726461452
step: 5000 epoch: 228 loss: 17.306880625074708 loss_input: 82.02271685555468
step: 6000 epoch: 228 loss: 17.272133256729155 loss_input: 82.09058980078841
step: 7000 epoch: 228 loss: 17.273667095933806 loss_input: 81.94277875203096
step: 8000 epoch: 228 loss: 17.282337818692856 loss_input: 81.91793492549272
step: 9000 epoch: 228 loss: 17.299314546076513 loss_input: 82.04202954364662
step: 10000 epoch: 228 loss: 17.314309416145292 loss_input: 81.98859680682322
step: 11000 epoch: 228 loss: 17.339406203144346 loss_input: 82.12775257957728
step: 12000 epoch: 228 loss: 17.356054170719137 loss_input: 82.10684365576957
step: 13000 epoch: 228 loss: 17.35763554794588 loss_input: 81.99478472640776
step: 14000 epoch: 228 loss: 17.369408131718558 loss_input: 82.08516815013353
step: 15000 epoch: 228 loss: 17.38725200272204 loss_input: 82.22635572260677
Save loss: 17.40514475877583 Name: 228_train_model.pth
step: 0 epoch: 229 loss: 20.21784210205078 loss_input: 195.83636474609375
step: 1000 epoch: 229 loss: 17.791899673469537 loss_input: 83.62833595847512
step: 2000 epoch: 229 loss: 17.514296760444697 loss_input: 83.33027776272996
step: 3000 epoch: 229 loss: 17.481702578937718 loss_input: 82.8921202698059
step: 4000 epoch: 229 loss: 17.491808677250013 loss_input: 83.12681226055791
step: 5000 epoch: 229 loss: 17.474776614215273 loss_input: 82.80806106845037
step: 6000 epoch: 229 loss: 17.43895391868842 loss_input: 82.51764959542875
step: 7000 epoch: 229 loss: 17.418770122010443 loss_input: 82.2984089779864
step: 8000 epoch: 229 loss: 17.411603931605317 loss_input: 82.37626783926179
step: 9000 epoch: 229 loss: 17.419265900488867 loss_input: 82.33998480691815
step: 10000 epoch: 229 loss: 17.40477879759479 loss_input: 82.25274758290773
step: 11000 epoch: 229 loss: 17.404810246397457 loss_input: 82.31574539366272
step: 12000 epoch: 229 loss: 17.403388439620617 loss_input: 82.27219724690912
step: 13000 epoch: 229 loss: 17.399375016648406 loss_input: 82.20194146912077
step: 14000 epoch: 229 loss: 17.392910390111023 loss_input: 82.1576397235577
step: 15000 epoch: 229 loss: 17.404944763573937 loss_input: 82.23759922759707
Save loss: 17.408713239073755 Name: 229_train_model.pth
step: 0 epoch: 230 loss: 14.241264343261719 loss_input: 80.737060546875
step: 1000 epoch: 230 loss: 17.439260794328046 loss_input: 82.76483946961243
step: 2000 epoch: 230 loss: 17.44185765953674 loss_input: 82.71640092429371
step: 3000 epoch: 230 loss: 17.42928461335731 loss_input: 82.28406801004483
step: 4000 epoch: 230 loss: 17.46331194209266 loss_input: 82.61187461923582
step: 5000 epoch: 230 loss: 17.496032643380154 loss_input: 82.56929429543779
step: 6000 epoch: 230 loss: 17.463148062754783 loss_input: 82.33226086667847
step: 7000 epoch: 230 loss: 17.444568691279542 loss_input: 82.37047514243221
step: 8000 epoch: 230 loss: 17.428406887181982 loss_input: 82.38181918544362
step: 9000 epoch: 230 loss: 17.42950572070645 loss_input: 82.38594971109981
step: 10000 epoch: 230 loss: 17.430960190986326 loss_input: 82.26241601303737
step: 11000 epoch: 230 loss: 17.413166383344166 loss_input: 82.26025213639136
step: 12000 epoch: 230 loss: 17.41148061248106 loss_input: 82.29859358135357
step: 13000 epoch: 230 loss: 17.400064479405216 loss_input: 82.2103113650212
step: 14000 epoch: 230 loss: 17.39316385218556 loss_input: 82.11460345605894
step: 15000 epoch: 230 loss: 17.395720280803733 loss_input: 82.21333726270463
Save loss: 17.401414668515326 Name: 230_train_model.pth
step: 0 epoch: 231 loss: 7.671451568603516 loss_input: 55.27825927734375
step: 1000 epoch: 231 loss: 17.47805270329341 loss_input: 82.87056754280876
step: 2000 epoch: 231 loss: 17.513087740187522 loss_input: 83.38613313582466
step: 3000 epoch: 231 loss: 17.511731335895135 loss_input: 82.9604919087843
step: 4000 epoch: 231 loss: 17.471589738444905 loss_input: 82.78880961827414
step: 5000 epoch: 231 loss: 17.455426991784414 loss_input: 82.76619627246426
step: 6000 epoch: 231 loss: 17.429701401618814 loss_input: 82.55533922650261
step: 7000 epoch: 231 loss: 17.405444367717564 loss_input: 82.59469526465527
step: 8000 epoch: 231 loss: 17.412967054624286 loss_input: 82.54344223207451
step: 9000 epoch: 231 loss: 17.426288003511473 loss_input: 82.47277108505956
step: 10000 epoch: 231 loss: 17.419465974311496 loss_input: 82.25764265004163
step: 11000 epoch: 231 loss: 17.443502815059507 loss_input: 82.3185694888444
step: 12000 epoch: 231 loss: 17.43143760717945 loss_input: 82.254095926373
step: 13000 epoch: 231 loss: 17.412497078691132 loss_input: 82.24353589762927
step: 14000 epoch: 231 loss: 17.39720944465633 loss_input: 82.25601085779726
step: 15000 epoch: 231 loss: 17.400474508844464 loss_input: 82.21119811901863
Save loss: 17.400607643947005 Name: 231_train_model.pth
step: 0 epoch: 232 loss: 17.716571807861328 loss_input: 118.85528564453125
step: 1000 epoch: 232 loss: 17.444342500322705 loss_input: 82.89007147637518
step: 2000 epoch: 232 loss: 17.472690530087817 loss_input: 83.34920402957701
step: 3000 epoch: 232 loss: 17.521532581949664 loss_input: 83.46024321238623
step: 4000 epoch: 232 loss: 17.498775531994763 loss_input: 83.03238156830422
step: 5000 epoch: 232 loss: 17.457777369811378 loss_input: 82.91862322115655
step: 6000 epoch: 232 loss: 17.383756392599246 loss_input: 82.66839724710596
step: 7000 epoch: 232 loss: 17.382064614461466 loss_input: 82.4773969232074
step: 8000 epoch: 232 loss: 17.392027996999147 loss_input: 82.35476493477866
step: 9000 epoch: 232 loss: 17.400114437485122 loss_input: 82.42314958439948
step: 10000 epoch: 232 loss: 17.39724440386791 loss_input: 82.38257125215213
step: 11000 epoch: 232 loss: 17.389297154781744 loss_input: 82.40245094018441
step: 12000 epoch: 232 loss: 17.397549988180288 loss_input: 82.4062757801189
step: 13000 epoch: 232 loss: 17.406373879935373 loss_input: 82.33006501753471
step: 14000 epoch: 232 loss: 17.411737146551935 loss_input: 82.3090306215223
step: 15000 epoch: 232 loss: 17.411642844673317 loss_input: 82.27444504188001
Save loss: 17.3999463686347 Name: 232_train_model.pth
step: 0 epoch: 233 loss: 19.198564529418945 loss_input: 69.5816650390625
step: 1000 epoch: 233 loss: 17.304536555077764 loss_input: 82.33805446262721
step: 2000 epoch: 233 loss: 17.34458169503429 loss_input: 82.74913754670516
step: 3000 epoch: 233 loss: 17.32164523371932 loss_input: 82.69155754641031
step: 4000 epoch: 233 loss: 17.372054250739094 loss_input: 82.77173081912628
step: 5000 epoch: 233 loss: 17.405510577505243 loss_input: 82.83900437929992
step: 6000 epoch: 233 loss: 17.387404319704064 loss_input: 82.50692037573816
step: 7000 epoch: 233 loss: 17.36765181815653 loss_input: 82.5130959510667
step: 8000 epoch: 233 loss: 17.373047461495997 loss_input: 82.46437084768701
step: 9000 epoch: 233 loss: 17.38131629105555 loss_input: 82.48832667545403
step: 10000 epoch: 233 loss: 17.394505825677808 loss_input: 82.48998602947323
step: 11000 epoch: 233 loss: 17.379560057157473 loss_input: 82.34727226835025
step: 12000 epoch: 233 loss: 17.3795547651436 loss_input: 82.27199177900539
step: 13000 epoch: 233 loss: 17.379127054579047 loss_input: 82.26156834277471
step: 14000 epoch: 233 loss: 17.396425621108186 loss_input: 82.26417292476458
step: 15000 epoch: 233 loss: 17.40195627233504 loss_input: 82.26602618175065
Save loss: 17.393797940284013 Name: 233_train_model.pth
step: 0 epoch: 234 loss: 18.167213439941406 loss_input: 91.1953125
step: 1000 epoch: 234 loss: 17.139996167544005 loss_input: 80.8706143100064
step: 2000 epoch: 234 loss: 17.18549967550862 loss_input: 81.6109215289816
step: 3000 epoch: 234 loss: 17.339512164018345 loss_input: 81.81316939356803
step: 4000 epoch: 234 loss: 17.315536438480255 loss_input: 81.70130505093454
step: 5000 epoch: 234 loss: 17.35651700159617 loss_input: 81.83459962060323
step: 6000 epoch: 234 loss: 17.360399340137086 loss_input: 81.86764855393567
step: 7000 epoch: 234 loss: 17.381414842135634 loss_input: 81.9124889420094
step: 8000 epoch: 234 loss: 17.401775558893867 loss_input: 81.89638305404695
step: 9000 epoch: 234 loss: 17.403491848191344 loss_input: 82.02639003245515
step: 10000 epoch: 234 loss: 17.389229769516486 loss_input: 81.96111210412163
step: 11000 epoch: 234 loss: 17.387910059003133 loss_input: 81.97044044247043
step: 12000 epoch: 234 loss: 17.39387101453996 loss_input: 82.04172373147777
step: 13000 epoch: 234 loss: 17.36908262999697 loss_input: 81.96223900745616
step: 14000 epoch: 234 loss: 17.375607599386885 loss_input: 82.03949817706241
step: 15000 epoch: 234 loss: 17.3916291881264 loss_input: 82.15374353867945
Save loss: 17.39807763659954 Name: 234_train_model.pth
step: 0 epoch: 235 loss: 16.135419845581055 loss_input: 116.1185302734375
step: 1000 epoch: 235 loss: 17.235280809583482 loss_input: 82.3909381633991
step: 2000 epoch: 235 loss: 17.38209327157291 loss_input: 82.26586871359243
step: 3000 epoch: 235 loss: 17.359971978989336 loss_input: 82.1829485182999
step: 4000 epoch: 235 loss: 17.36167018981434 loss_input: 81.98744413322939
step: 5000 epoch: 235 loss: 17.39016981426179 loss_input: 82.01212120857079
step: 6000 epoch: 235 loss: 17.40988210232491 loss_input: 82.33195364656021
step: 7000 epoch: 235 loss: 17.37386913593114 loss_input: 82.26918137671181
step: 8000 epoch: 235 loss: 17.394459430463105 loss_input: 82.41644895096121
step: 9000 epoch: 235 loss: 17.378124150683146 loss_input: 82.31380918748829
step: 10000 epoch: 235 loss: 17.39785680381337 loss_input: 82.31374130043051
step: 11000 epoch: 235 loss: 17.408235438075263 loss_input: 82.4109986361325
step: 12000 epoch: 235 loss: 17.39731074855602 loss_input: 82.3217047688246
step: 13000 epoch: 235 loss: 17.39273051639381 loss_input: 82.2891074794502
step: 14000 epoch: 235 loss: 17.397263402590095 loss_input: 82.29241249787961
step: 15000 epoch: 235 loss: 17.411761826844575 loss_input: 82.30990927916004
Save loss: 17.39798233832419 Name: 235_train_model.pth
step: 0 epoch: 236 loss: 16.20920753479004 loss_input: 108.7674560546875
step: 1000 epoch: 236 loss: 17.07160613348672 loss_input: 81.50046505008663
step: 2000 epoch: 236 loss: 17.239472972339897 loss_input: 81.83945261866137
step: 3000 epoch: 236 loss: 17.318546778994456 loss_input: 82.37680766567394
step: 4000 epoch: 236 loss: 17.33625225554821 loss_input: 82.49613216202785
step: 5000 epoch: 236 loss: 17.32062941478553 loss_input: 82.15853518749375
step: 6000 epoch: 236 loss: 17.321282506207588 loss_input: 82.17425689671839
step: 7000 epoch: 236 loss: 17.319073078854053 loss_input: 82.01667005159977
step: 8000 epoch: 236 loss: 17.319191218048257 loss_input: 81.99231094343187
step: 9000 epoch: 236 loss: 17.330901961183034 loss_input: 82.07064225665464
step: 10000 epoch: 236 loss: 17.336169541662855 loss_input: 82.17275907907256
step: 11000 epoch: 236 loss: 17.333523075381862 loss_input: 82.17262113689239
step: 12000 epoch: 236 loss: 17.334863283626042 loss_input: 82.16097685015507
step: 13000 epoch: 236 loss: 17.342202637087574 loss_input: 82.17317654501336
step: 14000 epoch: 236 loss: 17.354037172053083 loss_input: 82.18952384225625
step: 15000 epoch: 236 loss: 17.37373651115825 loss_input: 82.17872629699036
Save loss: 17.382430328547954 Name: 236_train_model.pth
step: 0 epoch: 237 loss: 9.233692169189453 loss_input: 64.46337890625
step: 1000 epoch: 237 loss: 17.31895576538025 loss_input: 82.7498908562141
step: 2000 epoch: 237 loss: 17.188957834172285 loss_input: 83.02363475854845
step: 3000 epoch: 237 loss: 17.303090196893596 loss_input: 82.49155491870549
step: 4000 epoch: 237 loss: 17.298512412321266 loss_input: 82.51592108381595
step: 5000 epoch: 237 loss: 17.303823310979435 loss_input: 82.44126373699869
step: 6000 epoch: 237 loss: 17.33500951402407 loss_input: 82.64150097298237
step: 7000 epoch: 237 loss: 17.354676834090643 loss_input: 82.57826433207644
step: 8000 epoch: 237 loss: 17.3440143174342 loss_input: 82.43963825200561
step: 9000 epoch: 237 loss: 17.361133321313485 loss_input: 82.42583780974206
step: 10000 epoch: 237 loss: 17.34285952322317 loss_input: 82.26156725758088
step: 11000 epoch: 237 loss: 17.378820701161686 loss_input: 82.37933676395186
step: 12000 epoch: 237 loss: 17.36632045459056 loss_input: 82.34589707856377
step: 13000 epoch: 237 loss: 17.37050116620571 loss_input: 82.36826321352684
step: 14000 epoch: 237 loss: 17.38395144905195 loss_input: 82.37396032729666
step: 15000 epoch: 237 loss: 17.382286366586104 loss_input: 82.31399034253836
Save loss: 17.38681083895266 Name: 237_train_model.pth
step: 0 epoch: 238 loss: 19.828487396240234 loss_input: 51.3299560546875
step: 1000 epoch: 238 loss: 17.15506254685866 loss_input: 81.10224413038134
step: 2000 epoch: 238 loss: 17.237219415385386 loss_input: 81.56392586177614
step: 3000 epoch: 238 loss: 17.33825737633176 loss_input: 82.20458201351502
step: 4000 epoch: 238 loss: 17.3769397617608 loss_input: 82.04862351001606
step: 5000 epoch: 238 loss: 17.38552592454303 loss_input: 81.98711906788564
step: 6000 epoch: 238 loss: 17.410037911826223 loss_input: 82.07246075239088
step: 7000 epoch: 238 loss: 17.372083793349173 loss_input: 81.88810272500135
step: 8000 epoch: 238 loss: 17.31761738080231 loss_input: 81.86518687269805
step: 9000 epoch: 238 loss: 17.32066317709694 loss_input: 81.9882790326357
step: 10000 epoch: 238 loss: 17.336676368783944 loss_input: 82.04063114196393
step: 11000 epoch: 238 loss: 17.346853789584483 loss_input: 82.05871729611938
step: 12000 epoch: 238 loss: 17.346150489362437 loss_input: 82.06331600255643
step: 13000 epoch: 238 loss: 17.36186701840102 loss_input: 82.18235324953218
step: 14000 epoch: 238 loss: 17.374123484277273 loss_input: 82.21274902919183
step: 15000 epoch: 238 loss: 17.393841273021273 loss_input: 82.12866026216741
Save loss: 17.40691202738881 Name: 238_train_model.pth
step: 0 epoch: 239 loss: 18.08753204345703 loss_input: 99.0474853515625
step: 1000 epoch: 239 loss: 17.36272729145778 loss_input: 82.45065883823209
step: 2000 epoch: 239 loss: 17.424177942843155 loss_input: 82.1763551817841
step: 3000 epoch: 239 loss: 17.401947885543176 loss_input: 82.85008230085414
step: 4000 epoch: 239 loss: 17.394220892055962 loss_input: 82.89953739897545
step: 5000 epoch: 239 loss: 17.3874045173494 loss_input: 82.90594620333746
step: 6000 epoch: 239 loss: 17.35474577078798 loss_input: 82.68407671190543
step: 7000 epoch: 239 loss: 17.358663065163448 loss_input: 82.57597598013342
step: 8000 epoch: 239 loss: 17.34940216332998 loss_input: 82.62964955003183
step: 9000 epoch: 239 loss: 17.34142953871939 loss_input: 82.50826516102161
step: 10000 epoch: 239 loss: 17.361430029191563 loss_input: 82.38113367618317
step: 11000 epoch: 239 loss: 17.354758478925287 loss_input: 82.24970378447485
step: 12000 epoch: 239 loss: 17.379706863164603 loss_input: 82.20477793334116
step: 13000 epoch: 239 loss: 17.380988366438256 loss_input: 82.18805040556526
step: 14000 epoch: 239 loss: 17.368127492417372 loss_input: 82.18310522026746
step: 15000 epoch: 239 loss: 17.37518523783773 loss_input: 82.25570947858033
Save loss: 17.37740598253906 Name: 239_train_model.pth
step: 0 epoch: 240 loss: 21.146995544433594 loss_input: 83.08734130859375
step: 1000 epoch: 240 loss: 17.52189980020056 loss_input: 82.01201697472449
step: 2000 epoch: 240 loss: 17.405342895111282 loss_input: 81.79104997062016
step: 3000 epoch: 240 loss: 17.356714983536218 loss_input: 81.50940968489019
step: 4000 epoch: 240 loss: 17.3958185188057 loss_input: 81.51094451423765
step: 5000 epoch: 240 loss: 17.37381273468741 loss_input: 81.53373535497978
step: 6000 epoch: 240 loss: 17.305979187419823 loss_input: 81.45166895401873
step: 7000 epoch: 240 loss: 17.321678205211406 loss_input: 81.73521237863062
step: 8000 epoch: 240 loss: 17.319236276358637 loss_input: 81.68645677255431
step: 9000 epoch: 240 loss: 17.32056819265544 loss_input: 81.84775728586581
step: 10000 epoch: 240 loss: 17.338893316779277 loss_input: 82.11441184983065
step: 11000 epoch: 240 loss: 17.337586058929674 loss_input: 82.14831822595318
step: 12000 epoch: 240 loss: 17.360639028634225 loss_input: 82.25108830855892
step: 13000 epoch: 240 loss: 17.36011372990869 loss_input: 82.29996735245876
step: 14000 epoch: 240 loss: 17.3790164715511 loss_input: 82.29836032830104
step: 15000 epoch: 240 loss: 17.368599018409455 loss_input: 82.23417856941047
Save loss: 17.37385208901763 Name: 240_train_model.pth
step: 0 epoch: 241 loss: 15.183314323425293 loss_input: 60.789306640625
step: 1000 epoch: 241 loss: 17.36498742146449 loss_input: 82.71282593139283
step: 2000 epoch: 241 loss: 17.37262510335904 loss_input: 82.55934953665745
step: 3000 epoch: 241 loss: 17.268160374789826 loss_input: 82.2755685645475
step: 4000 epoch: 241 loss: 17.343996287881957 loss_input: 82.49658155834577
step: 5000 epoch: 241 loss: 17.366448143915377 loss_input: 82.40069005241921
step: 6000 epoch: 241 loss: 17.368493528529775 loss_input: 82.38173649728625
step: 7000 epoch: 241 loss: 17.389013277668727 loss_input: 82.32935980023223
step: 8000 epoch: 241 loss: 17.42232618223442 loss_input: 82.6280936455819
step: 9000 epoch: 241 loss: 17.420526791328776 loss_input: 82.61004642740433
step: 10000 epoch: 241 loss: 17.392660767206515 loss_input: 82.48418379597587
step: 11000 epoch: 241 loss: 17.376397615735634 loss_input: 82.41998904640593
step: 12000 epoch: 241 loss: 17.38663360033718 loss_input: 82.46143982833787
step: 13000 epoch: 241 loss: 17.37014868831407 loss_input: 82.31327111518766
step: 14000 epoch: 241 loss: 17.380157010383787 loss_input: 82.37866288097865
step: 15000 epoch: 241 loss: 17.369496232103916 loss_input: 82.29052217645126
Save loss: 17.36934470155835 Name: 241_train_model.pth
step: 0 epoch: 242 loss: 19.335901260375977 loss_input: 94.23114013671875
step: 1000 epoch: 242 loss: 17.295093352978046 loss_input: 82.64883962473074
step: 2000 epoch: 242 loss: 17.437269006592818 loss_input: 83.16475432327782
step: 3000 epoch: 242 loss: 17.344326329445767 loss_input: 82.78402847244278
step: 4000 epoch: 242 loss: 17.379820674218347 loss_input: 82.56143749096906
step: 5000 epoch: 242 loss: 17.37603215974847 loss_input: 82.49718764743145
step: 6000 epoch: 242 loss: 17.363332840586242 loss_input: 82.51269630924143
step: 7000 epoch: 242 loss: 17.386980516776717 loss_input: 82.52918455148148
step: 8000 epoch: 242 loss: 17.371865880532557 loss_input: 82.43389640076728
step: 9000 epoch: 242 loss: 17.364928324107872 loss_input: 82.55840545711935
step: 10000 epoch: 242 loss: 17.368072376646 loss_input: 82.41204348803402
step: 11000 epoch: 242 loss: 17.374890082771696 loss_input: 82.43074084884329
step: 12000 epoch: 242 loss: 17.36849405296484 loss_input: 82.37178344065403
step: 13000 epoch: 242 loss: 17.3776883872855 loss_input: 82.33525100120002
step: 14000 epoch: 242 loss: 17.3623630900901 loss_input: 82.25347821486524
step: 15000 epoch: 242 loss: 17.358645599259194 loss_input: 82.18769743149936
Save loss: 17.369227832943203 Name: 242_train_model.pth
step: 0 epoch: 243 loss: 11.054760932922363 loss_input: 46.12762451171875
step: 1000 epoch: 243 loss: 17.21984602187897 loss_input: 81.0244857681381
step: 2000 epoch: 243 loss: 17.127479607078328 loss_input: 80.58039370207474
step: 3000 epoch: 243 loss: 17.253535842228157 loss_input: 81.1155999147745
step: 4000 epoch: 243 loss: 17.278067764834265 loss_input: 81.37802742612686
step: 5000 epoch: 243 loss: 17.267482378415597 loss_input: 81.56003707705725
step: 6000 epoch: 243 loss: 17.230092251545152 loss_input: 81.60722296406003
step: 7000 epoch: 243 loss: 17.28519485347902 loss_input: 81.77788491501772
step: 8000 epoch: 243 loss: 17.296306246236746 loss_input: 81.99645778683183
step: 9000 epoch: 243 loss: 17.311862614006216 loss_input: 81.99896373818707
step: 10000 epoch: 243 loss: 17.324517367851875 loss_input: 82.09945326878444
step: 11000 epoch: 243 loss: 17.355153906942444 loss_input: 82.23622476303646
step: 12000 epoch: 243 loss: 17.37469922722444 loss_input: 82.32615593597994
step: 13000 epoch: 243 loss: 17.37936975396456 loss_input: 82.2608159029083
step: 14000 epoch: 243 loss: 17.380362271343024 loss_input: 82.29123525304135
step: 15000 epoch: 243 loss: 17.369933565031186 loss_input: 82.24692664125126
Save loss: 17.3648161547333 Name: 243_train_model.pth
step: 0 epoch: 244 loss: 19.25554656982422 loss_input: 109.975830078125
step: 1000 epoch: 244 loss: 17.302555747799108 loss_input: 81.68699415818557
step: 2000 epoch: 244 loss: 17.418258071958512 loss_input: 82.06526329730643
step: 3000 epoch: 244 loss: 17.343137937956993 loss_input: 82.07203366700033
step: 4000 epoch: 244 loss: 17.29404121039957 loss_input: 81.84625675856427
step: 5000 epoch: 244 loss: 17.27766954810637 loss_input: 82.07818197102767
step: 6000 epoch: 244 loss: 17.262736200511586 loss_input: 81.93888241933637
step: 7000 epoch: 244 loss: 17.29725828437767 loss_input: 82.05362925797152
step: 8000 epoch: 244 loss: 17.28783995707979 loss_input: 81.97607912383755
step: 9000 epoch: 244 loss: 17.303574547001077 loss_input: 81.99435263079494
step: 10000 epoch: 244 loss: 17.31621239762487 loss_input: 81.9756134982205
step: 11000 epoch: 244 loss: 17.33152113027133 loss_input: 81.97159515039648
step: 12000 epoch: 244 loss: 17.344459832544057 loss_input: 81.96633466884758
step: 13000 epoch: 244 loss: 17.351076157219033 loss_input: 82.04144592261683
step: 14000 epoch: 244 loss: 17.352798356421378 loss_input: 82.0991395904408
step: 15000 epoch: 244 loss: 17.352878113602266 loss_input: 82.10102634596495
Save loss: 17.358511222407223 Name: 244_train_model.pth
step: 0 epoch: 245 loss: 19.88052749633789 loss_input: 76.6370849609375
step: 1000 epoch: 245 loss: 17.35134746978333 loss_input: 82.15637475317651
step: 2000 epoch: 245 loss: 17.310841939617312 loss_input: 82.204786126224
step: 3000 epoch: 245 loss: 17.33237071459947 loss_input: 81.9829539648893
step: 4000 epoch: 245 loss: 17.3024216565273 loss_input: 82.0739667530627
step: 5000 epoch: 245 loss: 17.30321551899604 loss_input: 82.23103490874091
step: 6000 epoch: 245 loss: 17.294631048592503 loss_input: 82.25177652886303
step: 7000 epoch: 245 loss: 17.303381045330184 loss_input: 82.18340550959238
step: 8000 epoch: 245 loss: 17.299916549230154 loss_input: 82.12664446302122
step: 9000 epoch: 245 loss: 17.27895618181681 loss_input: 82.13264144424491
step: 10000 epoch: 245 loss: 17.289698505411145 loss_input: 82.03373801413804
step: 11000 epoch: 245 loss: 17.301378654009774 loss_input: 81.95492438032089
step: 12000 epoch: 245 loss: 17.323564580793867 loss_input: 82.07937049659111
step: 13000 epoch: 245 loss: 17.342477833671797 loss_input: 82.1446714651382
step: 14000 epoch: 245 loss: 17.35456208404188 loss_input: 82.25589713126658
step: 15000 epoch: 245 loss: 17.355886312367385 loss_input: 82.26148059042157
Save loss: 17.355187156021596 Name: 245_train_model.pth
step: 0 epoch: 246 loss: 19.029216766357422 loss_input: 100.52001953125
step: 1000 epoch: 246 loss: 17.4403896855784 loss_input: 80.8348317332082
step: 2000 epoch: 246 loss: 17.39981232518735 loss_input: 81.31404616295367
step: 3000 epoch: 246 loss: 17.31508536030554 loss_input: 81.22142554044486
step: 4000 epoch: 246 loss: 17.3196272266653 loss_input: 81.55549938504143
step: 5000 epoch: 246 loss: 17.316200246765145 loss_input: 82.21512692090488
step: 6000 epoch: 246 loss: 17.294977759305805 loss_input: 82.3340768888779
step: 7000 epoch: 246 loss: 17.328413067230446 loss_input: 82.32882094550109
step: 8000 epoch: 246 loss: 17.31844184342332 loss_input: 82.2926966282535
step: 9000 epoch: 246 loss: 17.28350026778891 loss_input: 82.03498516712119
step: 10000 epoch: 246 loss: 17.305478056363256 loss_input: 82.11531908942895
step: 11000 epoch: 246 loss: 17.337092433254476 loss_input: 82.26411935518291
step: 12000 epoch: 246 loss: 17.343425158689087 loss_input: 82.20911514209753
step: 13000 epoch: 246 loss: 17.355308644707943 loss_input: 82.18793470119937
step: 14000 epoch: 246 loss: 17.368765744334688 loss_input: 82.31049057844919
step: 15000 epoch: 246 loss: 17.37415983945352 loss_input: 82.30908555070938
Save loss: 17.359705295607448 Name: 246_train_model.pth
step: 0 epoch: 247 loss: 16.479602813720703 loss_input: 90.143310546875
step: 1000 epoch: 247 loss: 17.47554465226241 loss_input: 83.32966923213505
step: 2000 epoch: 247 loss: 17.42647503305232 loss_input: 82.87107672970156
step: 3000 epoch: 247 loss: 17.34134602459301 loss_input: 82.28597647219807
step: 4000 epoch: 247 loss: 17.316288553574953 loss_input: 81.9264853586408
step: 5000 epoch: 247 loss: 17.29957713996904 loss_input: 82.01674346527179
step: 6000 epoch: 247 loss: 17.257042567226573 loss_input: 81.88044216751635
step: 7000 epoch: 247 loss: 17.262941825459674 loss_input: 81.83429887039017
step: 8000 epoch: 247 loss: 17.2867704631537 loss_input: 81.88780741244014
step: 9000 epoch: 247 loss: 17.2952048723174 loss_input: 81.89138455915922
step: 10000 epoch: 247 loss: 17.307563074230373 loss_input: 81.92037023104331
step: 11000 epoch: 247 loss: 17.318881070480575 loss_input: 82.06999804217017
step: 12000 epoch: 247 loss: 17.31420780032806 loss_input: 82.06029500389941
step: 13000 epoch: 247 loss: 17.328046432780024 loss_input: 82.05452898980728
step: 14000 epoch: 247 loss: 17.352452687404554 loss_input: 82.15457065915493
step: 15000 epoch: 247 loss: 17.34178331983653 loss_input: 82.19829880807426
Save loss: 17.35607180327177 Name: 247_train_model.pth
step: 0 epoch: 248 loss: 21.42676544189453 loss_input: 64.79315185546875
step: 1000 epoch: 248 loss: 17.311328837921568 loss_input: 81.72772796051605
step: 2000 epoch: 248 loss: 17.253323488745433 loss_input: 82.40245246029329
step: 3000 epoch: 248 loss: 17.250774864910525 loss_input: 82.64963249554836
step: 4000 epoch: 248 loss: 17.2743429139506 loss_input: 82.35339416488651
step: 5000 epoch: 248 loss: 17.30195585109548 loss_input: 82.35153330661993
step: 6000 epoch: 248 loss: 17.320759571943455 loss_input: 82.28639924758635
step: 7000 epoch: 248 loss: 17.330711480090148 loss_input: 82.12127777086532
step: 8000 epoch: 248 loss: 17.348989639233356 loss_input: 82.18244891934894
step: 9000 epoch: 248 loss: 17.35516695199629 loss_input: 82.09399116379647
step: 10000 epoch: 248 loss: 17.33682338422137 loss_input: 82.0658481759246
step: 11000 epoch: 248 loss: 17.326251438732267 loss_input: 82.09696605506566
step: 12000 epoch: 248 loss: 17.3494179462574 loss_input: 82.1715625006103
step: 13000 epoch: 248 loss: 17.35513187863168 loss_input: 82.09935439442609
step: 14000 epoch: 248 loss: 17.347374044565736 loss_input: 82.20120097797144
step: 15000 epoch: 248 loss: 17.33888041193537 loss_input: 82.13096041806848
Save loss: 17.356208857640624 Name: 248_train_model.pth
step: 0 epoch: 249 loss: 23.486820220947266 loss_input: 102.84808349609375
step: 1000 epoch: 249 loss: 17.436996285612885 loss_input: 82.55891947408061
step: 2000 epoch: 249 loss: 17.23119685412764 loss_input: 82.63823380058018
step: 3000 epoch: 249 loss: 17.321418038847764 loss_input: 82.80573341600977
step: 4000 epoch: 249 loss: 17.37180730117258 loss_input: 82.79643175143714
step: 5000 epoch: 249 loss: 17.357388800417176 loss_input: 82.49077425725793
step: 6000 epoch: 249 loss: 17.33834568811762 loss_input: 82.54915686659645
step: 7000 epoch: 249 loss: 17.35936121078342 loss_input: 82.4877923148953
step: 8000 epoch: 249 loss: 17.33874003196743 loss_input: 82.48415225715193
step: 9000 epoch: 249 loss: 17.314613226347134 loss_input: 82.29518826385826
step: 10000 epoch: 249 loss: 17.338450786102154 loss_input: 82.30224265781429
step: 11000 epoch: 249 loss: 17.339136964569807 loss_input: 82.25821272107365
step: 12000 epoch: 249 loss: 17.339171558904127 loss_input: 82.26139825567108
step: 13000 epoch: 249 loss: 17.326130838249657 loss_input: 82.19323103732856
step: 14000 epoch: 249 loss: 17.334752739450693 loss_input: 82.21577325339624
step: 15000 epoch: 249 loss: 17.350149770600265 loss_input: 82.30119280703552
Save loss: 17.34056639239192 Name: 249_train_model.pth
step: 0 epoch: 250 loss: 14.356021881103516 loss_input: 87.26776123046875
step: 1000 epoch: 250 loss: 17.266393954460913 loss_input: 81.29835008741259
step: 2000 epoch: 250 loss: 17.312101361991047 loss_input: 82.10675305047671
step: 3000 epoch: 250 loss: 17.345054547971824 loss_input: 82.09278025082133
step: 4000 epoch: 250 loss: 17.341452985785956 loss_input: 82.303885851047
step: 5000 epoch: 250 loss: 17.324275333055184 loss_input: 81.99367160259354
step: 6000 epoch: 250 loss: 17.31137551746454 loss_input: 82.01734584356383
step: 7000 epoch: 250 loss: 17.31892343038083 loss_input: 82.00170112068254
step: 8000 epoch: 250 loss: 17.323155567387076 loss_input: 82.17954780036041
step: 9000 epoch: 250 loss: 17.360690920687478 loss_input: 82.27122501118582
step: 10000 epoch: 250 loss: 17.362427524966773 loss_input: 82.4359395054147
step: 11000 epoch: 250 loss: 17.35558570119405 loss_input: 82.39673770890325
step: 12000 epoch: 250 loss: 17.342182357692092 loss_input: 82.50290705042971
step: 13000 epoch: 250 loss: 17.36322148360983 loss_input: 82.45849342249365
step: 14000 epoch: 250 loss: 17.35801817084983 loss_input: 82.3369363213035
step: 15000 epoch: 250 loss: 17.36207337128656 loss_input: 82.26412175955848
Save loss: 17.347253499120473 Name: 250_train_model.pth
step: 0 epoch: 251 loss: 17.95616912841797 loss_input: 83.3328857421875
step: 1000 epoch: 251 loss: 17.379369157892125 loss_input: 82.07098626471185
step: 2000 epoch: 251 loss: 17.36881912820998 loss_input: 81.9246484545813
step: 3000 epoch: 251 loss: 17.3604016207886 loss_input: 82.2443105309298
step: 4000 epoch: 251 loss: 17.385632075360046 loss_input: 82.21469347323814
step: 5000 epoch: 251 loss: 17.33258136366158 loss_input: 82.21878015754271
step: 6000 epoch: 251 loss: 17.314293278195304 loss_input: 82.10630065277206
step: 7000 epoch: 251 loss: 17.327666771580468 loss_input: 82.02198160970029
step: 8000 epoch: 251 loss: 17.330836691747322 loss_input: 82.10997344356613
step: 9000 epoch: 251 loss: 17.31783001005166 loss_input: 82.0452773991909
step: 10000 epoch: 251 loss: 17.309312876075904 loss_input: 82.00709741037615
step: 11000 epoch: 251 loss: 17.307282236725317 loss_input: 81.89549851380698
step: 12000 epoch: 251 loss: 17.331650567068657 loss_input: 81.96941564480629
step: 13000 epoch: 251 loss: 17.324327505842447 loss_input: 82.04094796569134
step: 14000 epoch: 251 loss: 17.323561523277565 loss_input: 82.05035576072133
step: 15000 epoch: 251 loss: 17.323363773886836 loss_input: 82.0713273208544
Save loss: 17.351754363358022 Name: 251_train_model.pth
step: 0 epoch: 252 loss: 22.096139907836914 loss_input: 130.503173828125
step: 1000 epoch: 252 loss: 17.28073236301586 loss_input: 83.40341409079203
step: 2000 epoch: 252 loss: 17.377564957831755 loss_input: 83.0564754548995
step: 3000 epoch: 252 loss: 17.31429855237997 loss_input: 82.75935706977883
step: 4000 epoch: 252 loss: 17.29804910692207 loss_input: 82.46906260495423
step: 5000 epoch: 252 loss: 17.306383872694838 loss_input: 82.56481462496563
step: 6000 epoch: 252 loss: 17.334722885666757 loss_input: 82.3248442764422
step: 7000 epoch: 252 loss: 17.338586008151996 loss_input: 82.11221598121307
step: 8000 epoch: 252 loss: 17.318411634439467 loss_input: 81.98575515279828
step: 9000 epoch: 252 loss: 17.348835212576986 loss_input: 82.10755526946659
step: 10000 epoch: 252 loss: 17.347647942706185 loss_input: 82.19693833839273
step: 11000 epoch: 252 loss: 17.339459068633655 loss_input: 82.14526919782861
step: 12000 epoch: 252 loss: 17.349270822067936 loss_input: 82.10588063685937
step: 13000 epoch: 252 loss: 17.337758538264712 loss_input: 82.16867956239695
step: 14000 epoch: 252 loss: 17.337983835187096 loss_input: 82.07923427832722
step: 15000 epoch: 252 loss: 17.343619617078552 loss_input: 82.17269303628456
Save loss: 17.35366989918053 Name: 252_train_model.pth
step: 0 epoch: 253 loss: 16.937698364257812 loss_input: 51.27178955078125
step: 1000 epoch: 253 loss: 16.96522331237793 loss_input: 79.83619914021526
step: 2000 epoch: 253 loss: 17.207529274837068 loss_input: 80.74411054970561
step: 3000 epoch: 253 loss: 17.257031091170802 loss_input: 81.40541635454197
step: 4000 epoch: 253 loss: 17.28506400745471 loss_input: 81.51112327203843
step: 5000 epoch: 253 loss: 17.321295178096264 loss_input: 81.68179883310447
step: 6000 epoch: 253 loss: 17.368864833106798 loss_input: 81.88768900658484
step: 7000 epoch: 253 loss: 17.351951916887664 loss_input: 81.98493998276474
step: 8000 epoch: 253 loss: 17.3345524903581 loss_input: 81.98353237766666
step: 9000 epoch: 253 loss: 17.367315000592967 loss_input: 82.0897357568994
step: 10000 epoch: 253 loss: 17.37826837021021 loss_input: 82.24672651753379
step: 11000 epoch: 253 loss: 17.33003824308302 loss_input: 82.1062084621391
step: 12000 epoch: 253 loss: 17.342668399384852 loss_input: 82.28006020860566
step: 13000 epoch: 253 loss: 17.344749520773263 loss_input: 82.27008887593833
step: 14000 epoch: 253 loss: 17.338814583278353 loss_input: 82.19166155490149
step: 15000 epoch: 253 loss: 17.337833336612142 loss_input: 82.21537089584653
Save loss: 17.344704344987868 Name: 253_train_model.pth
step: 0 epoch: 254 loss: 14.051065444946289 loss_input: 58.35406494140625
step: 1000 epoch: 254 loss: 17.036357098406963 loss_input: 80.9664748703445
step: 2000 epoch: 254 loss: 17.250680438284277 loss_input: 82.26250766218453
step: 3000 epoch: 254 loss: 17.278864613933113 loss_input: 82.32972855076953
step: 4000 epoch: 254 loss: 17.32043418095309 loss_input: 82.3640450055943
step: 5000 epoch: 254 loss: 17.32048961077421 loss_input: 82.33281760395968
step: 6000 epoch: 254 loss: 17.320862586091824 loss_input: 82.36334441085991
step: 7000 epoch: 254 loss: 17.33401791543011 loss_input: 82.3538732835181
step: 8000 epoch: 254 loss: 17.33479841553171 loss_input: 82.37630007705336
step: 9000 epoch: 254 loss: 17.33124737633081 loss_input: 82.34461154435532
step: 10000 epoch: 254 loss: 17.326368808460263 loss_input: 82.37248249662339
step: 11000 epoch: 254 loss: 17.33915506199418 loss_input: 82.38452248304131
step: 12000 epoch: 254 loss: 17.33611662345532 loss_input: 82.34594661972342
step: 13000 epoch: 254 loss: 17.34392058857587 loss_input: 82.3833778721123
step: 14000 epoch: 254 loss: 17.345539276927347 loss_input: 82.31599015074536
step: 15000 epoch: 254 loss: 17.332038578721065 loss_input: 82.21264691159261
Save loss: 17.33431129825115 Name: 254_train_model.pth
step: 0 epoch: 255 loss: 19.509626388549805 loss_input: 117.150634765625
step: 1000 epoch: 255 loss: 17.301848050478576 loss_input: 82.95108407217782
step: 2000 epoch: 255 loss: 17.278047733816845 loss_input: 81.93460499364575
step: 3000 epoch: 255 loss: 17.317893506923703 loss_input: 81.73703331392791
step: 4000 epoch: 255 loss: 17.34533774432645 loss_input: 81.949423334474
step: 5000 epoch: 255 loss: 17.344065948572904 loss_input: 82.02597096547487
step: 6000 epoch: 255 loss: 17.347609974030473 loss_input: 82.22601685526371
step: 7000 epoch: 255 loss: 17.342803086370864 loss_input: 82.24562515413535
step: 8000 epoch: 255 loss: 17.32579101486335 loss_input: 82.26659732871526
step: 9000 epoch: 255 loss: 17.305215631454683 loss_input: 82.25605918868173
step: 10000 epoch: 255 loss: 17.321156689863468 loss_input: 82.18650541562054
step: 11000 epoch: 255 loss: 17.33259913639831 loss_input: 82.19307158710892
step: 12000 epoch: 255 loss: 17.323932940935496 loss_input: 82.13676166939702
step: 13000 epoch: 255 loss: 17.32091052146538 loss_input: 82.15892366001894
step: 14000 epoch: 255 loss: 17.318044993308824 loss_input: 82.19585573948261
step: 15000 epoch: 255 loss: 17.31329727581315 loss_input: 82.14390023331127
Save loss: 17.338989332154394 Name: 255_train_model.pth
step: 0 epoch: 256 loss: 17.747135162353516 loss_input: 76.32000732421875
step: 1000 epoch: 256 loss: 17.354574941850448 loss_input: 82.79037844479738
step: 2000 epoch: 256 loss: 17.154546009427843 loss_input: 82.21530854445824
step: 3000 epoch: 256 loss: 17.197050913696646 loss_input: 82.63797732298869
step: 4000 epoch: 256 loss: 17.226059653943135 loss_input: 82.25432820261732
step: 5000 epoch: 256 loss: 17.273890744159516 loss_input: 82.56741920658837
step: 6000 epoch: 256 loss: 17.287424630154135 loss_input: 82.57463352665506
step: 7000 epoch: 256 loss: 17.27640275821705 loss_input: 82.3544539152052
step: 8000 epoch: 256 loss: 17.27224141793644 loss_input: 82.30391700457609
step: 9000 epoch: 256 loss: 17.30476435625928 loss_input: 82.35507291995768
step: 10000 epoch: 256 loss: 17.316876271905546 loss_input: 82.37295277430265
step: 11000 epoch: 256 loss: 17.31830156301327 loss_input: 82.38360500829825
step: 12000 epoch: 256 loss: 17.317628486584987 loss_input: 82.2856055838934
step: 13000 epoch: 256 loss: 17.33824709354662 loss_input: 82.32264322046592
step: 14000 epoch: 256 loss: 17.351402542844994 loss_input: 82.34724239580683
step: 15000 epoch: 256 loss: 17.33879991922607 loss_input: 82.27909549682364
Save loss: 17.33420752686262 Name: 256_train_model.pth
step: 0 epoch: 257 loss: 16.014148712158203 loss_input: 62.623779296875
step: 1000 epoch: 257 loss: 17.02425810245129 loss_input: 81.18711311881478
step: 2000 epoch: 257 loss: 17.011401278683092 loss_input: 81.68890478466821
step: 3000 epoch: 257 loss: 17.117010219062976 loss_input: 82.0311756839517
step: 4000 epoch: 257 loss: 17.214981397191632 loss_input: 82.48753914121568
step: 5000 epoch: 257 loss: 17.281027616250277 loss_input: 82.51892846528351
step: 6000 epoch: 257 loss: 17.311547294614158 loss_input: 82.48243845345438
step: 7000 epoch: 257 loss: 17.30747932557497 loss_input: 82.15404853226883
step: 8000 epoch: 257 loss: 17.305337272961577 loss_input: 82.29561282104142
step: 9000 epoch: 257 loss: 17.287269425516644 loss_input: 82.19176180809554
step: 10000 epoch: 257 loss: 17.308643370911476 loss_input: 82.29638607794494
step: 11000 epoch: 257 loss: 17.32148162105627 loss_input: 82.35875657033665
step: 12000 epoch: 257 loss: 17.327396830403103 loss_input: 82.26865879971429
step: 13000 epoch: 257 loss: 17.329812109942583 loss_input: 82.27000707164653
step: 14000 epoch: 257 loss: 17.31852948114741 loss_input: 82.21499324493702
step: 15000 epoch: 257 loss: 17.32426912550465 loss_input: 82.24711584982876
Save loss: 17.3239457051754 Name: 257_train_model.pth
step: 0 epoch: 258 loss: 21.38311767578125 loss_input: 83.51220703125
step: 1000 epoch: 258 loss: 17.354393312147447 loss_input: 83.59382798597886
step: 2000 epoch: 258 loss: 17.18143028595756 loss_input: 82.94033610636089
step: 3000 epoch: 258 loss: 17.246126302676533 loss_input: 82.76370042993481
step: 4000 epoch: 258 loss: 17.23091069688442 loss_input: 82.2255020656457
step: 5000 epoch: 258 loss: 17.275617204077648 loss_input: 82.25286184218234
step: 6000 epoch: 258 loss: 17.295998922766614 loss_input: 82.35756670763206
step: 7000 epoch: 258 loss: 17.29493962511167 loss_input: 82.29328057766301
step: 8000 epoch: 258 loss: 17.294929432877897 loss_input: 82.22184561371967
step: 9000 epoch: 258 loss: 17.274247806186928 loss_input: 82.13070584116956
step: 10000 epoch: 258 loss: 17.279995924066917 loss_input: 82.04503028090745
step: 11000 epoch: 258 loss: 17.30671468263062 loss_input: 82.21317115036773
step: 12000 epoch: 258 loss: 17.298252417002008 loss_input: 82.1261944703664
step: 13000 epoch: 258 loss: 17.30234124220185 loss_input: 82.08748567849798
step: 14000 epoch: 258 loss: 17.31112121510579 loss_input: 82.16543443743974
step: 15000 epoch: 258 loss: 17.330995015021713 loss_input: 82.19439387634574
Save loss: 17.335462201833725 Name: 258_train_model.pth
step: 0 epoch: 259 loss: 27.870502471923828 loss_input: 83.4180908203125
step: 1000 epoch: 259 loss: 17.26114709608324 loss_input: 82.08272617275303
step: 2000 epoch: 259 loss: 17.376288774071902 loss_input: 82.14951578776042
step: 3000 epoch: 259 loss: 17.31806479077147 loss_input: 81.70166068504508
step: 4000 epoch: 259 loss: 17.287017868924398 loss_input: 81.7195881693639
step: 5000 epoch: 259 loss: 17.291173206332015 loss_input: 81.72352795065987
step: 6000 epoch: 259 loss: 17.28641867915743 loss_input: 81.67955194316572
step: 7000 epoch: 259 loss: 17.287739845092528 loss_input: 81.72242091461823
step: 8000 epoch: 259 loss: 17.294622995930602 loss_input: 81.86838141204029
step: 9000 epoch: 259 loss: 17.328696539237516 loss_input: 82.03486879279039
step: 10000 epoch: 259 loss: 17.33226919052613 loss_input: 82.16587921896287
step: 11000 epoch: 259 loss: 17.334344285345654 loss_input: 82.23330380831857
step: 12000 epoch: 259 loss: 17.331730957200513 loss_input: 82.25285671079331
step: 13000 epoch: 259 loss: 17.342990111978263 loss_input: 82.24260651167609
step: 14000 epoch: 259 loss: 17.328480493033855 loss_input: 82.178783878021
step: 15000 epoch: 259 loss: 17.33357512079138 loss_input: 82.1744950401744
Save loss: 17.31962487269938 Name: 259_train_model.pth
step: 0 epoch: 260 loss: 15.133604049682617 loss_input: 115.1077880859375
step: 1000 epoch: 260 loss: 17.209347720627303 loss_input: 81.79410701603085
step: 2000 epoch: 260 loss: 17.306580948627097 loss_input: 81.49926712059009
step: 3000 epoch: 260 loss: 17.333830770990208 loss_input: 81.7547416242112
step: 4000 epoch: 260 loss: 17.30270061812321 loss_input: 81.50270954295118
step: 5000 epoch: 260 loss: 17.310467778480284 loss_input: 81.60916029684688
step: 6000 epoch: 260 loss: 17.289100712089812 loss_input: 81.49875134743168
step: 7000 epoch: 260 loss: 17.27854865950323 loss_input: 81.76393319892638
step: 8000 epoch: 260 loss: 17.267350050676257 loss_input: 81.86100788304782
step: 9000 epoch: 260 loss: 17.27374963616811 loss_input: 82.00000237332571
step: 10000 epoch: 260 loss: 17.287500146293315 loss_input: 82.15473395897715
step: 11000 epoch: 260 loss: 17.273871242907923 loss_input: 82.14011396580247
step: 12000 epoch: 260 loss: 17.28178079765386 loss_input: 82.11471387495867
step: 13000 epoch: 260 loss: 17.291938456395307 loss_input: 82.13360327638559
step: 14000 epoch: 260 loss: 17.306648889377062 loss_input: 82.21399227011827
step: 15000 epoch: 260 loss: 17.31891564073328 loss_input: 82.2286492668067
Save loss: 17.329429689630867 Name: 260_train_model.pth
step: 0 epoch: 261 loss: 19.263023376464844 loss_input: 85.31524658203125
step: 1000 epoch: 261 loss: 17.015910093839114 loss_input: 81.24443549614448
step: 2000 epoch: 261 loss: 17.112474373612983 loss_input: 81.47017348259465
step: 3000 epoch: 261 loss: 17.13077161551237 loss_input: 81.58873862927852
step: 4000 epoch: 261 loss: 17.178464285584276 loss_input: 81.8444827269745
step: 5000 epoch: 261 loss: 17.181921896041096 loss_input: 81.76939593770699
step: 6000 epoch: 261 loss: 17.220063157368454 loss_input: 82.00847704174956
step: 7000 epoch: 261 loss: 17.250224753663023 loss_input: 82.09323294914343
step: 8000 epoch: 261 loss: 17.260872342470005 loss_input: 82.25603645400038
step: 9000 epoch: 261 loss: 17.259005943254262 loss_input: 82.32118410725631
step: 10000 epoch: 261 loss: 17.279019328883 loss_input: 82.2235765131983
step: 11000 epoch: 261 loss: 17.282954769431 loss_input: 82.19189650084189
step: 12000 epoch: 261 loss: 17.297578829883008 loss_input: 82.26506864987813
step: 13000 epoch: 261 loss: 17.315257471475718 loss_input: 82.3327376306465
step: 14000 epoch: 261 loss: 17.31813251431538 loss_input: 82.25464800958693
step: 15000 epoch: 261 loss: 17.31875186431854 loss_input: 82.18959232063565
Save loss: 17.315908855482935 Name: 261_train_model.pth
step: 0 epoch: 262 loss: 16.310888290405273 loss_input: 102.52557373046875
step: 1000 epoch: 262 loss: 17.290336210172732 loss_input: 81.99610124982439
step: 2000 epoch: 262 loss: 17.176237397525146 loss_input: 81.598284933461
step: 3000 epoch: 262 loss: 17.178081079627308 loss_input: 81.6315877088822
step: 4000 epoch: 262 loss: 17.190675031957074 loss_input: 81.83575837607981
step: 5000 epoch: 262 loss: 17.209172462468338 loss_input: 82.23468884104038
step: 6000 epoch: 262 loss: 17.270510546745133 loss_input: 82.43677384335683
step: 7000 epoch: 262 loss: 17.302899184490574 loss_input: 82.53889242803483
step: 8000 epoch: 262 loss: 17.30027894013644 loss_input: 82.47832717199412
step: 9000 epoch: 262 loss: 17.33713340295737 loss_input: 82.58556935649729
step: 10000 epoch: 262 loss: 17.3299023081405 loss_input: 82.43473216765237
step: 11000 epoch: 262 loss: 17.33739821209321 loss_input: 82.4383860833319
step: 12000 epoch: 262 loss: 17.344012749849703 loss_input: 82.41756432915258
step: 13000 epoch: 262 loss: 17.3404685286098 loss_input: 82.45833335367682
step: 14000 epoch: 262 loss: 17.32112455921474 loss_input: 82.31669779411682
step: 15000 epoch: 262 loss: 17.329593597495645 loss_input: 82.32402647359316
Save loss: 17.316869269892575 Name: 262_train_model.pth
step: 0 epoch: 263 loss: 13.59006118774414 loss_input: 80.43524169921875
step: 1000 epoch: 263 loss: 17.161558037633068 loss_input: 82.56681825254823
step: 2000 epoch: 263 loss: 17.165211844718318 loss_input: 81.37451867328055
step: 3000 epoch: 263 loss: 17.213198112909176 loss_input: 81.64504279823392
step: 4000 epoch: 263 loss: 17.2739036422883 loss_input: 82.10915523682556
step: 5000 epoch: 263 loss: 17.2796599135068 loss_input: 82.16843612478677
step: 6000 epoch: 263 loss: 17.238193233298016 loss_input: 82.0117691362963
step: 7000 epoch: 263 loss: 17.268855841053227 loss_input: 82.09618130201545
step: 8000 epoch: 263 loss: 17.260855259947174 loss_input: 82.06116981349011
step: 9000 epoch: 263 loss: 17.28704238565693 loss_input: 82.0468016710166
step: 10000 epoch: 263 loss: 17.273296644420412 loss_input: 81.90805386014134
step: 11000 epoch: 263 loss: 17.264486259898753 loss_input: 81.93687576697573
step: 12000 epoch: 263 loss: 17.286803483665015 loss_input: 82.07155992921
step: 13000 epoch: 263 loss: 17.28637869304331 loss_input: 82.09436454388575
step: 14000 epoch: 263 loss: 17.28084032185273 loss_input: 82.14560739296384
step: 15000 epoch: 263 loss: 17.296696414074002 loss_input: 82.1164005407452
Save loss: 17.309224220484495 Name: 263_train_model.pth
step: 0 epoch: 264 loss: 16.27501678466797 loss_input: 77.044677734375
step: 1000 epoch: 264 loss: 17.359936063939873 loss_input: 82.8023368843071
step: 2000 epoch: 264 loss: 17.27033786997683 loss_input: 82.5857769662532
step: 3000 epoch: 264 loss: 17.27761976085715 loss_input: 82.85949080612453
step: 4000 epoch: 264 loss: 17.29068919915731 loss_input: 82.46987386454168
step: 5000 epoch: 264 loss: 17.29018964483318 loss_input: 82.48389175924581
step: 6000 epoch: 264 loss: 17.305041302523797 loss_input: 82.5326015610354
step: 7000 epoch: 264 loss: 17.293119202169617 loss_input: 82.54968932137764
step: 8000 epoch: 264 loss: 17.283962421038197 loss_input: 82.47351000985716
step: 9000 epoch: 264 loss: 17.29118141064762 loss_input: 82.4140758448714
step: 10000 epoch: 264 loss: 17.316743675583233 loss_input: 82.37894214728918
step: 11000 epoch: 264 loss: 17.3189270137256 loss_input: 82.41568281390258
step: 12000 epoch: 264 loss: 17.314959193615007 loss_input: 82.34135215396276
step: 13000 epoch: 264 loss: 17.318281598737006 loss_input: 82.22123583642605
step: 14000 epoch: 264 loss: 17.320494005845433 loss_input: 82.24653807163885
step: 15000 epoch: 264 loss: 17.30993431015465 loss_input: 82.22535266854605
Save loss: 17.309532874539496 Name: 264_train_model.pth
step: 0 epoch: 265 loss: 23.36703872680664 loss_input: 85.19952392578125
step: 1000 epoch: 265 loss: 17.28012265215863 loss_input: 82.29067105989714
step: 2000 epoch: 265 loss: 17.25750900661272 loss_input: 82.07130660133801
step: 3000 epoch: 265 loss: 17.227819652805245 loss_input: 82.22463367923582
step: 4000 epoch: 265 loss: 17.275340070130973 loss_input: 82.50084334080054
step: 5000 epoch: 265 loss: 17.281967434542725 loss_input: 82.24277002509655
step: 6000 epoch: 265 loss: 17.314530744172796 loss_input: 82.13179090106075
step: 7000 epoch: 265 loss: 17.292881915712677 loss_input: 82.04630047095672
step: 8000 epoch: 265 loss: 17.296730497094664 loss_input: 82.06970377329543
step: 9000 epoch: 265 loss: 17.298493736837322 loss_input: 82.1427601997673
step: 10000 epoch: 265 loss: 17.28896418951378 loss_input: 82.1160367056556
step: 11000 epoch: 265 loss: 17.287381422301877 loss_input: 82.08716786244752
step: 12000 epoch: 265 loss: 17.30054334979903 loss_input: 82.13445781475404
step: 13000 epoch: 265 loss: 17.283564775634314 loss_input: 82.10702547745726
step: 14000 epoch: 265 loss: 17.297184369741323 loss_input: 82.1271209814882
step: 15000 epoch: 265 loss: 17.316732600389912 loss_input: 82.22519327975792
Save loss: 17.31037193673849 Name: 265_train_model.pth
step: 0 epoch: 266 loss: 20.00809097290039 loss_input: 78.65155029296875
step: 1000 epoch: 266 loss: 17.243461953771938 loss_input: 82.68520529763205
step: 2000 epoch: 266 loss: 17.269383601341648 loss_input: 82.89384827239701
step: 3000 epoch: 266 loss: 17.279488168212424 loss_input: 82.7283028053149
step: 4000 epoch: 266 loss: 17.245509481942527 loss_input: 82.58339118528474
step: 5000 epoch: 266 loss: 17.243280468070967 loss_input: 82.35082636792954
step: 6000 epoch: 266 loss: 17.26947344869122 loss_input: 82.39405296667678
step: 7000 epoch: 266 loss: 17.28286582738089 loss_input: 82.26753674035685
step: 8000 epoch: 266 loss: 17.29653996244816 loss_input: 82.45300713295848
step: 9000 epoch: 266 loss: 17.29168249847968 loss_input: 82.26393928777878
step: 10000 epoch: 266 loss: 17.291681213577252 loss_input: 82.29188228638074
step: 11000 epoch: 266 loss: 17.2803437763556 loss_input: 82.22153494746216
step: 12000 epoch: 266 loss: 17.277135654445807 loss_input: 82.25331660797472
step: 13000 epoch: 266 loss: 17.28519371270895 loss_input: 82.17825896937465
step: 14000 epoch: 266 loss: 17.286169684756594 loss_input: 82.174720181643
step: 15000 epoch: 266 loss: 17.299279863075785 loss_input: 82.18666895595632
Save loss: 17.308750493243338 Name: 266_train_model.pth
step: 0 epoch: 267 loss: 12.3031644821167 loss_input: 54.3846435546875
step: 1000 epoch: 267 loss: 17.28389074323656 loss_input: 83.40997905902691
step: 2000 epoch: 267 loss: 17.146053919251237 loss_input: 82.03516570572135
step: 3000 epoch: 267 loss: 17.155021950468466 loss_input: 82.11637134545646
step: 4000 epoch: 267 loss: 17.226789859317893 loss_input: 81.897417048936
step: 5000 epoch: 267 loss: 17.239742311375828 loss_input: 82.0072366142006
step: 6000 epoch: 267 loss: 17.22035076220499 loss_input: 81.91896593894508
step: 7000 epoch: 267 loss: 17.257559634160867 loss_input: 82.22627089824222
step: 8000 epoch: 267 loss: 17.275318548867382 loss_input: 82.2265904811215
step: 9000 epoch: 267 loss: 17.26952457139259 loss_input: 82.14089852239725
step: 10000 epoch: 267 loss: 17.273553438013096 loss_input: 82.22225713219693
step: 11000 epoch: 267 loss: 17.27792633120531 loss_input: 82.24217585998929
step: 12000 epoch: 267 loss: 17.290957067660635 loss_input: 82.32094271685042
step: 13000 epoch: 267 loss: 17.293155626575448 loss_input: 82.26952924538406
step: 14000 epoch: 267 loss: 17.28501535653983 loss_input: 82.22626344909516
step: 15000 epoch: 267 loss: 17.29454906652247 loss_input: 82.24705542905212
Save loss: 17.2993600038141 Name: 267_train_model.pth
step: 0 epoch: 268 loss: 15.86080551147461 loss_input: 57.8118896484375
step: 1000 epoch: 268 loss: 17.30482068857351 loss_input: 82.03707736355442
step: 2000 epoch: 268 loss: 17.270784738598795 loss_input: 82.41618845118457
step: 3000 epoch: 268 loss: 17.219880595202447 loss_input: 82.02193722697903
step: 4000 epoch: 268 loss: 17.245939846248575 loss_input: 82.068879173029
step: 5000 epoch: 268 loss: 17.243018572532137 loss_input: 82.35556309148326
step: 6000 epoch: 268 loss: 17.262908284613697 loss_input: 82.152078016701
step: 7000 epoch: 268 loss: 17.25785492781383 loss_input: 82.19289377814889
step: 8000 epoch: 268 loss: 17.279591363722822 loss_input: 82.39395646222933
step: 9000 epoch: 268 loss: 17.29194455970037 loss_input: 82.46176836742532
step: 10000 epoch: 268 loss: 17.28323744881238 loss_input: 82.44151807353444
step: 11000 epoch: 268 loss: 17.29908919889226 loss_input: 82.49326010091058
step: 12000 epoch: 268 loss: 17.288934759433722 loss_input: 82.37075999180244
step: 13000 epoch: 268 loss: 17.304222458500593 loss_input: 82.4428460759554
step: 14000 epoch: 268 loss: 17.301537928057776 loss_input: 82.3646869549078
step: 15000 epoch: 268 loss: 17.303486693438654 loss_input: 82.29890714398337
Save loss: 17.29946873228252 Name: 268_train_model.pth
step: 0 epoch: 269 loss: 6.09892463684082 loss_input: 52.7947998046875
step: 1000 epoch: 269 loss: 17.478109913748817 loss_input: 82.025124045876
step: 2000 epoch: 269 loss: 17.29631826223462 loss_input: 82.26766029826884
step: 3000 epoch: 269 loss: 17.245932536298376 loss_input: 81.88282598427477
step: 4000 epoch: 269 loss: 17.273175072354157 loss_input: 81.99947673909159
step: 5000 epoch: 269 loss: 17.279499682396132 loss_input: 82.00614291204259
step: 6000 epoch: 269 loss: 17.285504421379702 loss_input: 82.19576205072592
step: 7000 epoch: 269 loss: 17.295493195796247 loss_input: 82.10736034291418
step: 8000 epoch: 269 loss: 17.289588464139296 loss_input: 82.20451286515211
step: 9000 epoch: 269 loss: 17.295518992040147 loss_input: 82.10973975303742
step: 10000 epoch: 269 loss: 17.286604206343434 loss_input: 82.05082848648729
step: 11000 epoch: 269 loss: 17.290247808552994 loss_input: 82.08978640990911
step: 12000 epoch: 269 loss: 17.276703371662723 loss_input: 82.04273949562634
step: 13000 epoch: 269 loss: 17.28302174098271 loss_input: 82.184658703387
step: 14000 epoch: 269 loss: 17.288486534268095 loss_input: 82.18532559476779
step: 15000 epoch: 269 loss: 17.296068909470062 loss_input: 82.21732139650976
Save loss: 17.29848616591096 Name: 269_train_model.pth
step: 0 epoch: 270 loss: 11.4232177734375 loss_input: 52.74853515625
step: 1000 epoch: 270 loss: 16.986724220432126 loss_input: 81.32456325412869
step: 2000 epoch: 270 loss: 17.116936341218505 loss_input: 81.56069697695098
step: 3000 epoch: 270 loss: 17.277753692672714 loss_input: 82.04156860229533
step: 4000 epoch: 270 loss: 17.272656928774893 loss_input: 82.25121333497192
step: 5000 epoch: 270 loss: 17.247375692231397 loss_input: 82.15584572294526
step: 6000 epoch: 270 loss: 17.264302062463848 loss_input: 82.23479072254234
step: 7000 epoch: 270 loss: 17.282223629008154 loss_input: 82.20377872392393
step: 8000 epoch: 270 loss: 17.281866875220473 loss_input: 82.16161774486442
step: 9000 epoch: 270 loss: 17.27920480439854 loss_input: 82.15224939335083
step: 10000 epoch: 270 loss: 17.27730376045056 loss_input: 82.08852269680258
step: 11000 epoch: 270 loss: 17.273973665891067 loss_input: 82.11567413771155
step: 12000 epoch: 270 loss: 17.262430474039178 loss_input: 82.00264147299248
step: 13000 epoch: 270 loss: 17.2809435000888 loss_input: 82.11776552889111
step: 14000 epoch: 270 loss: 17.281789941997513 loss_input: 82.20657045876466
step: 15000 epoch: 270 loss: 17.282037925405522 loss_input: 82.25486975184727
Save loss: 17.29037025131285 Name: 270_train_model.pth
step: 0 epoch: 271 loss: 16.23158836364746 loss_input: 99.08404541015625
step: 1000 epoch: 271 loss: 17.240011660130946 loss_input: 81.86311253443822
step: 2000 epoch: 271 loss: 17.228131328803904 loss_input: 82.33115382006262
step: 3000 epoch: 271 loss: 17.182601703718795 loss_input: 81.95434055754201
step: 4000 epoch: 271 loss: 17.21988103586982 loss_input: 82.02327492772802
step: 5000 epoch: 271 loss: 17.227806056268072 loss_input: 82.09400110944608
step: 6000 epoch: 271 loss: 17.22942251965237 loss_input: 82.16123868067888
step: 7000 epoch: 271 loss: 17.265495835534335 loss_input: 82.29162067598044
step: 8000 epoch: 271 loss: 17.260924090267554 loss_input: 82.23480814287862
step: 9000 epoch: 271 loss: 17.277911946927425 loss_input: 82.2153019510419
step: 10000 epoch: 271 loss: 17.29558295703461 loss_input: 82.16327760534493
step: 11000 epoch: 271 loss: 17.294035013475565 loss_input: 82.15280496739288
step: 12000 epoch: 271 loss: 17.30328288400941 loss_input: 82.18419108181415
step: 13000 epoch: 271 loss: 17.298555228446283 loss_input: 82.20224720356548
step: 14000 epoch: 271 loss: 17.297033869073303 loss_input: 82.34662905373528
step: 15000 epoch: 271 loss: 17.29722148869389 loss_input: 82.2772760827989
Save loss: 17.294682038113475 Name: 271_train_model.pth
step: 0 epoch: 272 loss: 21.6020450592041 loss_input: 51.12530517578125
step: 1000 epoch: 272 loss: 17.228565854864282 loss_input: 82.73647641421078
step: 2000 epoch: 272 loss: 17.293881394397253 loss_input: 82.65736824580874
step: 3000 epoch: 272 loss: 17.33149327631197 loss_input: 82.79007422737342
step: 4000 epoch: 272 loss: 17.287547278243345 loss_input: 82.35944500508174
step: 5000 epoch: 272 loss: 17.231231158267402 loss_input: 82.18455836420607
step: 6000 epoch: 272 loss: 17.26028874841775 loss_input: 82.24915649230988
step: 7000 epoch: 272 loss: 17.279485810196345 loss_input: 82.25988329269089
step: 8000 epoch: 272 loss: 17.27155452518966 loss_input: 82.17543894659651
step: 9000 epoch: 272 loss: 17.261232552190926 loss_input: 82.16782548249847
step: 10000 epoch: 272 loss: 17.269504678665356 loss_input: 82.23469687333025
step: 11000 epoch: 272 loss: 17.263056192492563 loss_input: 82.21990323766731
step: 12000 epoch: 272 loss: 17.259971677099763 loss_input: 82.20924740443179
step: 13000 epoch: 272 loss: 17.278738670739365 loss_input: 82.1795864898914
step: 14000 epoch: 272 loss: 17.27421948777106 loss_input: 82.09836944281463
step: 15000 epoch: 272 loss: 17.274903189507366 loss_input: 82.1689999638045
Save loss: 17.291955567747355 Name: 272_train_model.pth
step: 0 epoch: 273 loss: 13.007244110107422 loss_input: 77.651611328125
step: 1000 epoch: 273 loss: 17.095436907910205 loss_input: 82.12165056623064
step: 2000 epoch: 273 loss: 17.2759501105961 loss_input: 82.50991639800218
step: 3000 epoch: 273 loss: 17.274947185192215 loss_input: 82.390382872864
step: 4000 epoch: 273 loss: 17.288182162428104 loss_input: 82.3511361539498
step: 5000 epoch: 273 loss: 17.216591863435784 loss_input: 82.14570535599911
step: 6000 epoch: 273 loss: 17.180897736068648 loss_input: 82.02270966516974
step: 7000 epoch: 273 loss: 17.186693496115634 loss_input: 82.07608294204344
step: 8000 epoch: 273 loss: 17.19703906644152 loss_input: 82.11036693380856
step: 9000 epoch: 273 loss: 17.19456624629802 loss_input: 82.12959771505423
step: 10000 epoch: 273 loss: 17.230082114354786 loss_input: 82.11811014943427
step: 11000 epoch: 273 loss: 17.23542269627925 loss_input: 82.0253629508468
step: 12000 epoch: 273 loss: 17.230493062615903 loss_input: 82.01474454968921
step: 13000 epoch: 273 loss: 17.238413111777152 loss_input: 82.01492407923249
step: 14000 epoch: 273 loss: 17.242467318370625 loss_input: 82.07909048105103
step: 15000 epoch: 273 loss: 17.267455840315804 loss_input: 82.10769034668522
Save loss: 17.292554306954145 Name: 273_train_model.pth
step: 0 epoch: 274 loss: 7.842053413391113 loss_input: 47.72021484375
step: 1000 epoch: 274 loss: 16.924612997056958 loss_input: 82.46456241512394
step: 2000 epoch: 274 loss: 17.175799574749522 loss_input: 82.55120526284709
step: 3000 epoch: 274 loss: 17.235241717713865 loss_input: 82.12603977385143
step: 4000 epoch: 274 loss: 17.202258237806806 loss_input: 82.24911810987683
step: 5000 epoch: 274 loss: 17.211083916086505 loss_input: 82.34049542630537
step: 6000 epoch: 274 loss: 17.202250486849707 loss_input: 82.29532259556676
step: 7000 epoch: 274 loss: 17.242809516875134 loss_input: 82.37377220037204
step: 8000 epoch: 274 loss: 17.257307292669807 loss_input: 82.28297971847758
step: 9000 epoch: 274 loss: 17.25348479900608 loss_input: 82.17941981598399
step: 10000 epoch: 274 loss: 17.24880113392851 loss_input: 82.28688528852193
step: 11000 epoch: 274 loss: 17.262852782975997 loss_input: 82.30211118502721
step: 12000 epoch: 274 loss: 17.253260714621774 loss_input: 82.28668230691062
step: 13000 epoch: 274 loss: 17.253995858763798 loss_input: 82.22912511274673
step: 14000 epoch: 274 loss: 17.267597152355627 loss_input: 82.22154563771393
step: 15000 epoch: 274 loss: 17.26969421773694 loss_input: 82.21764354300208
Save loss: 17.283322037950157 Name: 274_train_model.pth
step: 0 epoch: 275 loss: 19.95443344116211 loss_input: 63.70123291015625
step: 1000 epoch: 275 loss: 17.104542846089 loss_input: 81.87139288933723
step: 2000 epoch: 275 loss: 17.293039585935183 loss_input: 82.04604051733898
step: 3000 epoch: 275 loss: 17.272515675259687 loss_input: 81.9195022557585
step: 4000 epoch: 275 loss: 17.199559532919697 loss_input: 81.95487738477978
step: 5000 epoch: 275 loss: 17.17720283519933 loss_input: 81.93350005536003
step: 6000 epoch: 275 loss: 17.218351734298366 loss_input: 82.14766981585426
step: 7000 epoch: 275 loss: 17.238388579601118 loss_input: 82.05891664088445
step: 8000 epoch: 275 loss: 17.250413251152604 loss_input: 82.02648797623442
step: 9000 epoch: 275 loss: 17.277314393576773 loss_input: 82.07240772618147
step: 10000 epoch: 275 loss: 17.269909655424417 loss_input: 82.25127925439878
step: 11000 epoch: 275 loss: 17.258962377376225 loss_input: 82.13150582834977
step: 12000 epoch: 275 loss: 17.272432402068105 loss_input: 82.20203230236184
step: 13000 epoch: 275 loss: 17.273985665941705 loss_input: 82.24247698625797
step: 14000 epoch: 275 loss: 17.283592354339426 loss_input: 82.25492062107868
step: 15000 epoch: 275 loss: 17.28000045700015 loss_input: 82.25443618293286
Save loss: 17.287811260923743 Name: 275_train_model.pth
step: 0 epoch: 276 loss: 17.156841278076172 loss_input: 94.3282470703125
step: 1000 epoch: 276 loss: 17.292873323975982 loss_input: 81.42199249724885
step: 2000 epoch: 276 loss: 17.308850209037402 loss_input: 81.87312514397098
step: 3000 epoch: 276 loss: 17.279674461069842 loss_input: 81.95276564306039
step: 4000 epoch: 276 loss: 17.304826772084866 loss_input: 82.20244278224877
step: 5000 epoch: 276 loss: 17.324779689943664 loss_input: 82.50834260638106
step: 6000 epoch: 276 loss: 17.309857543637964 loss_input: 82.52889428300035
step: 7000 epoch: 276 loss: 17.28737463094289 loss_input: 82.43281657830968
step: 8000 epoch: 276 loss: 17.260785209731807 loss_input: 82.28599532510829
step: 9000 epoch: 276 loss: 17.254765933916946 loss_input: 82.27672517994962
step: 10000 epoch: 276 loss: 17.258956246704546 loss_input: 82.2186196968682
step: 11000 epoch: 276 loss: 17.27450004595755 loss_input: 82.19613687684998
step: 12000 epoch: 276 loss: 17.280109533438353 loss_input: 82.12102412486134
step: 13000 epoch: 276 loss: 17.281373310488892 loss_input: 82.05735925399068
step: 14000 epoch: 276 loss: 17.27796722456316 loss_input: 82.13434092912169
step: 15000 epoch: 276 loss: 17.286117158518753 loss_input: 82.16193796308762
Save loss: 17.281360639944673 Name: 276_train_model.pth
step: 0 epoch: 277 loss: 21.94300651550293 loss_input: 89.239501953125
step: 1000 epoch: 277 loss: 17.444586046925792 loss_input: 83.36767968359766
step: 2000 epoch: 277 loss: 17.402274524015763 loss_input: 83.23617994469562
step: 3000 epoch: 277 loss: 17.252265044507563 loss_input: 82.81081967225117
step: 4000 epoch: 277 loss: 17.215933183228127 loss_input: 82.40126758776614
step: 5000 epoch: 277 loss: 17.150730552017343 loss_input: 82.13808723850933
step: 6000 epoch: 277 loss: 17.207923369732644 loss_input: 82.50313915576245
step: 7000 epoch: 277 loss: 17.23808822010674 loss_input: 82.49497315638781
step: 8000 epoch: 277 loss: 17.25038488109981 loss_input: 82.6035773390875
step: 9000 epoch: 277 loss: 17.23907289268202 loss_input: 82.41576435761695
step: 10000 epoch: 277 loss: 17.259259327854256 loss_input: 82.40815330698375
step: 11000 epoch: 277 loss: 17.25574146798606 loss_input: 82.37938437814248
step: 12000 epoch: 277 loss: 17.257057851279303 loss_input: 82.23202144145051
step: 13000 epoch: 277 loss: 17.27405443344471 loss_input: 82.13992088217523
step: 14000 epoch: 277 loss: 17.2810872520313 loss_input: 82.14628230639079
step: 15000 epoch: 277 loss: 17.275745284198944 loss_input: 82.17953266413099
Save loss: 17.27918713273108 Name: 277_train_model.pth
step: 0 epoch: 278 loss: 11.877785682678223 loss_input: 42.83343505859375
step: 1000 epoch: 278 loss: 17.16601292022339 loss_input: 82.13392564466783
step: 2000 epoch: 278 loss: 17.31537408533244 loss_input: 82.68512505832045
step: 3000 epoch: 278 loss: 17.223057734175786 loss_input: 82.23691537228476
step: 4000 epoch: 278 loss: 17.215078745863195 loss_input: 82.15314138689449
step: 5000 epoch: 278 loss: 17.204108409275175 loss_input: 81.98311378983969
step: 6000 epoch: 278 loss: 17.194531357381727 loss_input: 82.04249409766341
step: 7000 epoch: 278 loss: 17.24153919489686 loss_input: 82.06528257019366
step: 8000 epoch: 278 loss: 17.25253762764985 loss_input: 81.95947214514446
step: 9000 epoch: 278 loss: 17.243037591257593 loss_input: 81.91631738997316
step: 10000 epoch: 278 loss: 17.256050595091935 loss_input: 81.93296305745402
step: 11000 epoch: 278 loss: 17.26833954686176 loss_input: 82.04974968317866
step: 12000 epoch: 278 loss: 17.26805301463939 loss_input: 82.05608321193775
step: 13000 epoch: 278 loss: 17.269986152392185 loss_input: 82.18077889835547
step: 14000 epoch: 278 loss: 17.278658782556292 loss_input: 82.29863035034327
step: 15000 epoch: 278 loss: 17.279816455884294 loss_input: 82.27902829345882
Save loss: 17.278514183163644 Name: 278_train_model.pth
step: 0 epoch: 279 loss: 18.870800018310547 loss_input: 66.9566650390625
step: 1000 epoch: 279 loss: 17.060548097341805 loss_input: 81.53057239391468
step: 2000 epoch: 279 loss: 17.191819086246404 loss_input: 81.82038753572432
step: 3000 epoch: 279 loss: 17.21040523429586 loss_input: 81.76248188266672
step: 4000 epoch: 279 loss: 17.289076536424574 loss_input: 82.43700421330215
step: 5000 epoch: 279 loss: 17.277909094704267 loss_input: 82.15909034989878
step: 6000 epoch: 279 loss: 17.259535044277733 loss_input: 82.02281562488609
step: 7000 epoch: 279 loss: 17.23740470659697 loss_input: 82.08973198443749
step: 8000 epoch: 279 loss: 17.241499435691562 loss_input: 82.0549773655002
step: 9000 epoch: 279 loss: 17.263891413004952 loss_input: 82.17590909766537
step: 10000 epoch: 279 loss: 17.267001152872957 loss_input: 82.13655783469504
step: 11000 epoch: 279 loss: 17.26561950917743 loss_input: 82.07319912833741
step: 12000 epoch: 279 loss: 17.26840142960966 loss_input: 82.09624131867592
step: 13000 epoch: 279 loss: 17.26013282337296 loss_input: 82.12367954023819
step: 14000 epoch: 279 loss: 17.256269087289777 loss_input: 82.13182838711856
step: 15000 epoch: 279 loss: 17.264413296242616 loss_input: 82.0894918528869
Save loss: 17.269618564650415 Name: 279_train_model.pth
step: 0 epoch: 280 loss: 18.295385360717773 loss_input: 84.978515625
step: 1000 epoch: 280 loss: 17.43817884319431 loss_input: 83.82746513740166
step: 2000 epoch: 280 loss: 17.31046102524757 loss_input: 82.80631897796219
step: 3000 epoch: 280 loss: 17.29448726486262 loss_input: 82.73567376141547
step: 4000 epoch: 280 loss: 17.27760558651555 loss_input: 82.57776048236477
step: 5000 epoch: 280 loss: 17.232365212233585 loss_input: 82.11295309297516
step: 6000 epoch: 280 loss: 17.243021028992572 loss_input: 82.22567249127098
step: 7000 epoch: 280 loss: 17.28588571039 loss_input: 82.32617845713726
step: 8000 epoch: 280 loss: 17.27558817849757 loss_input: 82.30973226430193
step: 9000 epoch: 280 loss: 17.30146396081986 loss_input: 82.24935688298089
step: 10000 epoch: 280 loss: 17.27693164120935 loss_input: 82.27426879504432
step: 11000 epoch: 280 loss: 17.293157554888616 loss_input: 82.34271492120212
step: 12000 epoch: 280 loss: 17.277897741047404 loss_input: 82.32934474698723
step: 13000 epoch: 280 loss: 17.286738374344928 loss_input: 82.40856253338836
step: 14000 epoch: 280 loss: 17.285591488914417 loss_input: 82.39423741286895
step: 15000 epoch: 280 loss: 17.27155011485842 loss_input: 82.35831371414432
Save loss: 17.275271906539796 Name: 280_train_model.pth
step: 0 epoch: 281 loss: 12.049873352050781 loss_input: 88.63189697265625
step: 1000 epoch: 281 loss: 17.10250252657956 loss_input: 82.19512396782905
step: 2000 epoch: 281 loss: 17.221812733407617 loss_input: 82.03446403019194
step: 3000 epoch: 281 loss: 17.207332358285612 loss_input: 82.30028469107303
step: 4000 epoch: 281 loss: 17.169915421430364 loss_input: 82.49206224139766
step: 5000 epoch: 281 loss: 17.17280246200287 loss_input: 82.08601202122857
step: 6000 epoch: 281 loss: 17.135935909170804 loss_input: 81.87199938112528
step: 7000 epoch: 281 loss: 17.185106058526664 loss_input: 81.94999843249235
step: 8000 epoch: 281 loss: 17.178263377255313 loss_input: 81.9127450453581
step: 9000 epoch: 281 loss: 17.17135143298571 loss_input: 81.97556143121685
step: 10000 epoch: 281 loss: 17.186641544809866 loss_input: 81.92875686796555
step: 11000 epoch: 281 loss: 17.222588985424217 loss_input: 81.98462926044365
step: 12000 epoch: 281 loss: 17.252332558622758 loss_input: 82.20430911051194
step: 13000 epoch: 281 loss: 17.26410312432893 loss_input: 82.26445709761359
step: 14000 epoch: 281 loss: 17.26475088762033 loss_input: 82.28066278346887
step: 15000 epoch: 281 loss: 17.252726886806613 loss_input: 82.23756822787335
Save loss: 17.273060580685733 Name: 281_train_model.pth
step: 0 epoch: 282 loss: 18.068889617919922 loss_input: 66.5345458984375
step: 1000 epoch: 282 loss: 17.292942170496588 loss_input: 83.21582799524694
step: 2000 epoch: 282 loss: 17.28257245960264 loss_input: 82.6588236643397
step: 3000 epoch: 282 loss: 17.31610803626371 loss_input: 82.99476110502228
step: 4000 epoch: 282 loss: 17.25859556577111 loss_input: 82.56874796498629
step: 5000 epoch: 282 loss: 17.284567013904347 loss_input: 82.6570253332146
step: 6000 epoch: 282 loss: 17.268931497714654 loss_input: 82.50934770555818
step: 7000 epoch: 282 loss: 17.227790742342073 loss_input: 82.24046207335894
step: 8000 epoch: 282 loss: 17.231440393109125 loss_input: 82.11361696243048
step: 9000 epoch: 282 loss: 17.24749947902746 loss_input: 82.16398894046495
step: 10000 epoch: 282 loss: 17.259535542465116 loss_input: 82.1505155514722
step: 11000 epoch: 282 loss: 17.261076415671987 loss_input: 82.14539626702083
step: 12000 epoch: 282 loss: 17.26871320965111 loss_input: 82.13987674590757
step: 13000 epoch: 282 loss: 17.276083849877654 loss_input: 82.19116679933124
step: 14000 epoch: 282 loss: 17.272926804440573 loss_input: 82.13026757679975
step: 15000 epoch: 282 loss: 17.278179714897618 loss_input: 82.20601561946651
Save loss: 17.26593639010191 Name: 282_train_model.pth
step: 0 epoch: 283 loss: 19.683683395385742 loss_input: 96.01068115234375
step: 1000 epoch: 283 loss: 17.31762688738721 loss_input: 82.82841590782265
step: 2000 epoch: 283 loss: 17.27500256915381 loss_input: 82.14248481838183
step: 3000 epoch: 283 loss: 17.310937683489353 loss_input: 82.27507859522086
step: 4000 epoch: 283 loss: 17.323569165858828 loss_input: 82.27603261538131
step: 5000 epoch: 283 loss: 17.33091833676798 loss_input: 82.3459287024431
step: 6000 epoch: 283 loss: 17.32256529494497 loss_input: 82.13055935583002
step: 7000 epoch: 283 loss: 17.304480539800576 loss_input: 82.25508292690888
step: 8000 epoch: 283 loss: 17.292888714214993 loss_input: 82.27865290489812
step: 9000 epoch: 283 loss: 17.2791287232579 loss_input: 82.31406686963167
step: 10000 epoch: 283 loss: 17.277513685565914 loss_input: 82.3250514975358
step: 11000 epoch: 283 loss: 17.245458277406804 loss_input: 82.31676045474914
step: 12000 epoch: 283 loss: 17.232954345497625 loss_input: 82.25378861962513
step: 13000 epoch: 283 loss: 17.220978849145617 loss_input: 82.19856226410319
step: 14000 epoch: 283 loss: 17.226821140326496 loss_input: 82.23556902830741
step: 15000 epoch: 283 loss: 17.246016591145636 loss_input: 82.2324686207443
Save loss: 17.2589234662503 Name: 283_train_model.pth
step: 0 epoch: 284 loss: 19.584468841552734 loss_input: 80.185546875
step: 1000 epoch: 284 loss: 17.3780660938907 loss_input: 82.03319714953015
step: 2000 epoch: 284 loss: 17.292839340779974 loss_input: 82.08642535421743
step: 3000 epoch: 284 loss: 17.289423296507977 loss_input: 82.06929369490769
step: 4000 epoch: 284 loss: 17.304699262062687 loss_input: 82.36653991104275
step: 5000 epoch: 284 loss: 17.23904640334674 loss_input: 82.30219928914607
step: 6000 epoch: 284 loss: 17.243111540964893 loss_input: 82.40472513817684
step: 7000 epoch: 284 loss: 17.232507259296426 loss_input: 82.4089982471812
step: 8000 epoch: 284 loss: 17.246100014320778 loss_input: 82.38002430482322
step: 9000 epoch: 284 loss: 17.229276502785346 loss_input: 82.26223992554429
step: 10000 epoch: 284 loss: 17.225600278803068 loss_input: 82.23112201108991
step: 11000 epoch: 284 loss: 17.229275630610758 loss_input: 82.28130003318235
step: 12000 epoch: 284 loss: 17.248173105966032 loss_input: 82.23052895107863
step: 13000 epoch: 284 loss: 17.247761759058566 loss_input: 82.1994937016188
step: 14000 epoch: 284 loss: 17.253791118296512 loss_input: 82.1863364827317
step: 15000 epoch: 284 loss: 17.256108409142097 loss_input: 82.22608018759036
Save loss: 17.264865133434533 Name: 284_train_model.pth
step: 0 epoch: 285 loss: 16.54931640625 loss_input: 89.142822265625
step: 1000 epoch: 285 loss: 17.155867763332555 loss_input: 82.17860612288102
step: 2000 epoch: 285 loss: 17.115121238533106 loss_input: 82.15222543147371
step: 3000 epoch: 285 loss: 17.15539963449569 loss_input: 82.3417668963861
step: 4000 epoch: 285 loss: 17.22416207665832 loss_input: 82.32794735581241
step: 5000 epoch: 285 loss: 17.226637349846698 loss_input: 82.21626111789361
step: 6000 epoch: 285 loss: 17.217942161533042 loss_input: 82.3821573866107
step: 7000 epoch: 285 loss: 17.18843756166803 loss_input: 82.1118040266011
step: 8000 epoch: 285 loss: 17.20066453009721 loss_input: 82.07787278085601
step: 9000 epoch: 285 loss: 17.233581565960343 loss_input: 82.17750469131902
step: 10000 epoch: 285 loss: 17.226825790349967 loss_input: 82.11162971227292
step: 11000 epoch: 285 loss: 17.215123413367852 loss_input: 82.12470373454153
step: 12000 epoch: 285 loss: 17.228617339797363 loss_input: 82.13185817787483
step: 13000 epoch: 285 loss: 17.23889665179652 loss_input: 82.16623982274801
step: 14000 epoch: 285 loss: 17.250575894569042 loss_input: 82.22614888993002
step: 15000 epoch: 285 loss: 17.28701640834698 loss_input: 82.27008614464765
Save loss: 17.280308855324982 Name: 285_train_model.pth
step: 0 epoch: 286 loss: 17.68515396118164 loss_input: 101.8336181640625
step: 1000 epoch: 286 loss: 17.185304325420063 loss_input: 80.99956891253278
step: 2000 epoch: 286 loss: 17.13876989899368 loss_input: 81.50303107723482
step: 3000 epoch: 286 loss: 17.175643956490415 loss_input: 81.80113446478127
step: 4000 epoch: 286 loss: 17.194391749138177 loss_input: 81.90586015910573
step: 5000 epoch: 286 loss: 17.17581812998362 loss_input: 81.67912918881068
step: 6000 epoch: 286 loss: 17.189134066431865 loss_input: 81.66390201445918
step: 7000 epoch: 286 loss: 17.228913751743026 loss_input: 81.85221153360625
step: 8000 epoch: 286 loss: 17.226441228111124 loss_input: 81.97370582430858
step: 9000 epoch: 286 loss: 17.235018821229353 loss_input: 81.99070347977829
step: 10000 epoch: 286 loss: 17.246435543213256 loss_input: 81.96974084329848
step: 11000 epoch: 286 loss: 17.25617791505177 loss_input: 82.05892616717905
step: 12000 epoch: 286 loss: 17.263050524038768 loss_input: 82.0870924381383
step: 13000 epoch: 286 loss: 17.242573799715437 loss_input: 82.10491725997039
step: 14000 epoch: 286 loss: 17.2472778965972 loss_input: 82.19239969187468
step: 15000 epoch: 286 loss: 17.257015444836483 loss_input: 82.20513040422956
Save loss: 17.256075507313014 Name: 286_train_model.pth
step: 0 epoch: 287 loss: 17.218364715576172 loss_input: 106.28631591796875
step: 1000 epoch: 287 loss: 17.14640651716219 loss_input: 82.62018005188172
step: 2000 epoch: 287 loss: 17.07256227323617 loss_input: 81.7108286676974
step: 3000 epoch: 287 loss: 17.12985183905856 loss_input: 81.69269173052342
step: 4000 epoch: 287 loss: 17.21652890443504 loss_input: 82.28338249299979
step: 5000 epoch: 287 loss: 17.19743855889619 loss_input: 82.19072678384245
step: 6000 epoch: 287 loss: 17.21047952774663 loss_input: 82.28709833964847
step: 7000 epoch: 287 loss: 17.222803541871926 loss_input: 82.2607765453847
step: 8000 epoch: 287 loss: 17.249465014692873 loss_input: 82.30412074497768
step: 9000 epoch: 287 loss: 17.238473897509515 loss_input: 82.13282159322792
step: 10000 epoch: 287 loss: 17.219132274547203 loss_input: 82.1251443703286
step: 11000 epoch: 287 loss: 17.22865642537725 loss_input: 82.18105081822284
step: 12000 epoch: 287 loss: 17.233389150320555 loss_input: 82.16962964776536
step: 13000 epoch: 287 loss: 17.236332550772097 loss_input: 82.27740124538948
step: 14000 epoch: 287 loss: 17.23244601925325 loss_input: 82.29904648883199
step: 15000 epoch: 287 loss: 17.24386747175356 loss_input: 82.28711516481583
Save loss: 17.256081589400768 Name: 287_train_model.pth
step: 0 epoch: 288 loss: 20.315675735473633 loss_input: 80.57928466796875
step: 1000 epoch: 288 loss: 17.534029248472933 loss_input: 84.28777600573255
step: 2000 epoch: 288 loss: 17.370947188940242 loss_input: 83.47021652137798
step: 3000 epoch: 288 loss: 17.292292377385486 loss_input: 82.65558085049125
step: 4000 epoch: 288 loss: 17.29247936919998 loss_input: 82.4911823191663
step: 5000 epoch: 288 loss: 17.27675152430413 loss_input: 82.49884850910678
step: 6000 epoch: 288 loss: 17.26532947613863 loss_input: 82.45798260918102
step: 7000 epoch: 288 loss: 17.269624292603595 loss_input: 82.39412589496143
step: 8000 epoch: 288 loss: 17.30124628497666 loss_input: 82.50166749328453
step: 9000 epoch: 288 loss: 17.29524956646396 loss_input: 82.4308997812092
step: 10000 epoch: 288 loss: 17.300000433539907 loss_input: 82.4585065175123
step: 11000 epoch: 288 loss: 17.307179282485503 loss_input: 82.47485151829237
step: 12000 epoch: 288 loss: 17.28365655233836 loss_input: 82.44014215449494
step: 13000 epoch: 288 loss: 17.283318475048116 loss_input: 82.36871534596497
step: 14000 epoch: 288 loss: 17.267005364117168 loss_input: 82.34264369474447
step: 15000 epoch: 288 loss: 17.249253386998525 loss_input: 82.28740362214785
Save loss: 17.2548458878994 Name: 288_train_model.pth
step: 0 epoch: 289 loss: 16.80677032470703 loss_input: 51.98199462890625
step: 1000 epoch: 289 loss: 17.162864877031996 loss_input: 81.92079465944212
step: 2000 epoch: 289 loss: 17.23927926898062 loss_input: 82.2290458774519
step: 3000 epoch: 289 loss: 17.26941265435427 loss_input: 82.1797732664958
step: 4000 epoch: 289 loss: 17.215004682004587 loss_input: 82.11982982952992
step: 5000 epoch: 289 loss: 17.225292648656016 loss_input: 82.27262096408843
step: 6000 epoch: 289 loss: 17.232559263577084 loss_input: 82.22443992895715
step: 7000 epoch: 289 loss: 17.245887398873034 loss_input: 82.1581166243829
step: 8000 epoch: 289 loss: 17.247707981956616 loss_input: 82.02741813492796
step: 9000 epoch: 289 loss: 17.2524399828638 loss_input: 82.10219646954374
step: 10000 epoch: 289 loss: 17.230718419308925 loss_input: 82.14438968700786
step: 11000 epoch: 289 loss: 17.239445121361335 loss_input: 82.23348622581719
step: 12000 epoch: 289 loss: 17.235026737400197 loss_input: 82.18289074461894
step: 13000 epoch: 289 loss: 17.242081760305705 loss_input: 82.24352346149905
step: 14000 epoch: 289 loss: 17.2278354351848 loss_input: 82.18648089903455
step: 15000 epoch: 289 loss: 17.245379311809078 loss_input: 82.19685763217998
Save loss: 17.253676964834334 Name: 289_train_model.pth
step: 0 epoch: 290 loss: 28.248703002929688 loss_input: 89.74371337890625
step: 1000 epoch: 290 loss: 17.14095968156904 loss_input: 81.91011192176964
step: 2000 epoch: 290 loss: 17.212489704320813 loss_input: 82.62190346477152
step: 3000 epoch: 290 loss: 17.19402165478049 loss_input: 82.39035287391182
step: 4000 epoch: 290 loss: 17.14546432509419 loss_input: 82.30174423557196
step: 5000 epoch: 290 loss: 17.16583915015169 loss_input: 82.2685636578739
step: 6000 epoch: 290 loss: 17.20676281753728 loss_input: 82.37065453283827
step: 7000 epoch: 290 loss: 17.202302729090764 loss_input: 82.40917205047717
step: 8000 epoch: 290 loss: 17.226221893924638 loss_input: 82.66620133939557
step: 9000 epoch: 290 loss: 17.21234918384999 loss_input: 82.5489940611525
step: 10000 epoch: 290 loss: 17.21373107135087 loss_input: 82.38745366186038
step: 11000 epoch: 290 loss: 17.219759850380214 loss_input: 82.34764494287806
step: 12000 epoch: 290 loss: 17.223866790227618 loss_input: 82.36654007266972
step: 13000 epoch: 290 loss: 17.233591731791588 loss_input: 82.38916751276795
step: 14000 epoch: 290 loss: 17.225315158532847 loss_input: 82.2221011313087
step: 15000 epoch: 290 loss: 17.227925122781528 loss_input: 82.13620312275405
Save loss: 17.240989041358233 Name: 290_train_model.pth
step: 0 epoch: 291 loss: 12.66021728515625 loss_input: 58.01019287109375
step: 1000 epoch: 291 loss: 17.11277050643296 loss_input: 80.95479361946647
step: 2000 epoch: 291 loss: 17.155691611177023 loss_input: 81.76203344250726
step: 3000 epoch: 291 loss: 17.16116359995747 loss_input: 81.21537484331395
step: 4000 epoch: 291 loss: 17.208344866591254 loss_input: 81.75758895359168
step: 5000 epoch: 291 loss: 17.197505423698967 loss_input: 81.7543808987226
step: 6000 epoch: 291 loss: 17.214982648786556 loss_input: 82.04633087071751
step: 7000 epoch: 291 loss: 17.232984341411758 loss_input: 82.03867616777403
step: 8000 epoch: 291 loss: 17.2338635395831 loss_input: 82.17543828292216
step: 9000 epoch: 291 loss: 17.22717033999587 loss_input: 82.29050407159622
step: 10000 epoch: 291 loss: 17.230842333557057 loss_input: 82.23013933958167
step: 11000 epoch: 291 loss: 17.226060634179067 loss_input: 82.09780486264388
step: 12000 epoch: 291 loss: 17.21607619700318 loss_input: 82.1145547695651
step: 13000 epoch: 291 loss: 17.20797396678116 loss_input: 82.09385112747486
step: 14000 epoch: 291 loss: 17.217587178659887 loss_input: 82.22375273801592
step: 15000 epoch: 291 loss: 17.239355699307648 loss_input: 82.26596673340266
Save loss: 17.25238798978925 Name: 291_train_model.pth
step: 0 epoch: 292 loss: 19.487262725830078 loss_input: 103.062744140625
step: 1000 epoch: 292 loss: 17.203669086917415 loss_input: 81.18454866666536
step: 2000 epoch: 292 loss: 17.229985273104795 loss_input: 81.29944994543743
step: 3000 epoch: 292 loss: 17.18964109735384 loss_input: 81.66085297820648
step: 4000 epoch: 292 loss: 17.24332625500651 loss_input: 82.08762889538846
step: 5000 epoch: 292 loss: 17.1975232513636 loss_input: 82.19773533086351
step: 6000 epoch: 292 loss: 17.21271170423222 loss_input: 82.13541677176525
step: 7000 epoch: 292 loss: 17.225804177442118 loss_input: 82.3763410472611
step: 8000 epoch: 292 loss: 17.229506018936238 loss_input: 82.38541343983613
step: 9000 epoch: 292 loss: 17.23995749678377 loss_input: 82.27589867266268
step: 10000 epoch: 292 loss: 17.231426672224593 loss_input: 82.1572701494499
step: 11000 epoch: 292 loss: 17.220906238514296 loss_input: 82.21612474482966
step: 12000 epoch: 292 loss: 17.241547460844494 loss_input: 82.25364539730738
step: 13000 epoch: 292 loss: 17.257517115793434 loss_input: 82.2329453473531
step: 14000 epoch: 292 loss: 17.250998401137796 loss_input: 82.24433265404312
step: 15000 epoch: 292 loss: 17.239526195467636 loss_input: 82.19513965996016
Save loss: 17.250428431600334 Name: 292_train_model.pth
step: 0 epoch: 293 loss: 10.394587516784668 loss_input: 41.51654052734375
step: 1000 epoch: 293 loss: 17.199871978321514 loss_input: 81.83136419244818
step: 2000 epoch: 293 loss: 17.096263393886325 loss_input: 81.68892384862257
step: 3000 epoch: 293 loss: 17.23143102438678 loss_input: 82.23633888394623
step: 4000 epoch: 293 loss: 17.169926371284795 loss_input: 82.18694606133623
step: 5000 epoch: 293 loss: 17.200984823110222 loss_input: 82.02489752293683
step: 6000 epoch: 293 loss: 17.166741950772956 loss_input: 81.79359324918829
step: 7000 epoch: 293 loss: 17.193437433637836 loss_input: 81.8671862184448
step: 8000 epoch: 293 loss: 17.213142135920727 loss_input: 81.94938868115118
step: 9000 epoch: 293 loss: 17.221656513802145 loss_input: 82.03943784485101
step: 10000 epoch: 293 loss: 17.212517767021172 loss_input: 82.05955491486984
step: 11000 epoch: 293 loss: 17.230049634346063 loss_input: 82.20452576266149
step: 12000 epoch: 293 loss: 17.236174102922586 loss_input: 82.31962347050506
step: 13000 epoch: 293 loss: 17.236039070374837 loss_input: 82.28458169515055
step: 14000 epoch: 293 loss: 17.23671118954779 loss_input: 82.27512861978887
step: 15000 epoch: 293 loss: 17.23643140223541 loss_input: 82.24905987305924
Save loss: 17.240942824825645 Name: 293_train_model.pth
step: 0 epoch: 294 loss: 17.422565460205078 loss_input: 51.6558837890625
step: 1000 epoch: 294 loss: 17.34642676564006 loss_input: 82.60203419627248
step: 2000 epoch: 294 loss: 17.23214044408879 loss_input: 82.88262320255888
step: 3000 epoch: 294 loss: 17.201629794704562 loss_input: 82.43278985943488
step: 4000 epoch: 294 loss: 17.19751847132955 loss_input: 82.50281599216895
step: 5000 epoch: 294 loss: 17.193882264511224 loss_input: 82.16890753929292
step: 6000 epoch: 294 loss: 17.190628603724036 loss_input: 82.20183507419988
step: 7000 epoch: 294 loss: 17.204838900442823 loss_input: 82.4343527445298
step: 8000 epoch: 294 loss: 17.21929124483033 loss_input: 82.37414047304145
step: 9000 epoch: 294 loss: 17.231056981345784 loss_input: 82.3347344408564
step: 10000 epoch: 294 loss: 17.21582875274656 loss_input: 82.16154155458477
step: 11000 epoch: 294 loss: 17.238888158135907 loss_input: 82.35723093554883
step: 12000 epoch: 294 loss: 17.23491335175413 loss_input: 82.24059124325089
step: 13000 epoch: 294 loss: 17.239224934704477 loss_input: 82.1863500969051
step: 14000 epoch: 294 loss: 17.253840373504264 loss_input: 82.28243003481755
step: 15000 epoch: 294 loss: 17.25750802844439 loss_input: 82.26500760138126
Save loss: 17.245587986305356 Name: 294_train_model.pth
step: 0 epoch: 295 loss: 19.958740234375 loss_input: 98.63702392578125
step: 1000 epoch: 295 loss: 17.078703786943343 loss_input: 81.09149377234094
step: 2000 epoch: 295 loss: 17.136565611399394 loss_input: 82.56383929617223
step: 3000 epoch: 295 loss: 17.169950524952682 loss_input: 82.4204353757081
step: 4000 epoch: 295 loss: 17.140366306605262 loss_input: 82.26757104669146
step: 5000 epoch: 295 loss: 17.136991348964553 loss_input: 82.041930847229
step: 6000 epoch: 295 loss: 17.150975680752527 loss_input: 82.06490755772475
step: 7000 epoch: 295 loss: 17.156860225014373 loss_input: 82.10008511967598
step: 8000 epoch: 295 loss: 17.175924637871613 loss_input: 81.98939211120339
step: 9000 epoch: 295 loss: 17.174570872801937 loss_input: 81.91117672580651
step: 10000 epoch: 295 loss: 17.185176948966557 loss_input: 81.98502475802225
step: 11000 epoch: 295 loss: 17.16973174692186 loss_input: 81.85025115126253
step: 12000 epoch: 295 loss: 17.190264366058674 loss_input: 81.86361046654315
step: 13000 epoch: 295 loss: 17.19644081760357 loss_input: 81.88988263485442
step: 14000 epoch: 295 loss: 17.21411358398264 loss_input: 81.98355137195837
step: 15000 epoch: 295 loss: 17.22962254761426 loss_input: 82.12270097113968
Save loss: 17.236349282741546 Name: 295_train_model.pth
step: 0 epoch: 296 loss: 17.97405242919922 loss_input: 86.08319091796875
step: 1000 epoch: 296 loss: 17.01505086686347 loss_input: 82.1071580397142
step: 2000 epoch: 296 loss: 17.06479167568868 loss_input: 82.21663991002545
step: 3000 epoch: 296 loss: 17.089585079347557 loss_input: 81.99666448260736
step: 4000 epoch: 296 loss: 17.108273835040368 loss_input: 81.74886300092457
step: 5000 epoch: 296 loss: 17.145893189840805 loss_input: 82.02718897138541
step: 6000 epoch: 296 loss: 17.18765983099223 loss_input: 82.38938860328291
step: 7000 epoch: 296 loss: 17.173404242205393 loss_input: 82.4476109736137
step: 8000 epoch: 296 loss: 17.17254318870346 loss_input: 82.4072943106843
step: 9000 epoch: 296 loss: 17.178505762247706 loss_input: 82.33617056058442
step: 10000 epoch: 296 loss: 17.197930535940202 loss_input: 82.32504886108069
step: 11000 epoch: 296 loss: 17.204124326652185 loss_input: 82.27484134188262
step: 12000 epoch: 296 loss: 17.198492044588395 loss_input: 82.12951072741822
step: 13000 epoch: 296 loss: 17.198069497609392 loss_input: 82.08416149009422
step: 14000 epoch: 296 loss: 17.22080447448849 loss_input: 82.18550181683452
step: 15000 epoch: 296 loss: 17.239864262189318 loss_input: 82.19613730666653
Save loss: 17.23637165944278 Name: 296_train_model.pth
step: 0 epoch: 297 loss: 15.545645713806152 loss_input: 65.307861328125
step: 1000 epoch: 297 loss: 17.193330618051384 loss_input: 83.23403098366477
step: 2000 epoch: 297 loss: 17.186463356971263 loss_input: 83.48510104688866
step: 3000 epoch: 297 loss: 17.137467223697804 loss_input: 82.38380456026377
step: 4000 epoch: 297 loss: 17.103371855081008 loss_input: 82.46199871235804
step: 5000 epoch: 297 loss: 17.12068150554078 loss_input: 82.3206417310288
step: 6000 epoch: 297 loss: 17.134385883122796 loss_input: 82.37335643440937
step: 7000 epoch: 297 loss: 17.13183348254806 loss_input: 82.21018916766076
step: 8000 epoch: 297 loss: 17.132252554880978 loss_input: 82.16916929878394
step: 9000 epoch: 297 loss: 17.163624308292317 loss_input: 82.09166532494653
step: 10000 epoch: 297 loss: 17.164993421183432 loss_input: 81.95184642302371
step: 11000 epoch: 297 loss: 17.168372204971035 loss_input: 82.08716240307167
step: 12000 epoch: 297 loss: 17.163096439936748 loss_input: 82.08650552720867
step: 13000 epoch: 297 loss: 17.176286240997943 loss_input: 82.09742138131051
step: 14000 epoch: 297 loss: 17.188926981190395 loss_input: 82.19126522090637
step: 15000 epoch: 297 loss: 17.203021515084064 loss_input: 82.14518910816102
Save loss: 17.23124008369446 Name: 297_train_model.pth
step: 0 epoch: 298 loss: 11.04648208618164 loss_input: 35.98748779296875
step: 1000 epoch: 298 loss: 17.21946687869854 loss_input: 82.95102053708011
step: 2000 epoch: 298 loss: 17.323458061046686 loss_input: 82.68815555601105
step: 3000 epoch: 298 loss: 17.29428524273469 loss_input: 82.48937646598269
step: 4000 epoch: 298 loss: 17.24962718723357 loss_input: 82.38431284207756
step: 5000 epoch: 298 loss: 17.188461226383417 loss_input: 82.11856656988915
step: 6000 epoch: 298 loss: 17.220275079701427 loss_input: 82.29985682317921
step: 7000 epoch: 298 loss: 17.175685879843964 loss_input: 82.16986369065022
step: 8000 epoch: 298 loss: 17.16158925853749 loss_input: 82.0430927579365
step: 9000 epoch: 298 loss: 17.183316109088008 loss_input: 81.99067202304127
step: 10000 epoch: 298 loss: 17.17613490478192 loss_input: 82.0218565362702
step: 11000 epoch: 298 loss: 17.20421558956094 loss_input: 82.18600529057905
step: 12000 epoch: 298 loss: 17.212697199886716 loss_input: 82.22760321018984
step: 13000 epoch: 298 loss: 17.223611464015192 loss_input: 82.28572919458419
step: 14000 epoch: 298 loss: 17.225301272664254 loss_input: 82.25952229085338
step: 15000 epoch: 298 loss: 17.226007994425533 loss_input: 82.22041336160271
Save loss: 17.231917555123566 Name: 298_train_model.pth
step: 0 epoch: 299 loss: 26.229366302490234 loss_input: 70.79241943359375
step: 1000 epoch: 299 loss: 17.100839509830607 loss_input: 81.34706827596231
step: 2000 epoch: 299 loss: 17.197163811807094 loss_input: 81.43608786427099
step: 3000 epoch: 299 loss: 17.17222821700577 loss_input: 81.59691746224566
step: 4000 epoch: 299 loss: 17.200761091884928 loss_input: 82.05113693500454
step: 5000 epoch: 299 loss: 17.206260541276297 loss_input: 82.19647809939966
step: 6000 epoch: 299 loss: 17.243760790313964 loss_input: 82.39921975894643
step: 7000 epoch: 299 loss: 17.239967650983047 loss_input: 82.30192322030167
step: 8000 epoch: 299 loss: 17.261095436166755 loss_input: 82.38790913007212
step: 9000 epoch: 299 loss: 17.245711056209515 loss_input: 82.37864411278521
step: 10000 epoch: 299 loss: 17.262768333237858 loss_input: 82.28138108430367
step: 11000 epoch: 299 loss: 17.257802305454753 loss_input: 82.30718959187304
step: 12000 epoch: 299 loss: 17.262478139338697 loss_input: 82.2981259435249
step: 13000 epoch: 299 loss: 17.269731594190148 loss_input: 82.31404830055891
step: 14000 epoch: 299 loss: 17.271314395545986 loss_input: 82.30872678841857
step: 15000 epoch: 299 loss: 17.25250925687812 loss_input: 82.24298163244195
Save loss: 17.25124568273127 Name: 299_train_model.pth
step: 0 epoch: 300 loss: 12.947957038879395 loss_input: 47.918701171875
step: 1000 epoch: 300 loss: 16.867350213177556 loss_input: 80.73820899559425
step: 2000 epoch: 300 loss: 16.99713537265276 loss_input: 81.2454479852359
step: 3000 epoch: 300 loss: 17.07871109666287 loss_input: 81.81320103992029
step: 4000 epoch: 300 loss: 17.188050623984076 loss_input: 82.07923775891965
step: 5000 epoch: 300 loss: 17.203827876707145 loss_input: 82.26025295429196
step: 6000 epoch: 300 loss: 17.21693132086965 loss_input: 82.02560902325199
step: 7000 epoch: 300 loss: 17.19407374227818 loss_input: 82.0368040248575
step: 8000 epoch: 300 loss: 17.19811833681069 loss_input: 82.04457155410505
step: 9000 epoch: 300 loss: 17.201862781289655 loss_input: 82.12931064435766
step: 10000 epoch: 300 loss: 17.179398397674156 loss_input: 82.10886763496502
step: 11000 epoch: 300 loss: 17.21638734459216 loss_input: 82.2824777659151
step: 12000 epoch: 300 loss: 17.216661672116956 loss_input: 82.23524779612278
step: 13000 epoch: 300 loss: 17.22774016232722 loss_input: 82.32481224307115
step: 14000 epoch: 300 loss: 17.2349840357392 loss_input: 82.30927744750235
step: 15000 epoch: 300 loss: 17.224369596174643 loss_input: 82.26358950337556
Save loss: 17.230775467261672 Name: 300_train_model.pth
step: 0 epoch: 301 loss: 10.940919876098633 loss_input: 56.53546142578125
step: 1000 epoch: 301 loss: 17.153937298339326 loss_input: 81.65192766218156
step: 2000 epoch: 301 loss: 17.05568227620199 loss_input: 81.69141628526557
step: 3000 epoch: 301 loss: 17.174677225638533 loss_input: 82.33270591118065
step: 4000 epoch: 301 loss: 17.19564427712118 loss_input: 81.99055656007873
step: 5000 epoch: 301 loss: 17.16503518515886 loss_input: 82.28097456680538
step: 6000 epoch: 301 loss: 17.165036512799826 loss_input: 82.19239023720655
step: 7000 epoch: 301 loss: 17.188737968634985 loss_input: 82.1926533601366
step: 8000 epoch: 301 loss: 17.19089958879623 loss_input: 82.1549864784358
step: 9000 epoch: 301 loss: 17.205563214550732 loss_input: 82.25172776755274
step: 10000 epoch: 301 loss: 17.178444611574456 loss_input: 82.14283082800118
step: 11000 epoch: 301 loss: 17.18198813799132 loss_input: 82.21368331083893
step: 12000 epoch: 301 loss: 17.185835548146905 loss_input: 82.11463455112465
step: 13000 epoch: 301 loss: 17.210897151016013 loss_input: 82.18795184606442
step: 14000 epoch: 301 loss: 17.210288906342626 loss_input: 82.197323578101
step: 15000 epoch: 301 loss: 17.215860823378325 loss_input: 82.22813341578009
Save loss: 17.22832740741968 Name: 301_train_model.pth
step: 0 epoch: 302 loss: 19.044363021850586 loss_input: 109.975830078125
step: 1000 epoch: 302 loss: 17.355461968051326 loss_input: 83.18043211671142
step: 2000 epoch: 302 loss: 17.276490943780964 loss_input: 82.22654612025042
step: 3000 epoch: 302 loss: 17.208693535317583 loss_input: 82.20519452093443
step: 4000 epoch: 302 loss: 17.224850798213343 loss_input: 82.54189542519156
step: 5000 epoch: 302 loss: 17.209473788225754 loss_input: 82.29444089209503
step: 6000 epoch: 302 loss: 17.20908029547057 loss_input: 82.4826579501561
step: 7000 epoch: 302 loss: 17.225002898605972 loss_input: 82.27453195686051
step: 8000 epoch: 302 loss: 17.220945852754294 loss_input: 82.24125240570514
step: 9000 epoch: 302 loss: 17.2124636995701 loss_input: 82.21695974617187
step: 10000 epoch: 302 loss: 17.201030517265732 loss_input: 82.23002319519024
step: 11000 epoch: 302 loss: 17.18654988672655 loss_input: 82.18492048374686
step: 12000 epoch: 302 loss: 17.193678739279292 loss_input: 82.20273389896545
step: 13000 epoch: 302 loss: 17.208410316719917 loss_input: 82.298460280324
step: 14000 epoch: 302 loss: 17.19745044515487 loss_input: 82.2221377454275
step: 15000 epoch: 302 loss: 17.212323472925952 loss_input: 82.2222656469712
Save loss: 17.22565185007453 Name: 302_train_model.pth
step: 0 epoch: 303 loss: 20.047983169555664 loss_input: 77.024169921875
step: 1000 epoch: 303 loss: 16.87362190321847 loss_input: 80.17317417689732
step: 2000 epoch: 303 loss: 16.973971069008037 loss_input: 80.92621880588027
step: 3000 epoch: 303 loss: 16.97297386311166 loss_input: 81.14218344048078
step: 4000 epoch: 303 loss: 17.034528779018167 loss_input: 81.28983517975486
step: 5000 epoch: 303 loss: 17.0997114911887 loss_input: 81.66059727507623
step: 6000 epoch: 303 loss: 17.09915678105023 loss_input: 81.85575190593235
step: 7000 epoch: 303 loss: 17.09752093940237 loss_input: 81.95642471694892
step: 8000 epoch: 303 loss: 17.14504895805046 loss_input: 81.98165762726924
step: 9000 epoch: 303 loss: 17.177352003859966 loss_input: 82.12386263451197
step: 10000 epoch: 303 loss: 17.199946761119367 loss_input: 82.13833793622591
step: 11000 epoch: 303 loss: 17.21488263910223 loss_input: 82.17522118683458
step: 12000 epoch: 303 loss: 17.20738564766543 loss_input: 82.12424650240735
step: 13000 epoch: 303 loss: 17.22418070197078 loss_input: 82.22811821810733
step: 14000 epoch: 303 loss: 17.21672904957159 loss_input: 82.25130997810285
step: 15000 epoch: 303 loss: 17.22314244613243 loss_input: 82.24358913588704
Save loss: 17.213779847428203 Name: 303_train_model.pth
step: 0 epoch: 304 loss: 19.411819458007812 loss_input: 89.723388671875
step: 1000 epoch: 304 loss: 16.95063489014571 loss_input: 81.5382277634475
step: 2000 epoch: 304 loss: 17.23370362996221 loss_input: 82.07989858830351
step: 3000 epoch: 304 loss: 17.138080466949553 loss_input: 82.0546679345817
step: 4000 epoch: 304 loss: 17.120768587698315 loss_input: 82.41987455406657
step: 5000 epoch: 304 loss: 17.1302495240164 loss_input: 82.41018308789414
step: 6000 epoch: 304 loss: 17.152705394313884 loss_input: 82.29067173903633
step: 7000 epoch: 304 loss: 17.156865543577435 loss_input: 82.36037749873623
step: 8000 epoch: 304 loss: 17.17235782310048 loss_input: 82.26972171691429
step: 9000 epoch: 304 loss: 17.179969213867356 loss_input: 82.2302587242368
step: 10000 epoch: 304 loss: 17.194021314278256 loss_input: 82.29465278569799
step: 11000 epoch: 304 loss: 17.210540470974237 loss_input: 82.26711580907417
step: 12000 epoch: 304 loss: 17.211942865395386 loss_input: 82.21160636922835
step: 13000 epoch: 304 loss: 17.211342036600307 loss_input: 82.19370395298618
step: 14000 epoch: 304 loss: 17.217127803870333 loss_input: 82.21778620601458
step: 15000 epoch: 304 loss: 17.226007541015413 loss_input: 82.25539002559334
Save loss: 17.213858485534786 Name: 304_train_model.pth
step: 0 epoch: 305 loss: 20.860607147216797 loss_input: 69.182861328125
step: 1000 epoch: 305 loss: 16.993398586353223 loss_input: 81.858090334958
step: 2000 epoch: 305 loss: 16.918565526120606 loss_input: 81.76403222949072
step: 3000 epoch: 305 loss: 17.02864649326791 loss_input: 82.42440803016832
step: 4000 epoch: 305 loss: 16.993523258294083 loss_input: 82.25965263139781
step: 5000 epoch: 305 loss: 17.06082650643066 loss_input: 82.55406058680842
step: 6000 epoch: 305 loss: 17.07749307018541 loss_input: 82.63800465832192
step: 7000 epoch: 305 loss: 17.074116172355986 loss_input: 82.38132923498372
step: 8000 epoch: 305 loss: 17.118168344528673 loss_input: 82.37180877140113
step: 9000 epoch: 305 loss: 17.142527520795646 loss_input: 82.36707635376668
step: 10000 epoch: 305 loss: 17.15288258097122 loss_input: 82.29060474079544
step: 11000 epoch: 305 loss: 17.15859314171079 loss_input: 82.27186958279006
step: 12000 epoch: 305 loss: 17.17525302939649 loss_input: 82.2907636470968
step: 13000 epoch: 305 loss: 17.179637344312084 loss_input: 82.27866567449288
step: 14000 epoch: 305 loss: 17.191169837212342 loss_input: 82.29771470529728
step: 15000 epoch: 305 loss: 17.21374255132297 loss_input: 82.35477834192564
Save loss: 17.218875457003712 Name: 305_train_model.pth
step: 0 epoch: 306 loss: 15.066545486450195 loss_input: 197.25152587890625
step: 1000 epoch: 306 loss: 17.49867803352577 loss_input: 84.2720216185182
step: 2000 epoch: 306 loss: 17.352430817367196 loss_input: 83.7741244429055
step: 3000 epoch: 306 loss: 17.295139112539268 loss_input: 83.68096437743408
step: 4000 epoch: 306 loss: 17.27359532892808 loss_input: 83.14513903872664
step: 5000 epoch: 306 loss: 17.238117473741884 loss_input: 82.81008992293339
step: 6000 epoch: 306 loss: 17.225719688058277 loss_input: 82.6606248812047
step: 7000 epoch: 306 loss: 17.201233603242226 loss_input: 82.36050695324775
step: 8000 epoch: 306 loss: 17.196768514753327 loss_input: 82.31762139199152
step: 9000 epoch: 306 loss: 17.19106256332732 loss_input: 82.27410020034667
step: 10000 epoch: 306 loss: 17.186630423313833 loss_input: 82.22622169545741
step: 11000 epoch: 306 loss: 17.198267442683136 loss_input: 82.2093434399252
step: 12000 epoch: 306 loss: 17.20786760948209 loss_input: 82.21536764738192
step: 13000 epoch: 306 loss: 17.189127117787386 loss_input: 82.12495270139226
step: 14000 epoch: 306 loss: 17.18434626992605 loss_input: 82.1059524713776
step: 15000 epoch: 306 loss: 17.200276090688575 loss_input: 82.17469947397387
Save loss: 17.219450386479497 Name: 306_train_model.pth
step: 0 epoch: 307 loss: 19.787311553955078 loss_input: 86.647216796875
step: 1000 epoch: 307 loss: 17.06022681151475 loss_input: 82.09193882289586
step: 2000 epoch: 307 loss: 17.064873273821846 loss_input: 81.0053243641851
step: 3000 epoch: 307 loss: 17.113321493721454 loss_input: 81.79390884597713
step: 4000 epoch: 307 loss: 17.143416761964655 loss_input: 81.94821779479446
step: 5000 epoch: 307 loss: 17.142555536353285 loss_input: 82.12794818965894
step: 6000 epoch: 307 loss: 17.139717387747833 loss_input: 82.41806062311555
step: 7000 epoch: 307 loss: 17.18773019114591 loss_input: 82.6330769188386
step: 8000 epoch: 307 loss: 17.190550434486582 loss_input: 82.46240363295652
step: 9000 epoch: 307 loss: 17.20144375145879 loss_input: 82.37211428005395
step: 10000 epoch: 307 loss: 17.22702301396047 loss_input: 82.27009494485122
step: 11000 epoch: 307 loss: 17.227802046384934 loss_input: 82.34988956193081
step: 12000 epoch: 307 loss: 17.226761029805854 loss_input: 82.27898546906573
step: 13000 epoch: 307 loss: 17.216805377819657 loss_input: 82.31309387803252
step: 14000 epoch: 307 loss: 17.202482643676447 loss_input: 82.29886829198306
step: 15000 epoch: 307 loss: 17.20504074043913 loss_input: 82.2663846376713
Save loss: 17.213542121976612 Name: 307_train_model.pth
step: 0 epoch: 308 loss: 18.62900161743164 loss_input: 84.09381103515625
step: 1000 epoch: 308 loss: 16.907743450168606 loss_input: 83.74375106095077
step: 2000 epoch: 308 loss: 17.007314479452322 loss_input: 83.57321049558229
step: 3000 epoch: 308 loss: 17.016040834257183 loss_input: 83.31390832369028
step: 4000 epoch: 308 loss: 17.03150105053292 loss_input: 82.76066045134225
step: 5000 epoch: 308 loss: 17.049941538524877 loss_input: 82.80021360229907
step: 6000 epoch: 308 loss: 17.09395660922599 loss_input: 82.5951014593271
step: 7000 epoch: 308 loss: 17.12563452988314 loss_input: 82.45704198193506
step: 8000 epoch: 308 loss: 17.14352442607062 loss_input: 82.25604807211599
step: 9000 epoch: 308 loss: 17.162996043232916 loss_input: 82.26185689789894
step: 10000 epoch: 308 loss: 17.18855128037001 loss_input: 82.27783427101626
step: 11000 epoch: 308 loss: 17.186322464053063 loss_input: 82.24339100941995
step: 12000 epoch: 308 loss: 17.188993471223664 loss_input: 82.1715755000153
step: 13000 epoch: 308 loss: 17.175761439673543 loss_input: 82.09075749337642
step: 14000 epoch: 308 loss: 17.17880197409502 loss_input: 82.11705031148723
step: 15000 epoch: 308 loss: 17.19702340364377 loss_input: 82.18203510960598
Save loss: 17.212241700187324 Name: 308_train_model.pth
step: 0 epoch: 309 loss: 17.691822052001953 loss_input: 89.3963623046875
step: 1000 epoch: 309 loss: 17.107255577207447 loss_input: 82.12116112647118
step: 2000 epoch: 309 loss: 17.16013126752187 loss_input: 82.57417497916082
step: 3000 epoch: 309 loss: 17.10322982642858 loss_input: 81.9350727288495
step: 4000 epoch: 309 loss: 17.125671846572832 loss_input: 82.04423860465548
step: 5000 epoch: 309 loss: 17.147812728380302 loss_input: 82.08737832511623
step: 6000 epoch: 309 loss: 17.161834586999593 loss_input: 82.09782795396731
step: 7000 epoch: 309 loss: 17.13481904251067 loss_input: 82.0085079311354
step: 8000 epoch: 309 loss: 17.159025605299103 loss_input: 82.16283944731518
step: 9000 epoch: 309 loss: 17.164913037924908 loss_input: 82.3073540738312
step: 10000 epoch: 309 loss: 17.164351015779427 loss_input: 82.3676129823541
step: 11000 epoch: 309 loss: 17.142378313868015 loss_input: 82.20020200911246
step: 12000 epoch: 309 loss: 17.135175160473263 loss_input: 82.0897930238398
step: 13000 epoch: 309 loss: 17.141499143867765 loss_input: 82.13043675783203
step: 14000 epoch: 309 loss: 17.162172635958267 loss_input: 82.10515813811848
step: 15000 epoch: 309 loss: 17.177269687112844 loss_input: 82.1727220545323
Save loss: 17.208151803448796 Name: 309_train_model.pth
step: 0 epoch: 310 loss: 10.986429214477539 loss_input: 58.48321533203125
step: 1000 epoch: 310 loss: 17.168647200196656 loss_input: 83.02260959255588
step: 2000 epoch: 310 loss: 17.117162727344517 loss_input: 83.03047554591845
step: 3000 epoch: 310 loss: 17.171633215118987 loss_input: 82.44992328810318
step: 4000 epoch: 310 loss: 17.21131403605302 loss_input: 82.47507811462661
step: 5000 epoch: 310 loss: 17.19101881127528 loss_input: 82.56304914554198
step: 6000 epoch: 310 loss: 17.167815221426707 loss_input: 82.32629064996328
step: 7000 epoch: 310 loss: 17.19962048299005 loss_input: 82.50429265197047
step: 8000 epoch: 310 loss: 17.22909740796165 loss_input: 82.567741074602
step: 9000 epoch: 310 loss: 17.242546837643218 loss_input: 82.58028383131041
step: 10000 epoch: 310 loss: 17.232016882602245 loss_input: 82.48514206806858
step: 11000 epoch: 310 loss: 17.22428578897429 loss_input: 82.34153225061927
step: 12000 epoch: 310 loss: 17.208811424700542 loss_input: 82.24254267975228
step: 13000 epoch: 310 loss: 17.226832546057054 loss_input: 82.1838173374434
step: 14000 epoch: 310 loss: 17.210034723954834 loss_input: 82.18099770042932
step: 15000 epoch: 310 loss: 17.20582839015579 loss_input: 82.21392435735405
Save loss: 17.209177324846387 Name: 310_train_model.pth
step: 0 epoch: 311 loss: 19.912002563476562 loss_input: 106.218505859375
step: 1000 epoch: 311 loss: 17.030589927088368 loss_input: 81.25042627050684
step: 2000 epoch: 311 loss: 17.13219689095634 loss_input: 81.76613088109265
step: 3000 epoch: 311 loss: 17.128009722495786 loss_input: 82.07418858834164
step: 4000 epoch: 311 loss: 17.169529736384664 loss_input: 81.94529887730704
step: 5000 epoch: 311 loss: 17.19766025027378 loss_input: 82.04972544309497
step: 6000 epoch: 311 loss: 17.20263385363488 loss_input: 82.06567400102912
step: 7000 epoch: 311 loss: 17.185090758667354 loss_input: 82.21573033337593
step: 8000 epoch: 311 loss: 17.17919193248632 loss_input: 82.14625550651384
step: 9000 epoch: 311 loss: 17.20686689690343 loss_input: 82.1231326130864
step: 10000 epoch: 311 loss: 17.22302410373949 loss_input: 82.16481387139606
step: 11000 epoch: 311 loss: 17.220834857170694 loss_input: 82.11843817975887
step: 12000 epoch: 311 loss: 17.247613205113876 loss_input: 82.18632363522116
step: 13000 epoch: 311 loss: 17.242354824271406 loss_input: 82.15011744326458
step: 14000 epoch: 311 loss: 17.22485265852034 loss_input: 82.13920866701486
step: 15000 epoch: 311 loss: 17.225561267923858 loss_input: 82.2128774219414
Save loss: 17.20463561733067 Name: 311_train_model.pth
step: 0 epoch: 312 loss: 9.91208553314209 loss_input: 57.72601318359375
step: 1000 epoch: 312 loss: 17.202228465637603 loss_input: 82.1004053929469
step: 2000 epoch: 312 loss: 17.09580323220729 loss_input: 82.08593725598138
step: 3000 epoch: 312 loss: 17.185360242747656 loss_input: 82.26619772575808
step: 4000 epoch: 312 loss: 17.170272025666335 loss_input: 81.5892490347872
step: 5000 epoch: 312 loss: 17.189422866006634 loss_input: 81.59377531232035
step: 6000 epoch: 312 loss: 17.19070608030655 loss_input: 81.89045203484966
step: 7000 epoch: 312 loss: 17.167738136232384 loss_input: 81.86995121302425
step: 8000 epoch: 312 loss: 17.160516027092502 loss_input: 81.8115923757211
step: 9000 epoch: 312 loss: 17.163284799176154 loss_input: 81.84829783452881
step: 10000 epoch: 312 loss: 17.188570175036443 loss_input: 81.97835766350123
step: 11000 epoch: 312 loss: 17.20405280220542 loss_input: 82.1417419971764
step: 12000 epoch: 312 loss: 17.19925506632881 loss_input: 82.15527824870388
step: 13000 epoch: 312 loss: 17.17814397010498 loss_input: 82.10880007896779
step: 14000 epoch: 312 loss: 17.189193381351128 loss_input: 82.20259737568271
step: 15000 epoch: 312 loss: 17.213693312395176 loss_input: 82.18642741118691
Save loss: 17.209173154160382 Name: 312_train_model.pth
step: 0 epoch: 313 loss: 16.104717254638672 loss_input: 103.5777587890625
step: 1000 epoch: 313 loss: 17.01534414529562 loss_input: 81.78174267139111
step: 2000 epoch: 313 loss: 17.105182976081693 loss_input: 81.8160003250328
step: 3000 epoch: 313 loss: 17.098070299573756 loss_input: 82.07304362494403
step: 4000 epoch: 313 loss: 17.14431346723122 loss_input: 82.40055699576082
step: 5000 epoch: 313 loss: 17.11901269991668 loss_input: 82.38563960791826
step: 6000 epoch: 313 loss: 17.13375951734866 loss_input: 82.41355342957164
step: 7000 epoch: 313 loss: 17.1718537143734 loss_input: 82.28533676620415
step: 8000 epoch: 313 loss: 17.19911152445485 loss_input: 82.25407870616515
step: 9000 epoch: 313 loss: 17.15779287165343 loss_input: 82.20795591125726
step: 10000 epoch: 313 loss: 17.187035304285885 loss_input: 82.23458854065765
step: 11000 epoch: 313 loss: 17.196917963855842 loss_input: 82.24535414327568
step: 12000 epoch: 313 loss: 17.21344596844357 loss_input: 82.32115669336312
step: 13000 epoch: 313 loss: 17.220525123166926 loss_input: 82.4095001068127
step: 14000 epoch: 313 loss: 17.22171732650707 loss_input: 82.34003368655866
step: 15000 epoch: 313 loss: 17.206794495757727 loss_input: 82.2803165579977
Save loss: 17.198936354905367 Name: 313_train_model.pth
step: 0 epoch: 314 loss: 24.755226135253906 loss_input: 140.31201171875
step: 1000 epoch: 314 loss: 17.095602815325087 loss_input: 82.38396100969344
step: 2000 epoch: 314 loss: 17.18494447477456 loss_input: 81.96601952319739
step: 3000 epoch: 314 loss: 17.16000311440605 loss_input: 81.98486881326016
step: 4000 epoch: 314 loss: 17.16882652975386 loss_input: 81.91246400741123
step: 5000 epoch: 314 loss: 17.18119042340671 loss_input: 81.8388090082179
step: 6000 epoch: 314 loss: 17.124650965291565 loss_input: 81.77545645061761
step: 7000 epoch: 314 loss: 17.126718403628647 loss_input: 82.01433219849731
step: 8000 epoch: 314 loss: 17.1301206447023 loss_input: 82.0194837019274
step: 9000 epoch: 314 loss: 17.133818762975988 loss_input: 82.0934880594322
step: 10000 epoch: 314 loss: 17.15484897394965 loss_input: 82.1387116903544
step: 11000 epoch: 314 loss: 17.174440971277765 loss_input: 82.27684005045174
step: 12000 epoch: 314 loss: 17.188833884503104 loss_input: 82.33090523502925
step: 13000 epoch: 314 loss: 17.18589835023524 loss_input: 82.24264757109279
step: 14000 epoch: 314 loss: 17.195587634614565 loss_input: 82.26647504129866
step: 15000 epoch: 314 loss: 17.2090935612208 loss_input: 82.24214127505432
Save loss: 17.2004377617985 Name: 314_train_model.pth
step: 0 epoch: 315 loss: 18.374977111816406 loss_input: 86.0006103515625
step: 1000 epoch: 315 loss: 17.251042511794235 loss_input: 82.30543412885942
step: 2000 epoch: 315 loss: 17.2114866708053 loss_input: 82.69930113678512
step: 3000 epoch: 315 loss: 17.232975625467475 loss_input: 82.82819775000131
step: 4000 epoch: 315 loss: 17.20577376617607 loss_input: 82.30965336058563
step: 5000 epoch: 315 loss: 17.175879862517792 loss_input: 82.23272041148411
step: 6000 epoch: 315 loss: 17.19488235776215 loss_input: 82.13295534932004
step: 7000 epoch: 315 loss: 17.177244164810542 loss_input: 82.01901626532427
step: 8000 epoch: 315 loss: 17.184744848726453 loss_input: 82.10395278806702
step: 9000 epoch: 315 loss: 17.19473138823402 loss_input: 82.05021396412768
step: 10000 epoch: 315 loss: 17.199082261239894 loss_input: 82.15510501562149
step: 11000 epoch: 315 loss: 17.206226814574475 loss_input: 82.18645708167155
step: 12000 epoch: 315 loss: 17.184970804775034 loss_input: 82.1899752423382
step: 13000 epoch: 315 loss: 17.18967692179548 loss_input: 82.15082456566593
step: 14000 epoch: 315 loss: 17.215293024328894 loss_input: 82.22812510200862
step: 15000 epoch: 315 loss: 17.202640328293807 loss_input: 82.26130747777623
Save loss: 17.194511408001183 Name: 315_train_model.pth
step: 0 epoch: 316 loss: 16.4464111328125 loss_input: 57.74822998046875
step: 1000 epoch: 316 loss: 17.129360375704465 loss_input: 81.90872823465597
step: 2000 epoch: 316 loss: 17.049885487210922 loss_input: 81.59449178608938
step: 3000 epoch: 316 loss: 17.156874059081595 loss_input: 81.83887829219687
step: 4000 epoch: 316 loss: 17.189539584241132 loss_input: 81.87223242712271
step: 5000 epoch: 316 loss: 17.203223532615866 loss_input: 81.84734456917211
step: 6000 epoch: 316 loss: 17.18575260007884 loss_input: 81.75096307597822
step: 7000 epoch: 316 loss: 17.16765990166677 loss_input: 81.90194528223088
step: 8000 epoch: 316 loss: 17.152988283950943 loss_input: 81.81962018083921
step: 9000 epoch: 316 loss: 17.156013900711066 loss_input: 81.78729429097721
step: 10000 epoch: 316 loss: 17.148557425999496 loss_input: 81.9023038308962
step: 11000 epoch: 316 loss: 17.157372648570117 loss_input: 81.92020230604923
step: 12000 epoch: 316 loss: 17.189466841985123 loss_input: 82.01807915644888
step: 13000 epoch: 316 loss: 17.20730495148462 loss_input: 82.11459213267399
step: 14000 epoch: 316 loss: 17.206639981492913 loss_input: 82.13024713584214
step: 15000 epoch: 316 loss: 17.208392232030672 loss_input: 82.21000515248853
Save loss: 17.198105292350053 Name: 316_train_model.pth
step: 0 epoch: 317 loss: 11.614116668701172 loss_input: 72.657958984375
step: 1000 epoch: 317 loss: 17.12239630715354 loss_input: 82.24307205245925
step: 2000 epoch: 317 loss: 17.315835805013144 loss_input: 82.74502116617472
step: 3000 epoch: 317 loss: 17.235813686665754 loss_input: 82.24338233284217
step: 4000 epoch: 317 loss: 17.275153430990205 loss_input: 82.26523290136939
step: 5000 epoch: 317 loss: 17.286998734953784 loss_input: 82.13728337096252
step: 6000 epoch: 317 loss: 17.241659498715318 loss_input: 82.24966981414556
step: 7000 epoch: 317 loss: 17.22838621496422 loss_input: 82.02826042812801
step: 8000 epoch: 317 loss: 17.234387908394883 loss_input: 81.97407000608078
step: 9000 epoch: 317 loss: 17.245677218067954 loss_input: 82.02823495414572
step: 10000 epoch: 317 loss: 17.231721689028568 loss_input: 82.1043908243453
step: 11000 epoch: 317 loss: 17.239122711781015 loss_input: 82.20777545058937
step: 12000 epoch: 317 loss: 17.23440898618006 loss_input: 82.1306504334681
step: 13000 epoch: 317 loss: 17.22904349123457 loss_input: 82.12482337784414
step: 14000 epoch: 317 loss: 17.221499191932224 loss_input: 82.132369359743
step: 15000 epoch: 317 loss: 17.199798561272544 loss_input: 82.21075419516303
Save loss: 17.202718947023154 Name: 317_train_model.pth
step: 0 epoch: 318 loss: 22.032028198242188 loss_input: 162.26446533203125
step: 1000 epoch: 318 loss: 17.276896990738905 loss_input: 83.19826590645683
step: 2000 epoch: 318 loss: 17.18656046458449 loss_input: 82.73982466774426
step: 3000 epoch: 318 loss: 17.142096398632276 loss_input: 83.03303907649273
step: 4000 epoch: 318 loss: 17.185595458818955 loss_input: 82.9441098435108
step: 5000 epoch: 318 loss: 17.23869109010725 loss_input: 83.04297469363549
step: 6000 epoch: 318 loss: 17.20778877304864 loss_input: 82.72004875408135
step: 7000 epoch: 318 loss: 17.235390199489075 loss_input: 82.77451303866468
step: 8000 epoch: 318 loss: 17.25413382865387 loss_input: 82.79604909158084
step: 9000 epoch: 318 loss: 17.229835491632834 loss_input: 82.80756917173048
step: 10000 epoch: 318 loss: 17.20389625664032 loss_input: 82.63832808003379
step: 11000 epoch: 318 loss: 17.20329696039169 loss_input: 82.5794912199033
step: 12000 epoch: 318 loss: 17.183216341494838 loss_input: 82.43772256649511
step: 13000 epoch: 318 loss: 17.19506279443486 loss_input: 82.44638786142802
step: 14000 epoch: 318 loss: 17.202734289948545 loss_input: 82.3599858616056
step: 15000 epoch: 318 loss: 17.20230418993198 loss_input: 82.34292494107164
Save loss: 17.193206735149026 Name: 318_train_model.pth
step: 0 epoch: 319 loss: 11.043697357177734 loss_input: 59.5306396484375
step: 1000 epoch: 319 loss: 17.125245434420926 loss_input: 82.6641474980098
step: 2000 epoch: 319 loss: 17.183908981063972 loss_input: 83.02721646378959
step: 3000 epoch: 319 loss: 17.118305476257618 loss_input: 82.5511564707923
step: 4000 epoch: 319 loss: 17.1646181861927 loss_input: 82.72723146117768
step: 5000 epoch: 319 loss: 17.17235725132424 loss_input: 82.9311787300743
step: 6000 epoch: 319 loss: 17.151278916606703 loss_input: 82.60767458963545
step: 7000 epoch: 319 loss: 17.16232425310734 loss_input: 82.58093285257519
step: 8000 epoch: 319 loss: 17.166133212769065 loss_input: 82.4078114274412
step: 9000 epoch: 319 loss: 17.174370141204708 loss_input: 82.46414397152486
step: 10000 epoch: 319 loss: 17.183922853079835 loss_input: 82.44568345289494
step: 11000 epoch: 319 loss: 17.18422967504885 loss_input: 82.37550529741436
step: 12000 epoch: 319 loss: 17.18180911589261 loss_input: 82.3273290712509
step: 13000 epoch: 319 loss: 17.186497698691376 loss_input: 82.33722095652348
step: 14000 epoch: 319 loss: 17.180430216105034 loss_input: 82.32215396043837
step: 15000 epoch: 319 loss: 17.18087912338526 loss_input: 82.29257566910904
Save loss: 17.185865895420314 Name: 319_train_model.pth
step: 0 epoch: 320 loss: 17.95354461669922 loss_input: 77.649169921875
step: 1000 epoch: 320 loss: 16.776232009167437 loss_input: 80.67735328636208
step: 2000 epoch: 320 loss: 16.99877566805129 loss_input: 81.7491552380548
step: 3000 epoch: 320 loss: 17.124394187844622 loss_input: 82.07072337378426
step: 4000 epoch: 320 loss: 17.113292975951303 loss_input: 81.9146054855915
step: 5000 epoch: 320 loss: 17.190660917909117 loss_input: 82.47196128401274
step: 6000 epoch: 320 loss: 17.15724929347274 loss_input: 82.344447535925
step: 7000 epoch: 320 loss: 17.160417810165445 loss_input: 82.22025911407188
step: 8000 epoch: 320 loss: 17.13482517109053 loss_input: 82.01945971810896
step: 9000 epoch: 320 loss: 17.14021306718115 loss_input: 82.21786514280531
step: 10000 epoch: 320 loss: 17.138070489916608 loss_input: 82.25570574046111
step: 11000 epoch: 320 loss: 17.14966401183901 loss_input: 82.22295992778177
step: 12000 epoch: 320 loss: 17.15000561451616 loss_input: 82.3104107677713
step: 13000 epoch: 320 loss: 17.15955429626936 loss_input: 82.22648660157903
step: 14000 epoch: 320 loss: 17.168646524535035 loss_input: 82.36329193910814
step: 15000 epoch: 320 loss: 17.170925856176975 loss_input: 82.27743174055324
Save loss: 17.184217962220313 Name: 320_train_model.pth
step: 0 epoch: 321 loss: 15.706503868103027 loss_input: 64.3094482421875
step: 1000 epoch: 321 loss: 17.03703754979533 loss_input: 81.72179791643903
step: 2000 epoch: 321 loss: 17.153071663488095 loss_input: 82.04507484416972
step: 3000 epoch: 321 loss: 17.133802743484004 loss_input: 82.1699578737426
step: 4000 epoch: 321 loss: 17.182439028933715 loss_input: 82.41520038386399
step: 5000 epoch: 321 loss: 17.17715399147534 loss_input: 82.27131060604286
step: 6000 epoch: 321 loss: 17.141604275291034 loss_input: 82.21076284649173
step: 7000 epoch: 321 loss: 17.138271035815695 loss_input: 82.13286839149931
step: 8000 epoch: 321 loss: 17.17157859886278 loss_input: 82.15670186307278
step: 9000 epoch: 321 loss: 17.16154558927983 loss_input: 82.09228786862224
step: 10000 epoch: 321 loss: 17.157500583021037 loss_input: 81.86016921989442
step: 11000 epoch: 321 loss: 17.166735722123356 loss_input: 81.89787995569294
step: 12000 epoch: 321 loss: 17.166435077740505 loss_input: 81.99134551389109
step: 13000 epoch: 321 loss: 17.148430612525356 loss_input: 81.91270535154146
step: 14000 epoch: 321 loss: 17.158794893893877 loss_input: 81.97935083145568
step: 15000 epoch: 321 loss: 17.184968563121412 loss_input: 82.19137071770085
Save loss: 17.18792730922997 Name: 321_train_model.pth
step: 0 epoch: 322 loss: 12.026333808898926 loss_input: 50.1923828125
step: 1000 epoch: 322 loss: 17.372952520311415 loss_input: 83.73357434801527
step: 2000 epoch: 322 loss: 17.23203336686149 loss_input: 82.71197677528423
step: 3000 epoch: 322 loss: 17.19634066324002 loss_input: 82.30402371979523
step: 4000 epoch: 322 loss: 17.206574083000266 loss_input: 82.18343642543954
step: 5000 epoch: 322 loss: 17.215745608870016 loss_input: 82.24613965146423
step: 6000 epoch: 322 loss: 17.28721447257951 loss_input: 82.41561943045299
step: 7000 epoch: 322 loss: 17.264920604312408 loss_input: 82.2380626097081
step: 8000 epoch: 322 loss: 17.23102603297191 loss_input: 82.17565764637087
step: 9000 epoch: 322 loss: 17.229739877915677 loss_input: 82.39256039964744
step: 10000 epoch: 322 loss: 17.221580411157493 loss_input: 82.33767682191741
step: 11000 epoch: 322 loss: 17.211687096334913 loss_input: 82.34710317749878
step: 12000 epoch: 322 loss: 17.208226623460458 loss_input: 82.33697438055292
step: 13000 epoch: 322 loss: 17.201226650939596 loss_input: 82.32386256214215
step: 14000 epoch: 322 loss: 17.186247512431034 loss_input: 82.25959513982818
step: 15000 epoch: 322 loss: 17.17857781512508 loss_input: 82.2339816139536
Save loss: 17.192661342829467 Name: 322_train_model.pth
step: 0 epoch: 323 loss: 29.236473083496094 loss_input: 74.583251953125
step: 1000 epoch: 323 loss: 17.280890579585666 loss_input: 83.92370074993366
step: 2000 epoch: 323 loss: 17.165936325622283 loss_input: 82.95625746696965
step: 3000 epoch: 323 loss: 17.21227931245412 loss_input: 82.84234244916408
step: 4000 epoch: 323 loss: 17.1841886860524 loss_input: 82.51429623062984
step: 5000 epoch: 323 loss: 17.187967272096575 loss_input: 82.45626223876319
step: 6000 epoch: 323 loss: 17.18655347498312 loss_input: 82.44893759772134
step: 7000 epoch: 323 loss: 17.183878934650725 loss_input: 82.48873179075702
step: 8000 epoch: 323 loss: 17.147699926334862 loss_input: 82.1997031788873
step: 9000 epoch: 323 loss: 17.166570437376983 loss_input: 82.33575060399104
step: 10000 epoch: 323 loss: 17.179023681289134 loss_input: 82.33046363942707
step: 11000 epoch: 323 loss: 17.171123790823323 loss_input: 82.19198234730791
step: 12000 epoch: 323 loss: 17.154483369087917 loss_input: 82.06266967885325
step: 13000 epoch: 323 loss: 17.150167688407603 loss_input: 82.09423400442304
step: 14000 epoch: 323 loss: 17.167471601591647 loss_input: 82.15037796546899
step: 15000 epoch: 323 loss: 17.1695123794325 loss_input: 82.24721410987965
Save loss: 17.178583966985347 Name: 323_train_model.pth
step: 0 epoch: 324 loss: 15.74665641784668 loss_input: 82.3909912109375
step: 1000 epoch: 324 loss: 17.361521017777694 loss_input: 83.40468735366197
step: 2000 epoch: 324 loss: 17.284815160111748 loss_input: 82.86834545983844
step: 3000 epoch: 324 loss: 17.243267276850354 loss_input: 82.72016684542652
step: 4000 epoch: 324 loss: 17.21747961356562 loss_input: 82.42662015160272
step: 5000 epoch: 324 loss: 17.22931207806748 loss_input: 82.56139475229953
step: 6000 epoch: 324 loss: 17.180247831813418 loss_input: 82.36549389392133
step: 7000 epoch: 324 loss: 17.166600058171056 loss_input: 82.25456830839177
step: 8000 epoch: 324 loss: 17.19419873519147 loss_input: 82.34440661805702
step: 9000 epoch: 324 loss: 17.20259817300035 loss_input: 82.2697151217139
step: 10000 epoch: 324 loss: 17.193686426478546 loss_input: 82.36965112692832
step: 11000 epoch: 324 loss: 17.162993677225277 loss_input: 82.23421648391518
step: 12000 epoch: 324 loss: 17.159901793366043 loss_input: 82.21941421059964
step: 13000 epoch: 324 loss: 17.168072580328428 loss_input: 82.24273614976582
step: 14000 epoch: 324 loss: 17.15699886758024 loss_input: 82.14792581779872
step: 15000 epoch: 324 loss: 17.17879973444237 loss_input: 82.20388770550062
Save loss: 17.183790184631945 Name: 324_train_model.pth
step: 0 epoch: 325 loss: 21.991378784179688 loss_input: 113.043212890625
step: 1000 epoch: 325 loss: 17.259803682416827 loss_input: 82.6203749253676
step: 2000 epoch: 325 loss: 17.248578328242722 loss_input: 83.09721189210083
step: 3000 epoch: 325 loss: 17.20522245475746 loss_input: 82.6021995557145
step: 4000 epoch: 325 loss: 17.116320365132527 loss_input: 81.99873906950508
step: 5000 epoch: 325 loss: 17.077135223313537 loss_input: 82.03570716520758
step: 6000 epoch: 325 loss: 17.118235234398977 loss_input: 81.99592344960736
step: 7000 epoch: 325 loss: 17.16838815685273 loss_input: 82.02831405293105
step: 8000 epoch: 325 loss: 17.176563084267062 loss_input: 82.14154028323364
step: 9000 epoch: 325 loss: 17.16758916791499 loss_input: 82.20630932500555
step: 10000 epoch: 325 loss: 17.160445525853376 loss_input: 82.24905691803866
step: 11000 epoch: 325 loss: 17.1466500839963 loss_input: 82.156765150922
step: 12000 epoch: 325 loss: 17.149661522072144 loss_input: 82.1483442714816
step: 13000 epoch: 325 loss: 17.174381721790876 loss_input: 82.29545060536209
step: 14000 epoch: 325 loss: 17.14537090189533 loss_input: 82.17765231037487
step: 15000 epoch: 325 loss: 17.163372178513498 loss_input: 82.21259882723314
Save loss: 17.176483076423406 Name: 325_train_model.pth
step: 0 epoch: 326 loss: 29.89800262451172 loss_input: 78.38873291015625
step: 1000 epoch: 326 loss: 17.31427859831285 loss_input: 81.79061971963583
step: 2000 epoch: 326 loss: 17.174878796239543 loss_input: 81.6008696701454
step: 3000 epoch: 326 loss: 17.210220259215504 loss_input: 81.77781061504889
step: 4000 epoch: 326 loss: 17.159167771934122 loss_input: 81.77230318580023
step: 5000 epoch: 326 loss: 17.200326787307485 loss_input: 82.17813780993801
step: 6000 epoch: 326 loss: 17.20535871931323 loss_input: 82.09780266928168
step: 7000 epoch: 326 loss: 17.223209004728407 loss_input: 82.20593550286623
step: 8000 epoch: 326 loss: 17.1771192703824 loss_input: 82.1172502591839
step: 9000 epoch: 326 loss: 17.171743425312684 loss_input: 81.98222677406504
step: 10000 epoch: 326 loss: 17.155705415610136 loss_input: 82.02940482929246
step: 11000 epoch: 326 loss: 17.175002863792688 loss_input: 82.12749322038728
step: 12000 epoch: 326 loss: 17.177270223136624 loss_input: 82.13041411994963
step: 13000 epoch: 326 loss: 17.18325368656249 loss_input: 82.18334611655473
step: 14000 epoch: 326 loss: 17.174788769445506 loss_input: 82.19395684034566
step: 15000 epoch: 326 loss: 17.180394018453708 loss_input: 82.2650033047927
Save loss: 17.178030065760016 Name: 326_train_model.pth
step: 0 epoch: 327 loss: 8.484489440917969 loss_input: 65.59881591796875
step: 1000 epoch: 327 loss: 17.169430352114773 loss_input: 82.66325440916505
step: 2000 epoch: 327 loss: 17.042985386636364 loss_input: 81.38583930690905
step: 3000 epoch: 327 loss: 17.045405760799714 loss_input: 81.5561517539401
step: 4000 epoch: 327 loss: 17.033592719550253 loss_input: 81.92402404464802
step: 5000 epoch: 327 loss: 17.06713075753189 loss_input: 81.83119995873872
step: 6000 epoch: 327 loss: 17.10150964556883 loss_input: 81.8970641835573
step: 7000 epoch: 327 loss: 17.11498624344346 loss_input: 82.01709102940652
step: 8000 epoch: 327 loss: 17.14001019643763 loss_input: 82.06337130527142
step: 9000 epoch: 327 loss: 17.127267443621214 loss_input: 82.12641293572632
step: 10000 epoch: 327 loss: 17.126798409126412 loss_input: 82.11192841897451
step: 11000 epoch: 327 loss: 17.14450820315243 loss_input: 82.11165285244843
step: 12000 epoch: 327 loss: 17.1351087138053 loss_input: 82.01082162830832
step: 13000 epoch: 327 loss: 17.15159556220361 loss_input: 82.0871363119679
step: 14000 epoch: 327 loss: 17.166859754382962 loss_input: 82.22783290399039
step: 15000 epoch: 327 loss: 17.174049994522917 loss_input: 82.21778900042732
Save loss: 17.177256913125515 Name: 327_train_model.pth
step: 0 epoch: 328 loss: 25.012374877929688 loss_input: 113.4405517578125
step: 1000 epoch: 328 loss: 17.155675697993566 loss_input: 83.05899349673764
step: 2000 epoch: 328 loss: 17.12296381883178 loss_input: 82.34024885140438
step: 3000 epoch: 328 loss: 17.095715516569296 loss_input: 82.04459070690629
step: 4000 epoch: 328 loss: 17.16360915490789 loss_input: 82.29753840836666
step: 5000 epoch: 328 loss: 17.197808755776617 loss_input: 82.24669364222859
step: 6000 epoch: 328 loss: 17.20347877526915 loss_input: 82.29846460933268
step: 7000 epoch: 328 loss: 17.17690534834146 loss_input: 82.29851999379552
step: 8000 epoch: 328 loss: 17.166541033693438 loss_input: 82.25108517624261
step: 9000 epoch: 328 loss: 17.190848036244663 loss_input: 82.22148799737312
step: 10000 epoch: 328 loss: 17.190733119161496 loss_input: 82.23095248458553
step: 11000 epoch: 328 loss: 17.16996966415227 loss_input: 82.16109440823206
step: 12000 epoch: 328 loss: 17.17066842130339 loss_input: 82.20809184073528
step: 13000 epoch: 328 loss: 17.17737720282424 loss_input: 82.23383889903602
step: 14000 epoch: 328 loss: 17.18523066541602 loss_input: 82.19874123632631
step: 15000 epoch: 328 loss: 17.18100852338833 loss_input: 82.15639902164075
Save loss: 17.182400154560803 Name: 328_train_model.pth
step: 0 epoch: 329 loss: 14.578513145446777 loss_input: 69.30853271484375
step: 1000 epoch: 329 loss: 17.196449266923416 loss_input: 82.48067703780595
step: 2000 epoch: 329 loss: 17.134565728953454 loss_input: 82.76801500982907
step: 3000 epoch: 329 loss: 17.099387269463392 loss_input: 82.18318672014928
step: 4000 epoch: 329 loss: 17.054622013608803 loss_input: 81.89394341478108
step: 5000 epoch: 329 loss: 17.098239645913132 loss_input: 82.2179028427713
step: 6000 epoch: 329 loss: 17.106897760204347 loss_input: 82.22108580251273
step: 7000 epoch: 329 loss: 17.1105825939173 loss_input: 82.28822338647493
step: 8000 epoch: 329 loss: 17.16404039215228 loss_input: 82.31912938220368
step: 9000 epoch: 329 loss: 17.149287451480365 loss_input: 82.1264705397317
step: 10000 epoch: 329 loss: 17.14919083483898 loss_input: 82.10047139206978
step: 11000 epoch: 329 loss: 17.156811446712705 loss_input: 82.14427783842271
step: 12000 epoch: 329 loss: 17.173514979013394 loss_input: 82.11399452369041
step: 13000 epoch: 329 loss: 17.166257614723012 loss_input: 82.217729490873
step: 14000 epoch: 329 loss: 17.180414291630726 loss_input: 82.32987511477278
step: 15000 epoch: 329 loss: 17.170550706473822 loss_input: 82.24079143421689
Save loss: 17.176798071488736 Name: 329_train_model.pth
step: 0 epoch: 330 loss: 14.634808540344238 loss_input: 49.75445556640625
step: 1000 epoch: 330 loss: 17.045096643678434 loss_input: 83.3986639581122
step: 2000 epoch: 330 loss: 17.125200557208313 loss_input: 82.22161291790628
step: 3000 epoch: 330 loss: 17.091751454949815 loss_input: 81.85043767149232
step: 4000 epoch: 330 loss: 17.098046456834908 loss_input: 81.8469351930816
step: 5000 epoch: 330 loss: 17.120405854141442 loss_input: 81.80689212499297
step: 6000 epoch: 330 loss: 17.1252297699958 loss_input: 81.63543454020684
step: 7000 epoch: 330 loss: 17.111771421149157 loss_input: 81.85209121562433
step: 8000 epoch: 330 loss: 17.140965490307813 loss_input: 81.82347339028911
step: 9000 epoch: 330 loss: 17.147836760856908 loss_input: 81.86753883357578
step: 10000 epoch: 330 loss: 17.148129666856427 loss_input: 81.93595732589827
step: 11000 epoch: 330 loss: 17.169118867333548 loss_input: 82.05243197883999
step: 12000 epoch: 330 loss: 17.15620931576971 loss_input: 81.9818915186023
step: 13000 epoch: 330 loss: 17.164626467329715 loss_input: 82.01968316420822
step: 14000 epoch: 330 loss: 17.16801485299434 loss_input: 82.09245489161488
step: 15000 epoch: 330 loss: 17.169116898271515 loss_input: 82.17678441391668
Save loss: 17.1752199665457 Name: 330_train_model.pth
step: 0 epoch: 331 loss: 17.355640411376953 loss_input: 91.42095947265625
step: 1000 epoch: 331 loss: 17.110520786815112 loss_input: 82.72634835867258
step: 2000 epoch: 331 loss: 16.991802023983432 loss_input: 81.99298114004522
step: 3000 epoch: 331 loss: 16.997336538105717 loss_input: 82.0670761113483
step: 4000 epoch: 331 loss: 17.049661564010584 loss_input: 82.275071079032
step: 5000 epoch: 331 loss: 17.109331279915587 loss_input: 82.06862853503517
step: 6000 epoch: 331 loss: 17.148104964365782 loss_input: 82.27357200923869
step: 7000 epoch: 331 loss: 17.16451890787692 loss_input: 82.3793073331879
step: 8000 epoch: 331 loss: 17.164119355247372 loss_input: 82.35332825523274
step: 9000 epoch: 331 loss: 17.171867381253648 loss_input: 82.45102276233631
step: 10000 epoch: 331 loss: 17.144388464090525 loss_input: 82.42379415622891
step: 11000 epoch: 331 loss: 17.13571399169102 loss_input: 82.40372164875103
step: 12000 epoch: 331 loss: 17.14772450122304 loss_input: 82.42225450023533
step: 13000 epoch: 331 loss: 17.128557069128675 loss_input: 82.29131613429826
step: 14000 epoch: 331 loss: 17.143174750797577 loss_input: 82.26769656833943
step: 15000 epoch: 331 loss: 17.16044347602982 loss_input: 82.30707220430097
Save loss: 17.16543180975318 Name: 331_train_model.pth
step: 0 epoch: 332 loss: 11.769525527954102 loss_input: 79.089599609375
step: 1000 epoch: 332 loss: 17.264059485969963 loss_input: 82.41179340678852
step: 2000 epoch: 332 loss: 17.126845382202394 loss_input: 82.00717268318965
step: 3000 epoch: 332 loss: 17.078677363730954 loss_input: 81.67282060764902
step: 4000 epoch: 332 loss: 17.135479074989192 loss_input: 82.12617664675628
step: 5000 epoch: 332 loss: 17.15172233368916 loss_input: 82.21788936890356
step: 6000 epoch: 332 loss: 17.19653141730985 loss_input: 82.46052072986208
step: 7000 epoch: 332 loss: 17.181030524183964 loss_input: 82.19982795077824
step: 8000 epoch: 332 loss: 17.16465800217637 loss_input: 82.13591520054133
step: 9000 epoch: 332 loss: 17.165929855763707 loss_input: 82.2189167566451
step: 10000 epoch: 332 loss: 17.163537016631054 loss_input: 82.19360817214177
step: 11000 epoch: 332 loss: 17.16856733918916 loss_input: 82.30481445822929
step: 12000 epoch: 332 loss: 17.15809296496798 loss_input: 82.33218669867517
step: 13000 epoch: 332 loss: 17.168798725216785 loss_input: 82.37227017787244
step: 14000 epoch: 332 loss: 17.151027591149642 loss_input: 82.3145375175141
step: 15000 epoch: 332 loss: 17.168003167353362 loss_input: 82.25260536423222
Save loss: 17.165238955274223 Name: 332_train_model.pth
step: 0 epoch: 333 loss: 13.54318618774414 loss_input: 38.51983642578125
step: 1000 epoch: 333 loss: 17.243175955323668 loss_input: 81.89720441911604
step: 2000 epoch: 333 loss: 17.041437899452756 loss_input: 81.36143610323744
step: 3000 epoch: 333 loss: 16.964420991514334 loss_input: 81.32210623395717
step: 4000 epoch: 333 loss: 17.010540201198577 loss_input: 81.3914636117582
step: 5000 epoch: 333 loss: 17.03416432416146 loss_input: 81.72114770161203
step: 6000 epoch: 333 loss: 17.097894649071765 loss_input: 81.96274589407625
step: 7000 epoch: 333 loss: 17.113780377030697 loss_input: 81.97611717264442
step: 8000 epoch: 333 loss: 17.109240031573137 loss_input: 82.0033911242662
step: 9000 epoch: 333 loss: 17.121506413411147 loss_input: 82.11417306442948
step: 10000 epoch: 333 loss: 17.160235374382218 loss_input: 82.22705162955384
step: 11000 epoch: 333 loss: 17.157750122136022 loss_input: 82.26479642320336
step: 12000 epoch: 333 loss: 17.17543864111117 loss_input: 82.35547071821978
step: 13000 epoch: 333 loss: 17.184274899667066 loss_input: 82.48893775937007
step: 14000 epoch: 333 loss: 17.175816533276205 loss_input: 82.34478700042631
step: 15000 epoch: 333 loss: 17.170927802194523 loss_input: 82.26750699530187
Save loss: 17.172142517358065 Name: 333_train_model.pth
step: 0 epoch: 334 loss: 19.548715591430664 loss_input: 102.67010498046875
step: 1000 epoch: 334 loss: 16.780307777397162 loss_input: 80.29643348594765
step: 2000 epoch: 334 loss: 17.09116462443484 loss_input: 82.10068522257426
step: 3000 epoch: 334 loss: 17.129058778623627 loss_input: 81.97069236605377
step: 4000 epoch: 334 loss: 17.15183091932343 loss_input: 81.80504420708787
step: 5000 epoch: 334 loss: 17.124960483252774 loss_input: 81.90084868768434
step: 6000 epoch: 334 loss: 17.131959340906484 loss_input: 82.14415257979465
step: 7000 epoch: 334 loss: 17.077909423970883 loss_input: 82.14657050945847
step: 8000 epoch: 334 loss: 17.083864353996653 loss_input: 82.16535330849757
step: 9000 epoch: 334 loss: 17.110085270111064 loss_input: 82.255954335152
step: 10000 epoch: 334 loss: 17.11159552198543 loss_input: 82.2864057893527
step: 11000 epoch: 334 loss: 17.096481608494923 loss_input: 82.21171073403838
step: 12000 epoch: 334 loss: 17.10948573445372 loss_input: 82.14757283104827
step: 13000 epoch: 334 loss: 17.124617941498197 loss_input: 82.12753004125885
step: 14000 epoch: 334 loss: 17.146478271715964 loss_input: 82.21036271631407
step: 15000 epoch: 334 loss: 17.169923376142243 loss_input: 82.26131238467566
Save loss: 17.165945422530175 Name: 334_train_model.pth
step: 0 epoch: 335 loss: 18.98523712158203 loss_input: 103.23944091796875
step: 1000 epoch: 335 loss: 17.25783359230339 loss_input: 83.06125332187344
step: 2000 epoch: 335 loss: 17.177789268465055 loss_input: 82.62333381038972
step: 3000 epoch: 335 loss: 17.212098574964095 loss_input: 82.74289100085127
step: 4000 epoch: 335 loss: 17.19223989054311 loss_input: 82.44296973242041
step: 5000 epoch: 335 loss: 17.22745248156294 loss_input: 82.60004125883808
step: 6000 epoch: 335 loss: 17.17436835301397 loss_input: 82.59877865204432
step: 7000 epoch: 335 loss: 17.22067192172582 loss_input: 82.847189554673
step: 8000 epoch: 335 loss: 17.219373416966192 loss_input: 82.72877049121301
step: 9000 epoch: 335 loss: 17.231004946126685 loss_input: 82.62626163282205
step: 10000 epoch: 335 loss: 17.213232622255315 loss_input: 82.56904184459484
step: 11000 epoch: 335 loss: 17.186341882152956 loss_input: 82.46191212619698
step: 12000 epoch: 335 loss: 17.157276023815953 loss_input: 82.33118236749314
step: 13000 epoch: 335 loss: 17.15678370843419 loss_input: 82.33611351654591
step: 14000 epoch: 335 loss: 17.148662297114857 loss_input: 82.26366953839235
step: 15000 epoch: 335 loss: 17.14658264010503 loss_input: 82.17121693433329
Save loss: 17.152024824678897 Name: 335_train_model.pth
step: 0 epoch: 336 loss: 15.269027709960938 loss_input: 87.51605224609375
step: 1000 epoch: 336 loss: 17.036650266561594 loss_input: 82.1603952687937
step: 2000 epoch: 336 loss: 16.974009457854613 loss_input: 82.0508990194844
step: 3000 epoch: 336 loss: 17.027304980326637 loss_input: 82.3055628999318
step: 4000 epoch: 336 loss: 17.06156111609724 loss_input: 81.85449658093289
step: 5000 epoch: 336 loss: 17.04213880162505 loss_input: 81.85317174846281
step: 6000 epoch: 336 loss: 17.06288527790814 loss_input: 81.95508439023183
step: 7000 epoch: 336 loss: 17.098657111715646 loss_input: 82.05388191614368
step: 8000 epoch: 336 loss: 17.149365208950837 loss_input: 82.42614640821265
step: 9000 epoch: 336 loss: 17.147786800682034 loss_input: 82.3006733653943
step: 10000 epoch: 336 loss: 17.144981751119168 loss_input: 82.23062230520112
step: 11000 epoch: 336 loss: 17.118485390451884 loss_input: 82.07584091143525
step: 12000 epoch: 336 loss: 17.115691111213156 loss_input: 82.10904280824091
step: 13000 epoch: 336 loss: 17.12213045761646 loss_input: 82.06719235520795
step: 14000 epoch: 336 loss: 17.12270261893808 loss_input: 82.07800182677381
step: 15000 epoch: 336 loss: 17.145295538049435 loss_input: 82.11259202186
Save loss: 17.161762648165226 Name: 336_train_model.pth
step: 0 epoch: 337 loss: 18.59130096435547 loss_input: 119.2789306640625
step: 1000 epoch: 337 loss: 17.237083476501983 loss_input: 82.83471771148773
step: 2000 epoch: 337 loss: 17.215406037044193 loss_input: 82.2684539993187
step: 3000 epoch: 337 loss: 17.203002570113195 loss_input: 82.0852608660069
step: 4000 epoch: 337 loss: 17.158186510067466 loss_input: 81.88232899355728
step: 5000 epoch: 337 loss: 17.16287128430942 loss_input: 82.01005471513119
step: 6000 epoch: 337 loss: 17.14232458954671 loss_input: 82.0054553489132
step: 7000 epoch: 337 loss: 17.14089361481489 loss_input: 82.10580187588994
step: 8000 epoch: 337 loss: 17.14744786992101 loss_input: 82.22834548788092
step: 9000 epoch: 337 loss: 17.152543597480427 loss_input: 82.25490164992519
step: 10000 epoch: 337 loss: 17.15957463349048 loss_input: 82.32143442149926
step: 11000 epoch: 337 loss: 17.15643677563247 loss_input: 82.31159421520096
step: 12000 epoch: 337 loss: 17.161573096520005 loss_input: 82.30788165622428
step: 13000 epoch: 337 loss: 17.164208209768606 loss_input: 82.27766231962025
step: 14000 epoch: 337 loss: 17.157813457614754 loss_input: 82.29290249235127
step: 15000 epoch: 337 loss: 17.159097871306134 loss_input: 82.26964442197995
Save loss: 17.155085233196615 Name: 337_train_model.pth
step: 0 epoch: 338 loss: 18.645915985107422 loss_input: 66.40582275390625
step: 1000 epoch: 338 loss: 17.318076581507178 loss_input: 83.2832283073372
step: 2000 epoch: 338 loss: 17.301092801244184 loss_input: 82.86644385326868
step: 3000 epoch: 338 loss: 17.234184200944046 loss_input: 82.58393948843224
step: 4000 epoch: 338 loss: 17.187800919821907 loss_input: 82.76094893818049
step: 5000 epoch: 338 loss: 17.170758439264066 loss_input: 82.44281090071048
step: 6000 epoch: 338 loss: 17.214039900246867 loss_input: 82.61006761324285
step: 7000 epoch: 338 loss: 17.17412232572803 loss_input: 82.56111731527193
step: 8000 epoch: 338 loss: 17.188507860920577 loss_input: 82.45928993983173
step: 9000 epoch: 338 loss: 17.175724961336236 loss_input: 82.31096297324386
step: 10000 epoch: 338 loss: 17.173592951307057 loss_input: 82.33275422457754
step: 11000 epoch: 338 loss: 17.17995745944344 loss_input: 82.2936149925202
step: 12000 epoch: 338 loss: 17.172748196155823 loss_input: 82.27336635928125
step: 13000 epoch: 338 loss: 17.170378674544626 loss_input: 82.23433408611747
step: 14000 epoch: 338 loss: 17.1709918594047 loss_input: 82.19758578820124
step: 15000 epoch: 338 loss: 17.16811308489824 loss_input: 82.19787074416394
Save loss: 17.16357211819291 Name: 338_train_model.pth
step: 0 epoch: 339 loss: 13.252297401428223 loss_input: 56.43890380859375
step: 1000 epoch: 339 loss: 16.85731899774039 loss_input: 83.4125592425153
step: 2000 epoch: 339 loss: 16.988849215004695 loss_input: 82.65656838328883
step: 3000 epoch: 339 loss: 16.997717185880056 loss_input: 81.87308338219943
step: 4000 epoch: 339 loss: 17.053126297364145 loss_input: 81.89301545463243
step: 5000 epoch: 339 loss: 17.093091543663313 loss_input: 82.2049600944069
step: 6000 epoch: 339 loss: 17.085179851643545 loss_input: 81.94900258933082
step: 7000 epoch: 339 loss: 17.076074077578685 loss_input: 81.99736914153168
step: 8000 epoch: 339 loss: 17.052479842799467 loss_input: 81.97428094773065
step: 9000 epoch: 339 loss: 17.084292000895697 loss_input: 81.98552518144686
step: 10000 epoch: 339 loss: 17.111416050248497 loss_input: 82.11634612905897
step: 11000 epoch: 339 loss: 17.10195619751568 loss_input: 82.07718583755347
step: 12000 epoch: 339 loss: 17.097088816900786 loss_input: 82.1029407827668
step: 13000 epoch: 339 loss: 17.11071642671454 loss_input: 82.13404804291648
step: 14000 epoch: 339 loss: 17.124616537900593 loss_input: 82.12243244745214
step: 15000 epoch: 339 loss: 17.144562419545895 loss_input: 82.1705896405409
Save loss: 17.15295619674027 Name: 339_train_model.pth
step: 0 epoch: 340 loss: 16.45539093017578 loss_input: 100.0137939453125
step: 1000 epoch: 340 loss: 17.011660076640585 loss_input: 82.30271443644246
step: 2000 epoch: 340 loss: 17.110015333443506 loss_input: 82.08490758893015
step: 3000 epoch: 340 loss: 17.141626533608722 loss_input: 82.34517827553576
step: 4000 epoch: 340 loss: 17.19974264422824 loss_input: 82.93415560599597
step: 5000 epoch: 340 loss: 17.201083186721114 loss_input: 82.68106461715469
step: 6000 epoch: 340 loss: 17.145504995259298 loss_input: 82.58485723570811
step: 7000 epoch: 340 loss: 17.155677740377385 loss_input: 82.46104689424948
step: 8000 epoch: 340 loss: 17.15840395729686 loss_input: 82.509246067142
step: 9000 epoch: 340 loss: 17.15536052205141 loss_input: 82.37638280805172
step: 10000 epoch: 340 loss: 17.15331345981937 loss_input: 82.33280424398967
step: 11000 epoch: 340 loss: 17.1550506722135 loss_input: 82.36837049792348
step: 12000 epoch: 340 loss: 17.15248621438863 loss_input: 82.40593768878635
step: 13000 epoch: 340 loss: 17.145746157450105 loss_input: 82.29948339498223
step: 14000 epoch: 340 loss: 17.14384335153878 loss_input: 82.24950534975041
step: 15000 epoch: 340 loss: 17.150068011762905 loss_input: 82.26974175435831
Save loss: 17.15354940186441 Name: 340_train_model.pth
step: 0 epoch: 341 loss: 17.003379821777344 loss_input: 106.713134765625
step: 1000 epoch: 341 loss: 17.24386649603372 loss_input: 83.30280504407702
step: 2000 epoch: 341 loss: 17.120702240956778 loss_input: 82.49276887435188
step: 3000 epoch: 341 loss: 17.107665243247318 loss_input: 82.30193438938322
step: 4000 epoch: 341 loss: 17.13151646458903 loss_input: 82.2954555855665
step: 5000 epoch: 341 loss: 17.089439848427105 loss_input: 82.06827528537261
step: 6000 epoch: 341 loss: 17.119972339850072 loss_input: 82.03491287218732
step: 7000 epoch: 341 loss: 17.139638511236523 loss_input: 82.14159088287332
step: 8000 epoch: 341 loss: 17.104530840184655 loss_input: 81.9949176732547
step: 9000 epoch: 341 loss: 17.112338404697308 loss_input: 82.07285325366409
step: 10000 epoch: 341 loss: 17.101234519497154 loss_input: 82.1509904490508
step: 11000 epoch: 341 loss: 17.100145273279704 loss_input: 82.19132738310189
step: 12000 epoch: 341 loss: 17.109030253270717 loss_input: 82.26672430922912
step: 13000 epoch: 341 loss: 17.12863100054594 loss_input: 82.31300097089137
step: 14000 epoch: 341 loss: 17.12219189952147 loss_input: 82.25876000312199
step: 15000 epoch: 341 loss: 17.129508054397096 loss_input: 82.24754251409281
Save loss: 17.145513816788792 Name: 341_train_model.pth
step: 0 epoch: 342 loss: 21.75200653076172 loss_input: 99.8018798828125
step: 1000 epoch: 342 loss: 17.16845258775648 loss_input: 80.96373158091907
step: 2000 epoch: 342 loss: 16.967034164993002 loss_input: 81.36816839383043
step: 3000 epoch: 342 loss: 17.020470591236535 loss_input: 81.96796894931508
step: 4000 epoch: 342 loss: 17.046540699968098 loss_input: 82.29143504529141
step: 5000 epoch: 342 loss: 17.070612831226327 loss_input: 81.93328881831056
step: 6000 epoch: 342 loss: 17.102968751976956 loss_input: 82.04285179561545
step: 7000 epoch: 342 loss: 17.13668790583508 loss_input: 82.23088814111935
step: 8000 epoch: 342 loss: 17.15234132922153 loss_input: 82.20204622401936
step: 9000 epoch: 342 loss: 17.14483494690267 loss_input: 82.135740526172
step: 10000 epoch: 342 loss: 17.137282210223116 loss_input: 82.21136797453067
step: 11000 epoch: 342 loss: 17.147615864411037 loss_input: 82.16794942580681
step: 12000 epoch: 342 loss: 17.168521195190685 loss_input: 82.19194497768903
step: 13000 epoch: 342 loss: 17.163659815952947 loss_input: 82.22245178856434
step: 14000 epoch: 342 loss: 17.163603963430297 loss_input: 82.18304780772495
step: 15000 epoch: 342 loss: 17.153411842665587 loss_input: 82.21441092566485
Save loss: 17.15333488419652 Name: 342_train_model.pth
step: 0 epoch: 343 loss: 14.38420295715332 loss_input: 66.60687255859375
step: 1000 epoch: 343 loss: 16.845110642683732 loss_input: 82.3258923693494
step: 2000 epoch: 343 loss: 17.04227043520743 loss_input: 82.00039939746924
step: 3000 epoch: 343 loss: 17.05319838275992 loss_input: 81.97705389300572
step: 4000 epoch: 343 loss: 17.094354217811038 loss_input: 82.1131387227656
step: 5000 epoch: 343 loss: 17.128197502884905 loss_input: 82.30498194794635
step: 6000 epoch: 343 loss: 17.108687669987322 loss_input: 82.27251207643778
step: 7000 epoch: 343 loss: 17.10952174743573 loss_input: 82.18167163514866
step: 8000 epoch: 343 loss: 17.140148288651474 loss_input: 82.15865200535235
step: 9000 epoch: 343 loss: 17.163461700225852 loss_input: 82.29154616500945
step: 10000 epoch: 343 loss: 17.15426026357554 loss_input: 82.27591023856598
step: 11000 epoch: 343 loss: 17.17844932866675 loss_input: 82.37915490126785
step: 12000 epoch: 343 loss: 17.160533067216754 loss_input: 82.21044785065925
step: 13000 epoch: 343 loss: 17.13749347045948 loss_input: 82.18661828353197
step: 14000 epoch: 343 loss: 17.129233831848794 loss_input: 82.10748070868821
step: 15000 epoch: 343 loss: 17.143451512030747 loss_input: 82.20231132133509
Save loss: 17.149165315926076 Name: 343_train_model.pth
step: 0 epoch: 344 loss: 27.0306339263916 loss_input: 90.39776611328125
step: 1000 epoch: 344 loss: 17.21067559111726 loss_input: 82.89136351929321
step: 2000 epoch: 344 loss: 17.02263301411371 loss_input: 82.41420122458302
step: 3000 epoch: 344 loss: 17.094118579154568 loss_input: 82.60702031614461
step: 4000 epoch: 344 loss: 17.126163328745697 loss_input: 82.62382306339919
step: 5000 epoch: 344 loss: 17.093330786862722 loss_input: 82.64066306611724
step: 6000 epoch: 344 loss: 17.113746148072725 loss_input: 82.6883556109789
step: 7000 epoch: 344 loss: 17.13005907323391 loss_input: 82.71004150111646
step: 8000 epoch: 344 loss: 17.149317173343377 loss_input: 82.73559378075787
step: 9000 epoch: 344 loss: 17.16796612474683 loss_input: 82.64809803983368
step: 10000 epoch: 344 loss: 17.137092514975453 loss_input: 82.5509479386534
step: 11000 epoch: 344 loss: 17.137782960011304 loss_input: 82.47559822664381
step: 12000 epoch: 344 loss: 17.153223168382087 loss_input: 82.45123536661659
step: 13000 epoch: 344 loss: 17.149546832141944 loss_input: 82.35038615193663
step: 14000 epoch: 344 loss: 17.15120802652103 loss_input: 82.24221718276401
step: 15000 epoch: 344 loss: 17.12886832736427 loss_input: 82.19252964515907
Save loss: 17.141945742234586 Name: 344_train_model.pth
step: 0 epoch: 345 loss: 22.907949447631836 loss_input: 98.5421142578125
step: 1000 epoch: 345 loss: 17.176038864966515 loss_input: 81.88637577022587
step: 2000 epoch: 345 loss: 17.170120443003825 loss_input: 81.04942288987343
step: 3000 epoch: 345 loss: 17.121729968349683 loss_input: 81.49619188216558
step: 4000 epoch: 345 loss: 17.077383524177968 loss_input: 81.2756920022924
step: 5000 epoch: 345 loss: 17.050889235643165 loss_input: 81.47071205876013
step: 6000 epoch: 345 loss: 17.119710700469422 loss_input: 81.80227348379762
step: 7000 epoch: 345 loss: 17.099444792962316 loss_input: 81.95898772273604
step: 8000 epoch: 345 loss: 17.107548657305134 loss_input: 82.2582599156917
step: 9000 epoch: 345 loss: 17.13416525512626 loss_input: 82.38752213431364
step: 10000 epoch: 345 loss: 17.161097217447672 loss_input: 82.46860443960487
step: 11000 epoch: 345 loss: 17.16601638518272 loss_input: 82.46067010709432
step: 12000 epoch: 345 loss: 17.160928816628868 loss_input: 82.4235454133705
step: 13000 epoch: 345 loss: 17.14474626273689 loss_input: 82.32988763954005
step: 14000 epoch: 345 loss: 17.162007137005965 loss_input: 82.3648381238324
step: 15000 epoch: 345 loss: 17.158054891423173 loss_input: 82.27879363299608
Save loss: 17.15339195215702 Name: 345_train_model.pth
step: 0 epoch: 346 loss: 16.14751434326172 loss_input: 64.95770263671875
step: 1000 epoch: 346 loss: 17.290090985826918 loss_input: 81.78054050441746
step: 2000 epoch: 346 loss: 17.18348503851521 loss_input: 82.04374408864904
step: 3000 epoch: 346 loss: 17.207935644681754 loss_input: 82.59600583985026
step: 4000 epoch: 346 loss: 17.18601879534856 loss_input: 82.36713760097574
step: 5000 epoch: 346 loss: 17.133008891976946 loss_input: 82.09241503750032
step: 6000 epoch: 346 loss: 17.10382071417027 loss_input: 82.14118750797392
step: 7000 epoch: 346 loss: 17.115661983914997 loss_input: 82.30362707845586
step: 8000 epoch: 346 loss: 17.13501741587855 loss_input: 82.24317797677634
step: 9000 epoch: 346 loss: 17.128814048865625 loss_input: 82.08170708083216
step: 10000 epoch: 346 loss: 17.123795798773433 loss_input: 82.14562596474728
step: 11000 epoch: 346 loss: 17.128838669266575 loss_input: 82.23558516719538
step: 12000 epoch: 346 loss: 17.135485560007368 loss_input: 82.26266262581146
step: 13000 epoch: 346 loss: 17.125155922358115 loss_input: 82.2579327065361
step: 14000 epoch: 346 loss: 17.142757954882534 loss_input: 82.24112956601036
step: 15000 epoch: 346 loss: 17.147443567432646 loss_input: 82.19787837304987
Save loss: 17.147105690672994 Name: 346_train_model.pth
step: 0 epoch: 347 loss: 17.075393676757812 loss_input: 67.448974609375
step: 1000 epoch: 347 loss: 17.198096457775776 loss_input: 82.43852223216237
step: 2000 epoch: 347 loss: 17.196052840565038 loss_input: 82.48661075956163
step: 3000 epoch: 347 loss: 17.155805633529667 loss_input: 82.50459588014932
step: 4000 epoch: 347 loss: 17.12538118727116 loss_input: 82.39948217596479
step: 5000 epoch: 347 loss: 17.126204825143674 loss_input: 82.43804959711183
step: 6000 epoch: 347 loss: 17.14448618280989 loss_input: 82.36173691103566
step: 7000 epoch: 347 loss: 17.110286712305935 loss_input: 82.30273904788156
step: 8000 epoch: 347 loss: 17.087314828785193 loss_input: 82.13230308034005
step: 9000 epoch: 347 loss: 17.10678029073608 loss_input: 82.23284937198818
step: 10000 epoch: 347 loss: 17.10485021434609 loss_input: 82.20552623082418
step: 11000 epoch: 347 loss: 17.129933722679556 loss_input: 82.33722586831595
step: 12000 epoch: 347 loss: 17.12008429801362 loss_input: 82.2655016327985
step: 13000 epoch: 347 loss: 17.128305876697908 loss_input: 82.26416553631992
step: 14000 epoch: 347 loss: 17.119752142175795 loss_input: 82.22368033805343
step: 15000 epoch: 347 loss: 17.132218469215164 loss_input: 82.20464580111977
Save loss: 17.138997894153 Name: 347_train_model.pth
step: 0 epoch: 348 loss: 23.63117218017578 loss_input: 109.65625
step: 1000 epoch: 348 loss: 17.130353024908594 loss_input: 81.82746044238964
step: 2000 epoch: 348 loss: 17.17349155231573 loss_input: 82.47758780521849
step: 3000 epoch: 348 loss: 17.185220625034294 loss_input: 82.8353346532601
step: 4000 epoch: 348 loss: 17.16746270951555 loss_input: 82.68714628288728
step: 5000 epoch: 348 loss: 17.121155480388833 loss_input: 82.41690973328772
step: 6000 epoch: 348 loss: 17.087673780382165 loss_input: 82.21999052607443
step: 7000 epoch: 348 loss: 17.09273305914603 loss_input: 82.18771517050298
step: 8000 epoch: 348 loss: 17.101831343513386 loss_input: 82.32726039249977
step: 9000 epoch: 348 loss: 17.125270048044428 loss_input: 82.34067023694416
step: 10000 epoch: 348 loss: 17.116792748515792 loss_input: 82.35047831276442
step: 11000 epoch: 348 loss: 17.108848369941853 loss_input: 82.23753957381716
step: 12000 epoch: 348 loss: 17.122463710605476 loss_input: 82.32768795857994
step: 13000 epoch: 348 loss: 17.120247374698845 loss_input: 82.32508411030432
step: 14000 epoch: 348 loss: 17.119329653810702 loss_input: 82.24689769996897
step: 15000 epoch: 348 loss: 17.124822833444124 loss_input: 82.26797970141834
Save loss: 17.129863636627793 Name: 348_train_model.pth
step: 0 epoch: 349 loss: 17.86871910095215 loss_input: 72.8265380859375
step: 1000 epoch: 349 loss: 17.168434982175953 loss_input: 82.7153844080724
step: 2000 epoch: 349 loss: 17.068379264066124 loss_input: 82.13047104987545
step: 3000 epoch: 349 loss: 17.130030537398408 loss_input: 81.98072653760596
step: 4000 epoch: 349 loss: 17.18422880908067 loss_input: 81.94151007536202
step: 5000 epoch: 349 loss: 17.13211421813042 loss_input: 81.86025491434916
step: 6000 epoch: 349 loss: 17.124048202241624 loss_input: 82.05334453331672
step: 7000 epoch: 349 loss: 17.16878252498696 loss_input: 82.32840804933838
step: 8000 epoch: 349 loss: 17.145837382277374 loss_input: 82.18780439005182
step: 9000 epoch: 349 loss: 17.157121669370696 loss_input: 82.23622741461887
step: 10000 epoch: 349 loss: 17.16174484271906 loss_input: 82.23580779129118
step: 11000 epoch: 349 loss: 17.160630858298834 loss_input: 82.24303934017362
step: 12000 epoch: 349 loss: 17.14125892885744 loss_input: 82.29828593385447
step: 13000 epoch: 349 loss: 17.14117660301299 loss_input: 82.2786241925576
step: 14000 epoch: 349 loss: 17.14983975770652 loss_input: 82.33374340361709
step: 15000 epoch: 349 loss: 17.143743785188466 loss_input: 82.3120316433657
Save loss: 17.141556138321757 Name: 349_train_model.pth
step: 0 epoch: 350 loss: 16.73352813720703 loss_input: 101.79754638671875
step: 1000 epoch: 350 loss: 17.177103122869333 loss_input: 81.2941561612216
step: 2000 epoch: 350 loss: 17.258965179957134 loss_input: 82.4791403431585
step: 3000 epoch: 350 loss: 17.184139449292125 loss_input: 81.81969631135324
step: 4000 epoch: 350 loss: 17.20490526437938 loss_input: 82.25828921797871
step: 5000 epoch: 350 loss: 17.163573096070714 loss_input: 82.19907432087801
step: 6000 epoch: 350 loss: 17.17829754173388 loss_input: 82.37699098083262
step: 7000 epoch: 350 loss: 17.190235120196835 loss_input: 82.40551111804062
step: 8000 epoch: 350 loss: 17.19015059481858 loss_input: 82.49235676747682
step: 9000 epoch: 350 loss: 17.17321411372158 loss_input: 82.4489746297178
step: 10000 epoch: 350 loss: 17.17192493342314 loss_input: 82.4137439954735
step: 11000 epoch: 350 loss: 17.145791013136137 loss_input: 82.17053629890181
step: 12000 epoch: 350 loss: 17.140795109262267 loss_input: 82.206703894874
step: 13000 epoch: 350 loss: 17.129985714021092 loss_input: 82.12735450356733
step: 14000 epoch: 350 loss: 17.128287881044447 loss_input: 82.16744025817353
step: 15000 epoch: 350 loss: 17.141487816462284 loss_input: 82.23207548289099
Save loss: 17.146825803622605 Name: 350_train_model.pth
step: 0 epoch: 351 loss: 16.499298095703125 loss_input: 62.43212890625
step: 1000 epoch: 351 loss: 16.96851483115426 loss_input: 81.93130965713974
step: 2000 epoch: 351 loss: 17.074465863172083 loss_input: 82.51759724995901
step: 3000 epoch: 351 loss: 17.01812987119426 loss_input: 82.23592081041346
step: 4000 epoch: 351 loss: 17.00051653322593 loss_input: 82.17072781060106
step: 5000 epoch: 351 loss: 17.02378476855517 loss_input: 82.24159876276698
step: 6000 epoch: 351 loss: 17.05685806059873 loss_input: 82.26864972374396
step: 7000 epoch: 351 loss: 17.03601781151462 loss_input: 82.31219080519189
step: 8000 epoch: 351 loss: 17.078090914456997 loss_input: 82.29310842675427
step: 9000 epoch: 351 loss: 17.084656721273404 loss_input: 82.27810105789027
step: 10000 epoch: 351 loss: 17.104105745955785 loss_input: 82.40870897041155
step: 11000 epoch: 351 loss: 17.109865002909114 loss_input: 82.50140790867378
step: 12000 epoch: 351 loss: 17.111693799798662 loss_input: 82.5469403225187
step: 13000 epoch: 351 loss: 17.115458170750188 loss_input: 82.39314649229956
step: 14000 epoch: 351 loss: 17.125690824738076 loss_input: 82.33511563749214
step: 15000 epoch: 351 loss: 17.132924971473383 loss_input: 82.21959959749428
Save loss: 17.131864946827292 Name: 351_train_model.pth
step: 0 epoch: 352 loss: 9.613292694091797 loss_input: 65.74462890625
step: 1000 epoch: 352 loss: 16.855133109516675 loss_input: 81.48029015662073
step: 2000 epoch: 352 loss: 16.890999491127772 loss_input: 81.54843421794962
step: 3000 epoch: 352 loss: 16.938941174608516 loss_input: 81.50652317554305
step: 4000 epoch: 352 loss: 16.899098630429386 loss_input: 81.45373588548664
step: 5000 epoch: 352 loss: 16.979966071481062 loss_input: 81.88034292555551
step: 6000 epoch: 352 loss: 17.017885358189368 loss_input: 81.85392173610714
step: 7000 epoch: 352 loss: 17.067423578603425 loss_input: 82.00689810600439
step: 8000 epoch: 352 loss: 17.09161203212402 loss_input: 82.1832369601871
step: 9000 epoch: 352 loss: 17.08827626544917 loss_input: 82.21920396296028
step: 10000 epoch: 352 loss: 17.090474707283338 loss_input: 82.16962689107066
step: 11000 epoch: 352 loss: 17.114139086137566 loss_input: 82.31849524910022
step: 12000 epoch: 352 loss: 17.11463021340047 loss_input: 82.32025635233522
step: 13000 epoch: 352 loss: 17.129391740921893 loss_input: 82.23661296355358
step: 14000 epoch: 352 loss: 17.12571748119875 loss_input: 82.2165632360314
step: 15000 epoch: 352 loss: 17.14097081334676 loss_input: 82.24501961506603
Save loss: 17.133160527408123 Name: 352_train_model.pth
step: 0 epoch: 353 loss: 18.92343521118164 loss_input: 78.07110595703125
step: 1000 epoch: 353 loss: 17.03532189875097 loss_input: 82.41684560556631
step: 2000 epoch: 353 loss: 17.181829891462197 loss_input: 82.927072199984
step: 3000 epoch: 353 loss: 17.110952870840233 loss_input: 82.76266386953047
step: 4000 epoch: 353 loss: 17.128911097506766 loss_input: 82.6158506607479
step: 5000 epoch: 353 loss: 17.087217385090486 loss_input: 82.47515299186256
step: 6000 epoch: 353 loss: 17.0648558616956 loss_input: 82.32626977941827
step: 7000 epoch: 353 loss: 17.107209219998623 loss_input: 82.36766600830182
step: 8000 epoch: 353 loss: 17.080653752137565 loss_input: 82.1770644681988
step: 9000 epoch: 353 loss: 17.059619341276974 loss_input: 82.11793749126616
step: 10000 epoch: 353 loss: 17.098134828250345 loss_input: 82.14512589878707
step: 11000 epoch: 353 loss: 17.094103001104834 loss_input: 82.2450650349258
step: 12000 epoch: 353 loss: 17.11515708991204 loss_input: 82.37811576818468
step: 13000 epoch: 353 loss: 17.10419458829846 loss_input: 82.31126056991259
step: 14000 epoch: 353 loss: 17.11010316507909 loss_input: 82.27339364065374
step: 15000 epoch: 353 loss: 17.122414141780144 loss_input: 82.25083379044261
Save loss: 17.135031778216362 Name: 353_train_model.pth
step: 0 epoch: 354 loss: 18.152048110961914 loss_input: 113.5418701171875
step: 1000 epoch: 354 loss: 17.25692345498206 loss_input: 83.51332145637565
step: 2000 epoch: 354 loss: 16.989316719165746 loss_input: 82.14190881243948
step: 3000 epoch: 354 loss: 16.986269268581207 loss_input: 82.0491610828617
step: 4000 epoch: 354 loss: 17.011695941547487 loss_input: 82.18808144338428
step: 5000 epoch: 354 loss: 17.052508710980582 loss_input: 82.3370368772925
step: 6000 epoch: 354 loss: 17.086406338594294 loss_input: 82.3331358734319
step: 7000 epoch: 354 loss: 17.070369118299677 loss_input: 82.33137915635378
step: 8000 epoch: 354 loss: 17.07182481649175 loss_input: 82.38925729681739
step: 9000 epoch: 354 loss: 17.079744205410222 loss_input: 82.32719293396654
step: 10000 epoch: 354 loss: 17.114509649484614 loss_input: 82.30609168087098
step: 11000 epoch: 354 loss: 17.103743096718233 loss_input: 82.23910678086438
step: 12000 epoch: 354 loss: 17.114406699310212 loss_input: 82.2956195656791
step: 13000 epoch: 354 loss: 17.106204898602577 loss_input: 82.26657712684211
step: 14000 epoch: 354 loss: 17.128548778676024 loss_input: 82.39115547532738
step: 15000 epoch: 354 loss: 17.141467417997404 loss_input: 82.35032268600713
Save loss: 17.135376949444414 Name: 354_train_model.pth
step: 0 epoch: 355 loss: 16.211612701416016 loss_input: 83.09869384765625
step: 1000 epoch: 355 loss: 17.07517502095911 loss_input: 82.07193758390048
step: 2000 epoch: 355 loss: 17.020121619678747 loss_input: 82.02126247057136
step: 3000 epoch: 355 loss: 17.10285362995533 loss_input: 82.32804686035644
step: 4000 epoch: 355 loss: 17.13258211388048 loss_input: 82.40283392286693
step: 5000 epoch: 355 loss: 17.10740699102535 loss_input: 82.06832776511104
step: 6000 epoch: 355 loss: 17.071791800950454 loss_input: 82.09224565274278
step: 7000 epoch: 355 loss: 17.066332778595562 loss_input: 82.05950026286702
step: 8000 epoch: 355 loss: 17.08057113409549 loss_input: 82.08112543366161
step: 9000 epoch: 355 loss: 17.08694039868085 loss_input: 82.16278510151433
step: 10000 epoch: 355 loss: 17.081652797254225 loss_input: 82.31855520502637
step: 11000 epoch: 355 loss: 17.102326169424455 loss_input: 82.29539352845586
step: 12000 epoch: 355 loss: 17.111342031353168 loss_input: 82.32354170782806
step: 13000 epoch: 355 loss: 17.110367760916102 loss_input: 82.3117344620576
step: 14000 epoch: 355 loss: 17.114276948277247 loss_input: 82.24329097604218
step: 15000 epoch: 355 loss: 17.113909483321482 loss_input: 82.22052237126249
Save loss: 17.132492921978237 Name: 355_train_model.pth
step: 0 epoch: 356 loss: 19.76348876953125 loss_input: 97.26873779296875
step: 1000 epoch: 356 loss: 16.994871137620922 loss_input: 81.94962380149148
step: 2000 epoch: 356 loss: 16.93962508806403 loss_input: 82.12160780571628
step: 3000 epoch: 356 loss: 16.931989500737913 loss_input: 81.91067239468354
step: 4000 epoch: 356 loss: 17.014152779158458 loss_input: 82.13130928998707
step: 5000 epoch: 356 loss: 17.05359956832486 loss_input: 82.0617723867336
step: 6000 epoch: 356 loss: 17.071630063523372 loss_input: 82.0561141014258
step: 7000 epoch: 356 loss: 17.089512564083318 loss_input: 82.04455772152528
step: 8000 epoch: 356 loss: 17.097972111200754 loss_input: 81.94429263084058
step: 9000 epoch: 356 loss: 17.106688202096283 loss_input: 81.97468275144992
step: 10000 epoch: 356 loss: 17.12390603228648 loss_input: 82.16033441211543
step: 11000 epoch: 356 loss: 17.132176980594323 loss_input: 82.09042433575819
step: 12000 epoch: 356 loss: 17.133228017651888 loss_input: 82.15425597976699
step: 13000 epoch: 356 loss: 17.135064081397115 loss_input: 82.22086422194536
step: 14000 epoch: 356 loss: 17.129948955460552 loss_input: 82.24496152409314
step: 15000 epoch: 356 loss: 17.127762835690614 loss_input: 82.22971202147721
Save loss: 17.131483383387327 Name: 356_train_model.pth
step: 0 epoch: 357 loss: 18.895998001098633 loss_input: 87.9229736328125
step: 1000 epoch: 357 loss: 17.120171719855005 loss_input: 82.91369824023633
step: 2000 epoch: 357 loss: 17.134859523315658 loss_input: 83.13302708387018
step: 3000 epoch: 357 loss: 17.188929307067525 loss_input: 82.89622603826068
step: 4000 epoch: 357 loss: 17.06412992594213 loss_input: 82.39641447670309
step: 5000 epoch: 357 loss: 17.101102627794447 loss_input: 82.57413276866112
step: 6000 epoch: 357 loss: 17.086481093645215 loss_input: 82.56653760628112
step: 7000 epoch: 357 loss: 17.075724080909474 loss_input: 82.42503194995581
step: 8000 epoch: 357 loss: 17.09460429182173 loss_input: 82.40170468498566
step: 9000 epoch: 357 loss: 17.10651690090329 loss_input: 82.44508558971962
step: 10000 epoch: 357 loss: 17.105887161709166 loss_input: 82.38218751977341
step: 11000 epoch: 357 loss: 17.102156144230488 loss_input: 82.31161782811115
step: 12000 epoch: 357 loss: 17.086646667054055 loss_input: 82.31401820946074
step: 13000 epoch: 357 loss: 17.087851008840016 loss_input: 82.32868205319092
step: 14000 epoch: 357 loss: 17.107650256890857 loss_input: 82.30305817389299
step: 15000 epoch: 357 loss: 17.112116432739857 loss_input: 82.3198134772786
Save loss: 17.122464138403537 Name: 357_train_model.pth
step: 0 epoch: 358 loss: 21.513042449951172 loss_input: 161.265380859375
step: 1000 epoch: 358 loss: 16.98342012239622 loss_input: 82.11451695277378
step: 2000 epoch: 358 loss: 17.00498192242418 loss_input: 82.00036929167253
step: 3000 epoch: 358 loss: 17.027167559385063 loss_input: 82.08486732043094
step: 4000 epoch: 358 loss: 17.0730278206539 loss_input: 82.28396073283925
step: 5000 epoch: 358 loss: 17.034437880757284 loss_input: 82.2128672654141
step: 6000 epoch: 358 loss: 17.053660679530985 loss_input: 81.94260518754686
step: 7000 epoch: 358 loss: 17.07000930703311 loss_input: 82.02297355086272
step: 8000 epoch: 358 loss: 17.083267839889707 loss_input: 82.38095897192538
step: 9000 epoch: 358 loss: 17.078676016300786 loss_input: 82.3132106754306
step: 10000 epoch: 358 loss: 17.078405775936613 loss_input: 82.33732972527943
step: 11000 epoch: 358 loss: 17.11901921300972 loss_input: 82.3448027385472
step: 12000 epoch: 358 loss: 17.122477510612953 loss_input: 82.32234110902542
step: 13000 epoch: 358 loss: 17.11579893453425 loss_input: 82.24205739244518
step: 14000 epoch: 358 loss: 17.100245110725183 loss_input: 82.19071191134228
step: 15000 epoch: 358 loss: 17.108793447148155 loss_input: 82.27718018668547
Save loss: 17.12111441414058 Name: 358_train_model.pth
step: 0 epoch: 359 loss: 14.499248504638672 loss_input: 78.4549560546875
step: 1000 epoch: 359 loss: 17.056561253287576 loss_input: 81.99350704227413
step: 2000 epoch: 359 loss: 16.98608525355776 loss_input: 81.9252600567392
step: 3000 epoch: 359 loss: 17.01142032287074 loss_input: 81.8091250263584
step: 4000 epoch: 359 loss: 17.014687992578384 loss_input: 81.87539008497387
step: 5000 epoch: 359 loss: 16.980411711656387 loss_input: 81.81873870587211
step: 6000 epoch: 359 loss: 16.981887186791297 loss_input: 81.89581148299828
step: 7000 epoch: 359 loss: 16.980457830899034 loss_input: 82.05932355645622
step: 8000 epoch: 359 loss: 17.004745411464622 loss_input: 82.11834288078492
step: 9000 epoch: 359 loss: 17.02341041309597 loss_input: 82.14065556165417
step: 10000 epoch: 359 loss: 17.04323516313034 loss_input: 82.17536043026557
step: 11000 epoch: 359 loss: 17.073233586811455 loss_input: 82.12274768012034
step: 12000 epoch: 359 loss: 17.084774696432582 loss_input: 82.10533391423111
step: 13000 epoch: 359 loss: 17.098621641926634 loss_input: 82.16017316952181
step: 14000 epoch: 359 loss: 17.11372921255842 loss_input: 82.25890819897491
step: 15000 epoch: 359 loss: 17.11284815161302 loss_input: 82.1934103807142
Save loss: 17.113293725803494 Name: 359_train_model.pth
step: 0 epoch: 360 loss: 27.107254028320312 loss_input: 99.69781494140625
step: 1000 epoch: 360 loss: 17.176790260292076 loss_input: 82.59887792871191
step: 2000 epoch: 360 loss: 17.093383399204633 loss_input: 82.4492323235355
step: 3000 epoch: 360 loss: 17.059457003851804 loss_input: 82.46251141790627
step: 4000 epoch: 360 loss: 17.005630906240192 loss_input: 82.308472610241
step: 5000 epoch: 360 loss: 17.046982673758674 loss_input: 82.42056964583645
step: 6000 epoch: 360 loss: 17.077430414331097 loss_input: 82.33235169219843
step: 7000 epoch: 360 loss: 17.08609645384718 loss_input: 82.28904801929802
step: 8000 epoch: 360 loss: 17.068540210590378 loss_input: 82.30195873037843
step: 9000 epoch: 360 loss: 17.090102853754892 loss_input: 82.36827486968679
step: 10000 epoch: 360 loss: 17.076563430635847 loss_input: 82.33179148515812
step: 11000 epoch: 360 loss: 17.09343765159269 loss_input: 82.40649137210005
step: 12000 epoch: 360 loss: 17.087459263429274 loss_input: 82.28846158266921
step: 13000 epoch: 360 loss: 17.085080598979793 loss_input: 82.29831505599329
step: 14000 epoch: 360 loss: 17.102493292160965 loss_input: 82.3019138682477
step: 15000 epoch: 360 loss: 17.10436148866321 loss_input: 82.26004871159631
Save loss: 17.119573839902877 Name: 360_train_model.pth
step: 0 epoch: 361 loss: 23.593990325927734 loss_input: 82.82513427734375
step: 1000 epoch: 361 loss: 16.769823597384978 loss_input: 82.3827754276973
step: 2000 epoch: 361 loss: 16.91765869694433 loss_input: 81.82627704702337
step: 3000 epoch: 361 loss: 16.960115344871564 loss_input: 81.8905029296875
step: 4000 epoch: 361 loss: 16.98588791241559 loss_input: 82.18076747597769
step: 5000 epoch: 361 loss: 16.99362402328418 loss_input: 81.84125366088891
step: 6000 epoch: 361 loss: 17.03343689590668 loss_input: 81.75254460967118
step: 7000 epoch: 361 loss: 17.046728422123778 loss_input: 81.92544855129513
step: 8000 epoch: 361 loss: 17.04359889870777 loss_input: 82.09917789512151
step: 9000 epoch: 361 loss: 17.050491156571177 loss_input: 82.0161276229182
step: 10000 epoch: 361 loss: 17.05365741447191 loss_input: 82.15426172226528
step: 11000 epoch: 361 loss: 17.054371048955307 loss_input: 82.11052597402367
step: 12000 epoch: 361 loss: 17.056023083411795 loss_input: 82.08175691983351
step: 13000 epoch: 361 loss: 17.06774197097155 loss_input: 82.13744333650266
step: 14000 epoch: 361 loss: 17.086450005538055 loss_input: 82.1185642806694
step: 15000 epoch: 361 loss: 17.097400411241303 loss_input: 82.18756668256621
Save loss: 17.124636741727592 Name: 361_train_model.pth
step: 0 epoch: 362 loss: 18.375896453857422 loss_input: 73.8194580078125
step: 1000 epoch: 362 loss: 16.84989103761229 loss_input: 81.94425941490151
step: 2000 epoch: 362 loss: 17.019850461141043 loss_input: 82.62850566294001
step: 3000 epoch: 362 loss: 17.02860224846799 loss_input: 82.24449286354418
step: 4000 epoch: 362 loss: 17.023142252943273 loss_input: 82.17004772854638
step: 5000 epoch: 362 loss: 17.02647298604244 loss_input: 82.29192358549774
step: 6000 epoch: 362 loss: 17.029185642661343 loss_input: 82.29909177327868
step: 7000 epoch: 362 loss: 17.038546046739373 loss_input: 82.15548662028472
step: 8000 epoch: 362 loss: 17.0328497880698 loss_input: 82.12815857887149
step: 9000 epoch: 362 loss: 17.073224695347875 loss_input: 82.20817695603266
step: 10000 epoch: 362 loss: 17.099191265480005 loss_input: 82.2582869398607
step: 11000 epoch: 362 loss: 17.092470733634602 loss_input: 82.24801105062004
step: 12000 epoch: 362 loss: 17.100934144606462 loss_input: 82.31220562654121
step: 13000 epoch: 362 loss: 17.106432075895132 loss_input: 82.2959699839796
step: 14000 epoch: 362 loss: 17.10903855441562 loss_input: 82.33702880911687
step: 15000 epoch: 362 loss: 17.11150433579951 loss_input: 82.27520236850746
Save loss: 17.118511335283518 Name: 362_train_model.pth
step: 0 epoch: 363 loss: 18.099720001220703 loss_input: 48.20806884765625
step: 1000 epoch: 363 loss: 17.159537881285278 loss_input: 81.71184699089973
step: 2000 epoch: 363 loss: 17.02263610878925 loss_input: 81.86154054320497
step: 3000 epoch: 363 loss: 17.050436354048927 loss_input: 82.41422915180617
step: 4000 epoch: 363 loss: 17.059247256219162 loss_input: 82.61601931975503
step: 5000 epoch: 363 loss: 17.09196480139092 loss_input: 82.65472407227539
step: 6000 epoch: 363 loss: 17.05270669452748 loss_input: 82.35059219256101
step: 7000 epoch: 363 loss: 17.053871541717292 loss_input: 82.29276508733965
step: 8000 epoch: 363 loss: 17.086348902358218 loss_input: 82.39252843333549
step: 9000 epoch: 363 loss: 17.094032852163103 loss_input: 82.28867506474975
step: 10000 epoch: 363 loss: 17.095842371796525 loss_input: 82.28077546859572
step: 11000 epoch: 363 loss: 17.08414641324742 loss_input: 82.24405359677884
step: 12000 epoch: 363 loss: 17.110870626864557 loss_input: 82.32123681058805
step: 13000 epoch: 363 loss: 17.098137948322496 loss_input: 82.20657085028165
step: 14000 epoch: 363 loss: 17.103796646378907 loss_input: 82.22243793412078
step: 15000 epoch: 363 loss: 17.105846378375876 loss_input: 82.23688901696919
Save loss: 17.114152742251754 Name: 363_train_model.pth
step: 0 epoch: 364 loss: 19.275115966796875 loss_input: 61.86505126953125
step: 1000 epoch: 364 loss: 17.057003865351568 loss_input: 82.29205560064935
step: 2000 epoch: 364 loss: 17.095667894812358 loss_input: 82.52183097294126
step: 3000 epoch: 364 loss: 17.08017126570857 loss_input: 82.50465547128822
step: 4000 epoch: 364 loss: 16.977243027011326 loss_input: 82.24593802721195
step: 5000 epoch: 364 loss: 17.007613361227442 loss_input: 82.37895304239933
step: 6000 epoch: 364 loss: 17.026688916864764 loss_input: 82.31383237884832
step: 7000 epoch: 364 loss: 17.057800450506864 loss_input: 82.37752217038191
step: 8000 epoch: 364 loss: 17.04124567029238 loss_input: 82.45026795051497
step: 9000 epoch: 364 loss: 17.070298223651232 loss_input: 82.45862343962649
step: 10000 epoch: 364 loss: 17.057065220764073 loss_input: 82.36040672055842
step: 11000 epoch: 364 loss: 17.085757582027927 loss_input: 82.40690998527833
step: 12000 epoch: 364 loss: 17.082065915596523 loss_input: 82.38090420158672
step: 13000 epoch: 364 loss: 17.094462797665777 loss_input: 82.36031614605366
step: 14000 epoch: 364 loss: 17.104006947862125 loss_input: 82.26618342743237
step: 15000 epoch: 364 loss: 17.097066335237532 loss_input: 82.21418564364589
Save loss: 17.10525430199504 Name: 364_train_model.pth
step: 0 epoch: 365 loss: 17.70494842529297 loss_input: 59.59405517578125
step: 1000 epoch: 365 loss: 16.97869613763693 loss_input: 81.73656744866462
step: 2000 epoch: 365 loss: 17.029199630722054 loss_input: 82.40794254362076
step: 3000 epoch: 365 loss: 16.977280430379054 loss_input: 82.15303693934705
step: 4000 epoch: 365 loss: 16.99984637608203 loss_input: 82.08758978163173
step: 5000 epoch: 365 loss: 17.004970422293564 loss_input: 82.22272368036158
step: 6000 epoch: 365 loss: 17.022465535152595 loss_input: 82.1361471391324
step: 7000 epoch: 365 loss: 17.037812261917885 loss_input: 82.0845921634521
step: 8000 epoch: 365 loss: 17.024145398344373 loss_input: 82.07833887859965
step: 9000 epoch: 365 loss: 17.051088767480273 loss_input: 82.13785399983769
step: 10000 epoch: 365 loss: 17.06421651094988 loss_input: 82.12565219918437
step: 11000 epoch: 365 loss: 17.068219812314474 loss_input: 82.16411615612182
step: 12000 epoch: 365 loss: 17.064562658261938 loss_input: 82.09282229207136
step: 13000 epoch: 365 loss: 17.067107850373244 loss_input: 82.04304261563713
step: 14000 epoch: 365 loss: 17.088564127856056 loss_input: 82.13192781064606
step: 15000 epoch: 365 loss: 17.099534951260182 loss_input: 82.19460981653957
Save loss: 17.106844397529958 Name: 365_train_model.pth
step: 0 epoch: 366 loss: 13.930705070495605 loss_input: 55.0816650390625
step: 1000 epoch: 366 loss: 17.03343838864154 loss_input: 82.33079530332948
step: 2000 epoch: 366 loss: 17.018499229861046 loss_input: 82.37148424078975
step: 3000 epoch: 366 loss: 17.08218663495924 loss_input: 82.46880521841027
step: 4000 epoch: 366 loss: 17.081430677055447 loss_input: 82.50131067696942
step: 5000 epoch: 366 loss: 17.07701057983479 loss_input: 82.61106551882983
step: 6000 epoch: 366 loss: 17.109360731555867 loss_input: 82.45360596842258
step: 7000 epoch: 366 loss: 17.08299647156059 loss_input: 82.25992977740066
step: 8000 epoch: 366 loss: 17.10692213443112 loss_input: 82.32153823789605
step: 9000 epoch: 366 loss: 17.103446250861914 loss_input: 82.33852315020553
step: 10000 epoch: 366 loss: 17.09534006046779 loss_input: 82.27342978592766
step: 11000 epoch: 366 loss: 17.098935184408543 loss_input: 82.211475686825
step: 12000 epoch: 366 loss: 17.088372668368887 loss_input: 82.19246343322301
step: 13000 epoch: 366 loss: 17.105982267951042 loss_input: 82.25514148776416
step: 14000 epoch: 366 loss: 17.117642416493243 loss_input: 82.25428743527945
step: 15000 epoch: 366 loss: 17.118084736183782 loss_input: 82.21958205715973
Save loss: 17.114390558943153 Name: 366_train_model.pth
step: 0 epoch: 367 loss: 14.096931457519531 loss_input: 65.68878173828125
step: 1000 epoch: 367 loss: 16.75804083354466 loss_input: 81.86605709963864
step: 2000 epoch: 367 loss: 16.897971884600228 loss_input: 82.23294389182362
step: 3000 epoch: 367 loss: 17.04956455891707 loss_input: 82.54734715299978
step: 4000 epoch: 367 loss: 17.050617780425615 loss_input: 82.340809191623
step: 5000 epoch: 367 loss: 17.11306814922378 loss_input: 82.70283293194876
step: 6000 epoch: 367 loss: 17.097857242901114 loss_input: 82.67695000052075
step: 7000 epoch: 367 loss: 17.092059312829154 loss_input: 82.63331241986357
step: 8000 epoch: 367 loss: 17.128243256622426 loss_input: 82.58229319540624
step: 9000 epoch: 367 loss: 17.09565371079599 loss_input: 82.38570919827268
step: 10000 epoch: 367 loss: 17.121204239739715 loss_input: 82.45860786235829
step: 11000 epoch: 367 loss: 17.10966071498664 loss_input: 82.35704876216863
step: 12000 epoch: 367 loss: 17.127362266876986 loss_input: 82.35470343800289
step: 13000 epoch: 367 loss: 17.112024738700395 loss_input: 82.20990816962397
step: 14000 epoch: 367 loss: 17.121463485108418 loss_input: 82.25740872024767
step: 15000 epoch: 367 loss: 17.113420978306724 loss_input: 82.23037075441715
Save loss: 17.10667780995369 Name: 367_train_model.pth
step: 0 epoch: 368 loss: 11.16092300415039 loss_input: 57.31536865234375
step: 1000 epoch: 368 loss: 16.967866883768544 loss_input: 80.15477692473542
step: 2000 epoch: 368 loss: 16.933294853647013 loss_input: 80.69246022740583
step: 3000 epoch: 368 loss: 17.030293287177436 loss_input: 81.32961683263783
step: 4000 epoch: 368 loss: 17.004033786241664 loss_input: 81.52054058018699
step: 5000 epoch: 368 loss: 17.00128437056348 loss_input: 81.58042960499697
step: 6000 epoch: 368 loss: 17.011852687129455 loss_input: 81.74520481937907
step: 7000 epoch: 368 loss: 17.01801099158784 loss_input: 81.75222049926182
step: 8000 epoch: 368 loss: 17.02792569080482 loss_input: 81.83849411859406
step: 9000 epoch: 368 loss: 17.045635742394744 loss_input: 82.00125259384157
step: 10000 epoch: 368 loss: 17.052498471247006 loss_input: 82.02316740371862
step: 11000 epoch: 368 loss: 17.041323590198438 loss_input: 82.01321921284855
step: 12000 epoch: 368 loss: 17.05932382539594 loss_input: 82.10173128374556
step: 13000 epoch: 368 loss: 17.07822191806016 loss_input: 82.09859139628983
step: 14000 epoch: 368 loss: 17.091042981608563 loss_input: 82.16049586894742
step: 15000 epoch: 368 loss: 17.129986294825294 loss_input: 82.23378006895895
Save loss: 17.134503315910695 Name: 368_train_model.pth
step: 0 epoch: 369 loss: 14.299171447753906 loss_input: 77.087646484375
step: 1000 epoch: 369 loss: 16.912218605007205 loss_input: 82.27441387957745
step: 2000 epoch: 369 loss: 16.95943793125715 loss_input: 81.97295703344616
step: 3000 epoch: 369 loss: 17.035295717321368 loss_input: 81.83447442567972
step: 4000 epoch: 369 loss: 17.066298623408 loss_input: 81.99919325588525
step: 5000 epoch: 369 loss: 17.00818124563068 loss_input: 81.85814923197002
step: 6000 epoch: 369 loss: 17.027700352640952 loss_input: 81.80922538911157
step: 7000 epoch: 369 loss: 17.03572906362825 loss_input: 81.90011860401081
step: 8000 epoch: 369 loss: 17.060200054069533 loss_input: 81.999391715745
step: 9000 epoch: 369 loss: 17.071001632043274 loss_input: 82.08840434830158
step: 10000 epoch: 369 loss: 17.07385766464476 loss_input: 82.202062630651
step: 11000 epoch: 369 loss: 17.07980865392346 loss_input: 82.18004606559205
step: 12000 epoch: 369 loss: 17.082994137274387 loss_input: 82.17756235157486
step: 13000 epoch: 369 loss: 17.071851582501854 loss_input: 82.1975329384218
step: 14000 epoch: 369 loss: 17.07875338517532 loss_input: 82.22166700181293
step: 15000 epoch: 369 loss: 17.09393689683434 loss_input: 82.19648848361281
Save loss: 17.103964072346688 Name: 369_train_model.pth
step: 0 epoch: 370 loss: 18.395652770996094 loss_input: 173.13372802734375
step: 1000 epoch: 370 loss: 16.899022866200497 loss_input: 83.12584412657655
step: 2000 epoch: 370 loss: 16.949867984046346 loss_input: 82.45680022537559
step: 3000 epoch: 370 loss: 17.05875108401722 loss_input: 82.81812743733859
step: 4000 epoch: 370 loss: 17.125239766975906 loss_input: 82.79356132951625
step: 5000 epoch: 370 loss: 17.06530325776504 loss_input: 82.59749267187578
step: 6000 epoch: 370 loss: 17.07983827265158 loss_input: 82.53062155452932
step: 7000 epoch: 370 loss: 17.075191557126427 loss_input: 82.28838936095067
step: 8000 epoch: 370 loss: 17.0771431233969 loss_input: 82.19620136594254
step: 9000 epoch: 370 loss: 17.07151916040048 loss_input: 82.22375226537329
step: 10000 epoch: 370 loss: 17.069870785538786 loss_input: 82.23970361142597
step: 11000 epoch: 370 loss: 17.06632840981235 loss_input: 82.17520160742667
step: 12000 epoch: 370 loss: 17.07849974954499 loss_input: 82.12107088915279
step: 13000 epoch: 370 loss: 17.093876832302293 loss_input: 82.2539217658212
step: 14000 epoch: 370 loss: 17.10652969205527 loss_input: 82.24357006987643
step: 15000 epoch: 370 loss: 17.105354608309188 loss_input: 82.26292972070091
Save loss: 17.103554995939135 Name: 370_train_model.pth
step: 0 epoch: 371 loss: 11.812637329101562 loss_input: 56.04315185546875
step: 1000 epoch: 371 loss: 16.9903665636922 loss_input: 82.68683910084057
step: 2000 epoch: 371 loss: 17.041776187416318 loss_input: 82.5599762374672
step: 3000 epoch: 371 loss: 16.99050472005929 loss_input: 82.58736935507174
step: 4000 epoch: 371 loss: 17.082246138018032 loss_input: 82.74230198858977
step: 5000 epoch: 371 loss: 17.118443388386837 loss_input: 82.75204586768193
step: 6000 epoch: 371 loss: 17.104572955736852 loss_input: 82.5502288213195
step: 7000 epoch: 371 loss: 17.095454353585342 loss_input: 82.4416035833469
step: 8000 epoch: 371 loss: 17.08991896225622 loss_input: 82.33139860050453
step: 9000 epoch: 371 loss: 17.086218158320154 loss_input: 82.27267454359455
step: 10000 epoch: 371 loss: 17.098765764864858 loss_input: 82.2451416784591
step: 11000 epoch: 371 loss: 17.08128724498279 loss_input: 82.04896194628613
step: 12000 epoch: 371 loss: 17.08686751177406 loss_input: 82.03974019315936
step: 13000 epoch: 371 loss: 17.099580737849617 loss_input: 82.11068308466206
step: 14000 epoch: 371 loss: 17.102686794219156 loss_input: 82.11750646874135
step: 15000 epoch: 371 loss: 17.127537102645558 loss_input: 82.22574477700836
Save loss: 17.106837336540224 Name: 371_train_model.pth
step: 0 epoch: 372 loss: 19.32847785949707 loss_input: 77.06158447265625
step: 1000 epoch: 372 loss: 16.888218410007962 loss_input: 82.13956862229567
step: 2000 epoch: 372 loss: 17.09052979046556 loss_input: 82.70807510063328
step: 3000 epoch: 372 loss: 17.110546710450663 loss_input: 82.83976129864264
step: 4000 epoch: 372 loss: 17.138421570888255 loss_input: 82.92619218852514
step: 5000 epoch: 372 loss: 17.0869645966551 loss_input: 82.73468808435578
step: 6000 epoch: 372 loss: 17.087610769661204 loss_input: 82.62912442449371
step: 7000 epoch: 372 loss: 17.109927974450827 loss_input: 82.55508425031078
step: 8000 epoch: 372 loss: 17.077772620617935 loss_input: 82.50623088007553
step: 9000 epoch: 372 loss: 17.081796407116848 loss_input: 82.54440817204122
step: 10000 epoch: 372 loss: 17.11475946519175 loss_input: 82.55587958049898
step: 11000 epoch: 372 loss: 17.109313728093948 loss_input: 82.37134617400726
step: 12000 epoch: 372 loss: 17.09838886207744 loss_input: 82.3366451029797
step: 13000 epoch: 372 loss: 17.100969431574697 loss_input: 82.26546316599445
step: 14000 epoch: 372 loss: 17.101689835240453 loss_input: 82.22789499410756
step: 15000 epoch: 372 loss: 17.099671348350284 loss_input: 82.31263708396003
Save loss: 17.102952782422303 Name: 372_train_model.pth
step: 0 epoch: 373 loss: 13.596418380737305 loss_input: 51.730712890625
step: 1000 epoch: 373 loss: 17.061766171907927 loss_input: 82.47307630114027
step: 2000 epoch: 373 loss: 17.118741161760124 loss_input: 82.16889477014422
step: 3000 epoch: 373 loss: 17.07938657987519 loss_input: 81.81309371485547
step: 4000 epoch: 373 loss: 17.093929443321237 loss_input: 82.15818455969503
step: 5000 epoch: 373 loss: 17.1056744407306 loss_input: 82.48654907690337
step: 6000 epoch: 373 loss: 17.113486010915853 loss_input: 82.38868564630505
step: 7000 epoch: 373 loss: 17.10262052407692 loss_input: 82.496288311003
step: 8000 epoch: 373 loss: 17.0651130772519 loss_input: 82.22430680814557
step: 9000 epoch: 373 loss: 17.05787117999602 loss_input: 82.19694779600226
step: 10000 epoch: 373 loss: 17.079309335888272 loss_input: 82.22288310109418
step: 11000 epoch: 373 loss: 17.06968567991417 loss_input: 82.1635618352877
step: 12000 epoch: 373 loss: 17.070024610171664 loss_input: 82.2031933587647
step: 13000 epoch: 373 loss: 17.0713760670896 loss_input: 82.14937278243471
step: 14000 epoch: 373 loss: 17.092510489121256 loss_input: 82.21411471097488
step: 15000 epoch: 373 loss: 17.103151713488188 loss_input: 82.2818568402406
Save loss: 17.093198500737547 Name: 373_train_model.pth
step: 0 epoch: 374 loss: 15.799851417541504 loss_input: 89.26617431640625
step: 1000 epoch: 374 loss: 17.019762121594987 loss_input: 81.79353739522196
step: 2000 epoch: 374 loss: 16.90463364023021 loss_input: 81.62937568081193
step: 3000 epoch: 374 loss: 16.987379271759266 loss_input: 81.73471735414367
step: 4000 epoch: 374 loss: 17.062873138841525 loss_input: 82.08387055762914
step: 5000 epoch: 374 loss: 17.04521549742023 loss_input: 82.05248662181626
step: 6000 epoch: 374 loss: 17.052294747667737 loss_input: 82.21548388175539
step: 7000 epoch: 374 loss: 17.10371474064583 loss_input: 82.09770739887327
step: 8000 epoch: 374 loss: 17.113145698146393 loss_input: 82.1492704342133
step: 9000 epoch: 374 loss: 17.096050281151708 loss_input: 82.09975184235172
step: 10000 epoch: 374 loss: 17.085429011601327 loss_input: 82.20585691674974
step: 11000 epoch: 374 loss: 17.097839823246826 loss_input: 82.2084773798529
step: 12000 epoch: 374 loss: 17.12036345086687 loss_input: 82.21902118203204
step: 13000 epoch: 374 loss: 17.128140060711473 loss_input: 82.34183792972205
step: 14000 epoch: 374 loss: 17.1169925577921 loss_input: 82.25218287543495
step: 15000 epoch: 374 loss: 17.10705571876797 loss_input: 82.18407602985972
Save loss: 17.106172806948422 Name: 374_train_model.pth
step: 0 epoch: 375 loss: 21.355941772460938 loss_input: 74.75439453125
step: 1000 epoch: 375 loss: 17.161517947822897 loss_input: 82.19707831231268
step: 2000 epoch: 375 loss: 17.04034199993471 loss_input: 81.3841315121248
step: 3000 epoch: 375 loss: 16.98999935736143 loss_input: 81.36602112040127
step: 4000 epoch: 375 loss: 17.042661855531257 loss_input: 81.69811673058298
step: 5000 epoch: 375 loss: 17.018018679055327 loss_input: 81.8494744288447
step: 6000 epoch: 375 loss: 17.079056285615962 loss_input: 81.9913969925097
step: 7000 epoch: 375 loss: 17.054277790629715 loss_input: 81.8970628620437
step: 8000 epoch: 375 loss: 17.034841075120546 loss_input: 81.92894454324801
step: 9000 epoch: 375 loss: 17.060017390113952 loss_input: 82.09968253446014
step: 10000 epoch: 375 loss: 17.03758268191354 loss_input: 81.97043538444007
step: 11000 epoch: 375 loss: 17.03793021707402 loss_input: 82.02177753904475
step: 12000 epoch: 375 loss: 17.072336011782813 loss_input: 82.09418726011114
step: 13000 epoch: 375 loss: 17.08468678419191 loss_input: 82.14179672383224
step: 14000 epoch: 375 loss: 17.097696609747732 loss_input: 82.30262727467012
step: 15000 epoch: 375 loss: 17.100128831310943 loss_input: 82.27446501125185
Save loss: 17.10232638065517 Name: 375_train_model.pth
step: 0 epoch: 376 loss: 11.345105171203613 loss_input: 96.1541748046875
step: 1000 epoch: 376 loss: 17.046712919667765 loss_input: 83.01058139858189
step: 2000 epoch: 376 loss: 17.07586667526966 loss_input: 82.7669156144584
step: 3000 epoch: 376 loss: 17.0842609253775 loss_input: 82.48028153620017
step: 4000 epoch: 376 loss: 17.10007780219757 loss_input: 82.32507928315772
step: 5000 epoch: 376 loss: 17.135560445179106 loss_input: 82.32250543299543
step: 6000 epoch: 376 loss: 17.1192936797556 loss_input: 82.30385508868639
step: 7000 epoch: 376 loss: 17.088275416478687 loss_input: 82.32987601415479
step: 8000 epoch: 376 loss: 17.070756888213776 loss_input: 82.30900686059262
step: 9000 epoch: 376 loss: 17.07296846183376 loss_input: 82.35810876970702
step: 10000 epoch: 376 loss: 17.078713384297785 loss_input: 82.32309449719091
step: 11000 epoch: 376 loss: 17.09117135888023 loss_input: 82.3019673604335
step: 12000 epoch: 376 loss: 17.09501213006661 loss_input: 82.31356918071769
step: 13000 epoch: 376 loss: 17.10290486804706 loss_input: 82.28133530650136
step: 14000 epoch: 376 loss: 17.09902242077869 loss_input: 82.27148950158694
step: 15000 epoch: 376 loss: 17.093191418184627 loss_input: 82.25509339823309
Save loss: 17.095898573815823 Name: 376_train_model.pth
step: 0 epoch: 377 loss: 24.533531188964844 loss_input: 107.685546875
step: 1000 epoch: 377 loss: 16.949337585822686 loss_input: 82.43630728402456
step: 2000 epoch: 377 loss: 16.90290669260592 loss_input: 81.87150723954429
step: 3000 epoch: 377 loss: 16.986117634523794 loss_input: 81.87221666671006
step: 4000 epoch: 377 loss: 17.050941937924982 loss_input: 82.16678005181322
step: 5000 epoch: 377 loss: 17.04071123393577 loss_input: 82.14722481529085
step: 6000 epoch: 377 loss: 17.07675057088429 loss_input: 82.22031138527693
step: 7000 epoch: 377 loss: 17.049562398373546 loss_input: 82.05622431135674
step: 8000 epoch: 377 loss: 17.092334778543382 loss_input: 82.23888010550195
step: 9000 epoch: 377 loss: 17.116992945406732 loss_input: 82.1577420420626
step: 10000 epoch: 377 loss: 17.118294130145472 loss_input: 82.26402951745841
step: 11000 epoch: 377 loss: 17.116344545355883 loss_input: 82.30011553348666
step: 12000 epoch: 377 loss: 17.120260184213087 loss_input: 82.27776490834422
step: 13000 epoch: 377 loss: 17.098940046683943 loss_input: 82.27392473434286
step: 14000 epoch: 377 loss: 17.09809603875692 loss_input: 82.23574730362452
step: 15000 epoch: 377 loss: 17.10279403046905 loss_input: 82.23378332395028
Save loss: 17.105297243520617 Name: 377_train_model.pth
step: 0 epoch: 378 loss: 19.36477279663086 loss_input: 74.3184814453125
step: 1000 epoch: 378 loss: 17.098267263227648 loss_input: 82.33305884789038
step: 2000 epoch: 378 loss: 17.014976057036407 loss_input: 82.55149796317662
step: 3000 epoch: 378 loss: 17.024192747772634 loss_input: 82.66721283742008
step: 4000 epoch: 378 loss: 17.023886796087957 loss_input: 82.55729822122106
step: 5000 epoch: 378 loss: 17.04102475117312 loss_input: 82.47366963374903
step: 6000 epoch: 378 loss: 17.03643105026484 loss_input: 82.23718726525722
step: 7000 epoch: 378 loss: 17.05685815741685 loss_input: 82.2262609247786
step: 8000 epoch: 378 loss: 17.067577355683646 loss_input: 82.28298942948294
step: 9000 epoch: 378 loss: 17.09469242628886 loss_input: 82.2699589504161
step: 10000 epoch: 378 loss: 17.11405220981503 loss_input: 82.32292608141529
step: 11000 epoch: 378 loss: 17.1131386413172 loss_input: 82.33304372010215
step: 12000 epoch: 378 loss: 17.07431239380895 loss_input: 82.20132848324651
step: 13000 epoch: 378 loss: 17.08867715844814 loss_input: 82.25720788060624
step: 14000 epoch: 378 loss: 17.088309019244182 loss_input: 82.21951961142703
step: 15000 epoch: 378 loss: 17.100194492798458 loss_input: 82.23094281507535
Save loss: 17.096490996554493 Name: 378_train_model.pth
step: 0 epoch: 379 loss: 15.753984451293945 loss_input: 70.6680908203125
step: 1000 epoch: 379 loss: 16.983935407110742 loss_input: 82.64761575094828
step: 2000 epoch: 379 loss: 16.959132631321896 loss_input: 81.89432770797218
step: 3000 epoch: 379 loss: 16.98614224558153 loss_input: 82.19727205189416
step: 4000 epoch: 379 loss: 17.01337525606811 loss_input: 82.26275422721469
step: 5000 epoch: 379 loss: 17.037983587040372 loss_input: 82.25958203134763
step: 6000 epoch: 379 loss: 16.988121794255807 loss_input: 82.03103246626864
step: 7000 epoch: 379 loss: 17.02172848967514 loss_input: 82.16289514270004
step: 8000 epoch: 379 loss: 17.04442733795162 loss_input: 82.1677943485824
step: 9000 epoch: 379 loss: 17.041749500642947 loss_input: 82.04894854754636
step: 10000 epoch: 379 loss: 17.055627665583128 loss_input: 82.14668931666404
step: 11000 epoch: 379 loss: 17.068918960634747 loss_input: 82.14608155184578
step: 12000 epoch: 379 loss: 17.083948219212857 loss_input: 82.18141260139943
step: 13000 epoch: 379 loss: 17.080623886711955 loss_input: 82.1121021039247
step: 14000 epoch: 379 loss: 17.094401192132104 loss_input: 82.15112537476396
step: 15000 epoch: 379 loss: 17.089699986600674 loss_input: 82.14448793827157
Save loss: 17.098775200814007 Name: 379_train_model.pth
step: 0 epoch: 380 loss: 20.706947326660156 loss_input: 104.3646240234375
step: 1000 epoch: 380 loss: 17.070944875389426 loss_input: 82.5594394009311
step: 2000 epoch: 380 loss: 17.135828011158644 loss_input: 82.50723365734125
step: 3000 epoch: 380 loss: 17.084323320179056 loss_input: 82.40837284788535
step: 4000 epoch: 380 loss: 17.074352927816953 loss_input: 82.00789239030574
step: 5000 epoch: 380 loss: 17.025061185062945 loss_input: 81.96751379704529
step: 6000 epoch: 380 loss: 17.03900268685319 loss_input: 82.0400623333611
step: 7000 epoch: 380 loss: 17.050235738006425 loss_input: 81.87331435387857
step: 8000 epoch: 380 loss: 17.074745519952973 loss_input: 81.96633548400445
step: 9000 epoch: 380 loss: 17.073093594319158 loss_input: 81.94071521892533
step: 10000 epoch: 380 loss: 17.100138321052537 loss_input: 82.08656315764908
step: 11000 epoch: 380 loss: 17.091947008009313 loss_input: 81.98835879643931
step: 12000 epoch: 380 loss: 17.094035010945348 loss_input: 82.0621946360427
step: 13000 epoch: 380 loss: 17.063973997292283 loss_input: 81.95363714416048
step: 14000 epoch: 380 loss: 17.066805052081225 loss_input: 82.07052048859651
step: 15000 epoch: 380 loss: 17.08314253034516 loss_input: 82.1697541086779
Save loss: 17.088843067988755 Name: 380_train_model.pth
step: 0 epoch: 381 loss: 25.151630401611328 loss_input: 118.7408447265625
step: 1000 epoch: 381 loss: 16.95305089350347 loss_input: 83.50715160084057
step: 2000 epoch: 381 loss: 17.038587328554808 loss_input: 83.53907088813992
step: 3000 epoch: 381 loss: 17.09137659397017 loss_input: 82.91990090624088
step: 4000 epoch: 381 loss: 17.0214106673689 loss_input: 82.45628052215461
step: 5000 epoch: 381 loss: 17.015078120745557 loss_input: 82.49921871094531
step: 6000 epoch: 381 loss: 17.034956554674423 loss_input: 82.41392353593737
step: 7000 epoch: 381 loss: 17.031367946362398 loss_input: 82.39013494898329
step: 8000 epoch: 381 loss: 17.048267820629086 loss_input: 82.36611333159772
step: 9000 epoch: 381 loss: 17.033934903428257 loss_input: 82.22781685688352
step: 10000 epoch: 381 loss: 17.05380332151683 loss_input: 82.34910536656295
step: 11000 epoch: 381 loss: 17.057956230509294 loss_input: 82.33145985197191
step: 12000 epoch: 381 loss: 17.07983546896325 loss_input: 82.34580985642147
step: 13000 epoch: 381 loss: 17.10535277198731 loss_input: 82.43692483484594
step: 14000 epoch: 381 loss: 17.09850235348403 loss_input: 82.29267838726503
step: 15000 epoch: 381 loss: 17.087170951756292 loss_input: 82.2136429918344
Save loss: 17.091447171032428 Name: 381_train_model.pth
step: 0 epoch: 382 loss: 18.065048217773438 loss_input: 68.086181640625
step: 1000 epoch: 382 loss: 17.155190134858277 loss_input: 83.11238651485233
step: 2000 epoch: 382 loss: 16.989567027456577 loss_input: 82.14804536208459
step: 3000 epoch: 382 loss: 16.91421032746368 loss_input: 81.48294082636517
step: 4000 epoch: 382 loss: 16.938632557494017 loss_input: 81.7725192877806
step: 5000 epoch: 382 loss: 16.96669264455672 loss_input: 81.89835348360016
step: 6000 epoch: 382 loss: 16.990453202174994 loss_input: 81.97804513253325
step: 7000 epoch: 382 loss: 16.953374623945823 loss_input: 81.65758523231335
step: 8000 epoch: 382 loss: 16.99489369712432 loss_input: 81.8529423781744
step: 9000 epoch: 382 loss: 17.018102377073696 loss_input: 81.9030389988135
step: 10000 epoch: 382 loss: 17.0352544172825 loss_input: 82.05468397252072
step: 11000 epoch: 382 loss: 17.05293040166778 loss_input: 82.15838480459517
step: 12000 epoch: 382 loss: 17.04526921540794 loss_input: 82.20296486726691
step: 13000 epoch: 382 loss: 17.041380996255175 loss_input: 82.16200908786765
step: 14000 epoch: 382 loss: 17.044605449424218 loss_input: 82.15656149682808
step: 15000 epoch: 382 loss: 17.06213152814679 loss_input: 82.10663587596764
Save loss: 17.085955681800844 Name: 382_train_model.pth
step: 0 epoch: 383 loss: 16.808961868286133 loss_input: 68.91864013671875
step: 1000 epoch: 383 loss: 17.07927582194874 loss_input: 81.90854031222683
step: 2000 epoch: 383 loss: 16.996564255542363 loss_input: 81.42911262264376
step: 3000 epoch: 383 loss: 17.008127290222973 loss_input: 81.94402785334576
step: 4000 epoch: 383 loss: 17.029439581122823 loss_input: 82.14664084528184
step: 5000 epoch: 383 loss: 17.035280785925792 loss_input: 82.08619421135305
step: 6000 epoch: 383 loss: 17.058941265837866 loss_input: 81.8735360817956
step: 7000 epoch: 383 loss: 17.048524263466685 loss_input: 81.97708871354037
step: 8000 epoch: 383 loss: 17.035653558913925 loss_input: 81.9498688304831
step: 9000 epoch: 383 loss: 17.04779973002013 loss_input: 82.03609101484383
step: 10000 epoch: 383 loss: 17.06770585439358 loss_input: 82.00930327499476
step: 11000 epoch: 383 loss: 17.072996443287373 loss_input: 82.00596621010305
step: 12000 epoch: 383 loss: 17.09819860500491 loss_input: 82.17504713962639
step: 13000 epoch: 383 loss: 17.086385377103866 loss_input: 82.11652828651101
step: 14000 epoch: 383 loss: 17.07089057025565 loss_input: 82.14209151337823
step: 15000 epoch: 383 loss: 17.064758359460097 loss_input: 82.11775390511076
Save loss: 17.093067316949366 Name: 383_train_model.pth
step: 0 epoch: 384 loss: 20.138736724853516 loss_input: 113.21099853515625
step: 1000 epoch: 384 loss: 16.973888824988794 loss_input: 82.39854694865681
step: 2000 epoch: 384 loss: 17.070067247231563 loss_input: 82.87058219547453
step: 3000 epoch: 384 loss: 17.032127218936054 loss_input: 82.74501323858844
step: 4000 epoch: 384 loss: 17.038369003280167 loss_input: 82.7741990283679
step: 5000 epoch: 384 loss: 17.09494005048592 loss_input: 82.9095789606727
step: 6000 epoch: 384 loss: 17.079380419786286 loss_input: 82.55170389692539
step: 7000 epoch: 384 loss: 17.085869540079816 loss_input: 82.58290063263841
step: 8000 epoch: 384 loss: 17.102029228132974 loss_input: 82.64682657923375
step: 9000 epoch: 384 loss: 17.104904172553205 loss_input: 82.65653315131868
step: 10000 epoch: 384 loss: 17.085056615202394 loss_input: 82.48699326234369
step: 11000 epoch: 384 loss: 17.08163553707687 loss_input: 82.45581450270323
step: 12000 epoch: 384 loss: 17.08344816855615 loss_input: 82.4013161104169
step: 13000 epoch: 384 loss: 17.06846387585661 loss_input: 82.37639566806911
step: 14000 epoch: 384 loss: 17.052921681700074 loss_input: 82.25410044127707
step: 15000 epoch: 384 loss: 17.069371338566416 loss_input: 82.32251770373512
Save loss: 17.072500363126398 Name: 384_train_model.pth
step: 0 epoch: 385 loss: 14.95193099975586 loss_input: 76.22845458984375
step: 1000 epoch: 385 loss: 17.05118359671487 loss_input: 81.86530394654174
step: 2000 epoch: 385 loss: 16.974461819993323 loss_input: 81.89334833925811
step: 3000 epoch: 385 loss: 17.0044115349993 loss_input: 81.92113733085066
step: 4000 epoch: 385 loss: 17.079884813714404 loss_input: 82.13966927579628
step: 5000 epoch: 385 loss: 17.03398690360042 loss_input: 82.23451458389962
step: 6000 epoch: 385 loss: 17.047800902465326 loss_input: 82.15123700086444
step: 7000 epoch: 385 loss: 17.04841891507935 loss_input: 82.13023429862977
step: 8000 epoch: 385 loss: 17.03100899192277 loss_input: 81.94314033957426
step: 9000 epoch: 385 loss: 17.033998992625587 loss_input: 81.99465758280547
step: 10000 epoch: 385 loss: 17.032234970730144 loss_input: 82.02809581102437
step: 11000 epoch: 385 loss: 17.050211175031396 loss_input: 82.11582131893373
step: 12000 epoch: 385 loss: 17.044974047088115 loss_input: 82.16006216197478
step: 13000 epoch: 385 loss: 17.0464672078977 loss_input: 82.0911057519689
step: 14000 epoch: 385 loss: 17.05944135955177 loss_input: 82.15296414548793
step: 15000 epoch: 385 loss: 17.082223585483018 loss_input: 82.18386890662367
Save loss: 17.07462727110088 Name: 385_train_model.pth
step: 0 epoch: 386 loss: 15.340429306030273 loss_input: 107.3551025390625
step: 1000 epoch: 386 loss: 17.079690023854777 loss_input: 83.11966939310689
step: 2000 epoch: 386 loss: 17.073757738783502 loss_input: 82.86130503473849
step: 3000 epoch: 386 loss: 17.053141850386012 loss_input: 82.24965908987369
step: 4000 epoch: 386 loss: 17.025939522728684 loss_input: 82.16151273766032
step: 5000 epoch: 386 loss: 17.05519905077937 loss_input: 82.43983412790098
step: 6000 epoch: 386 loss: 17.059834254660064 loss_input: 82.23822150653928
step: 7000 epoch: 386 loss: 17.082831275580595 loss_input: 82.20904170497964
step: 8000 epoch: 386 loss: 17.06833565147232 loss_input: 82.15181638824689
step: 9000 epoch: 386 loss: 17.072725872876394 loss_input: 82.12669959956706
step: 10000 epoch: 386 loss: 17.064836734796142 loss_input: 82.18589531427716
step: 11000 epoch: 386 loss: 17.069905803264483 loss_input: 82.18244935607771
step: 12000 epoch: 386 loss: 17.057775323743193 loss_input: 82.21309709989988
step: 13000 epoch: 386 loss: 17.081453837923668 loss_input: 82.22167475811656
step: 14000 epoch: 386 loss: 17.064963833075506 loss_input: 82.21877603835357
step: 15000 epoch: 386 loss: 17.07273220399262 loss_input: 82.2727461723906
Save loss: 17.076130931243302 Name: 386_train_model.pth
step: 0 epoch: 387 loss: 17.850074768066406 loss_input: 58.2501220703125
step: 1000 epoch: 387 loss: 17.01368865528545 loss_input: 82.68671044531641
step: 2000 epoch: 387 loss: 16.934649737104067 loss_input: 81.78180825358805
step: 3000 epoch: 387 loss: 16.84097481083767 loss_input: 81.43254498663207
step: 4000 epoch: 387 loss: 16.92795848923902 loss_input: 81.81185314328157
step: 5000 epoch: 387 loss: 16.990163842622483 loss_input: 81.93507050543016
step: 6000 epoch: 387 loss: 17.013696728815855 loss_input: 82.14998056761047
step: 7000 epoch: 387 loss: 17.023212836173887 loss_input: 82.06319380085769
step: 8000 epoch: 387 loss: 17.003525867177284 loss_input: 81.9505150738604
step: 9000 epoch: 387 loss: 17.04354652091273 loss_input: 82.09766604844471
step: 10000 epoch: 387 loss: 17.053787331404227 loss_input: 82.169853473637
step: 11000 epoch: 387 loss: 17.06811351269421 loss_input: 82.25547983741535
step: 12000 epoch: 387 loss: 17.08028876697031 loss_input: 82.20868649231217
step: 13000 epoch: 387 loss: 17.07844885808946 loss_input: 82.25592353679374
step: 14000 epoch: 387 loss: 17.068922012416426 loss_input: 82.19876523014818
step: 15000 epoch: 387 loss: 17.085515808927227 loss_input: 82.27126766581446
Save loss: 17.087228991433978 Name: 387_train_model.pth
step: 0 epoch: 388 loss: 19.68413543701172 loss_input: 86.65234375
step: 1000 epoch: 388 loss: 16.755497487036738 loss_input: 81.45265787679118
step: 2000 epoch: 388 loss: 16.974708528056375 loss_input: 81.97120452225059
step: 3000 epoch: 388 loss: 16.967596095230373 loss_input: 81.7168739367151
step: 4000 epoch: 388 loss: 16.998415780466694 loss_input: 81.77466514413877
step: 5000 epoch: 388 loss: 16.991286739543114 loss_input: 82.01311506497528
step: 6000 epoch: 388 loss: 16.989520675399824 loss_input: 82.06257338254497
step: 7000 epoch: 388 loss: 17.021498209475585 loss_input: 82.06664735673921
step: 8000 epoch: 388 loss: 17.011311192316438 loss_input: 82.12037892693833
step: 9000 epoch: 388 loss: 17.0321427899564 loss_input: 82.22501126041334
step: 10000 epoch: 388 loss: 17.03967389113044 loss_input: 82.19223650755042
step: 11000 epoch: 388 loss: 17.033235257153684 loss_input: 82.24043555632905
step: 12000 epoch: 388 loss: 17.050730755146162 loss_input: 82.19248213385376
step: 13000 epoch: 388 loss: 17.057591193126317 loss_input: 82.23680049607813
step: 14000 epoch: 388 loss: 17.060048482169613 loss_input: 82.2376182114224
step: 15000 epoch: 388 loss: 17.063658353527597 loss_input: 82.19764258961511
Save loss: 17.082279322102664 Name: 388_train_model.pth
step: 0 epoch: 389 loss: 13.946861267089844 loss_input: 69.22991943359375
step: 1000 epoch: 389 loss: 17.437729997472925 loss_input: 81.58562897063874
step: 2000 epoch: 389 loss: 17.22951185363701 loss_input: 82.32597522649807
step: 3000 epoch: 389 loss: 17.092542211042566 loss_input: 82.35302931656244
step: 4000 epoch: 389 loss: 17.12907362854263 loss_input: 82.6096465843256
step: 5000 epoch: 389 loss: 17.157997843790618 loss_input: 82.71985312410175
step: 6000 epoch: 389 loss: 17.096958735052336 loss_input: 82.53948742714431
step: 7000 epoch: 389 loss: 17.091799828175596 loss_input: 82.64619612097826
step: 8000 epoch: 389 loss: 17.09689276135872 loss_input: 82.60593682350985
step: 9000 epoch: 389 loss: 17.10116729849167 loss_input: 82.51653108150744
step: 10000 epoch: 389 loss: 17.093337727837437 loss_input: 82.43196003275649
step: 11000 epoch: 389 loss: 17.076934733463194 loss_input: 82.31964196769576
step: 12000 epoch: 389 loss: 17.089829443315796 loss_input: 82.38733302274453
step: 13000 epoch: 389 loss: 17.092265808254965 loss_input: 82.36634339238027
step: 14000 epoch: 389 loss: 17.089161728171057 loss_input: 82.32296525589969
step: 15000 epoch: 389 loss: 17.108237866949743 loss_input: 82.31640104201387
Save loss: 17.10698540146649 Name: 389_train_model.pth
step: 0 epoch: 390 loss: 20.604690551757812 loss_input: 101.60699462890625
step: 1000 epoch: 390 loss: 17.16596239310997 loss_input: 82.33037452049903
step: 2000 epoch: 390 loss: 17.140794590316613 loss_input: 82.76235176479143
step: 3000 epoch: 390 loss: 17.173739072284235 loss_input: 82.76916135783515
step: 4000 epoch: 390 loss: 17.10213020985438 loss_input: 82.36755483980568
step: 5000 epoch: 390 loss: 17.069562282878813 loss_input: 82.21658709029678
step: 6000 epoch: 390 loss: 17.05258624836954 loss_input: 82.36066326795469
step: 7000 epoch: 390 loss: 17.039336200850876 loss_input: 82.27041276818156
step: 8000 epoch: 390 loss: 17.025930715760445 loss_input: 82.190069937852
step: 9000 epoch: 390 loss: 17.021785992328994 loss_input: 82.15132583060432
step: 10000 epoch: 390 loss: 17.061519197697233 loss_input: 82.23026399142313
step: 11000 epoch: 390 loss: 17.04881830582152 loss_input: 82.27193984451343
step: 12000 epoch: 390 loss: 17.05076640729616 loss_input: 82.24695371455049
step: 13000 epoch: 390 loss: 17.07685713859331 loss_input: 82.23937462949816
step: 14000 epoch: 390 loss: 17.07892337106618 loss_input: 82.27932344519337
step: 15000 epoch: 390 loss: 17.078589977355698 loss_input: 82.20549270510872
Save loss: 17.07384839692712 Name: 390_train_model.pth
step: 0 epoch: 391 loss: 13.288182258605957 loss_input: 63.7880859375
step: 1000 epoch: 391 loss: 16.877891579112568 loss_input: 81.77445948826563
step: 2000 epoch: 391 loss: 16.941463423990594 loss_input: 81.56262402246142
step: 3000 epoch: 391 loss: 16.95933897349883 loss_input: 81.79859378742242
step: 4000 epoch: 391 loss: 16.96549199974796 loss_input: 81.70288674779547
step: 5000 epoch: 391 loss: 16.99841280909353 loss_input: 81.93088180459611
step: 6000 epoch: 391 loss: 17.0220432042718 loss_input: 81.84493365078802
step: 7000 epoch: 391 loss: 17.032215521107773 loss_input: 81.89948796551664
step: 8000 epoch: 391 loss: 17.057301002299095 loss_input: 81.9969354340351
step: 9000 epoch: 391 loss: 17.063854639509785 loss_input: 81.96974881073326
step: 10000 epoch: 391 loss: 17.08004096834293 loss_input: 82.09794777358202
step: 11000 epoch: 391 loss: 17.06167596507621 loss_input: 82.14959100952647
step: 12000 epoch: 391 loss: 17.065857690370596 loss_input: 82.09749116874538
step: 13000 epoch: 391 loss: 17.084841746603946 loss_input: 82.13377062660595
step: 14000 epoch: 391 loss: 17.060165081949847 loss_input: 82.11418920952494
step: 15000 epoch: 391 loss: 17.07206543011154 loss_input: 82.16787258941852
Save loss: 17.082432808592916 Name: 391_train_model.pth
step: 0 epoch: 392 loss: 17.15009117126465 loss_input: 134.9368896484375
step: 1000 epoch: 392 loss: 17.048533849782878 loss_input: 83.05532650942807
step: 2000 epoch: 392 loss: 17.049098182117742 loss_input: 82.81780816744948
step: 3000 epoch: 392 loss: 16.961231339100955 loss_input: 82.07630179588574
step: 4000 epoch: 392 loss: 16.93090225773673 loss_input: 81.55910958173543
step: 5000 epoch: 392 loss: 16.95733742007397 loss_input: 81.78938880192712
step: 6000 epoch: 392 loss: 16.976940281290627 loss_input: 81.93143567258647
step: 7000 epoch: 392 loss: 17.01205981089343 loss_input: 81.89756612964331
step: 8000 epoch: 392 loss: 17.038841356859848 loss_input: 82.04530636368207
step: 9000 epoch: 392 loss: 17.028504042609004 loss_input: 81.96093112592524
step: 10000 epoch: 392 loss: 17.041682284923212 loss_input: 82.00093834610679
step: 11000 epoch: 392 loss: 17.05116975659642 loss_input: 81.98456121797877
step: 12000 epoch: 392 loss: 17.073422473824508 loss_input: 82.16178657678591
step: 13000 epoch: 392 loss: 17.068165156705756 loss_input: 82.16873287015783
step: 14000 epoch: 392 loss: 17.06429755882692 loss_input: 82.20492362989697
step: 15000 epoch: 392 loss: 17.068218650058796 loss_input: 82.25654587301602
Save loss: 17.072999305069448 Name: 392_train_model.pth
step: 0 epoch: 393 loss: 17.603452682495117 loss_input: 80.0308837890625
step: 1000 epoch: 393 loss: 16.93299236497679 loss_input: 81.41572721711881
step: 2000 epoch: 393 loss: 16.8884253115847 loss_input: 81.30896395674233
step: 3000 epoch: 393 loss: 16.874860898131967 loss_input: 81.52517969677426
step: 4000 epoch: 393 loss: 16.87988216452109 loss_input: 81.62521074748403
step: 5000 epoch: 393 loss: 16.90068121200513 loss_input: 81.65648786746557
step: 6000 epoch: 393 loss: 16.925008889020155 loss_input: 81.74753075618582
step: 7000 epoch: 393 loss: 16.96454624488786 loss_input: 81.95987660768048
step: 8000 epoch: 393 loss: 17.009791826787644 loss_input: 82.25051604877515
step: 9000 epoch: 393 loss: 17.019174079844163 loss_input: 82.23175718784067
step: 10000 epoch: 393 loss: 17.027325700037647 loss_input: 82.15220201612651
step: 11000 epoch: 393 loss: 17.033929125567628 loss_input: 82.17285029197453
step: 12000 epoch: 393 loss: 17.04930535637034 loss_input: 82.09230580475716
step: 13000 epoch: 393 loss: 17.05967667419886 loss_input: 82.09627891454264
step: 14000 epoch: 393 loss: 17.063193727242897 loss_input: 82.1336223002068
step: 15000 epoch: 393 loss: 17.063458739499332 loss_input: 82.15011025632145
Save loss: 17.068617436006665 Name: 393_train_model.pth
step: 0 epoch: 394 loss: 12.587532043457031 loss_input: 59.56982421875
step: 1000 epoch: 394 loss: 16.793890260673546 loss_input: 81.95347554057271
step: 2000 epoch: 394 loss: 17.03594301379603 loss_input: 82.48201781067475
step: 3000 epoch: 394 loss: 17.03702231074762 loss_input: 82.5757796310735
step: 4000 epoch: 394 loss: 17.074635072816346 loss_input: 82.5832256230376
step: 5000 epoch: 394 loss: 17.113906990597425 loss_input: 82.5241247404816
step: 6000 epoch: 394 loss: 17.04986061006561 loss_input: 82.26303288237767
step: 7000 epoch: 394 loss: 17.07573600514176 loss_input: 82.36504853205959
step: 8000 epoch: 394 loss: 17.04953396193699 loss_input: 82.25793764457421
step: 9000 epoch: 394 loss: 17.069999904556283 loss_input: 82.24937998561141
step: 10000 epoch: 394 loss: 17.065704286009083 loss_input: 82.2186461163454
step: 11000 epoch: 394 loss: 17.062930870940388 loss_input: 82.3849862470333
step: 12000 epoch: 394 loss: 17.082313039701944 loss_input: 82.29237970810375
step: 13000 epoch: 394 loss: 17.06951052738551 loss_input: 82.28261655184427
step: 14000 epoch: 394 loss: 17.07606508777649 loss_input: 82.24294746856111
step: 15000 epoch: 394 loss: 17.07168855395971 loss_input: 82.28311098840156
Save loss: 17.061981348261238 Name: 394_train_model.pth
step: 0 epoch: 395 loss: 9.667020797729492 loss_input: 47.6298828125
step: 1000 epoch: 395 loss: 16.937494274857755 loss_input: 81.27079390336226
step: 2000 epoch: 395 loss: 16.871825217843234 loss_input: 81.10634599692341
step: 3000 epoch: 395 loss: 16.986744117514366 loss_input: 81.35581132786467
step: 4000 epoch: 395 loss: 16.986375538893444 loss_input: 81.54922634087572
step: 5000 epoch: 395 loss: 16.97045577485379 loss_input: 81.68641194856731
step: 6000 epoch: 395 loss: 17.00043557783183 loss_input: 82.02055506618038
step: 7000 epoch: 395 loss: 17.018814339669767 loss_input: 82.0768463479129
step: 8000 epoch: 395 loss: 17.044725929196364 loss_input: 82.10114694067782
step: 9000 epoch: 395 loss: 17.039732256010893 loss_input: 82.04034814409727
step: 10000 epoch: 395 loss: 17.059395957977674 loss_input: 82.17607049719832
step: 11000 epoch: 395 loss: 17.076322280301927 loss_input: 82.23901922002119
step: 12000 epoch: 395 loss: 17.068094447119794 loss_input: 82.12167571240808
step: 13000 epoch: 395 loss: 17.067044612087017 loss_input: 82.19344101497902
step: 14000 epoch: 395 loss: 17.075759015486213 loss_input: 82.21942013122808
step: 15000 epoch: 395 loss: 17.082126328678306 loss_input: 82.33216453782639
Save loss: 17.07308186212182 Name: 395_train_model.pth
step: 0 epoch: 396 loss: 11.733867645263672 loss_input: 59.44012451171875
step: 1000 epoch: 396 loss: 16.975303351700486 loss_input: 82.28834611433489
step: 2000 epoch: 396 loss: 17.164265668970057 loss_input: 83.1409149551201
step: 3000 epoch: 396 loss: 17.012487423257088 loss_input: 82.21000098355887
step: 4000 epoch: 396 loss: 17.006924734923636 loss_input: 82.03054332852334
step: 5000 epoch: 396 loss: 16.995472624072598 loss_input: 82.08443631098
step: 6000 epoch: 396 loss: 16.936537778530493 loss_input: 81.99270487999722
step: 7000 epoch: 396 loss: 16.93119368058003 loss_input: 81.82818361153485
step: 8000 epoch: 396 loss: 16.963793558622058 loss_input: 81.97666851244425
step: 9000 epoch: 396 loss: 16.99208332466928 loss_input: 82.09496697361317
step: 10000 epoch: 396 loss: 16.998070960354298 loss_input: 82.14148202489321
step: 11000 epoch: 396 loss: 16.99480897233157 loss_input: 82.0024800213475
step: 12000 epoch: 396 loss: 16.99717388368827 loss_input: 82.07520294634465
step: 13000 epoch: 396 loss: 17.0105064763224 loss_input: 82.12437789772632
step: 14000 epoch: 396 loss: 17.040801767927945 loss_input: 82.1492777284184
step: 15000 epoch: 396 loss: 17.05864608709784 loss_input: 82.24545895752132
Save loss: 17.060632555291058 Name: 396_train_model.pth
step: 0 epoch: 397 loss: 33.862220764160156 loss_input: 148.88861083984375
step: 1000 epoch: 397 loss: 17.040727078497827 loss_input: 83.3446299793956
step: 2000 epoch: 397 loss: 16.98319189480577 loss_input: 83.46408424205866
step: 3000 epoch: 397 loss: 17.00791247071365 loss_input: 82.83506293393738
step: 4000 epoch: 397 loss: 16.97704111114498 loss_input: 82.04658286722265
step: 5000 epoch: 397 loss: 16.99684734016484 loss_input: 82.14155989602861
step: 6000 epoch: 397 loss: 17.006971366881054 loss_input: 82.17955029549111
step: 7000 epoch: 397 loss: 17.015965678388433 loss_input: 82.24591435842592
step: 8000 epoch: 397 loss: 17.013191852610106 loss_input: 82.32044972039628
step: 9000 epoch: 397 loss: 17.014381790860416 loss_input: 82.25481244678326
step: 10000 epoch: 397 loss: 17.02187677622199 loss_input: 82.29141572537083
step: 11000 epoch: 397 loss: 17.01532653409474 loss_input: 82.22427503261422
step: 12000 epoch: 397 loss: 17.030108958798124 loss_input: 82.23389312690739
step: 13000 epoch: 397 loss: 17.036550770868807 loss_input: 82.15148569457247
step: 14000 epoch: 397 loss: 17.038138579372067 loss_input: 82.156932598948
step: 15000 epoch: 397 loss: 17.052020476870627 loss_input: 82.19003440542114
Save loss: 17.069556665942073 Name: 397_train_model.pth
step: 0 epoch: 398 loss: 16.883922576904297 loss_input: 107.299072265625
step: 1000 epoch: 398 loss: 17.05951048301293 loss_input: 82.12532169978459
step: 2000 epoch: 398 loss: 17.074596296841356 loss_input: 82.13250616513032
step: 3000 epoch: 398 loss: 17.062909985414866 loss_input: 82.27886645613572
step: 4000 epoch: 398 loss: 16.990394486513356 loss_input: 82.08775030973702
step: 5000 epoch: 398 loss: 16.97294915392265 loss_input: 82.13053818679623
step: 6000 epoch: 398 loss: 16.975121468509204 loss_input: 82.21074166065095
step: 7000 epoch: 398 loss: 16.99250135975486 loss_input: 82.16477136566986
step: 8000 epoch: 398 loss: 16.983708148717195 loss_input: 81.92599547187949
step: 9000 epoch: 398 loss: 16.98436358443579 loss_input: 81.94242968229224
step: 10000 epoch: 398 loss: 16.990801757603286 loss_input: 82.01763548010433
step: 11000 epoch: 398 loss: 17.003498466434657 loss_input: 81.99863937262806
step: 12000 epoch: 398 loss: 17.020479723211427 loss_input: 82.07260592358678
step: 13000 epoch: 398 loss: 17.01658223168812 loss_input: 82.07233859395882
step: 14000 epoch: 398 loss: 17.016631239014483 loss_input: 82.12264295138935
step: 15000 epoch: 398 loss: 17.039022666836935 loss_input: 82.09641758339157
Save loss: 17.06303914363682 Name: 398_train_model.pth
step: 0 epoch: 399 loss: 23.860857009887695 loss_input: 92.4058837890625
step: 1000 epoch: 399 loss: 17.318005790005436 loss_input: 82.41205553479723
step: 2000 epoch: 399 loss: 17.19311668013764 loss_input: 81.97890086938952
step: 3000 epoch: 399 loss: 17.078928050022448 loss_input: 81.69224347499402
step: 4000 epoch: 399 loss: 17.034801924714326 loss_input: 82.03230428659924
step: 5000 epoch: 399 loss: 17.037404845509855 loss_input: 82.32428513076681
step: 6000 epoch: 399 loss: 17.045518951045732 loss_input: 82.28108543595599
step: 7000 epoch: 399 loss: 17.036096536130707 loss_input: 82.2458961812654
step: 8000 epoch: 399 loss: 17.055769635206342 loss_input: 82.32323032527428
step: 9000 epoch: 399 loss: 17.014315572238978 loss_input: 82.2431726200344
step: 10000 epoch: 399 loss: 17.027895716521375 loss_input: 82.32522124374476
step: 11000 epoch: 399 loss: 17.031147692552665 loss_input: 82.3511766985583
step: 12000 epoch: 399 loss: 17.03227562791516 loss_input: 82.36242990086113
step: 13000 epoch: 399 loss: 17.039440819377415 loss_input: 82.3721950681528
step: 14000 epoch: 399 loss: 17.044922286770564 loss_input: 82.31971109816793
step: 15000 epoch: 399 loss: 17.0498926423723 loss_input: 82.30715051125497
Save loss: 17.05153551118076 Name: 399_train_model.pth
step: 0 epoch: 400 loss: 19.83865737915039 loss_input: 73.689697265625
step: 1000 epoch: 400 loss: 17.021962162497992 loss_input: 82.31755567430616
step: 2000 epoch: 400 loss: 17.00692719950907 loss_input: 82.40327605362359
step: 3000 epoch: 400 loss: 16.95038098043221 loss_input: 82.12256188855017
step: 4000 epoch: 400 loss: 16.963504529541595 loss_input: 82.25661094186903
step: 5000 epoch: 400 loss: 16.98347276469465 loss_input: 82.22376879604548
step: 6000 epoch: 400 loss: 17.015994585984707 loss_input: 82.3139271608215
step: 7000 epoch: 400 loss: 17.033448996160427 loss_input: 82.28077706254017
step: 8000 epoch: 400 loss: 17.048665595224477 loss_input: 82.35128342111533
step: 9000 epoch: 400 loss: 17.0086638202218 loss_input: 82.33001612017067
step: 10000 epoch: 400 loss: 17.01359106082819 loss_input: 82.16901390915596
step: 11000 epoch: 400 loss: 17.027652154477245 loss_input: 82.15572354195606
step: 12000 epoch: 400 loss: 17.04698120914949 loss_input: 82.2646403154148
step: 13000 epoch: 400 loss: 17.060563453334396 loss_input: 82.22735062390784
step: 14000 epoch: 400 loss: 17.061264067886132 loss_input: 82.26061504291268
step: 15000 epoch: 400 loss: 17.061315843355068 loss_input: 82.22381598306858
Save loss: 17.07212268932164 Name: 400_train_model.pth
step: 0 epoch: 401 loss: 17.650739669799805 loss_input: 75.4591064453125
step: 1000 epoch: 401 loss: 17.06340760022372 loss_input: 82.3957318926191
step: 2000 epoch: 401 loss: 17.029823187409132 loss_input: 81.53740915186938
step: 3000 epoch: 401 loss: 16.99357466600768 loss_input: 81.86405626021795
step: 4000 epoch: 401 loss: 17.007045599020472 loss_input: 81.82176603981328
step: 5000 epoch: 401 loss: 17.037918015685804 loss_input: 81.94260477738436
step: 6000 epoch: 401 loss: 17.043348061603382 loss_input: 81.91494271421885
step: 7000 epoch: 401 loss: 17.042783828517944 loss_input: 81.95170284860254
step: 8000 epoch: 401 loss: 17.04158559695972 loss_input: 81.94783739953648
step: 9000 epoch: 401 loss: 17.06396858984332 loss_input: 82.01571983809525
step: 10000 epoch: 401 loss: 17.054535424562708 loss_input: 81.91794110383883
step: 11000 epoch: 401 loss: 17.044853997679148 loss_input: 82.05533070768425
step: 12000 epoch: 401 loss: 17.049220955696676 loss_input: 82.1489193069767
step: 13000 epoch: 401 loss: 17.04636121579257 loss_input: 82.16603574627372
step: 14000 epoch: 401 loss: 17.072354887989654 loss_input: 82.28702350466875
step: 15000 epoch: 401 loss: 17.058932684555142 loss_input: 82.17433477660734
Save loss: 17.069675257623196 Name: 401_train_model.pth
step: 0 epoch: 402 loss: 29.940088272094727 loss_input: 95.6708984375
step: 1000 epoch: 402 loss: 17.145823963634022 loss_input: 83.15430346021166
step: 2000 epoch: 402 loss: 17.136800326090466 loss_input: 82.39179113446207
step: 3000 epoch: 402 loss: 17.161177338063418 loss_input: 82.68817100029158
step: 4000 epoch: 402 loss: 17.146306324171977 loss_input: 82.68457206682216
step: 5000 epoch: 402 loss: 17.120480036740304 loss_input: 82.52661436706799
step: 6000 epoch: 402 loss: 17.135941485726143 loss_input: 82.67701013047305
step: 7000 epoch: 402 loss: 17.13900318310578 loss_input: 82.75303490579749
step: 8000 epoch: 402 loss: 17.125418253532576 loss_input: 82.54567924950543
step: 9000 epoch: 402 loss: 17.127495504514254 loss_input: 82.48813906744736
step: 10000 epoch: 402 loss: 17.117789110294904 loss_input: 82.54631820753471
step: 11000 epoch: 402 loss: 17.11368216277144 loss_input: 82.46320627013311
step: 12000 epoch: 402 loss: 17.09873715683516 loss_input: 82.2903574512915
step: 13000 epoch: 402 loss: 17.08466207061435 loss_input: 82.20600043141964
step: 14000 epoch: 402 loss: 17.0823960020903 loss_input: 82.29991325692835
step: 15000 epoch: 402 loss: 17.068405488754667 loss_input: 82.15463338414158
Save loss: 17.063551455035807 Name: 402_train_model.pth
step: 0 epoch: 403 loss: 14.642948150634766 loss_input: 77.92083740234375
step: 1000 epoch: 403 loss: 16.85326358583662 loss_input: 80.1106875302432
step: 2000 epoch: 403 loss: 17.08672853066646 loss_input: 82.47401409158702
step: 3000 epoch: 403 loss: 17.11484078302736 loss_input: 82.50216372781338
step: 4000 epoch: 403 loss: 16.979752102722916 loss_input: 82.2367097282076
step: 5000 epoch: 403 loss: 16.96911907129301 loss_input: 82.14606966302053
step: 6000 epoch: 403 loss: 16.958645663843853 loss_input: 82.12001735062385
step: 7000 epoch: 403 loss: 16.993861205849267 loss_input: 82.12786576666502
step: 8000 epoch: 403 loss: 16.971716286197125 loss_input: 82.06323700656803
step: 9000 epoch: 403 loss: 17.015139566634897 loss_input: 82.31692253649334
step: 10000 epoch: 403 loss: 17.048194865943934 loss_input: 82.52539537306035
step: 11000 epoch: 403 loss: 17.03155475884673 loss_input: 82.41283092805661
step: 12000 epoch: 403 loss: 17.04713713762194 loss_input: 82.36902341674977
step: 13000 epoch: 403 loss: 17.046367845153103 loss_input: 82.34596159839417
step: 14000 epoch: 403 loss: 17.046747891683765 loss_input: 82.3094033148023
step: 15000 epoch: 403 loss: 17.051302812519204 loss_input: 82.30420506552818
Save loss: 17.049133155286313 Name: 403_train_model.pth
step: 0 epoch: 404 loss: 12.277942657470703 loss_input: 52.5911865234375
step: 1000 epoch: 404 loss: 16.98569948118288 loss_input: 81.75010798527644
step: 2000 epoch: 404 loss: 17.099935326440402 loss_input: 82.7377037189413
step: 3000 epoch: 404 loss: 16.95294623381295 loss_input: 82.16822696470967
step: 4000 epoch: 404 loss: 16.975902732507315 loss_input: 82.0573182815136
step: 5000 epoch: 404 loss: 17.011221355401236 loss_input: 82.08947434145983
step: 6000 epoch: 404 loss: 17.032395969250068 loss_input: 82.16725839882805
step: 7000 epoch: 404 loss: 17.06056496371986 loss_input: 82.22542082611245
step: 8000 epoch: 404 loss: 17.064863794402115 loss_input: 82.36322997161976
step: 9000 epoch: 404 loss: 17.047870895330437 loss_input: 82.19610133243658
step: 10000 epoch: 404 loss: 17.060252212975076 loss_input: 82.28255748643886
step: 11000 epoch: 404 loss: 17.059661120460767 loss_input: 82.14039932360552
step: 12000 epoch: 404 loss: 17.060016018501393 loss_input: 82.2668234932652
step: 13000 epoch: 404 loss: 17.05946472226432 loss_input: 82.2617558612153
step: 14000 epoch: 404 loss: 17.053288836980173 loss_input: 82.27179734528589
step: 15000 epoch: 404 loss: 17.056039118574475 loss_input: 82.25415035400634
Save loss: 17.05664905564487 Name: 404_train_model.pth
step: 0 epoch: 405 loss: 11.515931129455566 loss_input: 73.16705322265625
step: 1000 epoch: 405 loss: 17.06919318550712 loss_input: 82.36642946896853
step: 2000 epoch: 405 loss: 16.917879760890887 loss_input: 82.21688103092009
step: 3000 epoch: 405 loss: 16.9702953203882 loss_input: 81.95640816667881
step: 4000 epoch: 405 loss: 17.01585535060403 loss_input: 82.2014304163217
step: 5000 epoch: 405 loss: 17.021975220262803 loss_input: 82.09898111930349
step: 6000 epoch: 405 loss: 17.07083058512185 loss_input: 82.16840745862018
step: 7000 epoch: 405 loss: 17.06330099761052 loss_input: 82.2684073958324
step: 8000 epoch: 405 loss: 17.08012613936106 loss_input: 82.23000363632525
step: 9000 epoch: 405 loss: 17.083961226863075 loss_input: 82.194526346031
step: 10000 epoch: 405 loss: 17.115629521814206 loss_input: 82.2531828604249
step: 11000 epoch: 405 loss: 17.106467041511145 loss_input: 82.38989461984279
step: 12000 epoch: 405 loss: 17.090239259026745 loss_input: 82.3945379836511
step: 13000 epoch: 405 loss: 17.085760234163484 loss_input: 82.41199753902644
step: 14000 epoch: 405 loss: 17.07682276000279 loss_input: 82.36660268546666
step: 15000 epoch: 405 loss: 17.078005529866125 loss_input: 82.35210054193001
Save loss: 17.06239991286397 Name: 405_train_model.pth
step: 0 epoch: 406 loss: 19.521461486816406 loss_input: 112.88330078125
step: 1000 epoch: 406 loss: 16.687524214372054 loss_input: 82.43280443921313
step: 2000 epoch: 406 loss: 16.754953220687707 loss_input: 82.283236616555
step: 3000 epoch: 406 loss: 16.861513774500654 loss_input: 82.11097794062135
step: 4000 epoch: 406 loss: 16.903255349664324 loss_input: 82.22660326129404
step: 5000 epoch: 406 loss: 16.92196365013954 loss_input: 82.37541325963323
step: 6000 epoch: 406 loss: 16.972253265708233 loss_input: 82.60510957727749
step: 7000 epoch: 406 loss: 16.99200804692271 loss_input: 82.6065398075805
step: 8000 epoch: 406 loss: 16.995758410826756 loss_input: 82.44089580050053
step: 9000 epoch: 406 loss: 16.985826142720814 loss_input: 82.31749849517588
step: 10000 epoch: 406 loss: 17.008509721509004 loss_input: 82.33182666230448
step: 11000 epoch: 406 loss: 17.023413241857746 loss_input: 82.28453244973113
step: 12000 epoch: 406 loss: 17.036601556180845 loss_input: 82.24631903233325
step: 13000 epoch: 406 loss: 17.05357177249652 loss_input: 82.32047731041422
step: 14000 epoch: 406 loss: 17.046533991273716 loss_input: 82.2733494979502
step: 15000 epoch: 406 loss: 17.05553959493724 loss_input: 82.23509273779534
Save loss: 17.054441192537546 Name: 406_train_model.pth
step: 0 epoch: 407 loss: 16.065776824951172 loss_input: 86.675048828125
step: 1000 epoch: 407 loss: 16.98576603069172 loss_input: 84.20230465335446
step: 2000 epoch: 407 loss: 17.017914435793198 loss_input: 82.66030031249024
step: 3000 epoch: 407 loss: 17.078519068889243 loss_input: 82.38830106761287
step: 4000 epoch: 407 loss: 17.078424172173797 loss_input: 82.41288640242283
step: 5000 epoch: 407 loss: 17.056449575248752 loss_input: 82.197534760626
step: 6000 epoch: 407 loss: 17.069702022533736 loss_input: 82.25351954483823
step: 7000 epoch: 407 loss: 17.08990676436079 loss_input: 82.27429634250073
step: 8000 epoch: 407 loss: 17.07074229512538 loss_input: 82.09096200316269
step: 9000 epoch: 407 loss: 17.053155073999314 loss_input: 81.99089370522415
step: 10000 epoch: 407 loss: 17.023166866233833 loss_input: 82.0327687263012
step: 11000 epoch: 407 loss: 17.042554263775592 loss_input: 82.10255939645579
step: 12000 epoch: 407 loss: 17.05001357908736 loss_input: 82.1310252445597
step: 13000 epoch: 407 loss: 17.046195062304744 loss_input: 82.17603497240746
step: 14000 epoch: 407 loss: 17.029138859216932 loss_input: 82.18258762679758
step: 15000 epoch: 407 loss: 17.035660573684584 loss_input: 82.20977324249347
Save loss: 17.050906178057193 Name: 407_train_model.pth
step: 0 epoch: 408 loss: 21.994544982910156 loss_input: 72.79803466796875
step: 1000 epoch: 408 loss: 17.0508300433983 loss_input: 81.7659962717946
step: 2000 epoch: 408 loss: 17.035095424189798 loss_input: 82.21353459012681
step: 3000 epoch: 408 loss: 17.034492763031803 loss_input: 81.87363418329957
step: 4000 epoch: 408 loss: 17.060615794773668 loss_input: 81.80854119661002
step: 5000 epoch: 408 loss: 17.05385237377993 loss_input: 81.90325153963347
step: 6000 epoch: 408 loss: 17.047111335267147 loss_input: 81.94700230218791
step: 7000 epoch: 408 loss: 17.04274329317074 loss_input: 82.11365439179727
step: 8000 epoch: 408 loss: 17.013435374348035 loss_input: 82.13310155688546
step: 9000 epoch: 408 loss: 17.02416326620621 loss_input: 82.12791056527573
step: 10000 epoch: 408 loss: 17.0274605814689 loss_input: 82.24394964678336
step: 11000 epoch: 408 loss: 17.053052880310837 loss_input: 82.31041370284697
step: 12000 epoch: 408 loss: 17.07316913611491 loss_input: 82.36051191392417
step: 13000 epoch: 408 loss: 17.071876678436723 loss_input: 82.36490210638846
step: 14000 epoch: 408 loss: 17.064678272871653 loss_input: 82.36431604023959
step: 15000 epoch: 408 loss: 17.050991891789504 loss_input: 82.25985559296046
Save loss: 17.05439265845716 Name: 408_train_model.pth
step: 0 epoch: 409 loss: 16.42678451538086 loss_input: 88.9095458984375
step: 1000 epoch: 409 loss: 17.092184781790017 loss_input: 82.36164573951439
step: 2000 epoch: 409 loss: 17.080454375969058 loss_input: 82.33334147745464
step: 3000 epoch: 409 loss: 17.040945279284422 loss_input: 82.65659869968474
step: 4000 epoch: 409 loss: 17.087542874668276 loss_input: 82.70294239794544
step: 5000 epoch: 409 loss: 17.033141316234815 loss_input: 82.4156963895307
step: 6000 epoch: 409 loss: 17.047440692119412 loss_input: 82.40281115053416
step: 7000 epoch: 409 loss: 17.033059285072202 loss_input: 82.3566354447272
step: 8000 epoch: 409 loss: 17.04440831655324 loss_input: 82.41682822178966
step: 9000 epoch: 409 loss: 17.045795627255373 loss_input: 82.44269996340468
step: 10000 epoch: 409 loss: 17.038947886603438 loss_input: 82.41743782775043
step: 11000 epoch: 409 loss: 17.043702122146744 loss_input: 82.40089494594064
step: 12000 epoch: 409 loss: 17.036551461876577 loss_input: 82.36245742033704
step: 13000 epoch: 409 loss: 17.035741263339634 loss_input: 82.30771256872218
step: 14000 epoch: 409 loss: 17.042200724675514 loss_input: 82.28093314989917
step: 15000 epoch: 409 loss: 17.043110137684774 loss_input: 82.22589151608642
Save loss: 17.05164808754623 Name: 409_train_model.pth
step: 0 epoch: 410 loss: 23.492305755615234 loss_input: 103.9722900390625
step: 1000 epoch: 410 loss: 17.181580943661135 loss_input: 82.70264799897369
step: 2000 epoch: 410 loss: 17.139092424403184 loss_input: 83.44639826083052
step: 3000 epoch: 410 loss: 17.18402130355123 loss_input: 83.06251686042803
step: 4000 epoch: 410 loss: 17.096367316614298 loss_input: 82.6755215340452
step: 5000 epoch: 410 loss: 17.104686544552205 loss_input: 82.6007024425193
step: 6000 epoch: 410 loss: 17.100401636561323 loss_input: 82.47372469578637
step: 7000 epoch: 410 loss: 17.083181775445478 loss_input: 82.57368547482893
step: 8000 epoch: 410 loss: 17.100168577567768 loss_input: 82.58803221649296
step: 9000 epoch: 410 loss: 17.081624948876446 loss_input: 82.39725651278548
step: 10000 epoch: 410 loss: 17.088840363323897 loss_input: 82.38495286213566
step: 11000 epoch: 410 loss: 17.08040393685094 loss_input: 82.2498761875696
step: 12000 epoch: 410 loss: 17.07403114026571 loss_input: 82.28256219737223
step: 13000 epoch: 410 loss: 17.0761437759373 loss_input: 82.32357090226083
step: 14000 epoch: 410 loss: 17.068572142593997 loss_input: 82.26209948190782
step: 15000 epoch: 410 loss: 17.081041312441812 loss_input: 82.29053204314374
Save loss: 17.06216945540905 Name: 410_train_model.pth
step: 0 epoch: 411 loss: 12.16108226776123 loss_input: 45.310302734375
step: 1000 epoch: 411 loss: 16.920616576959798 loss_input: 81.94688154862716
step: 2000 epoch: 411 loss: 16.92967199754977 loss_input: 82.0594311608844
step: 3000 epoch: 411 loss: 16.988026538319446 loss_input: 81.67640240118529
step: 4000 epoch: 411 loss: 16.98575200911791 loss_input: 81.73992308197364
step: 5000 epoch: 411 loss: 16.982303692564635 loss_input: 81.56621756703346
step: 6000 epoch: 411 loss: 17.019170169650742 loss_input: 81.54725703675038
step: 7000 epoch: 411 loss: 17.06552628577905 loss_input: 81.8109228763218
step: 8000 epoch: 411 loss: 17.052173565364065 loss_input: 81.99037944506205
step: 9000 epoch: 411 loss: 17.061577254382122 loss_input: 82.06017548343152
step: 10000 epoch: 411 loss: 17.045493576457744 loss_input: 81.96186276147776
step: 11000 epoch: 411 loss: 17.050260403927602 loss_input: 82.09496225329835
step: 12000 epoch: 411 loss: 17.04474398327136 loss_input: 82.15766505847006
step: 13000 epoch: 411 loss: 17.04686621304393 loss_input: 82.116997526257
step: 14000 epoch: 411 loss: 17.039644692320625 loss_input: 82.1185570092859
step: 15000 epoch: 411 loss: 17.061441658290338 loss_input: 82.22242177263053
Save loss: 17.051292267680168 Name: 411_train_model.pth
step: 0 epoch: 412 loss: 16.666397094726562 loss_input: 93.50933837890625
step: 1000 epoch: 412 loss: 16.89456624965687 loss_input: 81.73463743287962
step: 2000 epoch: 412 loss: 16.912236049734073 loss_input: 81.5886410432479
step: 3000 epoch: 412 loss: 16.949427449595962 loss_input: 82.04386192558209
step: 4000 epoch: 412 loss: 16.963117711277672 loss_input: 82.19871496969508
step: 5000 epoch: 412 loss: 16.971183909103647 loss_input: 82.18405155638794
step: 6000 epoch: 412 loss: 16.99323535605165 loss_input: 82.30346842420795
step: 7000 epoch: 412 loss: 17.024093089997983 loss_input: 82.37597678045157
step: 8000 epoch: 412 loss: 17.055349414638542 loss_input: 82.43654647539175
step: 9000 epoch: 412 loss: 17.070486263677342 loss_input: 82.38495302957874
step: 10000 epoch: 412 loss: 17.068959470677765 loss_input: 82.34653092918248
step: 11000 epoch: 412 loss: 17.060653587003564 loss_input: 82.27524106917127
step: 12000 epoch: 412 loss: 17.03894424915274 loss_input: 82.25483424782146
step: 13000 epoch: 412 loss: 17.029021498918734 loss_input: 82.12050437413768
step: 14000 epoch: 412 loss: 17.02508182013412 loss_input: 82.1270574048352
step: 15000 epoch: 412 loss: 17.042910475165407 loss_input: 82.18991290473214
Save loss: 17.051832651019097 Name: 412_train_model.pth
step: 0 epoch: 413 loss: 9.774995803833008 loss_input: 96.60870361328125
step: 1000 epoch: 413 loss: 16.620771159897078 loss_input: 80.53788039353225
step: 2000 epoch: 413 loss: 16.93332723258198 loss_input: 81.65455962204445
step: 3000 epoch: 413 loss: 16.9546179669732 loss_input: 81.92259224952359
step: 4000 epoch: 413 loss: 16.98154059209635 loss_input: 81.93632376512805
step: 5000 epoch: 413 loss: 16.977950446249555 loss_input: 81.94464543370623
step: 6000 epoch: 413 loss: 17.003890870193942 loss_input: 82.04502339006325
step: 7000 epoch: 413 loss: 16.996418746296566 loss_input: 81.90289266537538
step: 8000 epoch: 413 loss: 17.020633493329775 loss_input: 82.00778197860765
step: 9000 epoch: 413 loss: 17.04303553191546 loss_input: 82.19033201497577
step: 10000 epoch: 413 loss: 17.064534277167873 loss_input: 82.28458418196266
step: 11000 epoch: 413 loss: 17.062138732655807 loss_input: 82.38924429943513
step: 12000 epoch: 413 loss: 17.064049342524736 loss_input: 82.38342025778451
step: 13000 epoch: 413 loss: 17.047786538155847 loss_input: 82.27200575353879
step: 14000 epoch: 413 loss: 17.04916928523047 loss_input: 82.29386331762436
step: 15000 epoch: 413 loss: 17.03818312275212 loss_input: 82.276301265788
Save loss: 17.042409403696656 Name: 413_train_model.pth
step: 0 epoch: 414 loss: 14.388446807861328 loss_input: 51.518310546875
step: 1000 epoch: 414 loss: 17.047273418167375 loss_input: 80.99852027854958
step: 2000 epoch: 414 loss: 17.04370857750637 loss_input: 81.95027108028017
step: 3000 epoch: 414 loss: 17.04441648934214 loss_input: 82.16397809616846
step: 4000 epoch: 414 loss: 17.061385335698183 loss_input: 82.2488550683374
step: 5000 epoch: 414 loss: 17.03591946191107 loss_input: 82.41055180077265
step: 6000 epoch: 414 loss: 17.01509191250527 loss_input: 82.24345335144338
step: 7000 epoch: 414 loss: 16.98543839358616 loss_input: 82.2125714741644
step: 8000 epoch: 414 loss: 17.029283634589145 loss_input: 82.31815561934152
step: 9000 epoch: 414 loss: 17.04331959990472 loss_input: 82.29520235463204
step: 10000 epoch: 414 loss: 17.036573600654613 loss_input: 82.24699648565417
step: 11000 epoch: 414 loss: 17.038378775114364 loss_input: 82.22639909045023
step: 12000 epoch: 414 loss: 17.057506136691188 loss_input: 82.29300402029486
step: 13000 epoch: 414 loss: 17.042006532409246 loss_input: 82.26750122192374
step: 14000 epoch: 414 loss: 17.03181886901158 loss_input: 82.19368954289259
step: 15000 epoch: 414 loss: 17.028451687669317 loss_input: 82.18302976578906
Save loss: 17.047386636257173 Name: 414_train_model.pth
step: 0 epoch: 415 loss: 28.875329971313477 loss_input: 105.5380859375
step: 1000 epoch: 415 loss: 16.85053124794593 loss_input: 81.45208270733173
step: 2000 epoch: 415 loss: 16.879608145479796 loss_input: 81.66892457091767
step: 3000 epoch: 415 loss: 16.968416477274552 loss_input: 82.13659753389496
step: 4000 epoch: 415 loss: 17.021574000482765 loss_input: 82.04411203412526
step: 5000 epoch: 415 loss: 16.984971868922724 loss_input: 81.89778666581137
step: 6000 epoch: 415 loss: 17.008656281349044 loss_input: 81.87546726831336
step: 7000 epoch: 415 loss: 17.024459693929398 loss_input: 81.94366345230536
step: 8000 epoch: 415 loss: 16.995444654330747 loss_input: 81.97231017814667
step: 9000 epoch: 415 loss: 16.975891573563402 loss_input: 82.03727163588812
step: 10000 epoch: 415 loss: 16.98247224830911 loss_input: 81.97808147652032
step: 11000 epoch: 415 loss: 16.981202861177152 loss_input: 82.01815679664774
step: 12000 epoch: 415 loss: 16.99043178317964 loss_input: 82.01202201350571
step: 13000 epoch: 415 loss: 17.026865610901407 loss_input: 82.21243723909008
step: 14000 epoch: 415 loss: 17.030776770861948 loss_input: 82.22814735645278
step: 15000 epoch: 415 loss: 17.041396390755217 loss_input: 82.27773240410275
Save loss: 17.048161310508846 Name: 415_train_model.pth
step: 0 epoch: 416 loss: 20.53001594543457 loss_input: 105.33740234375
step: 1000 epoch: 416 loss: 16.79439201435962 loss_input: 81.78952505014517
step: 2000 epoch: 416 loss: 16.971698301425878 loss_input: 82.038719470831
step: 3000 epoch: 416 loss: 16.97717723771756 loss_input: 82.12322074689297
step: 4000 epoch: 416 loss: 16.925690754685693 loss_input: 81.80595674445647
step: 5000 epoch: 416 loss: 16.99158560245234 loss_input: 82.19561619902582
step: 6000 epoch: 416 loss: 17.053308686183463 loss_input: 82.45492803404836
step: 7000 epoch: 416 loss: 17.04458068091773 loss_input: 82.35224947461468
step: 8000 epoch: 416 loss: 17.051129438477506 loss_input: 82.26836360268199
step: 9000 epoch: 416 loss: 17.055802838244237 loss_input: 82.35658744758612
step: 10000 epoch: 416 loss: 17.04589009346956 loss_input: 82.31291173861129
step: 11000 epoch: 416 loss: 17.06326590274835 loss_input: 82.42001582730761
step: 12000 epoch: 416 loss: 17.04921008286779 loss_input: 82.3696431109989
step: 13000 epoch: 416 loss: 17.058980646376224 loss_input: 82.420194685858
step: 14000 epoch: 416 loss: 17.062268751381584 loss_input: 82.42106946758965
step: 15000 epoch: 416 loss: 17.04810553682255 loss_input: 82.35029345211622
Save loss: 17.038609999120236 Name: 416_train_model.pth
step: 0 epoch: 417 loss: 16.66278839111328 loss_input: 58.807373046875
step: 1000 epoch: 417 loss: 16.805886772605447 loss_input: 81.8863933307903
step: 2000 epoch: 417 loss: 17.021300360895527 loss_input: 81.88282503645638
step: 3000 epoch: 417 loss: 16.99048596451736 loss_input: 82.13498005140865
step: 4000 epoch: 417 loss: 16.98642390777218 loss_input: 82.36349179916935
step: 5000 epoch: 417 loss: 17.01755281563545 loss_input: 82.59369896040323
step: 6000 epoch: 417 loss: 17.032214897589927 loss_input: 82.48538244114262
step: 7000 epoch: 417 loss: 17.00948107097851 loss_input: 82.51801109886088
step: 8000 epoch: 417 loss: 17.020530553180535 loss_input: 82.4553424275945
step: 9000 epoch: 417 loss: 17.005066220514802 loss_input: 82.37078568554232
step: 10000 epoch: 417 loss: 17.01649051379614 loss_input: 82.20098724380492
step: 11000 epoch: 417 loss: 17.02188038756637 loss_input: 82.29028090619248
step: 12000 epoch: 417 loss: 17.029362137183956 loss_input: 82.25579717010073
step: 13000 epoch: 417 loss: 17.02676589813024 loss_input: 82.16821656653664
step: 14000 epoch: 417 loss: 17.033135835634504 loss_input: 82.24216925179172
step: 15000 epoch: 417 loss: 17.04184865282421 loss_input: 82.24522778803679
Save loss: 17.042176788583397 Name: 417_train_model.pth
step: 0 epoch: 418 loss: 14.856313705444336 loss_input: 112.13739013671875
step: 1000 epoch: 418 loss: 17.08709744378165 loss_input: 82.05783169467252
step: 2000 epoch: 418 loss: 16.972514555967788 loss_input: 82.02068548855455
step: 3000 epoch: 418 loss: 16.882844601341663 loss_input: 81.70899291707451
step: 4000 epoch: 418 loss: 16.932721104808998 loss_input: 82.01864336467153
step: 5000 epoch: 418 loss: 16.950406396086276 loss_input: 81.97818491721577
step: 6000 epoch: 418 loss: 16.96147657028736 loss_input: 81.96264258894676
step: 7000 epoch: 418 loss: 16.973808307202265 loss_input: 82.10056906804887
step: 8000 epoch: 418 loss: 16.950492633698715 loss_input: 82.01271819019449
step: 9000 epoch: 418 loss: 16.968000320047423 loss_input: 81.9907002452744
step: 10000 epoch: 418 loss: 16.980379743726715 loss_input: 82.1221243049035
step: 11000 epoch: 418 loss: 16.983323238208264 loss_input: 82.11126363334867
step: 12000 epoch: 418 loss: 17.000668163945623 loss_input: 82.15586954362689
step: 13000 epoch: 418 loss: 16.999620531405057 loss_input: 82.16166960827232
step: 14000 epoch: 418 loss: 17.01903206402468 loss_input: 82.24624323621833
step: 15000 epoch: 418 loss: 17.01709066948154 loss_input: 82.20039014814377
Save loss: 17.03943534591794 Name: 418_train_model.pth
step: 0 epoch: 419 loss: 16.915061950683594 loss_input: 82.5068359375
step: 1000 epoch: 419 loss: 16.956982765045318 loss_input: 81.12735939597512
step: 2000 epoch: 419 loss: 16.9984974458419 loss_input: 82.0352415650085
step: 3000 epoch: 419 loss: 17.045592175845663 loss_input: 82.50558200163748
step: 4000 epoch: 419 loss: 16.984563726927156 loss_input: 81.98758254162045
step: 5000 epoch: 419 loss: 16.922485643805228 loss_input: 82.02833438439265
step: 6000 epoch: 419 loss: 16.98447092523815 loss_input: 82.29689896247761
step: 7000 epoch: 419 loss: 16.98340854296053 loss_input: 82.24234241125431
step: 8000 epoch: 419 loss: 16.986440767334216 loss_input: 82.2602364447486
step: 9000 epoch: 419 loss: 17.016402418541123 loss_input: 82.23310859340282
step: 10000 epoch: 419 loss: 17.029784785355464 loss_input: 82.19690381425724
step: 11000 epoch: 419 loss: 17.026833431946777 loss_input: 82.21797916276077
step: 12000 epoch: 419 loss: 17.042545282049048 loss_input: 82.18695425385287
step: 13000 epoch: 419 loss: 17.041937638590788 loss_input: 82.14661347789954
step: 14000 epoch: 419 loss: 17.046389563783016 loss_input: 82.19266592564817
step: 15000 epoch: 419 loss: 17.044850254526107 loss_input: 82.19255624657575
Save loss: 17.03577084927261 Name: 419_train_model.pth
step: 0 epoch: 420 loss: 21.644668579101562 loss_input: 56.99334716796875
step: 1000 epoch: 420 loss: 16.795678614140986 loss_input: 81.58893194208136
step: 2000 epoch: 420 loss: 16.837540852433737 loss_input: 81.94983443946971
step: 3000 epoch: 420 loss: 16.942570563834018 loss_input: 82.82524050414185
step: 4000 epoch: 420 loss: 16.961263604058054 loss_input: 82.9960349115602
step: 5000 epoch: 420 loss: 16.91866641110407 loss_input: 82.78788336570967
step: 6000 epoch: 420 loss: 16.946245870477377 loss_input: 82.72639258609892
step: 7000 epoch: 420 loss: 16.944384499696845 loss_input: 82.64238216038892
step: 8000 epoch: 420 loss: 16.976072523805772 loss_input: 82.54572367754568
step: 9000 epoch: 420 loss: 16.996166436967023 loss_input: 82.52583094472793
step: 10000 epoch: 420 loss: 17.005476225734817 loss_input: 82.4467424595431
step: 11000 epoch: 420 loss: 17.010474559578306 loss_input: 82.39341706018558
step: 12000 epoch: 420 loss: 16.997267981467967 loss_input: 82.18731671144104
step: 13000 epoch: 420 loss: 17.000502585521176 loss_input: 82.1765139113022
step: 14000 epoch: 420 loss: 17.00969532831815 loss_input: 82.19590434615301
step: 15000 epoch: 420 loss: 17.01001885070633 loss_input: 82.16684467943023
Save loss: 17.03533075466752 Name: 420_train_model.pth
step: 0 epoch: 421 loss: 21.917400360107422 loss_input: 109.91815185546875
step: 1000 epoch: 421 loss: 17.120238353203344 loss_input: 82.72492808704968
step: 2000 epoch: 421 loss: 17.126617510756034 loss_input: 82.6420870070336
step: 3000 epoch: 421 loss: 17.112095180172716 loss_input: 82.64873372694127
step: 4000 epoch: 421 loss: 17.066414917984236 loss_input: 82.54732023170965
step: 5000 epoch: 421 loss: 17.068698245319123 loss_input: 82.44271317855569
step: 6000 epoch: 421 loss: 17.01898390351683 loss_input: 82.41097927363668
step: 7000 epoch: 421 loss: 17.022776901441002 loss_input: 82.31691558537118
step: 8000 epoch: 421 loss: 17.008523452194044 loss_input: 82.22559389396393
step: 9000 epoch: 421 loss: 17.02545986155936 loss_input: 82.279594259492
step: 10000 epoch: 421 loss: 17.03136517095418 loss_input: 82.32031531343937
step: 11000 epoch: 421 loss: 17.036395907152805 loss_input: 82.23860723132947
step: 12000 epoch: 421 loss: 17.032391669163953 loss_input: 82.23297197462549
step: 13000 epoch: 421 loss: 17.03036845600538 loss_input: 82.22881959425597
step: 14000 epoch: 421 loss: 17.02526202944975 loss_input: 82.19415776680711
step: 15000 epoch: 421 loss: 17.019697089090673 loss_input: 82.21489155768522
Save loss: 17.029600460484623 Name: 421_train_model.pth
step: 0 epoch: 422 loss: 18.38378143310547 loss_input: 97.934814453125
step: 1000 epoch: 422 loss: 16.87513264838037 loss_input: 81.50187306589895
step: 2000 epoch: 422 loss: 16.94528793943101 loss_input: 82.65851199156282
step: 3000 epoch: 422 loss: 16.99993625317999 loss_input: 82.90706550371881
step: 4000 epoch: 422 loss: 16.968476402851916 loss_input: 82.74084663343442
step: 5000 epoch: 422 loss: 17.001084446072745 loss_input: 82.57711936616583
step: 6000 epoch: 422 loss: 16.994011506103035 loss_input: 82.69687100693179
step: 7000 epoch: 422 loss: 17.01380087171107 loss_input: 82.46427235074118
step: 8000 epoch: 422 loss: 17.00830886647368 loss_input: 82.34634543973019
step: 9000 epoch: 422 loss: 17.02291997739069 loss_input: 82.33431377904625
step: 10000 epoch: 422 loss: 17.043551581202237 loss_input: 82.33610024007365
step: 11000 epoch: 422 loss: 17.063734500107575 loss_input: 82.25445086712642
step: 12000 epoch: 422 loss: 17.085163528328586 loss_input: 82.26083816237156
step: 13000 epoch: 422 loss: 17.08232781667176 loss_input: 82.24120079235942
step: 14000 epoch: 422 loss: 17.059589493573544 loss_input: 82.17791013933575
step: 15000 epoch: 422 loss: 17.066072472667116 loss_input: 82.19522983542633
Save loss: 17.062477657288312 Name: 422_train_model.pth
step: 0 epoch: 423 loss: 15.760297775268555 loss_input: 57.184326171875
step: 1000 epoch: 423 loss: 17.15834316602358 loss_input: 83.20096535544533
step: 2000 epoch: 423 loss: 17.046345325900816 loss_input: 82.56801433267741
step: 3000 epoch: 423 loss: 17.02311710769516 loss_input: 81.8843833468271
step: 4000 epoch: 423 loss: 17.02062258795481 loss_input: 82.157864418783
step: 5000 epoch: 423 loss: 17.01277289479238 loss_input: 82.16642906770208
step: 6000 epoch: 423 loss: 17.011606283176743 loss_input: 82.46754383099235
step: 7000 epoch: 423 loss: 17.027164292631788 loss_input: 82.37862879765781
step: 8000 epoch: 423 loss: 17.006465357015347 loss_input: 82.25864235232315
step: 9000 epoch: 423 loss: 17.037913544047424 loss_input: 82.29306026543925
step: 10000 epoch: 423 loss: 17.02211487253908 loss_input: 82.31974375194316
step: 11000 epoch: 423 loss: 17.029283384985952 loss_input: 82.3318985494394
step: 12000 epoch: 423 loss: 17.03105803702575 loss_input: 82.36353620311914
step: 13000 epoch: 423 loss: 17.045913721926258 loss_input: 82.44376777207042
step: 14000 epoch: 423 loss: 17.035150895058432 loss_input: 82.38658622162451
step: 15000 epoch: 423 loss: 17.024089689199453 loss_input: 82.27128384719013
Save loss: 17.025656397491694 Name: 423_train_model.pth
step: 0 epoch: 424 loss: 21.544170379638672 loss_input: 45.559326171875
step: 1000 epoch: 424 loss: 16.754213555590376 loss_input: 81.83750868272352
step: 2000 epoch: 424 loss: 16.85227883976141 loss_input: 82.07566675646552
step: 3000 epoch: 424 loss: 16.938153146783815 loss_input: 81.99997142472691
step: 4000 epoch: 424 loss: 16.954649094908394 loss_input: 82.36643117357868
step: 5000 epoch: 424 loss: 16.97286098281328 loss_input: 82.30995536732497
step: 6000 epoch: 424 loss: 17.002454840686 loss_input: 82.20562523433594
step: 7000 epoch: 424 loss: 16.983257485464495 loss_input: 82.12227053152563
step: 8000 epoch: 424 loss: 16.998959893450827 loss_input: 81.99384433635248
step: 9000 epoch: 424 loss: 17.026619095654503 loss_input: 82.07104912605196
step: 10000 epoch: 424 loss: 17.03883877418933 loss_input: 82.04316767419938
step: 11000 epoch: 424 loss: 17.038046883600494 loss_input: 82.11382458550466
step: 12000 epoch: 424 loss: 17.054817908605468 loss_input: 82.20163518477952
step: 13000 epoch: 424 loss: 17.06040790005653 loss_input: 82.24274323868867
step: 14000 epoch: 424 loss: 17.06004853875592 loss_input: 82.21447255197452
step: 15000 epoch: 424 loss: 17.056315223262054 loss_input: 82.28629932971917
Save loss: 17.046857836812734 Name: 424_train_model.pth
step: 0 epoch: 425 loss: 8.404948234558105 loss_input: 65.51446533203125
step: 1000 epoch: 425 loss: 16.95227671479369 loss_input: 81.81199915806849
step: 2000 epoch: 425 loss: 16.98259674126598 loss_input: 81.78157271461926
step: 3000 epoch: 425 loss: 16.930510944702036 loss_input: 82.09052496010445
step: 4000 epoch: 425 loss: 16.9858287350293 loss_input: 82.00583737732559
step: 5000 epoch: 425 loss: 16.97722231247644 loss_input: 82.01525710482903
step: 6000 epoch: 425 loss: 17.0248030360749 loss_input: 82.037017949401
step: 7000 epoch: 425 loss: 17.025529531900617 loss_input: 81.91970487370386
step: 8000 epoch: 425 loss: 17.014427796913196 loss_input: 81.94660430782334
step: 9000 epoch: 425 loss: 17.04830808253331 loss_input: 82.05470307579756
step: 10000 epoch: 425 loss: 17.043594346691545 loss_input: 82.16230709692702
step: 11000 epoch: 425 loss: 17.06112010140233 loss_input: 82.28312167821392
step: 12000 epoch: 425 loss: 17.0361288185269 loss_input: 82.26637254207495
step: 13000 epoch: 425 loss: 17.02724663865226 loss_input: 82.152436107867
step: 14000 epoch: 425 loss: 17.02444132177602 loss_input: 82.09763967577929
step: 15000 epoch: 425 loss: 17.028446759567938 loss_input: 82.15386208953257
Save loss: 17.050021119385956 Name: 425_train_model.pth
step: 0 epoch: 426 loss: 12.52572250366211 loss_input: 47.7449951171875
step: 1000 epoch: 426 loss: 16.985858788618913 loss_input: 82.39785281856815
step: 2000 epoch: 426 loss: 16.936406235406544 loss_input: 82.56028638512775
step: 3000 epoch: 426 loss: 16.949344551908855 loss_input: 82.46318064876812
step: 4000 epoch: 426 loss: 16.947458698522507 loss_input: 82.02101623031741
step: 5000 epoch: 426 loss: 16.986974750893328 loss_input: 82.3719776552502
step: 6000 epoch: 426 loss: 16.988741733733466 loss_input: 82.33355057889115
step: 7000 epoch: 426 loss: 16.98268585418943 loss_input: 82.41564152421584
step: 8000 epoch: 426 loss: 16.97800882493477 loss_input: 82.25201081889911
step: 9000 epoch: 426 loss: 17.010194344303898 loss_input: 82.24458849190368
step: 10000 epoch: 426 loss: 17.02994473172884 loss_input: 82.1773257183559
step: 11000 epoch: 426 loss: 17.036301113520846 loss_input: 82.23122926213917
step: 12000 epoch: 426 loss: 17.038703051837977 loss_input: 82.19084939638006
step: 13000 epoch: 426 loss: 17.042867672662535 loss_input: 82.2341492717958
step: 14000 epoch: 426 loss: 17.03690667169774 loss_input: 82.21765280577057
step: 15000 epoch: 426 loss: 17.04075541814783 loss_input: 82.17676131161568
Save loss: 17.040577931389212 Name: 426_train_model.pth
step: 0 epoch: 427 loss: 20.849340438842773 loss_input: 57.26361083984375
step: 1000 epoch: 427 loss: 17.0742479737822 loss_input: 81.42480602893201
step: 2000 epoch: 427 loss: 17.015146922731567 loss_input: 81.25064738138744
step: 3000 epoch: 427 loss: 16.94334589723983 loss_input: 81.570637424244
step: 4000 epoch: 427 loss: 17.034851683583508 loss_input: 81.79080957604837
step: 5000 epoch: 427 loss: 17.006985907506 loss_input: 81.89677442929383
step: 6000 epoch: 427 loss: 17.025827589084297 loss_input: 82.0259938976642
step: 7000 epoch: 427 loss: 17.02789492822344 loss_input: 82.10444275608506
step: 8000 epoch: 427 loss: 17.011096212986395 loss_input: 82.16063768091
step: 9000 epoch: 427 loss: 16.989765309715334 loss_input: 82.18594435280845
step: 10000 epoch: 427 loss: 16.988046808107388 loss_input: 82.16375179498651
step: 11000 epoch: 427 loss: 16.986129961840813 loss_input: 82.11817824910982
step: 12000 epoch: 427 loss: 17.01489844394519 loss_input: 82.17171530973494
step: 13000 epoch: 427 loss: 17.022042569963027 loss_input: 82.13004963222481
step: 14000 epoch: 427 loss: 17.047378139064683 loss_input: 82.27410267029614
step: 15000 epoch: 427 loss: 17.034398055030508 loss_input: 82.26395346836904
Save loss: 17.031213256120683 Name: 427_train_model.pth
step: 0 epoch: 428 loss: 16.37720489501953 loss_input: 70.74249267578125
step: 1000 epoch: 428 loss: 16.860257690126723 loss_input: 82.17920836487731
step: 2000 epoch: 428 loss: 16.924873936480132 loss_input: 81.8211144976828
step: 3000 epoch: 428 loss: 16.913580551659095 loss_input: 82.12168642760356
step: 4000 epoch: 428 loss: 16.838711482290922 loss_input: 81.6558744530176
step: 5000 epoch: 428 loss: 16.898354663297763 loss_input: 82.05047171718
step: 6000 epoch: 428 loss: 16.93059681848851 loss_input: 82.30055460320415
step: 7000 epoch: 428 loss: 16.935511882842192 loss_input: 82.24044958037524
step: 8000 epoch: 428 loss: 16.94753549108206 loss_input: 82.32958331380452
step: 9000 epoch: 428 loss: 16.959619076063124 loss_input: 82.31765368355339
step: 10000 epoch: 428 loss: 16.96845156175472 loss_input: 82.27205949449001
step: 11000 epoch: 428 loss: 16.958526663537047 loss_input: 82.09197092589416
step: 12000 epoch: 428 loss: 16.965297069264277 loss_input: 82.02646567986197
step: 13000 epoch: 428 loss: 16.982803232586758 loss_input: 82.11178203201617
step: 14000 epoch: 428 loss: 17.003885704537357 loss_input: 82.14012362764406
step: 15000 epoch: 428 loss: 17.016872669663336 loss_input: 82.15997659075333
Save loss: 17.031947561576963 Name: 428_train_model.pth
step: 0 epoch: 429 loss: 19.173076629638672 loss_input: 87.89617919921875
step: 1000 epoch: 429 loss: 16.95371318220735 loss_input: 83.06788408125078
step: 2000 epoch: 429 loss: 17.036392418757966 loss_input: 83.20349978209137
step: 3000 epoch: 429 loss: 17.11941044841437 loss_input: 83.16216510186709
step: 4000 epoch: 429 loss: 17.09420763054838 loss_input: 83.0369924303592
step: 5000 epoch: 429 loss: 17.085596429374405 loss_input: 82.78340974871432
step: 6000 epoch: 429 loss: 17.029414712260355 loss_input: 82.5266824527455
step: 7000 epoch: 429 loss: 16.96132893556187 loss_input: 82.2283423447878
step: 8000 epoch: 429 loss: 16.97400809746804 loss_input: 82.22949720701416
step: 9000 epoch: 429 loss: 16.99060542847551 loss_input: 82.23458724312749
step: 10000 epoch: 429 loss: 17.00148251046897 loss_input: 82.15550307762193
step: 11000 epoch: 429 loss: 17.0367631266826 loss_input: 82.17641865432333
step: 12000 epoch: 429 loss: 17.047412136178963 loss_input: 82.23706327797065
step: 13000 epoch: 429 loss: 17.020493390092337 loss_input: 82.2130739464227
step: 14000 epoch: 429 loss: 17.024546237734672 loss_input: 82.18571341496673
step: 15000 epoch: 429 loss: 17.03314049922103 loss_input: 82.20022301247626
Save loss: 17.031394874185324 Name: 429_train_model.pth
step: 0 epoch: 430 loss: 20.681533813476562 loss_input: 118.0386962890625
step: 1000 epoch: 430 loss: 16.944900360259858 loss_input: 82.36252047513034
step: 2000 epoch: 430 loss: 16.936273577450397 loss_input: 81.69760770859688
step: 3000 epoch: 430 loss: 17.051389572978696 loss_input: 82.08373598367601
step: 4000 epoch: 430 loss: 17.024249717611816 loss_input: 82.07229186355755
step: 5000 epoch: 430 loss: 17.04169376309789 loss_input: 82.03655334606907
step: 6000 epoch: 430 loss: 17.03872498665784 loss_input: 82.02760828314096
step: 7000 epoch: 430 loss: 17.07309755119762 loss_input: 82.05354273120022
step: 8000 epoch: 430 loss: 17.05152464249688 loss_input: 82.08378926999359
step: 9000 epoch: 430 loss: 17.062073507516626 loss_input: 82.25140489354264
step: 10000 epoch: 430 loss: 17.045744962327994 loss_input: 82.26151498009379
step: 11000 epoch: 430 loss: 17.03732698532183 loss_input: 82.3426040914338
step: 12000 epoch: 430 loss: 17.031758545557764 loss_input: 82.31699622260483
step: 13000 epoch: 430 loss: 17.021847628252424 loss_input: 82.23941228529296
step: 14000 epoch: 430 loss: 17.020644674726185 loss_input: 82.27144359334964
step: 15000 epoch: 430 loss: 17.03885860754946 loss_input: 82.32592742108208
Save loss: 17.02782679001987 Name: 430_train_model.pth
step: 0 epoch: 431 loss: 12.418011665344238 loss_input: 60.509033203125
step: 1000 epoch: 431 loss: 17.1009720303081 loss_input: 83.60022948243163
step: 2000 epoch: 431 loss: 17.09242309253851 loss_input: 83.02266945140711
step: 3000 epoch: 431 loss: 17.02748853117178 loss_input: 82.64551842907832
step: 4000 epoch: 431 loss: 17.049916615160786 loss_input: 82.5359710029768
step: 5000 epoch: 431 loss: 17.05253906665719 loss_input: 82.45814393029598
step: 6000 epoch: 431 loss: 17.024361185979057 loss_input: 82.29001329978533
step: 7000 epoch: 431 loss: 17.012866663330026 loss_input: 82.24905701075906
step: 8000 epoch: 431 loss: 17.023698223276355 loss_input: 82.20715328979874
step: 9000 epoch: 431 loss: 17.022132326372333 loss_input: 82.1805148616842
step: 10000 epoch: 431 loss: 17.01956390137792 loss_input: 82.23898403006379
step: 11000 epoch: 431 loss: 17.020010007380268 loss_input: 82.22959463956757
step: 12000 epoch: 431 loss: 17.029693428153504 loss_input: 82.38368042388883
step: 13000 epoch: 431 loss: 17.03129831699855 loss_input: 82.34918274390552
step: 14000 epoch: 431 loss: 17.01343656928103 loss_input: 82.26580885091634
step: 15000 epoch: 431 loss: 17.01072999092986 loss_input: 82.20253480904007
Save loss: 17.020270274832846 Name: 431_train_model.pth
step: 0 epoch: 432 loss: 17.57044219970703 loss_input: 65.34661865234375
step: 1000 epoch: 432 loss: 16.8804033529032 loss_input: 81.89816720145089
step: 2000 epoch: 432 loss: 16.880519547860423 loss_input: 82.08079151342298
step: 3000 epoch: 432 loss: 16.96555536121418 loss_input: 82.45830257508565
step: 4000 epoch: 432 loss: 17.0435610072549 loss_input: 82.64198874902856
step: 5000 epoch: 432 loss: 17.055371387651793 loss_input: 82.80074633266706
step: 6000 epoch: 432 loss: 17.05480216022651 loss_input: 82.69201820855378
step: 7000 epoch: 432 loss: 17.094444959678917 loss_input: 82.74508946406094
step: 8000 epoch: 432 loss: 17.090737782631972 loss_input: 82.85970275596654
step: 9000 epoch: 432 loss: 17.08786390455759 loss_input: 82.67092453052092
step: 10000 epoch: 432 loss: 17.06050919232494 loss_input: 82.69391671062385
step: 11000 epoch: 432 loss: 17.017467399779562 loss_input: 82.40525704272801
step: 12000 epoch: 432 loss: 16.99071109351511 loss_input: 82.41664884249555
step: 13000 epoch: 432 loss: 17.007263888268184 loss_input: 82.48522985367562
step: 14000 epoch: 432 loss: 17.01211792972631 loss_input: 82.37334412985568
step: 15000 epoch: 432 loss: 16.999501831999908 loss_input: 82.3136089796162
Save loss: 17.021080600887537 Name: 432_train_model.pth
step: 0 epoch: 433 loss: 22.46048927307129 loss_input: 106.46026611328125
step: 1000 epoch: 433 loss: 17.023061895465755 loss_input: 82.8750744494763
step: 2000 epoch: 433 loss: 17.02232600866944 loss_input: 82.1606220205327
step: 3000 epoch: 433 loss: 17.10254475094008 loss_input: 82.23484225147607
step: 4000 epoch: 433 loss: 17.00929642683028 loss_input: 81.7970506000209
step: 5000 epoch: 433 loss: 16.96076725335437 loss_input: 81.98802035247247
step: 6000 epoch: 433 loss: 16.99551273242491 loss_input: 82.151072101183
step: 7000 epoch: 433 loss: 16.998496004861178 loss_input: 82.17009728241948
step: 8000 epoch: 433 loss: 17.01501196417983 loss_input: 82.13607798384332
step: 9000 epoch: 433 loss: 16.99203170557682 loss_input: 82.07780705572645
step: 10000 epoch: 433 loss: 17.005515150351115 loss_input: 82.17118592095869
step: 11000 epoch: 433 loss: 17.006360072784453 loss_input: 82.22960399374205
step: 12000 epoch: 433 loss: 17.021724096865128 loss_input: 82.29536740468568
step: 13000 epoch: 433 loss: 17.02067604238057 loss_input: 82.2969541471194
step: 14000 epoch: 433 loss: 17.032797867702968 loss_input: 82.30810635369656
step: 15000 epoch: 433 loss: 17.02279564778206 loss_input: 82.35901873712976
Save loss: 17.02021735660732 Name: 433_train_model.pth
step: 0 epoch: 434 loss: 20.327316284179688 loss_input: 123.04248046875
step: 1000 epoch: 434 loss: 16.797118486581624 loss_input: 81.62726543476055
step: 2000 epoch: 434 loss: 16.908828219909896 loss_input: 81.86301941802536
step: 3000 epoch: 434 loss: 16.93478625426885 loss_input: 81.93780175895144
step: 4000 epoch: 434 loss: 16.903231556670008 loss_input: 81.51730832550652
step: 5000 epoch: 434 loss: 16.951144375769623 loss_input: 81.80384150220738
step: 6000 epoch: 434 loss: 16.933518996895838 loss_input: 81.75302247599033
step: 7000 epoch: 434 loss: 16.94121115797844 loss_input: 81.91179372882557
step: 8000 epoch: 434 loss: 16.954077810604055 loss_input: 81.98041076622015
step: 9000 epoch: 434 loss: 16.968769856339467 loss_input: 82.09014769843282
step: 10000 epoch: 434 loss: 16.974267965912187 loss_input: 82.17598733901023
step: 11000 epoch: 434 loss: 16.98911501927632 loss_input: 82.17336568146682
step: 12000 epoch: 434 loss: 16.98683706594203 loss_input: 82.15154908253902
step: 13000 epoch: 434 loss: 16.98362182628264 loss_input: 82.12256917549311
step: 14000 epoch: 434 loss: 16.99808195393202 loss_input: 82.1591612431213
step: 15000 epoch: 434 loss: 17.017133826295915 loss_input: 82.16817249211381
Save loss: 17.02466580928862 Name: 434_train_model.pth
step: 0 epoch: 435 loss: 17.356822967529297 loss_input: 85.0135498046875
step: 1000 epoch: 435 loss: 16.794003202722266 loss_input: 82.39196984655969
step: 2000 epoch: 435 loss: 16.89059740730907 loss_input: 82.37902880834973
step: 3000 epoch: 435 loss: 16.81932882959149 loss_input: 82.02394502125594
step: 4000 epoch: 435 loss: 16.849201903763905 loss_input: 81.69094840242576
step: 5000 epoch: 435 loss: 16.878168204764656 loss_input: 81.85915476816746
step: 6000 epoch: 435 loss: 16.89701346027277 loss_input: 81.82470540391705
step: 7000 epoch: 435 loss: 16.91263567599343 loss_input: 81.84665670668699
step: 8000 epoch: 435 loss: 16.944372751998568 loss_input: 81.93951535019305
step: 9000 epoch: 435 loss: 16.961624977284306 loss_input: 81.9573232911187
step: 10000 epoch: 435 loss: 16.973744496716556 loss_input: 81.98225190298353
step: 11000 epoch: 435 loss: 16.98408875419014 loss_input: 82.05207870063127
step: 12000 epoch: 435 loss: 17.00801722711945 loss_input: 82.17755776923374
step: 13000 epoch: 435 loss: 16.986950298940464 loss_input: 82.02439519469806
step: 14000 epoch: 435 loss: 16.995577612073753 loss_input: 82.08719705358658
step: 15000 epoch: 435 loss: 17.008069779728103 loss_input: 82.1092449102026
Save loss: 17.021735481917858 Name: 435_train_model.pth
step: 0 epoch: 436 loss: 17.6982364654541 loss_input: 74.0025634765625
step: 1000 epoch: 436 loss: 16.97744172388738 loss_input: 83.06498469791927
step: 2000 epoch: 436 loss: 16.96847266760068 loss_input: 82.6291510616762
step: 3000 epoch: 436 loss: 17.032623694921007 loss_input: 82.63773352811432
step: 4000 epoch: 436 loss: 16.95822010407356 loss_input: 82.21413869555072
step: 5000 epoch: 436 loss: 16.992137854586982 loss_input: 82.14286861865908
step: 6000 epoch: 436 loss: 16.98447114617561 loss_input: 82.08055572687755
step: 7000 epoch: 436 loss: 16.982085513721245 loss_input: 82.16173237110101
step: 8000 epoch: 436 loss: 16.999029079953843 loss_input: 82.28882312715061
step: 9000 epoch: 436 loss: 17.01438197479316 loss_input: 82.4751160325719
step: 10000 epoch: 436 loss: 17.01669377882997 loss_input: 82.45824528264947
step: 11000 epoch: 436 loss: 17.01985618311994 loss_input: 82.25803511381778
step: 12000 epoch: 436 loss: 17.020603676714824 loss_input: 82.19940434752996
step: 13000 epoch: 436 loss: 17.017929848648294 loss_input: 82.23624037258553
step: 14000 epoch: 436 loss: 17.010516098538226 loss_input: 82.21613576325184
step: 15000 epoch: 436 loss: 17.004506692593274 loss_input: 82.16621868356563
Save loss: 17.019340792506934 Name: 436_train_model.pth
step: 0 epoch: 437 loss: 13.990846633911133 loss_input: 80.94873046875
step: 1000 epoch: 437 loss: 17.148995111276815 loss_input: 82.62563919235062
step: 2000 epoch: 437 loss: 17.081977642636964 loss_input: 82.6572141175506
step: 3000 epoch: 437 loss: 16.9448757190698 loss_input: 82.20340589188369
step: 4000 epoch: 437 loss: 16.974261236679432 loss_input: 81.86392908303685
step: 5000 epoch: 437 loss: 16.957910335295153 loss_input: 81.9266599805039
step: 6000 epoch: 437 loss: 16.957099122338406 loss_input: 81.90423113074904
step: 7000 epoch: 437 loss: 16.956986454959733 loss_input: 82.05086212746673
step: 8000 epoch: 437 loss: 16.951648044967605 loss_input: 82.08896899667327
step: 9000 epoch: 437 loss: 16.929755794566255 loss_input: 82.05784826127176
step: 10000 epoch: 437 loss: 16.935904769203255 loss_input: 82.08506338086895
step: 11000 epoch: 437 loss: 16.948156345238438 loss_input: 82.05772989254173
step: 12000 epoch: 437 loss: 16.970057042635794 loss_input: 82.17052225323182
step: 13000 epoch: 437 loss: 16.978692689149913 loss_input: 82.18503023336396
step: 14000 epoch: 437 loss: 16.990346209099187 loss_input: 82.15154154812743
step: 15000 epoch: 437 loss: 17.001531694024557 loss_input: 82.20525329642737
Save loss: 17.009127963244914 Name: 437_train_model.pth
step: 0 epoch: 438 loss: 17.286239624023438 loss_input: 122.9632568359375
step: 1000 epoch: 438 loss: 16.96899684206708 loss_input: 82.95152759837818
step: 2000 epoch: 438 loss: 16.95311215446926 loss_input: 82.46155627020474
step: 3000 epoch: 438 loss: 16.99466121772733 loss_input: 82.4095111606678
step: 4000 epoch: 438 loss: 17.013557891373754 loss_input: 82.55878445244646
step: 5000 epoch: 438 loss: 17.03087075613327 loss_input: 82.76158369400339
step: 6000 epoch: 438 loss: 17.00801264228592 loss_input: 82.41494245994808
step: 7000 epoch: 438 loss: 17.00099940058879 loss_input: 82.39126407757877
step: 8000 epoch: 438 loss: 17.034510289024016 loss_input: 82.47831327297467
step: 9000 epoch: 438 loss: 17.01532311575133 loss_input: 82.51779953594566
step: 10000 epoch: 438 loss: 17.000446566294794 loss_input: 82.46412430900465
step: 11000 epoch: 438 loss: 16.98950650288662 loss_input: 82.41408192960796
step: 12000 epoch: 438 loss: 17.006838753803326 loss_input: 82.35431015514332
step: 13000 epoch: 438 loss: 17.017972902686456 loss_input: 82.34409985477379
step: 14000 epoch: 438 loss: 17.030541142091845 loss_input: 82.31303138642998
step: 15000 epoch: 438 loss: 17.020762583898087 loss_input: 82.26220692546278
Save loss: 17.020789325460793 Name: 438_train_model.pth
step: 0 epoch: 439 loss: 29.088375091552734 loss_input: 118.69012451171875
step: 1000 epoch: 439 loss: 16.972617219378066 loss_input: 82.39611328612793
step: 2000 epoch: 439 loss: 16.913741552847615 loss_input: 82.27240609431612
step: 3000 epoch: 439 loss: 17.00156868890459 loss_input: 82.56561612844547
step: 4000 epoch: 439 loss: 17.013162373006715 loss_input: 82.37797716098319
step: 5000 epoch: 439 loss: 17.051511900683828 loss_input: 82.61115095096216
step: 6000 epoch: 439 loss: 17.054487537332545 loss_input: 82.73596923706074
step: 7000 epoch: 439 loss: 17.0275622010827 loss_input: 82.45476199061406
step: 8000 epoch: 439 loss: 17.01494578161384 loss_input: 82.38442064401373
step: 9000 epoch: 439 loss: 17.02919906240717 loss_input: 82.48785631620358
step: 10000 epoch: 439 loss: 17.043025898273058 loss_input: 82.41740412140426
step: 11000 epoch: 439 loss: 17.015275280060155 loss_input: 82.41086861532652
step: 12000 epoch: 439 loss: 17.00413417216192 loss_input: 82.37922386574394
step: 13000 epoch: 439 loss: 17.006226718301967 loss_input: 82.30255530693101
step: 14000 epoch: 439 loss: 17.003332517767078 loss_input: 82.24500889706957
step: 15000 epoch: 439 loss: 16.999882911191083 loss_input: 82.18593434509965
Save loss: 17.01154071035981 Name: 439_train_model.pth
step: 0 epoch: 440 loss: 18.36536407470703 loss_input: 69.3863525390625
step: 1000 epoch: 440 loss: 16.90860085339694 loss_input: 81.58411364026598
step: 2000 epoch: 440 loss: 16.929370203118275 loss_input: 82.11203776163676
step: 3000 epoch: 440 loss: 16.876039056609528 loss_input: 81.97012055551795
step: 4000 epoch: 440 loss: 16.86535704466618 loss_input: 82.14568086494002
step: 5000 epoch: 440 loss: 16.910869511621662 loss_input: 82.20304715912286
step: 6000 epoch: 440 loss: 16.934360147217795 loss_input: 82.25011740190112
step: 7000 epoch: 440 loss: 16.950182414739373 loss_input: 82.43459499333521
step: 8000 epoch: 440 loss: 16.93811854295858 loss_input: 82.2925710412297
step: 9000 epoch: 440 loss: 16.9504096521535 loss_input: 82.40791341652142
step: 10000 epoch: 440 loss: 16.95955351259384 loss_input: 82.27856052895687
step: 11000 epoch: 440 loss: 16.94707742360232 loss_input: 82.22166827496328
step: 12000 epoch: 440 loss: 16.967084150514587 loss_input: 82.18771458172756
step: 13000 epoch: 440 loss: 16.990465037997488 loss_input: 82.23771322071529
step: 14000 epoch: 440 loss: 17.00724537763738 loss_input: 82.26490986291856
step: 15000 epoch: 440 loss: 17.017934607820553 loss_input: 82.28425286377393
Save loss: 17.010763483002783 Name: 440_train_model.pth
step: 0 epoch: 441 loss: 14.706148147583008 loss_input: 119.52459716796875
step: 1000 epoch: 441 loss: 17.130541042133526 loss_input: 83.36765407444119
step: 2000 epoch: 441 loss: 17.061415533850276 loss_input: 82.8796219260975
step: 3000 epoch: 441 loss: 16.992206428893923 loss_input: 82.55519249931012
step: 4000 epoch: 441 loss: 16.979458478652546 loss_input: 82.17887575225335
step: 5000 epoch: 441 loss: 17.00139147928776 loss_input: 82.2239357792504
step: 6000 epoch: 441 loss: 16.99928148236598 loss_input: 82.16409802671234
step: 7000 epoch: 441 loss: 17.02660451005517 loss_input: 82.02055841213532
step: 8000 epoch: 441 loss: 17.05540749258197 loss_input: 82.07479200445404
step: 9000 epoch: 441 loss: 17.051574366289486 loss_input: 82.02447344989011
step: 10000 epoch: 441 loss: 17.034418772440077 loss_input: 82.08425420885646
step: 11000 epoch: 441 loss: 17.01640397953387 loss_input: 82.14328729459432
step: 12000 epoch: 441 loss: 17.005233521561216 loss_input: 82.17660057869527
step: 13000 epoch: 441 loss: 16.99543578869911 loss_input: 82.18536961437161
step: 14000 epoch: 441 loss: 16.990424789587827 loss_input: 82.1722854626246
step: 15000 epoch: 441 loss: 16.997166732113946 loss_input: 82.20491407343677
Save loss: 17.01462801724672 Name: 441_train_model.pth
step: 0 epoch: 442 loss: 15.12929916381836 loss_input: 45.89361572265625
step: 1000 epoch: 442 loss: 16.96025515745927 loss_input: 82.12537499121972
step: 2000 epoch: 442 loss: 17.013619703629324 loss_input: 81.36750512073066
step: 3000 epoch: 442 loss: 17.012939539880445 loss_input: 81.5159230777241
step: 4000 epoch: 442 loss: 17.012826037031505 loss_input: 82.08891377619463
step: 5000 epoch: 442 loss: 17.019939771582425 loss_input: 82.07341870248997
step: 6000 epoch: 442 loss: 17.000373371917593 loss_input: 81.7554195069051
step: 7000 epoch: 442 loss: 16.973235387220466 loss_input: 81.67157956229467
step: 8000 epoch: 442 loss: 16.95565252801118 loss_input: 81.73757024115882
step: 9000 epoch: 442 loss: 16.97643035894605 loss_input: 81.88966498297064
step: 10000 epoch: 442 loss: 16.986999270367917 loss_input: 82.00879231534854
step: 11000 epoch: 442 loss: 16.999594698384854 loss_input: 82.12403739108767
step: 12000 epoch: 442 loss: 16.987671503731832 loss_input: 82.17216151583722
step: 13000 epoch: 442 loss: 16.996036950189072 loss_input: 82.22217553183259
step: 14000 epoch: 442 loss: 16.996474112555706 loss_input: 82.2461937358827
step: 15000 epoch: 442 loss: 17.01075151819967 loss_input: 82.23899456914461
Save loss: 17.017737778037787 Name: 442_train_model.pth
step: 0 epoch: 443 loss: 8.218541145324707 loss_input: 66.16143798828125
step: 1000 epoch: 443 loss: 16.68604543182876 loss_input: 81.92474566449175
step: 2000 epoch: 443 loss: 16.791712442557255 loss_input: 81.8235617908819
step: 3000 epoch: 443 loss: 16.834936297047104 loss_input: 81.32135268061688
step: 4000 epoch: 443 loss: 16.90976725873635 loss_input: 81.79358128302397
step: 5000 epoch: 443 loss: 16.975032184629814 loss_input: 82.0133492100408
step: 6000 epoch: 443 loss: 16.96303209553677 loss_input: 82.05216411685292
step: 7000 epoch: 443 loss: 16.979781392267338 loss_input: 82.15186822327559
step: 8000 epoch: 443 loss: 16.995322906200684 loss_input: 82.27756279305419
step: 9000 epoch: 443 loss: 16.99037885393067 loss_input: 82.22829249847891
step: 10000 epoch: 443 loss: 16.985861712581528 loss_input: 82.18421330474375
step: 11000 epoch: 443 loss: 16.97892426480034 loss_input: 82.09653399318101
step: 12000 epoch: 443 loss: 16.98998774960482 loss_input: 82.17302291459436
step: 13000 epoch: 443 loss: 16.983163613904836 loss_input: 82.15291576504332
step: 14000 epoch: 443 loss: 16.981742844974285 loss_input: 82.14275605156969
step: 15000 epoch: 443 loss: 16.99399144215136 loss_input: 82.17143642246766
Save loss: 17.007431917741894 Name: 443_train_model.pth
step: 0 epoch: 444 loss: 12.676313400268555 loss_input: 53.88787841796875
step: 1000 epoch: 444 loss: 16.92875179258379 loss_input: 81.7928630207683
step: 2000 epoch: 444 loss: 16.87521709649936 loss_input: 81.66521231571714
step: 3000 epoch: 444 loss: 16.921978515293233 loss_input: 82.1551729460948
step: 4000 epoch: 444 loss: 16.930416726791925 loss_input: 81.94272142718715
step: 5000 epoch: 444 loss: 16.92848145449264 loss_input: 81.95167714503974
step: 6000 epoch: 444 loss: 16.96473990088998 loss_input: 81.96290966479545
step: 7000 epoch: 444 loss: 16.949386096038133 loss_input: 81.87549122795515
step: 8000 epoch: 444 loss: 16.987123192317664 loss_input: 82.12239599353059
step: 9000 epoch: 444 loss: 16.98081770275397 loss_input: 82.07335418813057
step: 10000 epoch: 444 loss: 16.974637296387893 loss_input: 82.09785682198381
step: 11000 epoch: 444 loss: 16.96687645880095 loss_input: 82.09788285293489
step: 12000 epoch: 444 loss: 16.988767272720196 loss_input: 82.11094115875272
step: 13000 epoch: 444 loss: 16.992522311993685 loss_input: 82.15843847723853
step: 14000 epoch: 444 loss: 16.989973180609578 loss_input: 82.17447245327901
step: 15000 epoch: 444 loss: 17.00339330524518 loss_input: 82.2572227688076
Save loss: 17.007875987514854 Name: 444_train_model.pth
step: 0 epoch: 445 loss: 19.039016723632812 loss_input: 107.3604736328125
step: 1000 epoch: 445 loss: 17.03985671539764 loss_input: 80.49818845705076
step: 2000 epoch: 445 loss: 17.098056085463586 loss_input: 81.78068094858821
step: 3000 epoch: 445 loss: 17.03731142143852 loss_input: 81.90998526614176
step: 4000 epoch: 445 loss: 16.97793729923213 loss_input: 82.13600974451211
step: 5000 epoch: 445 loss: 17.01141004442239 loss_input: 82.23079125629953
step: 6000 epoch: 445 loss: 17.010295304630702 loss_input: 82.10893426738447
step: 7000 epoch: 445 loss: 17.03263772186118 loss_input: 82.22544698030025
step: 8000 epoch: 445 loss: 17.037262982956218 loss_input: 82.11533347607673
step: 9000 epoch: 445 loss: 17.017782594373312 loss_input: 82.09995801654634
step: 10000 epoch: 445 loss: 17.02647101012555 loss_input: 82.23021342885242
step: 11000 epoch: 445 loss: 17.00549006594299 loss_input: 82.21627659759092
step: 12000 epoch: 445 loss: 17.010967167066877 loss_input: 82.18871692434648
step: 13000 epoch: 445 loss: 17.021506407748515 loss_input: 82.200120019689
step: 14000 epoch: 445 loss: 17.015810092358358 loss_input: 82.2752432792189
step: 15000 epoch: 445 loss: 17.013210137425546 loss_input: 82.25738137639782
Save loss: 17.009378843829037 Name: 445_train_model.pth
step: 0 epoch: 446 loss: 17.137104034423828 loss_input: 82.02410888671875
step: 1000 epoch: 446 loss: 16.87485378295868 loss_input: 82.3992437006353
step: 2000 epoch: 446 loss: 16.842406534302658 loss_input: 82.0669415963405
step: 3000 epoch: 446 loss: 16.861712554342468 loss_input: 82.09260404002703
step: 4000 epoch: 446 loss: 16.863206765735725 loss_input: 82.23866552282827
step: 5000 epoch: 446 loss: 16.890926945140375 loss_input: 82.2003933807965
step: 6000 epoch: 446 loss: 16.91860015959248 loss_input: 82.13545419025215
step: 7000 epoch: 446 loss: 16.90166495555027 loss_input: 82.02928012760175
step: 8000 epoch: 446 loss: 16.930786653066214 loss_input: 82.19084305745962
step: 9000 epoch: 446 loss: 16.964553317312742 loss_input: 82.3567899329545
step: 10000 epoch: 446 loss: 16.973284335198873 loss_input: 82.28684617500164
step: 11000 epoch: 446 loss: 17.011678772594223 loss_input: 82.33403605043362
step: 12000 epoch: 446 loss: 17.008421865724223 loss_input: 82.24087447363598
step: 13000 epoch: 446 loss: 17.00129448959492 loss_input: 82.25667010834654
step: 14000 epoch: 446 loss: 16.999378578448823 loss_input: 82.27811650015373
step: 15000 epoch: 446 loss: 17.000512308251437 loss_input: 82.32786814863384
Save loss: 17.006010441735388 Name: 446_train_model.pth
step: 0 epoch: 447 loss: 8.541778564453125 loss_input: 37.36383056640625
step: 1000 epoch: 447 loss: 16.755968086726657 loss_input: 81.80072094653393
step: 2000 epoch: 447 loss: 16.81403714260538 loss_input: 82.2990816603417
step: 3000 epoch: 447 loss: 16.927723998985304 loss_input: 82.10618780962986
step: 4000 epoch: 447 loss: 16.95756464897171 loss_input: 81.84631518511974
step: 5000 epoch: 447 loss: 16.962807405950834 loss_input: 82.04735726438696
step: 6000 epoch: 447 loss: 16.971306328732975 loss_input: 82.26760263670248
step: 7000 epoch: 447 loss: 16.998291295181936 loss_input: 82.30977328643341
step: 8000 epoch: 447 loss: 16.99699942476525 loss_input: 82.33293428958588
step: 9000 epoch: 447 loss: 17.02058780573432 loss_input: 82.31337188527446
step: 10000 epoch: 447 loss: 17.01664035287622 loss_input: 82.36177753074303
step: 11000 epoch: 447 loss: 17.01254364176562 loss_input: 82.33842687552198
step: 12000 epoch: 447 loss: 17.023983327898023 loss_input: 82.2552415370822
step: 13000 epoch: 447 loss: 17.028311097762206 loss_input: 82.33895440941893
step: 14000 epoch: 447 loss: 17.019394559269674 loss_input: 82.33289702416624
step: 15000 epoch: 447 loss: 17.00946868785929 loss_input: 82.25223047060534
Save loss: 17.001315286532044 Name: 447_train_model.pth
step: 0 epoch: 448 loss: 17.290050506591797 loss_input: 121.68017578125
step: 1000 epoch: 448 loss: 16.864830276706478 loss_input: 82.52378243094796
step: 2000 epoch: 448 loss: 16.93548956970642 loss_input: 82.50705076538878
step: 3000 epoch: 448 loss: 16.98072704892284 loss_input: 82.49223106458004
step: 4000 epoch: 448 loss: 16.999890190158837 loss_input: 82.60669540256538
step: 5000 epoch: 448 loss: 17.004524545606625 loss_input: 82.53852781689756
step: 6000 epoch: 448 loss: 16.982331504624717 loss_input: 82.25852318680936
step: 7000 epoch: 448 loss: 16.997382063130758 loss_input: 82.21547893060477
step: 8000 epoch: 448 loss: 16.9890061820571 loss_input: 82.23984600344147
step: 9000 epoch: 448 loss: 17.025090972286716 loss_input: 82.44294275462448
step: 10000 epoch: 448 loss: 17.00894092452632 loss_input: 82.31121253719354
step: 11000 epoch: 448 loss: 17.019174567310067 loss_input: 82.2875039813497
step: 12000 epoch: 448 loss: 17.02266470570036 loss_input: 82.31605759029358
step: 13000 epoch: 448 loss: 17.008439980766497 loss_input: 82.26884078891467
step: 14000 epoch: 448 loss: 16.996210332581267 loss_input: 82.21711195520771
step: 15000 epoch: 448 loss: 17.002273457088943 loss_input: 82.2276858870216
Save loss: 17.00621512016654 Name: 448_train_model.pth
step: 0 epoch: 449 loss: 13.145772933959961 loss_input: 66.7283935546875
step: 1000 epoch: 449 loss: 16.86088683293178 loss_input: 82.16976126900444
step: 2000 epoch: 449 loss: 16.89748493353764 loss_input: 82.71812201809252
step: 3000 epoch: 449 loss: 16.983563042846928 loss_input: 82.76773572627484
step: 4000 epoch: 449 loss: 16.955768319971114 loss_input: 82.63400832226026
step: 5000 epoch: 449 loss: 16.95582851818194 loss_input: 82.58105574929935
step: 6000 epoch: 449 loss: 16.916259464671704 loss_input: 82.46309085814799
step: 7000 epoch: 449 loss: 16.956363836504224 loss_input: 82.54724098426787
step: 8000 epoch: 449 loss: 16.938966722134396 loss_input: 82.45387786609682
step: 9000 epoch: 449 loss: 16.933547688992657 loss_input: 82.38378068805072
step: 10000 epoch: 449 loss: 16.95526387290756 loss_input: 82.30024680637405
step: 11000 epoch: 449 loss: 16.990886082032432 loss_input: 82.35654803642673
step: 12000 epoch: 449 loss: 16.994138706744625 loss_input: 82.30440511895004
step: 13000 epoch: 449 loss: 17.007014474394907 loss_input: 82.26514693431034
step: 14000 epoch: 449 loss: 17.014130712721947 loss_input: 82.22416847980446
step: 15000 epoch: 449 loss: 17.016701413062357 loss_input: 82.2297820241346
Save loss: 17.005479497924448 Name: 449_train_model.pth
step: 0 epoch: 450 loss: 18.946937561035156 loss_input: 117.36578369140625
step: 1000 epoch: 450 loss: 16.922034238363718 loss_input: 82.17058838378418
step: 2000 epoch: 450 loss: 17.023782565199333 loss_input: 82.51104202537404
step: 3000 epoch: 450 loss: 17.03839289613741 loss_input: 82.89616372179326
step: 4000 epoch: 450 loss: 17.038286570518263 loss_input: 82.9704712798851
step: 5000 epoch: 450 loss: 17.044015446893454 loss_input: 82.80730874088854
step: 6000 epoch: 450 loss: 17.020877500550586 loss_input: 82.84992095840472
step: 7000 epoch: 450 loss: 17.006913673569517 loss_input: 82.7746767115719
step: 8000 epoch: 450 loss: 17.030793431758465 loss_input: 82.72177299096158
step: 9000 epoch: 450 loss: 17.046366280363 loss_input: 82.72629324958902
step: 10000 epoch: 450 loss: 17.049069329316993 loss_input: 82.66182459512838
step: 11000 epoch: 450 loss: 17.044475678519415 loss_input: 82.57867195844412
step: 12000 epoch: 450 loss: 17.03315507844134 loss_input: 82.48137422263964
step: 13000 epoch: 450 loss: 17.018661130346633 loss_input: 82.29137565308227
step: 14000 epoch: 450 loss: 17.034684171132398 loss_input: 82.28643052508666
step: 15000 epoch: 450 loss: 17.01931367976754 loss_input: 82.22693000479656
Save loss: 17.00346804267168 Name: 450_train_model.pth
step: 0 epoch: 451 loss: 13.992289543151855 loss_input: 64.46221923828125
step: 1000 epoch: 451 loss: 16.991474995722662 loss_input: 83.29857032615821
step: 2000 epoch: 451 loss: 16.928251764287 loss_input: 82.58312430088667
step: 3000 epoch: 451 loss: 16.923349853834683 loss_input: 82.30857627942379
step: 4000 epoch: 451 loss: 16.95019917784855 loss_input: 82.38100740928049
step: 5000 epoch: 451 loss: 16.951510804196737 loss_input: 82.35522910066425
step: 6000 epoch: 451 loss: 16.978850127577562 loss_input: 82.41344986000054
step: 7000 epoch: 451 loss: 16.96741041744152 loss_input: 82.43646182695161
step: 8000 epoch: 451 loss: 17.004104092365175 loss_input: 82.43118090463301
step: 9000 epoch: 451 loss: 16.977435363105847 loss_input: 82.38044165611373
step: 10000 epoch: 451 loss: 16.994418271338148 loss_input: 82.49910708392052
step: 11000 epoch: 451 loss: 17.004231600617942 loss_input: 82.35275203918785
step: 12000 epoch: 451 loss: 17.004424121377667 loss_input: 82.31553196372633
step: 13000 epoch: 451 loss: 17.021756735024805 loss_input: 82.39800329173897
step: 14000 epoch: 451 loss: 17.005534624986993 loss_input: 82.32187800097147
step: 15000 epoch: 451 loss: 17.000580480754966 loss_input: 82.27962446139658
Save loss: 16.99891006746888 Name: 451_train_model.pth
step: 0 epoch: 452 loss: 19.192882537841797 loss_input: 56.071533203125
step: 1000 epoch: 452 loss: 16.839479884186705 loss_input: 81.74167391446444
step: 2000 epoch: 452 loss: 16.998120917254003 loss_input: 81.62597845364427
step: 3000 epoch: 452 loss: 16.99824946930074 loss_input: 81.9624585790739
step: 4000 epoch: 452 loss: 16.9362437406262 loss_input: 81.72684021682568
step: 5000 epoch: 452 loss: 16.99595827866592 loss_input: 82.04432308118454
step: 6000 epoch: 452 loss: 16.99471294361439 loss_input: 82.18663832720588
step: 7000 epoch: 452 loss: 16.999143647527784 loss_input: 82.23560895458014
step: 8000 epoch: 452 loss: 16.998486753613214 loss_input: 82.23980034722223
step: 9000 epoch: 452 loss: 16.96613006752844 loss_input: 82.18121196847268
step: 10000 epoch: 452 loss: 16.996740595076922 loss_input: 82.23763627665554
step: 11000 epoch: 452 loss: 16.979909483773763 loss_input: 82.32385911361574
step: 12000 epoch: 452 loss: 16.978635193874275 loss_input: 82.30550163605344
step: 13000 epoch: 452 loss: 16.987969184085028 loss_input: 82.40782399907789
step: 14000 epoch: 452 loss: 16.997989136153873 loss_input: 82.39497653505357
step: 15000 epoch: 452 loss: 17.003150980151993 loss_input: 82.29472125360914
Save loss: 17.008714977681638 Name: 452_train_model.pth
step: 0 epoch: 453 loss: 18.492809295654297 loss_input: 64.494873046875
step: 1000 epoch: 453 loss: 17.19381467969744 loss_input: 81.36800540767825
step: 2000 epoch: 453 loss: 17.18959795958039 loss_input: 82.22450816827914
step: 3000 epoch: 453 loss: 17.088608781006766 loss_input: 82.40835936361057
step: 4000 epoch: 453 loss: 17.041988433345917 loss_input: 82.3047663072025
step: 5000 epoch: 453 loss: 17.004510912602484 loss_input: 81.9927886370968
step: 6000 epoch: 453 loss: 16.98872584387613 loss_input: 81.87965073448045
step: 7000 epoch: 453 loss: 16.973404260520134 loss_input: 82.06043161349439
step: 8000 epoch: 453 loss: 16.984302851575983 loss_input: 82.0362271076589
step: 9000 epoch: 453 loss: 16.994823781136287 loss_input: 81.97835936523546
step: 10000 epoch: 453 loss: 17.0000452391208 loss_input: 82.10669421339117
step: 11000 epoch: 453 loss: 17.00329857970918 loss_input: 82.10586649660046
step: 12000 epoch: 453 loss: 16.99928557520141 loss_input: 82.13039594316034
step: 13000 epoch: 453 loss: 16.998608903090464 loss_input: 82.16413624827086
step: 14000 epoch: 453 loss: 16.994226291634153 loss_input: 82.1386367953698
step: 15000 epoch: 453 loss: 17.007894183975736 loss_input: 82.21460294574112
Save loss: 17.00140011036396 Name: 453_train_model.pth
step: 0 epoch: 454 loss: 11.186556816101074 loss_input: 64.1912841796875
step: 1000 epoch: 454 loss: 16.933190707798367 loss_input: 82.28526173533498
step: 2000 epoch: 454 loss: 16.979532575202192 loss_input: 82.50041675329328
step: 3000 epoch: 454 loss: 17.013098856720358 loss_input: 82.87263557411201
step: 4000 epoch: 454 loss: 16.98819317492328 loss_input: 82.89386318886586
step: 5000 epoch: 454 loss: 16.969049693822527 loss_input: 82.5743810954606
step: 6000 epoch: 454 loss: 17.00139814693716 loss_input: 82.52761671475977
step: 7000 epoch: 454 loss: 17.020138693271306 loss_input: 82.52747468658217
step: 8000 epoch: 454 loss: 17.06172437528389 loss_input: 82.53963549508704
step: 9000 epoch: 454 loss: 17.050134228489263 loss_input: 82.50383898316854
step: 10000 epoch: 454 loss: 17.039082526969928 loss_input: 82.38333137511444
step: 11000 epoch: 454 loss: 17.025070083584527 loss_input: 82.38460874007015
step: 12000 epoch: 454 loss: 17.021367357667412 loss_input: 82.36328244008638
step: 13000 epoch: 454 loss: 17.015938916450995 loss_input: 82.30872274022792
step: 14000 epoch: 454 loss: 17.021411375934402 loss_input: 82.2892161537499
step: 15000 epoch: 454 loss: 17.009391936101355 loss_input: 82.26407719872712
Save loss: 16.997746340364216 Name: 454_train_model.pth
step: 0 epoch: 455 loss: 21.15192222595215 loss_input: 78.15936279296875
step: 1000 epoch: 455 loss: 17.218890067223427 loss_input: 82.94150344284621
step: 2000 epoch: 455 loss: 17.0961463869601 loss_input: 81.83523317076813
step: 3000 epoch: 455 loss: 17.013820907824122 loss_input: 81.77723382163627
step: 4000 epoch: 455 loss: 17.008253148066284 loss_input: 82.0681654232438
step: 5000 epoch: 455 loss: 16.975232366418105 loss_input: 82.21072649483776
step: 6000 epoch: 455 loss: 16.94440503319866 loss_input: 82.05376276805428
step: 7000 epoch: 455 loss: 16.913930490891808 loss_input: 82.0702657886206
step: 8000 epoch: 455 loss: 16.92676142933458 loss_input: 82.11216302735107
step: 9000 epoch: 455 loss: 16.90687157357882 loss_input: 81.85558204578831
step: 10000 epoch: 455 loss: 16.933857091604835 loss_input: 81.8967295409131
step: 11000 epoch: 455 loss: 16.919517110861516 loss_input: 81.90013179843203
step: 12000 epoch: 455 loss: 16.95766448394506 loss_input: 81.9704698172125
step: 13000 epoch: 455 loss: 16.979093088075643 loss_input: 82.06733461251929
step: 14000 epoch: 455 loss: 16.988476356398454 loss_input: 82.1583685576937
step: 15000 epoch: 455 loss: 17.00378269516987 loss_input: 82.2285763021072
Save loss: 17.00013365523517 Name: 455_train_model.pth
step: 0 epoch: 456 loss: 18.338172912597656 loss_input: 87.51837158203125
step: 1000 epoch: 456 loss: 16.827335320033512 loss_input: 82.63431728183926
step: 2000 epoch: 456 loss: 16.866076516366373 loss_input: 82.094756881813
step: 3000 epoch: 456 loss: 16.938872430452463 loss_input: 82.40664555906494
step: 4000 epoch: 456 loss: 16.94376485432961 loss_input: 82.03546595650892
step: 5000 epoch: 456 loss: 16.990414604333086 loss_input: 82.20705238835045
step: 6000 epoch: 456 loss: 17.00095796795651 loss_input: 82.3155001510165
step: 7000 epoch: 456 loss: 17.004306594570473 loss_input: 82.29273805262753
step: 8000 epoch: 456 loss: 16.983644574064865 loss_input: 82.33182209340532
step: 9000 epoch: 456 loss: 16.998047192962073 loss_input: 82.29618649143151
step: 10000 epoch: 456 loss: 16.98066842552424 loss_input: 82.24259888464279
step: 11000 epoch: 456 loss: 16.974392672471918 loss_input: 82.19790380051565
step: 12000 epoch: 456 loss: 16.985932307644255 loss_input: 82.17481730546706
step: 13000 epoch: 456 loss: 16.965215077630173 loss_input: 82.09453654650514
step: 14000 epoch: 456 loss: 16.972305887861342 loss_input: 82.08191507351874
step: 15000 epoch: 456 loss: 16.99602693052707 loss_input: 82.15377540097587
Save loss: 16.997829851299525 Name: 456_train_model.pth
step: 0 epoch: 457 loss: 18.462194442749023 loss_input: 164.7738037109375
step: 1000 epoch: 457 loss: 16.993801591398714 loss_input: 82.94564889456247
step: 2000 epoch: 457 loss: 17.03754839415791 loss_input: 82.93113312728401
step: 3000 epoch: 457 loss: 16.986583393281876 loss_input: 82.87596078000041
step: 4000 epoch: 457 loss: 16.95812823676491 loss_input: 82.63008381205479
step: 5000 epoch: 457 loss: 16.971140403553047 loss_input: 82.56298968477789
step: 6000 epoch: 457 loss: 16.97749812212453 loss_input: 82.60170753604868
step: 7000 epoch: 457 loss: 16.970699429154447 loss_input: 82.49207841960077
step: 8000 epoch: 457 loss: 16.950031092309636 loss_input: 82.2829140452292
step: 9000 epoch: 457 loss: 16.948878813102475 loss_input: 82.16240532193471
step: 10000 epoch: 457 loss: 16.967092616547824 loss_input: 82.1466461447117
step: 11000 epoch: 457 loss: 16.973558322265685 loss_input: 82.1755473068677
step: 12000 epoch: 457 loss: 16.968068687550854 loss_input: 82.16616117154625
step: 13000 epoch: 457 loss: 16.975786898431572 loss_input: 82.13791642586237
step: 14000 epoch: 457 loss: 16.99691495385547 loss_input: 82.24128835070654
step: 15000 epoch: 457 loss: 16.993184428017628 loss_input: 82.19692728492579
Save loss: 16.99075333826244 Name: 457_train_model.pth
step: 0 epoch: 458 loss: 10.656349182128906 loss_input: 44.22406005859375
step: 1000 epoch: 458 loss: 16.804912915358415 loss_input: 82.95607639430881
step: 2000 epoch: 458 loss: 16.829075239706732 loss_input: 82.33892114611639
step: 3000 epoch: 458 loss: 16.806852453194313 loss_input: 82.41154037114582
step: 4000 epoch: 458 loss: 16.824391087601406 loss_input: 82.14175412750816
step: 5000 epoch: 458 loss: 16.84499591513887 loss_input: 82.14091157598558
step: 6000 epoch: 458 loss: 16.877346641916848 loss_input: 81.78612612009327
step: 7000 epoch: 458 loss: 16.883172871402085 loss_input: 81.71610045000547
step: 8000 epoch: 458 loss: 16.912056901755594 loss_input: 81.95778515356479
step: 9000 epoch: 458 loss: 16.909064244699536 loss_input: 81.8328737941878
step: 10000 epoch: 458 loss: 16.907592693384547 loss_input: 81.84873443807963
step: 11000 epoch: 458 loss: 16.90840052700121 loss_input: 81.94284885123191
step: 12000 epoch: 458 loss: 16.939377068261408 loss_input: 82.0372234402016
step: 13000 epoch: 458 loss: 16.94660328366318 loss_input: 82.13355670544874
step: 14000 epoch: 458 loss: 16.961523195801696 loss_input: 82.25207288050156
step: 15000 epoch: 458 loss: 16.976544741066082 loss_input: 82.24601765644009
Save loss: 16.983538146167994 Name: 458_train_model.pth
step: 0 epoch: 459 loss: 16.556201934814453 loss_input: 70.0687255859375
step: 1000 epoch: 459 loss: 17.11688374520301 loss_input: 82.66991358251124
step: 2000 epoch: 459 loss: 16.921446057452613 loss_input: 82.2802500117129
step: 3000 epoch: 459 loss: 16.919673994675115 loss_input: 82.24992519583316
step: 4000 epoch: 459 loss: 16.8989652571932 loss_input: 81.98623659514809
step: 5000 epoch: 459 loss: 16.918511777418992 loss_input: 81.89948813215872
step: 6000 epoch: 459 loss: 16.968262680847037 loss_input: 81.88238828481386
step: 7000 epoch: 459 loss: 17.021224246742282 loss_input: 82.25424984628312
step: 8000 epoch: 459 loss: 17.01716121615894 loss_input: 82.2663528982095
step: 9000 epoch: 459 loss: 16.989897163877647 loss_input: 82.2616229896982
step: 10000 epoch: 459 loss: 16.95564378160153 loss_input: 82.18351128144022
step: 11000 epoch: 459 loss: 16.943548911442033 loss_input: 82.22265109577219
step: 12000 epoch: 459 loss: 16.969736326973536 loss_input: 82.24634178637783
step: 13000 epoch: 459 loss: 16.97617793372939 loss_input: 82.17550599322888
step: 14000 epoch: 459 loss: 16.99333307351583 loss_input: 82.23532518847705
step: 15000 epoch: 459 loss: 16.99087918078881 loss_input: 82.2526094736588
Save loss: 16.987820744290946 Name: 459_train_model.pth
step: 0 epoch: 460 loss: 24.374454498291016 loss_input: 103.749267578125
step: 1000 epoch: 460 loss: 16.877376904139865 loss_input: 83.37896911438172
step: 2000 epoch: 460 loss: 16.87058210277605 loss_input: 83.06329595822206
step: 3000 epoch: 460 loss: 16.953652226182072 loss_input: 82.81674417040023
step: 4000 epoch: 460 loss: 16.986487745315067 loss_input: 82.7304608936877
step: 5000 epoch: 460 loss: 17.00854311683516 loss_input: 82.49983078335505
step: 6000 epoch: 460 loss: 16.97671837886956 loss_input: 82.23102728117905
step: 7000 epoch: 460 loss: 16.965797666515083 loss_input: 82.2204549304759
step: 8000 epoch: 460 loss: 16.972617016570478 loss_input: 82.26619295268756
step: 9000 epoch: 460 loss: 16.97472708427354 loss_input: 82.33635811434362
step: 10000 epoch: 460 loss: 16.98586397330268 loss_input: 82.38605357603888
step: 11000 epoch: 460 loss: 16.97415054383012 loss_input: 82.2841027680012
step: 12000 epoch: 460 loss: 16.96950285935956 loss_input: 82.27677388171755
step: 13000 epoch: 460 loss: 16.976219974603207 loss_input: 82.31376401597413
step: 14000 epoch: 460 loss: 16.968058771443754 loss_input: 82.284044264521
step: 15000 epoch: 460 loss: 16.979565928931333 loss_input: 82.29643689851015
Save loss: 16.988884368017317 Name: 460_train_model.pth
step: 0 epoch: 461 loss: 12.824455261230469 loss_input: 150.2568359375
step: 1000 epoch: 461 loss: 17.110870175071053 loss_input: 82.96780611966159
step: 2000 epoch: 461 loss: 16.970444452160898 loss_input: 82.47460540969749
step: 3000 epoch: 461 loss: 16.89893678250769 loss_input: 81.92135779772627
step: 4000 epoch: 461 loss: 16.944920291962607 loss_input: 81.93219155843363
step: 5000 epoch: 461 loss: 16.910144006126142 loss_input: 81.92342057711505
step: 6000 epoch: 461 loss: 16.939749792007145 loss_input: 82.11898163457133
step: 7000 epoch: 461 loss: 16.95474707319164 loss_input: 82.2402806679124
step: 8000 epoch: 461 loss: 16.996766059164138 loss_input: 82.43104389020465
step: 9000 epoch: 461 loss: 17.001905471215633 loss_input: 82.39426610205203
step: 10000 epoch: 461 loss: 16.99734054595372 loss_input: 82.5171765599319
step: 11000 epoch: 461 loss: 17.00451231830695 loss_input: 82.45702964658575
step: 12000 epoch: 461 loss: 16.990419099792003 loss_input: 82.45026735341229
step: 13000 epoch: 461 loss: 16.99767338731694 loss_input: 82.3639248115229
step: 14000 epoch: 461 loss: 16.999451050902426 loss_input: 82.41553880882114
step: 15000 epoch: 461 loss: 16.99465773755825 loss_input: 82.3084657271221
Save loss: 16.993751545608042 Name: 461_train_model.pth
step: 0 epoch: 462 loss: 21.489164352416992 loss_input: 63.1707763671875
step: 1000 epoch: 462 loss: 16.904811503765703 loss_input: 81.22674627618476
step: 2000 epoch: 462 loss: 16.871602111313116 loss_input: 81.28393694998204
step: 3000 epoch: 462 loss: 16.87362501423425 loss_input: 81.94325296515744
step: 4000 epoch: 462 loss: 16.920558734465708 loss_input: 82.1613327594615
step: 5000 epoch: 462 loss: 16.937748235312732 loss_input: 82.38337831701237
step: 6000 epoch: 462 loss: 16.962618569059583 loss_input: 82.3540698640884
step: 7000 epoch: 462 loss: 17.003316224906943 loss_input: 82.41131035584482
step: 8000 epoch: 462 loss: 16.989052305667343 loss_input: 82.47618322917155
step: 9000 epoch: 462 loss: 16.989718131125866 loss_input: 82.43258826683103
step: 10000 epoch: 462 loss: 16.98306755873695 loss_input: 82.42345777629268
step: 11000 epoch: 462 loss: 16.995789967518636 loss_input: 82.43185913318959
step: 12000 epoch: 462 loss: 16.994212579790744 loss_input: 82.4686074439272
step: 13000 epoch: 462 loss: 16.984370303240183 loss_input: 82.411997867652
step: 14000 epoch: 462 loss: 16.980915523786116 loss_input: 82.31421192693128
step: 15000 epoch: 462 loss: 16.995693632685562 loss_input: 82.3153672242163
Save loss: 16.992647894605994 Name: 462_train_model.pth
step: 0 epoch: 463 loss: 17.498149871826172 loss_input: 98.86871337890625
step: 1000 epoch: 463 loss: 16.840278121498557 loss_input: 82.4277241923116
step: 2000 epoch: 463 loss: 16.867617377872648 loss_input: 82.85066518254544
step: 3000 epoch: 463 loss: 16.848503616959682 loss_input: 82.28904893437213
step: 4000 epoch: 463 loss: 16.900368873669606 loss_input: 82.47623981150709
step: 5000 epoch: 463 loss: 16.90072780598452 loss_input: 82.49758629817005
step: 6000 epoch: 463 loss: 16.862766151129456 loss_input: 82.17360178332908
step: 7000 epoch: 463 loss: 16.834464393433734 loss_input: 82.08868352407525
step: 8000 epoch: 463 loss: 16.871200615935678 loss_input: 82.10675554593762
step: 9000 epoch: 463 loss: 16.874703148234858 loss_input: 82.19366691054509
step: 10000 epoch: 463 loss: 16.908113487481284 loss_input: 82.18174480159406
step: 11000 epoch: 463 loss: 16.921380777292256 loss_input: 82.35002541938658
step: 12000 epoch: 463 loss: 16.942319540051297 loss_input: 82.38228099420263
step: 13000 epoch: 463 loss: 16.96647774946707 loss_input: 82.31783090753837
step: 14000 epoch: 463 loss: 16.97071146397972 loss_input: 82.28729891921782
step: 15000 epoch: 463 loss: 16.977087089057193 loss_input: 82.18774612616971
Save loss: 16.983401342242956 Name: 463_train_model.pth
step: 0 epoch: 464 loss: 7.511709690093994 loss_input: 66.73614501953125
step: 1000 epoch: 464 loss: 16.645599501949924 loss_input: 81.9386374123923
step: 2000 epoch: 464 loss: 16.708171692447387 loss_input: 82.12354766220406
step: 3000 epoch: 464 loss: 16.78017794660869 loss_input: 82.42560983903803
step: 4000 epoch: 464 loss: 16.788796917196215 loss_input: 82.01929182316238
step: 5000 epoch: 464 loss: 16.832460569348534 loss_input: 82.19064530599739
step: 6000 epoch: 464 loss: 16.84915700075607 loss_input: 82.21185535646403
step: 7000 epoch: 464 loss: 16.888447848171392 loss_input: 82.14844116158629
step: 8000 epoch: 464 loss: 16.90776621933327 loss_input: 82.21574257105802
step: 9000 epoch: 464 loss: 16.916414679268865 loss_input: 82.11633911743097
step: 10000 epoch: 464 loss: 16.951999448559402 loss_input: 82.31181090875288
step: 11000 epoch: 464 loss: 16.953788175982524 loss_input: 82.22669356940563
step: 12000 epoch: 464 loss: 16.99435067834402 loss_input: 82.33290138622888
step: 13000 epoch: 464 loss: 16.98613884569856 loss_input: 82.2428338548402
step: 14000 epoch: 464 loss: 16.984913037697016 loss_input: 82.21787961364925
step: 15000 epoch: 464 loss: 16.995488537509363 loss_input: 82.22564075969154
Save loss: 16.99905385431647 Name: 464_train_model.pth
step: 0 epoch: 465 loss: 15.708969116210938 loss_input: 69.65582275390625
step: 1000 epoch: 465 loss: 17.07148294039182 loss_input: 82.41850934709821
step: 2000 epoch: 465 loss: 17.01770319311932 loss_input: 82.25454338260558
step: 3000 epoch: 465 loss: 16.982213442343866 loss_input: 81.9493721412524
step: 4000 epoch: 465 loss: 16.933823738179186 loss_input: 81.79748886020683
step: 5000 epoch: 465 loss: 16.98021119042793 loss_input: 82.25265403462276
step: 6000 epoch: 465 loss: 16.982261795974576 loss_input: 82.19782672939927
step: 7000 epoch: 465 loss: 16.961478868257554 loss_input: 82.22763590273934
step: 8000 epoch: 465 loss: 16.972892600020057 loss_input: 82.37899638003013
step: 9000 epoch: 465 loss: 16.949010272884802 loss_input: 82.26653761832452
step: 10000 epoch: 465 loss: 16.944317525892828 loss_input: 82.28812738233013
step: 11000 epoch: 465 loss: 16.958319464918375 loss_input: 82.23976828633477
step: 12000 epoch: 465 loss: 16.97394085911828 loss_input: 82.2341906281573
step: 13000 epoch: 465 loss: 16.9954265907777 loss_input: 82.28999246339524
step: 14000 epoch: 465 loss: 16.990247579925647 loss_input: 82.22589371144946
step: 15000 epoch: 465 loss: 16.98705963223515 loss_input: 82.23777242161721
Save loss: 16.97878026601672 Name: 465_train_model.pth
step: 0 epoch: 466 loss: 8.560934066772461 loss_input: 78.2091064453125
step: 1000 epoch: 466 loss: 17.082077914303714 loss_input: 83.51181100393747
step: 2000 epoch: 466 loss: 16.968295801525887 loss_input: 82.38207791245979
step: 3000 epoch: 466 loss: 16.99216507387654 loss_input: 82.34450945143936
step: 4000 epoch: 466 loss: 17.033063989375655 loss_input: 82.29594450752546
step: 5000 epoch: 466 loss: 17.004889996999076 loss_input: 82.18022446536084
step: 6000 epoch: 466 loss: 17.00858204652023 loss_input: 82.2053112709568
step: 7000 epoch: 466 loss: 17.002739670787125 loss_input: 81.94728371497989
step: 8000 epoch: 466 loss: 16.99808364080766 loss_input: 81.94713260024626
step: 9000 epoch: 466 loss: 16.97371957530155 loss_input: 81.96252882844831
step: 10000 epoch: 466 loss: 16.98877214558207 loss_input: 82.08152509334028
step: 11000 epoch: 466 loss: 16.972501739546686 loss_input: 82.06225606465641
step: 12000 epoch: 466 loss: 16.98415424108366 loss_input: 82.11884407988788
step: 13000 epoch: 466 loss: 16.99516110870987 loss_input: 82.25786879355884
step: 14000 epoch: 466 loss: 16.992511362988953 loss_input: 82.20831981937116
step: 15000 epoch: 466 loss: 16.990751026169587 loss_input: 82.22820568472518
Save loss: 16.98590934020281 Name: 466_train_model.pth
step: 0 epoch: 467 loss: 14.685151100158691 loss_input: 93.43328857421875
step: 1000 epoch: 467 loss: 16.616036469405227 loss_input: 80.85874470987996
step: 2000 epoch: 467 loss: 16.7273302109226 loss_input: 81.48133272841119
step: 3000 epoch: 467 loss: 16.761154058177404 loss_input: 81.53076391528344
step: 4000 epoch: 467 loss: 16.825691125178988 loss_input: 81.72181625003905
step: 5000 epoch: 467 loss: 16.849382018070415 loss_input: 81.74729762299494
step: 6000 epoch: 467 loss: 16.86858358352984 loss_input: 81.98157592631046
step: 7000 epoch: 467 loss: 16.878600289252567 loss_input: 82.06342639004974
step: 8000 epoch: 467 loss: 16.892553351578332 loss_input: 82.10387867013449
step: 9000 epoch: 467 loss: 16.903337932562724 loss_input: 82.07919560722637
step: 10000 epoch: 467 loss: 16.945686867541998 loss_input: 82.19612255083085
step: 11000 epoch: 467 loss: 16.937934228544094 loss_input: 82.2480339034342
step: 12000 epoch: 467 loss: 16.935374241055634 loss_input: 82.13646104544185
step: 13000 epoch: 467 loss: 16.92440247579718 loss_input: 82.11579270044132
step: 14000 epoch: 467 loss: 16.947107313650097 loss_input: 82.07592973095393
step: 15000 epoch: 467 loss: 16.98176237877413 loss_input: 82.18737470317734
Save loss: 16.985454103589056 Name: 467_train_model.pth
step: 0 epoch: 468 loss: 18.77720832824707 loss_input: 153.78656005859375
step: 1000 epoch: 468 loss: 16.869259728061092 loss_input: 82.70652107901864
step: 2000 epoch: 468 loss: 16.886435634550125 loss_input: 82.10814975202828
step: 3000 epoch: 468 loss: 16.864361555804336 loss_input: 81.63509321355772
step: 4000 epoch: 468 loss: 16.854861750837507 loss_input: 81.72239531364717
step: 5000 epoch: 468 loss: 16.871805812997405 loss_input: 82.00431814271911
step: 6000 epoch: 468 loss: 16.922762668404932 loss_input: 82.13484462184363
step: 7000 epoch: 468 loss: 16.941247481207323 loss_input: 82.28357765296921
step: 8000 epoch: 468 loss: 16.95710622434422 loss_input: 82.41539733985888
step: 9000 epoch: 468 loss: 16.952190203741385 loss_input: 82.27997181492638
step: 10000 epoch: 468 loss: 16.948766783802117 loss_input: 82.38957137611435
step: 11000 epoch: 468 loss: 16.965954576294138 loss_input: 82.56159393779365
step: 12000 epoch: 468 loss: 16.97439472286535 loss_input: 82.38083031960065
step: 13000 epoch: 468 loss: 16.963955132725257 loss_input: 82.29813730712121
step: 14000 epoch: 468 loss: 16.95247235038299 loss_input: 82.20220877080209
step: 15000 epoch: 468 loss: 16.966046566184733 loss_input: 82.22355060769387
Save loss: 16.979197834968566 Name: 468_train_model.pth
step: 0 epoch: 469 loss: 12.604884147644043 loss_input: 54.327392578125
step: 1000 epoch: 469 loss: 16.75350021005987 loss_input: 81.70486623971732
step: 2000 epoch: 469 loss: 16.885839492306 loss_input: 81.78174297860835
step: 3000 epoch: 469 loss: 16.88850504078495 loss_input: 81.67073963626707
step: 4000 epoch: 469 loss: 16.86794462242117 loss_input: 81.77817813613002
step: 5000 epoch: 469 loss: 16.87687218282204 loss_input: 81.73530592953675
step: 6000 epoch: 469 loss: 16.888097021385462 loss_input: 81.82080495129067
step: 7000 epoch: 469 loss: 16.90572872435667 loss_input: 81.88496530350575
step: 8000 epoch: 469 loss: 16.912819497094514 loss_input: 82.18926044583783
step: 9000 epoch: 469 loss: 16.921551156793615 loss_input: 82.0282860416768
step: 10000 epoch: 469 loss: 16.935121491484352 loss_input: 82.10144518995367
step: 11000 epoch: 469 loss: 16.93983643512901 loss_input: 82.09541203041378
step: 12000 epoch: 469 loss: 16.939494271862856 loss_input: 82.10524302519875
step: 13000 epoch: 469 loss: 16.957307022650895 loss_input: 82.18591167284758
step: 14000 epoch: 469 loss: 16.988146606202417 loss_input: 82.18477623476811
step: 15000 epoch: 469 loss: 16.993261049496446 loss_input: 82.23563973908901
Save loss: 16.99855119262636 Name: 469_train_model.pth
step: 0 epoch: 470 loss: 17.7429141998291 loss_input: 99.8392333984375
step: 1000 epoch: 470 loss: 16.758818705956063 loss_input: 81.76739953162073
step: 2000 epoch: 470 loss: 16.871908624192468 loss_input: 81.92548525541916
step: 3000 epoch: 470 loss: 16.881530003164738 loss_input: 81.53292137890885
step: 4000 epoch: 470 loss: 16.883609424260698 loss_input: 81.73164054567413
step: 5000 epoch: 470 loss: 16.88392299453012 loss_input: 82.11840632361809
step: 6000 epoch: 470 loss: 16.92635701008984 loss_input: 82.08893329790087
step: 7000 epoch: 470 loss: 16.934199027581958 loss_input: 82.11394890538796
step: 8000 epoch: 470 loss: 16.955833931145644 loss_input: 82.0576401033367
step: 9000 epoch: 470 loss: 16.97060332887902 loss_input: 82.19294387347803
step: 10000 epoch: 470 loss: 16.981094104601684 loss_input: 82.2814719382554
step: 11000 epoch: 470 loss: 16.960744844134446 loss_input: 82.26371197145252
step: 12000 epoch: 470 loss: 16.96023325045579 loss_input: 82.23744846926004
step: 13000 epoch: 470 loss: 16.972627558494363 loss_input: 82.23948542795387
step: 14000 epoch: 470 loss: 16.998089078954013 loss_input: 82.3414648849022
step: 15000 epoch: 470 loss: 16.97833377680344 loss_input: 82.24687753156951
Save loss: 16.986643662571907 Name: 470_train_model.pth
step: 0 epoch: 471 loss: 24.144615173339844 loss_input: 114.352294921875
step: 1000 epoch: 471 loss: 17.146232750746872 loss_input: 81.11447500253652
step: 2000 epoch: 471 loss: 17.07111325661937 loss_input: 81.69717183034459
step: 3000 epoch: 471 loss: 17.03475064795957 loss_input: 81.78666593964121
step: 4000 epoch: 471 loss: 17.000530625843638 loss_input: 82.23796014802451
step: 5000 epoch: 471 loss: 16.98716863768741 loss_input: 82.49424274080731
step: 6000 epoch: 471 loss: 16.959870627712675 loss_input: 82.30194441673459
step: 7000 epoch: 471 loss: 16.935041478388207 loss_input: 82.28738380217446
step: 8000 epoch: 471 loss: 16.9331055430975 loss_input: 82.22621162222886
step: 9000 epoch: 471 loss: 16.950311379755302 loss_input: 82.26773466956362
step: 10000 epoch: 471 loss: 16.934304133163383 loss_input: 82.26756389302476
step: 11000 epoch: 471 loss: 16.94865143820325 loss_input: 82.35090456198762
step: 12000 epoch: 471 loss: 16.938701706224258 loss_input: 82.18662628334032
step: 13000 epoch: 471 loss: 16.941641922300022 loss_input: 82.17294588273107
step: 14000 epoch: 471 loss: 16.966200493036123 loss_input: 82.22829226972068
step: 15000 epoch: 471 loss: 16.97401846427058 loss_input: 82.2535992717673
Save loss: 16.9822393745929 Name: 471_train_model.pth
step: 0 epoch: 472 loss: 15.232397079467773 loss_input: 56.01953125
step: 1000 epoch: 472 loss: 16.8477825137166 loss_input: 80.93319558859109
step: 2000 epoch: 472 loss: 16.818966806202994 loss_input: 82.06211143085683
step: 3000 epoch: 472 loss: 16.854334211397155 loss_input: 82.07876758772467
step: 4000 epoch: 472 loss: 16.829480283709056 loss_input: 81.92293941590286
step: 5000 epoch: 472 loss: 16.85946870593876 loss_input: 82.0751541464168
step: 6000 epoch: 472 loss: 16.870444966204822 loss_input: 82.06074502312765
step: 7000 epoch: 472 loss: 16.878579189361563 loss_input: 82.06689216865503
step: 8000 epoch: 472 loss: 16.883403544335973 loss_input: 81.87878327953146
step: 9000 epoch: 472 loss: 16.91389039277686 loss_input: 82.07415660277114
step: 10000 epoch: 472 loss: 16.927302899497494 loss_input: 82.15231006196255
step: 11000 epoch: 472 loss: 16.925830453972893 loss_input: 82.02226233605894
step: 12000 epoch: 472 loss: 16.923505978071336 loss_input: 81.95956354019653
step: 13000 epoch: 472 loss: 16.940822616924407 loss_input: 82.12245865871645
step: 14000 epoch: 472 loss: 16.964472699664285 loss_input: 82.19054815699548
step: 15000 epoch: 472 loss: 16.963299502977396 loss_input: 82.15905790199925
Save loss: 16.980389215543866 Name: 472_train_model.pth
step: 0 epoch: 473 loss: 16.0832462310791 loss_input: 87.37689208984375
step: 1000 epoch: 473 loss: 16.666170951488848 loss_input: 81.8365254130635
step: 2000 epoch: 473 loss: 16.80466996759608 loss_input: 82.17109212727621
step: 3000 epoch: 473 loss: 16.88265624343455 loss_input: 82.09462287798598
step: 4000 epoch: 473 loss: 16.85507883235414 loss_input: 82.12736139390445
step: 5000 epoch: 473 loss: 16.843327215351454 loss_input: 81.94906825519662
step: 6000 epoch: 473 loss: 16.915840026557973 loss_input: 82.07723568288749
step: 7000 epoch: 473 loss: 16.936899801847375 loss_input: 82.11918727535296
step: 8000 epoch: 473 loss: 16.95707573471718 loss_input: 82.13393455212629
step: 9000 epoch: 473 loss: 16.93625847589518 loss_input: 82.07072517387391
step: 10000 epoch: 473 loss: 16.93226529009258 loss_input: 81.96848966836714
step: 11000 epoch: 473 loss: 16.92878511853353 loss_input: 82.0228231371278
step: 12000 epoch: 473 loss: 16.937526823530476 loss_input: 82.02425092403595
step: 13000 epoch: 473 loss: 16.94441345924323 loss_input: 82.02515409909874
step: 14000 epoch: 473 loss: 16.961398701558462 loss_input: 82.15613294293154
step: 15000 epoch: 473 loss: 16.966376134248012 loss_input: 82.21600212004095
Save loss: 16.977021264761685 Name: 473_train_model.pth
step: 0 epoch: 474 loss: 21.232791900634766 loss_input: 97.10284423828125
step: 1000 epoch: 474 loss: 16.906590154954603 loss_input: 81.7623420280891
step: 2000 epoch: 474 loss: 16.839360570979082 loss_input: 81.71053163603744
step: 3000 epoch: 474 loss: 16.872081826822395 loss_input: 82.14260049709992
step: 4000 epoch: 474 loss: 16.93047853208607 loss_input: 82.27847086385142
step: 5000 epoch: 474 loss: 16.926605298027614 loss_input: 82.17300530372441
step: 6000 epoch: 474 loss: 16.942588326772476 loss_input: 82.26165796911452
step: 7000 epoch: 474 loss: 16.923877630722792 loss_input: 82.17646253171979
step: 8000 epoch: 474 loss: 16.95812336293895 loss_input: 82.23473249162946
step: 9000 epoch: 474 loss: 16.939764176934393 loss_input: 82.20076472080359
step: 10000 epoch: 474 loss: 16.937903027476793 loss_input: 82.2623627468796
step: 11000 epoch: 474 loss: 16.941499656248652 loss_input: 82.2068201152967
step: 12000 epoch: 474 loss: 16.94980669734816 loss_input: 82.2055466325072
step: 13000 epoch: 474 loss: 16.959974963081294 loss_input: 82.21706687845676
step: 14000 epoch: 474 loss: 16.965442267309538 loss_input: 82.21114349637693
step: 15000 epoch: 474 loss: 16.959403029982784 loss_input: 82.19457897549668
Save loss: 16.96931697565317 Name: 474_train_model.pth
step: 0 epoch: 475 loss: 14.469991683959961 loss_input: 66.78887939453125
step: 1000 epoch: 475 loss: 16.983078689841957 loss_input: 82.2135265247448
step: 2000 epoch: 475 loss: 16.860880965176133 loss_input: 81.5893814872349
step: 3000 epoch: 475 loss: 16.962168207251203 loss_input: 81.87941741180674
step: 4000 epoch: 475 loss: 16.948949802341954 loss_input: 82.07309909583151
step: 5000 epoch: 475 loss: 16.98975014190773 loss_input: 81.96009820252746
step: 6000 epoch: 475 loss: 16.979699059538675 loss_input: 81.97924244274718
step: 7000 epoch: 475 loss: 16.96486990354075 loss_input: 82.07688272838813
step: 8000 epoch: 475 loss: 17.006575661381397 loss_input: 82.0030233494983
step: 9000 epoch: 475 loss: 17.00120049608428 loss_input: 82.10030571554333
step: 10000 epoch: 475 loss: 17.000363597010697 loss_input: 82.07498006913175
step: 11000 epoch: 475 loss: 16.979144508779488 loss_input: 82.13560419400793
step: 12000 epoch: 475 loss: 16.971447728632015 loss_input: 82.16565953996835
step: 13000 epoch: 475 loss: 16.978063901198222 loss_input: 82.18852425076376
step: 14000 epoch: 475 loss: 16.974216658901668 loss_input: 82.20510052330246
step: 15000 epoch: 475 loss: 16.989853741574166 loss_input: 82.27097443585141
Save loss: 16.985395903900265 Name: 475_train_model.pth
step: 0 epoch: 476 loss: 16.05056381225586 loss_input: 82.16748046875
step: 1000 epoch: 476 loss: 17.129073473123405 loss_input: 83.36775035267468
step: 2000 epoch: 476 loss: 17.042695211565892 loss_input: 82.55784802911045
step: 3000 epoch: 476 loss: 17.030301567555586 loss_input: 82.6108587380053
step: 4000 epoch: 476 loss: 16.96062426053414 loss_input: 82.16270942945415
step: 5000 epoch: 476 loss: 16.977910365802817 loss_input: 82.36599655166623
step: 6000 epoch: 476 loss: 16.96818220533941 loss_input: 82.44862383775882
step: 7000 epoch: 476 loss: 16.95116478210687 loss_input: 82.38814340348142
step: 8000 epoch: 476 loss: 16.94910549643695 loss_input: 82.30467004612704
step: 9000 epoch: 476 loss: 16.963087748850786 loss_input: 82.34928359197069
step: 10000 epoch: 476 loss: 16.96475110925587 loss_input: 82.32538702306528
step: 11000 epoch: 476 loss: 16.960745073493943 loss_input: 82.21909954985449
step: 12000 epoch: 476 loss: 16.969499544763195 loss_input: 82.2880236512273
step: 13000 epoch: 476 loss: 16.961013414723883 loss_input: 82.23457857414664
step: 14000 epoch: 476 loss: 16.96735057162605 loss_input: 82.20029714238464
step: 15000 epoch: 476 loss: 16.97032138284274 loss_input: 82.21587705920517
Save loss: 16.97105891357362 Name: 476_train_model.pth
step: 0 epoch: 477 loss: 19.15711784362793 loss_input: 80.69744873046875
step: 1000 epoch: 477 loss: 17.171476719977257 loss_input: 82.89745099870832
step: 2000 epoch: 477 loss: 17.080936702473767 loss_input: 83.0953327352437
step: 3000 epoch: 477 loss: 17.0117556754846 loss_input: 82.86157651632399
step: 4000 epoch: 477 loss: 16.98298975569819 loss_input: 82.1864962988989
step: 5000 epoch: 477 loss: 17.01204828008893 loss_input: 82.09307452474349
step: 6000 epoch: 477 loss: 16.996827960232856 loss_input: 82.13420180498888
step: 7000 epoch: 477 loss: 16.971683776441633 loss_input: 81.98678296376187
step: 8000 epoch: 477 loss: 16.93171867959545 loss_input: 81.8884002185884
step: 9000 epoch: 477 loss: 16.93633031932503 loss_input: 81.99432855849133
step: 10000 epoch: 477 loss: 16.945895982448988 loss_input: 82.16969133775099
step: 11000 epoch: 477 loss: 16.944790452667263 loss_input: 82.20525811795873
step: 12000 epoch: 477 loss: 16.949832550456808 loss_input: 82.23276637448996
step: 13000 epoch: 477 loss: 16.955087043387955 loss_input: 82.1269488904249
step: 14000 epoch: 477 loss: 16.970338954183767 loss_input: 82.24297519833962
step: 15000 epoch: 477 loss: 16.986359942015802 loss_input: 82.27196047444494
Save loss: 16.97385422490537 Name: 477_train_model.pth
step: 0 epoch: 478 loss: 17.58235740661621 loss_input: 120.58343505859375
step: 1000 epoch: 478 loss: 16.77425595215865 loss_input: 82.36022308989838
step: 2000 epoch: 478 loss: 16.839148202340404 loss_input: 82.48105338810088
step: 3000 epoch: 478 loss: 16.90984816981808 loss_input: 82.32596867531787
step: 4000 epoch: 478 loss: 16.902267875626098 loss_input: 82.1173206911895
step: 5000 epoch: 478 loss: 16.924343813612232 loss_input: 82.30938328935775
step: 6000 epoch: 478 loss: 16.948561846584187 loss_input: 82.20483233670039
step: 7000 epoch: 478 loss: 16.967150721374946 loss_input: 82.22845430214768
step: 8000 epoch: 478 loss: 16.991771224826117 loss_input: 82.21922733443719
step: 9000 epoch: 478 loss: 16.99483016376667 loss_input: 82.08545157173292
step: 10000 epoch: 478 loss: 16.987739152496378 loss_input: 82.11573648543349
step: 11000 epoch: 478 loss: 16.98536851215163 loss_input: 82.22609420872072
step: 12000 epoch: 478 loss: 16.964990297006317 loss_input: 82.10368666177233
step: 13000 epoch: 478 loss: 16.96975897508496 loss_input: 82.10956952755437
step: 14000 epoch: 478 loss: 16.991227750616087 loss_input: 82.21377072396615
step: 15000 epoch: 478 loss: 16.98007276722387 loss_input: 82.14903212588561
Save loss: 16.976741064503788 Name: 478_train_model.pth
step: 0 epoch: 479 loss: 9.481864929199219 loss_input: 56.3463134765625
step: 1000 epoch: 479 loss: 16.80044551329179 loss_input: 81.50592547101336
step: 2000 epoch: 479 loss: 16.875587138815085 loss_input: 81.8411706927298
step: 3000 epoch: 479 loss: 16.885092619934387 loss_input: 81.73653630755894
step: 4000 epoch: 479 loss: 16.880300721952956 loss_input: 82.01124537286088
step: 5000 epoch: 479 loss: 16.883160063086258 loss_input: 81.85549501427839
step: 6000 epoch: 479 loss: 16.899132068426166 loss_input: 81.90707817007713
step: 7000 epoch: 479 loss: 16.908723822594915 loss_input: 82.03313498462273
step: 8000 epoch: 479 loss: 16.92822424111821 loss_input: 82.03616280752992
step: 9000 epoch: 479 loss: 16.937188387155505 loss_input: 82.11546083773895
step: 10000 epoch: 479 loss: 16.95551305488996 loss_input: 82.2996901664814
step: 11000 epoch: 479 loss: 16.96293814539227 loss_input: 82.1267407419638
step: 12000 epoch: 479 loss: 16.96592137674701 loss_input: 82.23605476622879
step: 13000 epoch: 479 loss: 16.957150345224203 loss_input: 82.16523385764947
step: 14000 epoch: 479 loss: 16.955762638908464 loss_input: 82.11713878434604
step: 15000 epoch: 479 loss: 16.957521934174242 loss_input: 82.13868297045171
Save loss: 16.9698727722317 Name: 479_train_model.pth
step: 0 epoch: 480 loss: 23.237993240356445 loss_input: 112.507568359375
step: 1000 epoch: 480 loss: 16.99203324246478 loss_input: 82.63995117431396
step: 2000 epoch: 480 loss: 16.893421379939607 loss_input: 82.37273178596249
step: 3000 epoch: 480 loss: 16.93216491547953 loss_input: 82.33709800183793
step: 4000 epoch: 480 loss: 16.879394488345383 loss_input: 82.26895653405984
step: 5000 epoch: 480 loss: 16.902063917718014 loss_input: 82.30017199685064
step: 6000 epoch: 480 loss: 16.923562390429638 loss_input: 82.43350939312849
step: 7000 epoch: 480 loss: 16.919723540710528 loss_input: 82.3390579038374
step: 8000 epoch: 480 loss: 16.93289216687241 loss_input: 82.1961230752528
step: 9000 epoch: 480 loss: 16.948385884107292 loss_input: 82.35490349840686
step: 10000 epoch: 480 loss: 16.938276883542876 loss_input: 82.33360772199147
step: 11000 epoch: 480 loss: 16.93986224373973 loss_input: 82.1868148538787
step: 12000 epoch: 480 loss: 16.949028641807466 loss_input: 82.18174273339363
step: 13000 epoch: 480 loss: 16.971390834396395 loss_input: 82.29813557010037
step: 14000 epoch: 480 loss: 16.960057368883703 loss_input: 82.2885586776935
step: 15000 epoch: 480 loss: 16.97147311850251 loss_input: 82.26154366401614
Save loss: 16.971587909758092 Name: 480_train_model.pth
step: 0 epoch: 481 loss: 21.93267250061035 loss_input: 127.18743896484375
step: 1000 epoch: 481 loss: 16.851401529112064 loss_input: 82.99406410240151
step: 2000 epoch: 481 loss: 16.706307643774092 loss_input: 81.56734139033999
step: 3000 epoch: 481 loss: 16.874708720979115 loss_input: 81.70368375336476
step: 4000 epoch: 481 loss: 16.86009711785425 loss_input: 81.92466272469969
step: 5000 epoch: 481 loss: 16.88082060301883 loss_input: 81.78573046377053
step: 6000 epoch: 481 loss: 16.921659046720414 loss_input: 82.01678548163522
step: 7000 epoch: 481 loss: 16.926059509205693 loss_input: 82.26893179601575
step: 8000 epoch: 481 loss: 16.960615993782724 loss_input: 82.4386173987773
step: 9000 epoch: 481 loss: 16.98024606516647 loss_input: 82.4027817370878
step: 10000 epoch: 481 loss: 16.967717748512662 loss_input: 82.34966939707397
step: 11000 epoch: 481 loss: 16.95989794087902 loss_input: 82.27720104613182
step: 12000 epoch: 481 loss: 16.961974895633208 loss_input: 82.29975965239387
step: 13000 epoch: 481 loss: 16.940625103241974 loss_input: 82.14919771420828
step: 14000 epoch: 481 loss: 16.955957666906865 loss_input: 82.12955103600999
step: 15000 epoch: 481 loss: 16.954077104212214 loss_input: 82.09492450259175
Save loss: 16.97113771881163 Name: 481_train_model.pth
step: 0 epoch: 482 loss: 19.518882751464844 loss_input: 87.66265869140625
step: 1000 epoch: 482 loss: 16.85480781177898 loss_input: 81.01578402066683
step: 2000 epoch: 482 loss: 16.88949739259818 loss_input: 81.58344256216618
step: 3000 epoch: 482 loss: 16.92768764336957 loss_input: 81.82477245947314
step: 4000 epoch: 482 loss: 16.934691666901276 loss_input: 81.80544951652712
step: 5000 epoch: 482 loss: 16.942979668550695 loss_input: 81.81680177636348
step: 6000 epoch: 482 loss: 16.933111691113375 loss_input: 81.75645337186641
step: 7000 epoch: 482 loss: 16.932356114592523 loss_input: 81.87003161099753
step: 8000 epoch: 482 loss: 16.92683729212398 loss_input: 81.88359425500279
step: 9000 epoch: 482 loss: 16.90344528867965 loss_input: 81.75990494599992
step: 10000 epoch: 482 loss: 16.92822797137992 loss_input: 81.84139898388091
step: 11000 epoch: 482 loss: 16.95536867064483 loss_input: 81.88651671980459
step: 12000 epoch: 482 loss: 16.9561131147134 loss_input: 81.95679524671772
step: 13000 epoch: 482 loss: 16.97351225659752 loss_input: 82.01331734904858
step: 14000 epoch: 482 loss: 16.97155369968808 loss_input: 82.05669896172316
step: 15000 epoch: 482 loss: 16.96415566382476 loss_input: 82.06906392601171
Save loss: 16.96989598326385 Name: 482_train_model.pth
step: 0 epoch: 483 loss: 12.569207191467285 loss_input: 55.7884521484375
step: 1000 epoch: 483 loss: 16.982970207721202 loss_input: 83.3761234493046
step: 2000 epoch: 483 loss: 16.78626061248398 loss_input: 82.90712380016046
step: 3000 epoch: 483 loss: 16.839070898975702 loss_input: 82.05783037327402
step: 4000 epoch: 483 loss: 16.866083353944077 loss_input: 82.2592088031459
step: 5000 epoch: 483 loss: 16.897290665825423 loss_input: 81.95273495349758
step: 6000 epoch: 483 loss: 16.90057344012331 loss_input: 82.02180269149477
step: 7000 epoch: 483 loss: 16.921058539747595 loss_input: 82.09307008701603
step: 8000 epoch: 483 loss: 16.951318411987405 loss_input: 82.1848672877072
step: 9000 epoch: 483 loss: 16.933841093926016 loss_input: 82.17251686254696
step: 10000 epoch: 483 loss: 16.94280697204938 loss_input: 82.22067110305571
step: 11000 epoch: 483 loss: 16.96361055938496 loss_input: 82.40736962699769
step: 12000 epoch: 483 loss: 16.957096920481483 loss_input: 82.2267819895624
step: 13000 epoch: 483 loss: 16.96824487421423 loss_input: 82.23863840495594
step: 14000 epoch: 483 loss: 16.97853439777683 loss_input: 82.22001697754709
step: 15000 epoch: 483 loss: 16.968547698076815 loss_input: 82.19037090642524
Save loss: 16.968913738831876 Name: 483_train_model.pth
step: 0 epoch: 484 loss: 18.959735870361328 loss_input: 70.6656494140625
step: 1000 epoch: 484 loss: 16.9402410914967 loss_input: 82.02073866075331
step: 2000 epoch: 484 loss: 16.93754883613186 loss_input: 82.16178376265968
step: 3000 epoch: 484 loss: 16.977654174739857 loss_input: 82.44250976399793
step: 4000 epoch: 484 loss: 16.989963467375574 loss_input: 82.45322824567296
step: 5000 epoch: 484 loss: 16.956287302605702 loss_input: 82.26093528852823
step: 6000 epoch: 484 loss: 16.978633071875258 loss_input: 82.29291331897157
step: 7000 epoch: 484 loss: 16.91229384273414 loss_input: 82.13637486602082
step: 8000 epoch: 484 loss: 16.913747071594077 loss_input: 82.08602380818121
step: 9000 epoch: 484 loss: 16.92922837849975 loss_input: 82.04235413323215
step: 10000 epoch: 484 loss: 16.93624087312605 loss_input: 82.08259783152545
step: 11000 epoch: 484 loss: 16.939745767519003 loss_input: 82.09847673205135
step: 12000 epoch: 484 loss: 16.946178222634316 loss_input: 82.12699996049321
step: 13000 epoch: 484 loss: 16.950530496379724 loss_input: 82.17163530520939
step: 14000 epoch: 484 loss: 16.95188146614618 loss_input: 82.20369698062862
step: 15000 epoch: 484 loss: 16.9687424812561 loss_input: 82.22046459923506
Save loss: 16.975170265764 Name: 484_train_model.pth
step: 0 epoch: 485 loss: 11.397388458251953 loss_input: 47.2767333984375
step: 1000 epoch: 485 loss: 17.15191302599607 loss_input: 82.1155346118725
step: 2000 epoch: 485 loss: 16.92402305607793 loss_input: 82.16722449322214
step: 3000 epoch: 485 loss: 16.932698183558614 loss_input: 82.3884375577607
step: 4000 epoch: 485 loss: 16.93238368882921 loss_input: 82.3174427035331
step: 5000 epoch: 485 loss: 16.931324159543625 loss_input: 82.46037551522899
step: 6000 epoch: 485 loss: 16.919329723026966 loss_input: 82.4964841166609
step: 7000 epoch: 485 loss: 16.958785631778905 loss_input: 82.57393320729572
step: 8000 epoch: 485 loss: 16.973814295196725 loss_input: 82.57080871482862
step: 9000 epoch: 485 loss: 16.96706081377031 loss_input: 82.49864722468988
step: 10000 epoch: 485 loss: 16.98751840185206 loss_input: 82.44418016084134
step: 11000 epoch: 485 loss: 16.970442514009513 loss_input: 82.39189906519506
step: 12000 epoch: 485 loss: 16.97503989947338 loss_input: 82.33717603916308
step: 13000 epoch: 485 loss: 16.989826207894122 loss_input: 82.32496164564114
step: 14000 epoch: 485 loss: 16.973795294361484 loss_input: 82.24768061610973
step: 15000 epoch: 485 loss: 16.967542788432635 loss_input: 82.2219571429613
Save loss: 16.96064646539092 Name: 485_train_model.pth
step: 0 epoch: 486 loss: 13.645179748535156 loss_input: 75.87103271484375
step: 1000 epoch: 486 loss: 16.888758667699108 loss_input: 81.30912837162838
step: 2000 epoch: 486 loss: 17.02322926311598 loss_input: 82.34777365345647
step: 3000 epoch: 486 loss: 16.95787231138014 loss_input: 82.3224355309298
step: 4000 epoch: 486 loss: 16.949669203320852 loss_input: 82.3554879712689
step: 5000 epoch: 486 loss: 16.94268119718952 loss_input: 82.40019210513366
step: 6000 epoch: 486 loss: 16.96945911406517 loss_input: 82.47388317767351
step: 7000 epoch: 486 loss: 16.960358777228056 loss_input: 82.43343371252489
step: 8000 epoch: 486 loss: 16.94180071465657 loss_input: 82.27589256878778
step: 9000 epoch: 486 loss: 16.931771880877626 loss_input: 82.16919837682966
step: 10000 epoch: 486 loss: 16.94026625274408 loss_input: 82.17864249951958
step: 11000 epoch: 486 loss: 16.961531652059243 loss_input: 82.2545552887859
step: 12000 epoch: 486 loss: 16.969527726331936 loss_input: 82.16807289272931
step: 13000 epoch: 486 loss: 16.959471862193816 loss_input: 82.09220012203839
step: 14000 epoch: 486 loss: 16.967715599615058 loss_input: 82.11315916660948
step: 15000 epoch: 486 loss: 16.965725767017247 loss_input: 82.16751098022502
Save loss: 16.96638945749402 Name: 486_train_model.pth
step: 0 epoch: 487 loss: 17.75591278076172 loss_input: 69.77301025390625
step: 1000 epoch: 487 loss: 17.024757922588886 loss_input: 82.43745701320164
step: 2000 epoch: 487 loss: 17.102961140594026 loss_input: 82.35665281983032
step: 3000 epoch: 487 loss: 17.061853811129932 loss_input: 82.25689449138699
step: 4000 epoch: 487 loss: 17.125777852025756 loss_input: 82.69610409592426
step: 5000 epoch: 487 loss: 17.06022223184834 loss_input: 82.5550659521416
step: 6000 epoch: 487 loss: 17.05550485848387 loss_input: 82.74304612154485
step: 7000 epoch: 487 loss: 17.01257182192656 loss_input: 82.61398634074875
step: 8000 epoch: 487 loss: 16.999193078561838 loss_input: 82.59569328431114
step: 9000 epoch: 487 loss: 16.97334789530514 loss_input: 82.51027238428586
step: 10000 epoch: 487 loss: 16.97496248965096 loss_input: 82.46118820124823
step: 11000 epoch: 487 loss: 16.973634488798684 loss_input: 82.27634165493123
step: 12000 epoch: 487 loss: 16.975855198932404 loss_input: 82.28505134948064
step: 13000 epoch: 487 loss: 16.97841316917733 loss_input: 82.31251506982936
step: 14000 epoch: 487 loss: 16.969375772379472 loss_input: 82.25551360066078
step: 15000 epoch: 487 loss: 16.96203677390911 loss_input: 82.26631863865326
Save loss: 16.960878664061426 Name: 487_train_model.pth
step: 0 epoch: 488 loss: 19.121910095214844 loss_input: 70.20721435546875
step: 1000 epoch: 488 loss: 16.905528637793633 loss_input: 82.00460824575815
step: 2000 epoch: 488 loss: 16.891898551504354 loss_input: 82.07997460010229
step: 3000 epoch: 488 loss: 16.84831649992554 loss_input: 82.00555102644822
step: 4000 epoch: 488 loss: 16.84839662245827 loss_input: 81.99658070406714
step: 5000 epoch: 488 loss: 16.8742234381264 loss_input: 81.96317801869314
step: 6000 epoch: 488 loss: 16.877880452891386 loss_input: 81.9473902278494
step: 7000 epoch: 488 loss: 16.913887687894793 loss_input: 82.0589347395824
step: 8000 epoch: 488 loss: 16.925411410695865 loss_input: 82.01180452419034
step: 9000 epoch: 488 loss: 16.909324187515125 loss_input: 82.10324556765828
step: 10000 epoch: 488 loss: 16.913723844562146 loss_input: 82.16927643783474
step: 11000 epoch: 488 loss: 16.92196368722523 loss_input: 82.18403516051097
step: 12000 epoch: 488 loss: 16.933294693893835 loss_input: 82.20710491838082
step: 13000 epoch: 488 loss: 16.938929273132068 loss_input: 82.16532697135527
step: 14000 epoch: 488 loss: 16.9406387210616 loss_input: 82.14099521717338
step: 15000 epoch: 488 loss: 16.941373202127025 loss_input: 82.18372638682811
Save loss: 16.962572814241053 Name: 488_train_model.pth
step: 0 epoch: 489 loss: 17.606355667114258 loss_input: 94.66766357421875
step: 1000 epoch: 489 loss: 16.975104147142225 loss_input: 82.5775058071811
step: 2000 epoch: 489 loss: 16.95553215332832 loss_input: 82.87070438779634
step: 3000 epoch: 489 loss: 16.954221755335706 loss_input: 82.92306621262965
step: 4000 epoch: 489 loss: 16.92658357994463 loss_input: 82.80435866375203
step: 5000 epoch: 489 loss: 16.91882108617034 loss_input: 82.57413778474773
step: 6000 epoch: 489 loss: 16.938292292153275 loss_input: 82.6938931402058
step: 7000 epoch: 489 loss: 16.943401450345284 loss_input: 82.65882196795548
step: 8000 epoch: 489 loss: 16.95964510347557 loss_input: 82.56899162781654
step: 9000 epoch: 489 loss: 16.936360854093877 loss_input: 82.2786897375986
step: 10000 epoch: 489 loss: 16.947655483742093 loss_input: 82.21945665540868
step: 11000 epoch: 489 loss: 16.969046644206134 loss_input: 82.29554887654922
step: 12000 epoch: 489 loss: 16.983582982777776 loss_input: 82.34883573239827
step: 13000 epoch: 489 loss: 16.96235929886274 loss_input: 82.30547922939972
step: 14000 epoch: 489 loss: 16.94247809087163 loss_input: 82.2309261322294
step: 15000 epoch: 489 loss: 16.94532363607553 loss_input: 82.19829133786914
Save loss: 16.959814081162214 Name: 489_train_model.pth
step: 0 epoch: 490 loss: 20.351194381713867 loss_input: 151.34039306640625
step: 1000 epoch: 490 loss: 16.876220791251747 loss_input: 81.51556695257867
step: 2000 epoch: 490 loss: 16.8834511655858 loss_input: 82.17617500453875
step: 3000 epoch: 490 loss: 16.884816187852543 loss_input: 81.81157609328791
step: 4000 epoch: 490 loss: 16.919301376256964 loss_input: 81.91334400591747
step: 5000 epoch: 490 loss: 16.9190812424597 loss_input: 81.85502002382727
step: 6000 epoch: 490 loss: 16.927079820688558 loss_input: 81.99183860998752
step: 7000 epoch: 490 loss: 16.9216507198027 loss_input: 82.0111862071462
step: 8000 epoch: 490 loss: 16.941845992374027 loss_input: 82.00090999672405
step: 9000 epoch: 490 loss: 16.952454069643178 loss_input: 82.12591858554345
step: 10000 epoch: 490 loss: 16.960684933575163 loss_input: 82.30087934199744
step: 11000 epoch: 490 loss: 16.969073389456625 loss_input: 82.24436224015146
step: 12000 epoch: 490 loss: 16.968129076388326 loss_input: 82.30708701927834
step: 13000 epoch: 490 loss: 16.967233325085193 loss_input: 82.28929225152001
step: 14000 epoch: 490 loss: 16.95956488316693 loss_input: 82.230323174112
step: 15000 epoch: 490 loss: 16.967402129433616 loss_input: 82.22431038370229
Save loss: 16.963125150725247 Name: 490_train_model.pth
step: 0 epoch: 491 loss: 16.128393173217773 loss_input: 73.1143798828125
step: 1000 epoch: 491 loss: 16.8457997833694 loss_input: 84.22294197501716
step: 2000 epoch: 491 loss: 16.885633621139565 loss_input: 83.23384108726887
step: 3000 epoch: 491 loss: 16.945549910881248 loss_input: 83.031091117414
step: 4000 epoch: 491 loss: 16.90025715612227 loss_input: 82.51458892784129
step: 5000 epoch: 491 loss: 16.91586975740495 loss_input: 82.50047934748987
step: 6000 epoch: 491 loss: 16.914127101264107 loss_input: 82.26350044564512
step: 7000 epoch: 491 loss: 16.952059649447172 loss_input: 82.41811148212086
step: 8000 epoch: 491 loss: 16.954951103620715 loss_input: 82.38286565497673
step: 9000 epoch: 491 loss: 16.969927536065203 loss_input: 82.34697145738147
step: 10000 epoch: 491 loss: 16.953870405591545 loss_input: 82.28174140593754
step: 11000 epoch: 491 loss: 16.924104411128912 loss_input: 82.14396742519146
step: 12000 epoch: 491 loss: 16.914926022988997 loss_input: 81.99207879018708
step: 13000 epoch: 491 loss: 16.9325807296664 loss_input: 82.05480829336746
step: 14000 epoch: 491 loss: 16.947378805574115 loss_input: 82.14387029954752
step: 15000 epoch: 491 loss: 16.947057748935944 loss_input: 82.18809894281787
Save loss: 16.952137925729154 Name: 491_train_model.pth
step: 0 epoch: 492 loss: 15.07819938659668 loss_input: 85.41107177734375
step: 1000 epoch: 492 loss: 16.778793481441884 loss_input: 81.0481905789523
step: 2000 epoch: 492 loss: 16.864380042592266 loss_input: 81.62641277627787
step: 3000 epoch: 492 loss: 16.935520360168717 loss_input: 81.92086514374766
step: 4000 epoch: 492 loss: 16.943352306166222 loss_input: 81.78041643763864
step: 5000 epoch: 492 loss: 16.942019706438884 loss_input: 81.68136813311166
step: 6000 epoch: 492 loss: 16.94904769478073 loss_input: 81.99777085899095
step: 7000 epoch: 492 loss: 16.936004164661277 loss_input: 81.9067774863775
step: 8000 epoch: 492 loss: 16.942566366109858 loss_input: 82.00341465800662
step: 9000 epoch: 492 loss: 16.93868942303653 loss_input: 81.93785999282414
step: 10000 epoch: 492 loss: 16.955682185110767 loss_input: 81.92305023037736
step: 11000 epoch: 492 loss: 16.962194424609187 loss_input: 82.05628992667926
step: 12000 epoch: 492 loss: 16.964444450135726 loss_input: 82.07007303326122
step: 13000 epoch: 492 loss: 16.96661065153118 loss_input: 82.10401745031783
step: 14000 epoch: 492 loss: 16.959639366452468 loss_input: 82.14926403572318
step: 15000 epoch: 492 loss: 16.954767534473913 loss_input: 82.25514214579705
Save loss: 16.952647343322635 Name: 492_train_model.pth
step: 0 epoch: 493 loss: 6.013683795928955 loss_input: 61.20050048828125
step: 1000 epoch: 493 loss: 17.000060645016756 loss_input: 81.69271829244974
step: 2000 epoch: 493 loss: 16.96304267504881 loss_input: 81.79091975618636
step: 3000 epoch: 493 loss: 16.91653036745498 loss_input: 81.76544762992414
step: 4000 epoch: 493 loss: 16.87508944099291 loss_input: 81.87624884855447
step: 5000 epoch: 493 loss: 16.910824104920074 loss_input: 82.11152348387745
step: 6000 epoch: 493 loss: 16.936548252738213 loss_input: 82.19319547206
step: 7000 epoch: 493 loss: 16.97313627013648 loss_input: 82.35016749664948
step: 8000 epoch: 493 loss: 16.961617965874648 loss_input: 82.60729566244405
step: 9000 epoch: 493 loss: 16.95564424100286 loss_input: 82.53551873142567
step: 10000 epoch: 493 loss: 16.96762447983679 loss_input: 82.41664206382096
step: 11000 epoch: 493 loss: 16.973296508931234 loss_input: 82.42169955652113
step: 12000 epoch: 493 loss: 16.975759954137033 loss_input: 82.42754057909725
step: 13000 epoch: 493 loss: 16.977898274116466 loss_input: 82.30518879951693
step: 14000 epoch: 493 loss: 16.95178577864343 loss_input: 82.2094300305977
step: 15000 epoch: 493 loss: 16.95281592112177 loss_input: 82.23078985896383
Save loss: 16.959158143818378 Name: 493_train_model.pth
step: 0 epoch: 494 loss: 19.895599365234375 loss_input: 87.83953857421875
step: 1000 epoch: 494 loss: 17.018432063179894 loss_input: 80.7469036700604
step: 2000 epoch: 494 loss: 16.98497237925646 loss_input: 81.21448293797437
step: 3000 epoch: 494 loss: 16.959181893471676 loss_input: 81.86375881298031
step: 4000 epoch: 494 loss: 16.934121094355433 loss_input: 82.02881364314683
step: 5000 epoch: 494 loss: 16.90968556996227 loss_input: 82.02337509792963
step: 6000 epoch: 494 loss: 16.92017670707531 loss_input: 82.14190439899014
step: 7000 epoch: 494 loss: 16.94015570242666 loss_input: 82.30858760376587
step: 8000 epoch: 494 loss: 16.93569632485991 loss_input: 82.16107403536228
step: 9000 epoch: 494 loss: 16.93717406551224 loss_input: 82.21520707226107
step: 10000 epoch: 494 loss: 16.935136233528976 loss_input: 82.21435293604429
step: 11000 epoch: 494 loss: 16.944322129356113 loss_input: 82.24693913213146
step: 12000 epoch: 494 loss: 16.961371033224065 loss_input: 82.37900650206232
step: 13000 epoch: 494 loss: 16.95122323811178 loss_input: 82.31629459711783
step: 14000 epoch: 494 loss: 16.956613102535478 loss_input: 82.32451709905068
step: 15000 epoch: 494 loss: 16.940898389277812 loss_input: 82.24397343237014
Save loss: 16.96100125581026 Name: 494_train_model.pth
step: 0 epoch: 495 loss: 19.357662200927734 loss_input: 67.05426025390625
step: 1000 epoch: 495 loss: 16.99912522698973 loss_input: 82.5720180698208
step: 2000 epoch: 495 loss: 16.913402891587996 loss_input: 82.96040688152017
step: 3000 epoch: 495 loss: 16.99186102996783 loss_input: 83.19738763429768
step: 4000 epoch: 495 loss: 16.997380859999023 loss_input: 83.05739019346726
step: 5000 epoch: 495 loss: 16.959762416060414 loss_input: 82.96393781546425
step: 6000 epoch: 495 loss: 16.98260681189372 loss_input: 82.85332328441679
step: 7000 epoch: 495 loss: 16.974212218379826 loss_input: 82.72332076688542
step: 8000 epoch: 495 loss: 16.92334370037389 loss_input: 82.58744941885926
step: 9000 epoch: 495 loss: 16.93076000257593 loss_input: 82.57273034929713
step: 10000 epoch: 495 loss: 16.94565912182719 loss_input: 82.54851175649037
step: 11000 epoch: 495 loss: 16.925961552246214 loss_input: 82.43651320117027
step: 12000 epoch: 495 loss: 16.931638417944452 loss_input: 82.39024849023812
step: 13000 epoch: 495 loss: 16.947292878567076 loss_input: 82.33070666833471
step: 14000 epoch: 495 loss: 16.955230406118915 loss_input: 82.28920934881604
step: 15000 epoch: 495 loss: 16.95451219191067 loss_input: 82.31237200153453
Save loss: 16.95655476231873 Name: 495_train_model.pth
step: 0 epoch: 496 loss: 24.197656631469727 loss_input: 156.42510986328125
step: 1000 epoch: 496 loss: 16.829431307065736 loss_input: 81.06839772776053
step: 2000 epoch: 496 loss: 16.887514325513177 loss_input: 81.77634199257793
step: 3000 epoch: 496 loss: 16.804171457802283 loss_input: 82.30317396642525
step: 4000 epoch: 496 loss: 16.891954479679946 loss_input: 82.39465042186718
step: 5000 epoch: 496 loss: 16.90221488211208 loss_input: 82.47427709282363
step: 6000 epoch: 496 loss: 16.901067767852822 loss_input: 82.50654470592792
step: 7000 epoch: 496 loss: 16.880628527990154 loss_input: 82.29430312794322
step: 8000 epoch: 496 loss: 16.875317597088255 loss_input: 82.09577159025775
step: 9000 epoch: 496 loss: 16.901237192716536 loss_input: 82.05177244574821
step: 10000 epoch: 496 loss: 16.898952658874205 loss_input: 82.0689991242575
step: 11000 epoch: 496 loss: 16.903593117492264 loss_input: 82.07900475844525
step: 12000 epoch: 496 loss: 16.90629678860812 loss_input: 82.05050473800848
step: 13000 epoch: 496 loss: 16.921148214163427 loss_input: 82.13963213767211
step: 14000 epoch: 496 loss: 16.934911381541877 loss_input: 82.16619682979535
step: 15000 epoch: 496 loss: 16.928425922098814 loss_input: 82.20688109724976
Save loss: 16.950843801915646 Name: 496_train_model.pth
step: 0 epoch: 497 loss: 10.741941452026367 loss_input: 63.3636474609375
step: 1000 epoch: 497 loss: 16.8229950600928 loss_input: 81.52479917543394
step: 2000 epoch: 497 loss: 16.821814066645267 loss_input: 81.12242444451603
step: 3000 epoch: 497 loss: 16.895764313312977 loss_input: 82.13749762448975
step: 4000 epoch: 497 loss: 16.880157386383633 loss_input: 82.26785674186922
step: 5000 epoch: 497 loss: 16.90174890689625 loss_input: 82.20801738089882
step: 6000 epoch: 497 loss: 16.920061807079406 loss_input: 82.17104370747779
step: 7000 epoch: 497 loss: 16.914734068095317 loss_input: 82.07723844277827
step: 8000 epoch: 497 loss: 16.902488649047893 loss_input: 81.94737856406866
step: 9000 epoch: 497 loss: 16.906444276018867 loss_input: 82.06875938209556
step: 10000 epoch: 497 loss: 16.901816001428077 loss_input: 82.10529595233837
step: 11000 epoch: 497 loss: 16.930166952719027 loss_input: 82.22802564601727
step: 12000 epoch: 497 loss: 16.933678202743916 loss_input: 82.14223992686482
step: 13000 epoch: 497 loss: 16.941555963095478 loss_input: 82.10679282933765
step: 14000 epoch: 497 loss: 16.93862478353834 loss_input: 82.0769428639793
step: 15000 epoch: 497 loss: 16.94647369973779 loss_input: 82.17653943106266
Save loss: 16.943069071039556 Name: 497_train_model.pth
step: 0 epoch: 498 loss: 16.418136596679688 loss_input: 67.10894775390625
step: 1000 epoch: 498 loss: 16.57241313869541 loss_input: 81.53430145294159
step: 2000 epoch: 498 loss: 16.7656475919774 loss_input: 82.38855010971076
step: 3000 epoch: 498 loss: 16.764229592384 loss_input: 82.29115801324689
step: 4000 epoch: 498 loss: 16.863348991982313 loss_input: 82.36795333027095
step: 5000 epoch: 498 loss: 16.88619890367477 loss_input: 82.29369893843497
step: 6000 epoch: 498 loss: 16.86818594917459 loss_input: 82.30167585594458
step: 7000 epoch: 498 loss: 16.87370418334174 loss_input: 82.25359238231034
step: 8000 epoch: 498 loss: 16.92434421442044 loss_input: 82.4043162741254
step: 9000 epoch: 498 loss: 16.937699593142554 loss_input: 82.27616533953909
step: 10000 epoch: 498 loss: 16.959474633734366 loss_input: 82.35223629736636
step: 11000 epoch: 498 loss: 16.959212553478373 loss_input: 82.33079586093203
step: 12000 epoch: 498 loss: 16.970890695557436 loss_input: 82.22622590390417
step: 13000 epoch: 498 loss: 16.95702367246669 loss_input: 82.20414381435803
step: 14000 epoch: 498 loss: 16.950441134605192 loss_input: 82.20659809263763
step: 15000 epoch: 498 loss: 16.959162248229116 loss_input: 82.19518923754693
Save loss: 16.955932510733604 Name: 498_train_model.pth
step: 0 epoch: 499 loss: 20.286277770996094 loss_input: 79.0587158203125
step: 1000 epoch: 499 loss: 16.802635623977615 loss_input: 83.54256260097324
step: 2000 epoch: 499 loss: 16.766631587989803 loss_input: 81.79622465988686
step: 3000 epoch: 499 loss: 16.783559857984656 loss_input: 81.88394961282437
step: 4000 epoch: 499 loss: 16.851935146987753 loss_input: 81.75984067947857
step: 5000 epoch: 499 loss: 16.889327748540257 loss_input: 82.04406417300524
step: 6000 epoch: 499 loss: 16.894802308205744 loss_input: 81.80449392874625
step: 7000 epoch: 499 loss: 16.876390098622995 loss_input: 81.89123275357984
step: 8000 epoch: 499 loss: 16.870041151968124 loss_input: 81.74509867327374
step: 9000 epoch: 499 loss: 16.89883084697679 loss_input: 81.9471566554454
step: 10000 epoch: 499 loss: 16.92347976255746 loss_input: 82.01446812716189
step: 11000 epoch: 499 loss: 16.92707217386577 loss_input: 82.09683861969579
step: 12000 epoch: 499 loss: 16.9506548248463 loss_input: 82.12505257231982
step: 13000 epoch: 499 loss: 16.952512150901565 loss_input: 82.19625712009532
step: 14000 epoch: 499 loss: 16.961388920879426 loss_input: 82.28067587021546
step: 15000 epoch: 499 loss: 16.951487610637486 loss_input: 82.23731037559736
Save loss: 16.953066993638874 Name: 499_train_model.pth
step: 0 epoch: 500 loss: 20.767440795898438 loss_input: 83.60223388671875
step: 1000 epoch: 500 loss: 17.062432804545917 loss_input: 83.11116239717315
step: 2000 epoch: 500 loss: 16.996643535617828 loss_input: 82.94628375509511
step: 3000 epoch: 500 loss: 17.005063184218898 loss_input: 82.97130159901127
step: 4000 epoch: 500 loss: 17.007719122448794 loss_input: 82.67098464336553
step: 5000 epoch: 500 loss: 16.974917315740726 loss_input: 82.46346027425375
step: 6000 epoch: 500 loss: 16.969945595912428 loss_input: 82.44634755494853
step: 7000 epoch: 500 loss: 16.96571150668569 loss_input: 82.32734921448164
step: 8000 epoch: 500 loss: 16.95049600243017 loss_input: 82.36260660593695
step: 9000 epoch: 500 loss: 16.95144626781975 loss_input: 82.12070145960345
step: 10000 epoch: 500 loss: 16.93374468178621 loss_input: 82.06485316434177
step: 11000 epoch: 500 loss: 16.93503959033069 loss_input: 82.16270845272943
step: 12000 epoch: 500 loss: 16.93730565959459 loss_input: 82.17526769212917
step: 13000 epoch: 500 loss: 16.941544489533744 loss_input: 82.2292761725285
step: 14000 epoch: 500 loss: 16.93432203959281 loss_input: 82.22194994930608
step: 15000 epoch: 500 loss: 16.94843181242395 loss_input: 82.31327830097786
Save loss: 16.949603618770837 Name: 500_train_model.pth
step: 0 epoch: 501 loss: 21.923900604248047 loss_input: 128.4400634765625
step: 1000 epoch: 501 loss: 16.982854768351004 loss_input: 82.53100903003246
step: 2000 epoch: 501 loss: 16.998342491161342 loss_input: 82.29683153418408
step: 3000 epoch: 501 loss: 16.956671692855515 loss_input: 82.58309134178582
step: 4000 epoch: 501 loss: 16.97675871640496 loss_input: 82.58421426747805
step: 5000 epoch: 501 loss: 17.008177525662013 loss_input: 82.85828832768603
step: 6000 epoch: 501 loss: 17.0164878364802 loss_input: 82.6024669818214
step: 7000 epoch: 501 loss: 16.998381275429825 loss_input: 82.41708452921904
step: 8000 epoch: 501 loss: 16.96681160116893 loss_input: 82.35802682562628
step: 9000 epoch: 501 loss: 16.934003233657972 loss_input: 82.14951557981188
step: 10000 epoch: 501 loss: 16.93355589191409 loss_input: 82.15033550966193
step: 11000 epoch: 501 loss: 16.93513783738024 loss_input: 82.20558102006707
step: 12000 epoch: 501 loss: 16.94119786715549 loss_input: 82.24787618406474
step: 13000 epoch: 501 loss: 16.94089969628848 loss_input: 82.19657526250235
step: 14000 epoch: 501 loss: 16.94659570734566 loss_input: 82.11652834123734
step: 15000 epoch: 501 loss: 16.930387421612995 loss_input: 82.11965614238999
Save loss: 16.944390840947626 Name: 501_train_model.pth
step: 0 epoch: 502 loss: 17.483112335205078 loss_input: 89.61199951171875
step: 1000 epoch: 502 loss: 16.615811425370055 loss_input: 81.43742256278877
step: 2000 epoch: 502 loss: 16.75892011133925 loss_input: 82.29159425414247
step: 3000 epoch: 502 loss: 16.836727771946528 loss_input: 82.4247343170766
step: 4000 epoch: 502 loss: 16.814751453621334 loss_input: 82.28074310768011
step: 5000 epoch: 502 loss: 16.802392661869465 loss_input: 82.32971990806917
step: 6000 epoch: 502 loss: 16.81976864227075 loss_input: 82.20941565891361
step: 7000 epoch: 502 loss: 16.833585258450377 loss_input: 82.16310247566474
step: 8000 epoch: 502 loss: 16.841384118265726 loss_input: 82.20459912756267
step: 9000 epoch: 502 loss: 16.861495293454187 loss_input: 82.30091663127925
step: 10000 epoch: 502 loss: 16.86435155477086 loss_input: 82.24501133870987
step: 11000 epoch: 502 loss: 16.893569750045064 loss_input: 82.24308549520178
step: 12000 epoch: 502 loss: 16.919856219617103 loss_input: 82.31099340151016
step: 13000 epoch: 502 loss: 16.92773315583951 loss_input: 82.25774300038819
step: 14000 epoch: 502 loss: 16.930447915478883 loss_input: 82.25752130898992
step: 15000 epoch: 502 loss: 16.939852222046305 loss_input: 82.27135357317324
Save loss: 16.94505018107593 Name: 502_train_model.pth
step: 0 epoch: 503 loss: 15.844618797302246 loss_input: 64.16766357421875
step: 1000 epoch: 503 loss: 16.727095838312383 loss_input: 81.53342860752528
step: 2000 epoch: 503 loss: 16.926481480005084 loss_input: 82.23073186307236
step: 3000 epoch: 503 loss: 16.901676099962174 loss_input: 82.33723296661529
step: 4000 epoch: 503 loss: 16.883983863290922 loss_input: 82.31395410424737
step: 5000 epoch: 503 loss: 16.889042953471378 loss_input: 82.29235942362308
step: 6000 epoch: 503 loss: 16.88025941254397 loss_input: 82.17913289476168
step: 7000 epoch: 503 loss: 16.88543739701625 loss_input: 82.09685262770776
step: 8000 epoch: 503 loss: 16.896847460898975 loss_input: 82.07684175891588
step: 9000 epoch: 503 loss: 16.869496221609108 loss_input: 81.91891860241441
step: 10000 epoch: 503 loss: 16.88300647798532 loss_input: 82.01438571352826
step: 11000 epoch: 503 loss: 16.910146231695084 loss_input: 82.19806525156982
step: 12000 epoch: 503 loss: 16.92059372572211 loss_input: 82.32912434197378
step: 13000 epoch: 503 loss: 16.922918037876386 loss_input: 82.34442164492455
step: 14000 epoch: 503 loss: 16.936260496733077 loss_input: 82.3236561375987
step: 15000 epoch: 503 loss: 16.943350107397574 loss_input: 82.25831551415453
Save loss: 16.942684036761523 Name: 503_train_model.pth
step: 0 epoch: 504 loss: 17.327953338623047 loss_input: 133.9014892578125
step: 1000 epoch: 504 loss: 16.938468123768473 loss_input: 80.91245375718032
step: 2000 epoch: 504 loss: 16.90826153290504 loss_input: 81.55355574166042
step: 3000 epoch: 504 loss: 16.95758918602043 loss_input: 81.65624373581203
step: 4000 epoch: 504 loss: 16.87087913085091 loss_input: 81.43754570390605
step: 5000 epoch: 504 loss: 16.881847057025972 loss_input: 81.66082485407216
step: 6000 epoch: 504 loss: 16.828571405158083 loss_input: 81.62521975097249
step: 7000 epoch: 504 loss: 16.842096152671353 loss_input: 81.67157030371219
step: 8000 epoch: 504 loss: 16.828821905760925 loss_input: 81.6017315767658
step: 9000 epoch: 504 loss: 16.856419423012955 loss_input: 81.71201639352356
step: 10000 epoch: 504 loss: 16.878359752992978 loss_input: 81.87360378342049
step: 11000 epoch: 504 loss: 16.884169753622267 loss_input: 81.91474139336054
step: 12000 epoch: 504 loss: 16.891026630211687 loss_input: 82.05307219169724
step: 13000 epoch: 504 loss: 16.89735842299419 loss_input: 82.10473087294066
step: 14000 epoch: 504 loss: 16.917172825529732 loss_input: 82.18704322807858
step: 15000 epoch: 504 loss: 16.929973704951117 loss_input: 82.21599400290631
Save loss: 16.94149548818171 Name: 504_train_model.pth
step: 0 epoch: 505 loss: 12.020892143249512 loss_input: 50.5450439453125
step: 1000 epoch: 505 loss: 16.916112232875157 loss_input: 82.39397028752498
step: 2000 epoch: 505 loss: 16.73214376074025 loss_input: 82.09198531837596
step: 3000 epoch: 505 loss: 16.805666389166614 loss_input: 82.26372971704744
step: 4000 epoch: 505 loss: 16.843987316168775 loss_input: 81.8642865528735
step: 5000 epoch: 505 loss: 16.84615791342159 loss_input: 81.94555046510229
step: 6000 epoch: 505 loss: 16.857548760565255 loss_input: 81.84467113147157
step: 7000 epoch: 505 loss: 16.912034599257613 loss_input: 82.00735505453741
step: 8000 epoch: 505 loss: 16.899959481339085 loss_input: 82.08809998756259
step: 9000 epoch: 505 loss: 16.900225931744405 loss_input: 82.08593596750968
step: 10000 epoch: 505 loss: 16.907019601561 loss_input: 82.08493732535926
step: 11000 epoch: 505 loss: 16.93964565696418 loss_input: 82.21574883573261
step: 12000 epoch: 505 loss: 16.935142229541263 loss_input: 82.28860804975237
step: 13000 epoch: 505 loss: 16.95361127879801 loss_input: 82.31070730530494
step: 14000 epoch: 505 loss: 16.950961989374232 loss_input: 82.35008108202929
step: 15000 epoch: 505 loss: 16.942249023487598 loss_input: 82.33367570010307
Save loss: 16.94002473527193 Name: 505_train_model.pth
step: 0 epoch: 506 loss: 31.36419677734375 loss_input: 103.37371826171875
step: 1000 epoch: 506 loss: 16.97499206754473 loss_input: 82.66162091082745
step: 2000 epoch: 506 loss: 16.908891695967203 loss_input: 82.58032391275066
step: 3000 epoch: 506 loss: 16.87684270923275 loss_input: 82.57042918067025
step: 4000 epoch: 506 loss: 16.92715140117225 loss_input: 82.8149867135267
step: 5000 epoch: 506 loss: 16.91092339436356 loss_input: 82.59086543287046
step: 6000 epoch: 506 loss: 16.905387363321005 loss_input: 82.47296342943494
step: 7000 epoch: 506 loss: 16.91862259464866 loss_input: 82.3581877916715
step: 8000 epoch: 506 loss: 16.938067876075362 loss_input: 82.38545770767912
step: 9000 epoch: 506 loss: 16.90993690763549 loss_input: 82.3355509869563
step: 10000 epoch: 506 loss: 16.92570746937891 loss_input: 82.34806687788253
step: 11000 epoch: 506 loss: 16.933171732665603 loss_input: 82.33760161651675
step: 12000 epoch: 506 loss: 16.93443222431707 loss_input: 82.27062693302577
step: 13000 epoch: 506 loss: 16.926218978607345 loss_input: 82.26721837859678
step: 14000 epoch: 506 loss: 16.92744484205568 loss_input: 82.16547574657056
step: 15000 epoch: 506 loss: 16.93074227671537 loss_input: 82.24622850257248
Save loss: 16.94271308799088 Name: 506_train_model.pth
step: 0 epoch: 507 loss: 17.875200271606445 loss_input: 114.19219970703125
step: 1000 epoch: 507 loss: 17.003920968595917 loss_input: 82.60774503816496
step: 2000 epoch: 507 loss: 16.961279766610836 loss_input: 82.40986309213557
step: 3000 epoch: 507 loss: 16.93985475313262 loss_input: 82.21824268212679
step: 4000 epoch: 507 loss: 16.851109908241476 loss_input: 81.98766067760404
step: 5000 epoch: 507 loss: 16.84069137462638 loss_input: 82.0049564061797
step: 6000 epoch: 507 loss: 16.874826030797948 loss_input: 82.0102043743035
step: 7000 epoch: 507 loss: 16.897316629725548 loss_input: 82.09694006987561
step: 8000 epoch: 507 loss: 16.911781996641288 loss_input: 82.0673014399201
step: 9000 epoch: 507 loss: 16.93730186025456 loss_input: 82.25608322708067
step: 10000 epoch: 507 loss: 16.929713480926228 loss_input: 82.34358193208999
step: 11000 epoch: 507 loss: 16.931446684651913 loss_input: 82.33299757062213
step: 12000 epoch: 507 loss: 16.924278009952022 loss_input: 82.20446494571091
step: 13000 epoch: 507 loss: 16.93554903754692 loss_input: 82.15622531083096
step: 14000 epoch: 507 loss: 16.93961598111786 loss_input: 82.2256388294042
step: 15000 epoch: 507 loss: 16.939960366224863 loss_input: 82.20972214319826
Save loss: 16.94231012058258 Name: 507_train_model.pth
step: 0 epoch: 508 loss: 7.749978542327881 loss_input: 44.472900390625
step: 1000 epoch: 508 loss: 16.68388044822228 loss_input: 82.64427918273132
step: 2000 epoch: 508 loss: 16.80821382778993 loss_input: 82.79233210543165
step: 3000 epoch: 508 loss: 16.81170532378464 loss_input: 82.48787057959846
step: 4000 epoch: 508 loss: 16.81772354548349 loss_input: 82.31796172355837
step: 5000 epoch: 508 loss: 16.828947128188346 loss_input: 82.18686921795329
step: 6000 epoch: 508 loss: 16.8542642223102 loss_input: 82.28186975958108
step: 7000 epoch: 508 loss: 16.875262570541224 loss_input: 82.25407264294275
step: 8000 epoch: 508 loss: 16.89336656287467 loss_input: 82.18820167925787
step: 9000 epoch: 508 loss: 16.862803345163083 loss_input: 81.98861236284075
step: 10000 epoch: 508 loss: 16.87626428673737 loss_input: 82.00145965628046
step: 11000 epoch: 508 loss: 16.89003087438114 loss_input: 82.08514785454518
step: 12000 epoch: 508 loss: 16.93656442630451 loss_input: 82.12027353128288
step: 13000 epoch: 508 loss: 16.922792877346613 loss_input: 82.05970718504676
step: 14000 epoch: 508 loss: 16.94045083739572 loss_input: 82.10625503591261
step: 15000 epoch: 508 loss: 16.945851875904296 loss_input: 82.18510023322663
Save loss: 16.94509302687645 Name: 508_train_model.pth
step: 0 epoch: 509 loss: 17.872699737548828 loss_input: 86.4088134765625
step: 1000 epoch: 509 loss: 16.699777623156567 loss_input: 81.59101628352117
step: 2000 epoch: 509 loss: 16.69099656526355 loss_input: 81.9241652672199
step: 3000 epoch: 509 loss: 16.80542752910399 loss_input: 82.1248648318399
step: 4000 epoch: 509 loss: 16.884606313419415 loss_input: 82.31424317077469
step: 5000 epoch: 509 loss: 16.93739063881369 loss_input: 82.33077994996704
step: 6000 epoch: 509 loss: 16.933929547332284 loss_input: 82.31444807009704
step: 7000 epoch: 509 loss: 16.90991308453798 loss_input: 82.28131154080388
step: 8000 epoch: 509 loss: 16.90628803892294 loss_input: 82.1192973208061
step: 9000 epoch: 509 loss: 16.888106104347816 loss_input: 82.11759055851434
step: 10000 epoch: 509 loss: 16.908949484909527 loss_input: 82.34121172233363
step: 11000 epoch: 509 loss: 16.924644860839013 loss_input: 82.31295546396528
step: 12000 epoch: 509 loss: 16.935547129570093 loss_input: 82.20796739025003
step: 13000 epoch: 509 loss: 16.937916852356736 loss_input: 82.21871283244889
step: 14000 epoch: 509 loss: 16.928751827793356 loss_input: 82.2385400946877
step: 15000 epoch: 509 loss: 16.94628060136618 loss_input: 82.24592552797913
Save loss: 16.942793960869313 Name: 509_train_model.pth
step: 0 epoch: 510 loss: 14.723227500915527 loss_input: 114.7445068359375
step: 1000 epoch: 510 loss: 16.62822936083768 loss_input: 82.1343831339559
step: 2000 epoch: 510 loss: 16.73475212838756 loss_input: 81.98451454814585
step: 3000 epoch: 510 loss: 16.80240204762475 loss_input: 82.12964851559897
step: 4000 epoch: 510 loss: 16.794909370747007 loss_input: 81.74184253924312
step: 5000 epoch: 510 loss: 16.795490794314357 loss_input: 81.68441095716403
step: 6000 epoch: 510 loss: 16.832390810684092 loss_input: 81.93609244854048
step: 7000 epoch: 510 loss: 16.828081112693948 loss_input: 82.05262513767565
step: 8000 epoch: 510 loss: 16.85950290556327 loss_input: 82.05855697328606
step: 9000 epoch: 510 loss: 16.866165699713523 loss_input: 82.1389859744859
step: 10000 epoch: 510 loss: 16.885511711208526 loss_input: 82.12284960468797
step: 11000 epoch: 510 loss: 16.882859768818513 loss_input: 82.10690175848715
step: 12000 epoch: 510 loss: 16.90245456256903 loss_input: 82.09509134940252
step: 13000 epoch: 510 loss: 16.893145958613125 loss_input: 82.1592215403127
step: 14000 epoch: 510 loss: 16.92460541067511 loss_input: 82.25889822915829
step: 15000 epoch: 510 loss: 16.927978468357693 loss_input: 82.22283742281765
Save loss: 16.941542257189752 Name: 510_train_model.pth
step: 0 epoch: 511 loss: 18.216638565063477 loss_input: 104.6357421875
step: 1000 epoch: 511 loss: 17.07086600981035 loss_input: 82.79857806987934
step: 2000 epoch: 511 loss: 17.048528894193765 loss_input: 82.34317734931362
step: 3000 epoch: 511 loss: 16.983068334543876 loss_input: 82.07027316578068
step: 4000 epoch: 511 loss: 16.981348113458296 loss_input: 82.33819847040193
step: 5000 epoch: 511 loss: 16.967785999169948 loss_input: 82.2557559777107
step: 6000 epoch: 511 loss: 16.953114561946407 loss_input: 82.29433399080297
step: 7000 epoch: 511 loss: 16.97578862060429 loss_input: 82.38864863264511
step: 8000 epoch: 511 loss: 16.96230854271144 loss_input: 82.39565985495709
step: 9000 epoch: 511 loss: 16.957973545332138 loss_input: 82.51186565208033
step: 10000 epoch: 511 loss: 16.990809098897774 loss_input: 82.55646745506114
step: 11000 epoch: 511 loss: 16.976080689969013 loss_input: 82.55583312561075
step: 12000 epoch: 511 loss: 16.959055965497647 loss_input: 82.51027779952257
step: 13000 epoch: 511 loss: 16.96835440648958 loss_input: 82.51772873409527
step: 14000 epoch: 511 loss: 16.950395388991804 loss_input: 82.4084857106677
step: 15000 epoch: 511 loss: 16.946643154761464 loss_input: 82.36464961580687
Save loss: 16.943969125568866 Name: 511_train_model.pth
step: 0 epoch: 512 loss: 21.957077026367188 loss_input: 105.11297607421875
step: 1000 epoch: 512 loss: 16.822984852157273 loss_input: 82.50220616785558
step: 2000 epoch: 512 loss: 16.825768759702218 loss_input: 82.38140362801997
step: 3000 epoch: 512 loss: 16.808616538160603 loss_input: 82.09096628028287
step: 4000 epoch: 512 loss: 16.943444150652 loss_input: 82.58205010514949
step: 5000 epoch: 512 loss: 16.94021590565043 loss_input: 82.5267413558304
step: 6000 epoch: 512 loss: 16.91883033089907 loss_input: 82.50064730219117
step: 7000 epoch: 512 loss: 16.899984206936733 loss_input: 82.45589684697666
step: 8000 epoch: 512 loss: 16.911606740212534 loss_input: 82.40572726871055
step: 9000 epoch: 512 loss: 16.915403885121425 loss_input: 82.28520360123832
step: 10000 epoch: 512 loss: 16.892920469739487 loss_input: 82.2975205897379
step: 11000 epoch: 512 loss: 16.878962709214317 loss_input: 82.15615182555315
step: 12000 epoch: 512 loss: 16.894990596818126 loss_input: 82.2040626151434
step: 13000 epoch: 512 loss: 16.90348676404314 loss_input: 82.09282215858696
step: 14000 epoch: 512 loss: 16.90522029096591 loss_input: 82.14551918166559
step: 15000 epoch: 512 loss: 16.929019157413354 loss_input: 82.1656632925318
Save loss: 16.934159606054425 Name: 512_train_model.pth
step: 0 epoch: 513 loss: 20.575088500976562 loss_input: 93.57672119140625
step: 1000 epoch: 513 loss: 16.83404668870863 loss_input: 81.14497983705748
step: 2000 epoch: 513 loss: 16.834341441435196 loss_input: 81.40875414953656
step: 3000 epoch: 513 loss: 16.823163765980695 loss_input: 81.45006703901434
step: 4000 epoch: 513 loss: 16.81130027544555 loss_input: 81.72218035578996
step: 5000 epoch: 513 loss: 16.866543914384543 loss_input: 81.9911138621885
step: 6000 epoch: 513 loss: 16.870042913735976 loss_input: 82.01044186320509
step: 7000 epoch: 513 loss: 16.89797375042461 loss_input: 82.16546680552332
step: 8000 epoch: 513 loss: 16.888714788943705 loss_input: 82.21903597499531
step: 9000 epoch: 513 loss: 16.88722460946167 loss_input: 82.24243080657054
step: 10000 epoch: 513 loss: 16.882168861022414 loss_input: 82.1724470131112
step: 11000 epoch: 513 loss: 16.910306390182896 loss_input: 82.20803341719467
step: 12000 epoch: 513 loss: 16.916542686767237 loss_input: 82.1934838051419
step: 13000 epoch: 513 loss: 16.90328236791081 loss_input: 82.16309683378472
step: 14000 epoch: 513 loss: 16.908885916201832 loss_input: 82.1754261423082
step: 15000 epoch: 513 loss: 16.922808016437617 loss_input: 82.25051862997545
Save loss: 16.934458570301533 Name: 513_train_model.pth
step: 0 epoch: 514 loss: 20.935283660888672 loss_input: 90.50054931640625
step: 1000 epoch: 514 loss: 17.065703474439225 loss_input: 84.020328304508
step: 2000 epoch: 514 loss: 16.931232282008487 loss_input: 83.23273156512565
step: 3000 epoch: 514 loss: 16.914155481974706 loss_input: 82.9581641731402
step: 4000 epoch: 514 loss: 16.9368068326327 loss_input: 82.55560258494947
step: 5000 epoch: 514 loss: 16.940293590489972 loss_input: 82.41550176292867
step: 6000 epoch: 514 loss: 16.896910992011968 loss_input: 82.56806771625303
step: 7000 epoch: 514 loss: 16.914667207298066 loss_input: 82.51682937419375
step: 8000 epoch: 514 loss: 16.931943698609388 loss_input: 82.39842343315485
step: 9000 epoch: 514 loss: 16.909211133588833 loss_input: 82.19297745942725
step: 10000 epoch: 514 loss: 16.937707773269075 loss_input: 82.3791185883853
step: 11000 epoch: 514 loss: 16.939524588330464 loss_input: 82.38870490311427
step: 12000 epoch: 514 loss: 16.91720299394555 loss_input: 82.28240513648602
step: 13000 epoch: 514 loss: 16.91619869412188 loss_input: 82.22168855569554
step: 14000 epoch: 514 loss: 16.914721570758427 loss_input: 82.32869247742767
step: 15000 epoch: 514 loss: 16.934904843026438 loss_input: 82.28207473749796
Save loss: 16.92891208255291 Name: 514_train_model.pth
step: 0 epoch: 515 loss: 12.73739242553711 loss_input: 70.33428955078125
step: 1000 epoch: 515 loss: 16.812479516485713 loss_input: 79.8099065851141
step: 2000 epoch: 515 loss: 16.831226707040994 loss_input: 81.78467748547601
step: 3000 epoch: 515 loss: 16.91618620828961 loss_input: 82.31450047249875
step: 4000 epoch: 515 loss: 16.927525479684498 loss_input: 82.41956805110246
step: 5000 epoch: 515 loss: 16.875252217298698 loss_input: 82.38772235787218
step: 6000 epoch: 515 loss: 16.921361512967774 loss_input: 82.47073003701857
step: 7000 epoch: 515 loss: 16.941691941796773 loss_input: 82.62786916670944
step: 8000 epoch: 515 loss: 16.976302911066856 loss_input: 82.69445155415143
step: 9000 epoch: 515 loss: 16.95125578599643 loss_input: 82.67465725650709
step: 10000 epoch: 515 loss: 16.914331499188794 loss_input: 82.49544943576932
step: 11000 epoch: 515 loss: 16.899950507120398 loss_input: 82.38410519586998
step: 12000 epoch: 515 loss: 16.891063924889078 loss_input: 82.27228320776328
step: 13000 epoch: 515 loss: 16.902690069261546 loss_input: 82.23809010259427
step: 14000 epoch: 515 loss: 16.915786557994174 loss_input: 82.20008428004306
step: 15000 epoch: 515 loss: 16.923172026377948 loss_input: 82.18227167830739
Save loss: 16.93045370092988 Name: 515_train_model.pth
step: 0 epoch: 516 loss: 23.09282684326172 loss_input: 92.3701171875
step: 1000 epoch: 516 loss: 16.70801277189226 loss_input: 82.04071801001733
step: 2000 epoch: 516 loss: 16.801527391964648 loss_input: 82.46031333088338
step: 3000 epoch: 516 loss: 16.78779451229778 loss_input: 82.6245565036264
step: 4000 epoch: 516 loss: 16.789691531637317 loss_input: 82.6045746406416
step: 5000 epoch: 516 loss: 16.856340341390645 loss_input: 82.55627308376215
step: 6000 epoch: 516 loss: 16.85895932330586 loss_input: 82.53172359457018
step: 7000 epoch: 516 loss: 16.870465140226244 loss_input: 82.54577254867608
step: 8000 epoch: 516 loss: 16.86389074592557 loss_input: 82.50869208212987
step: 9000 epoch: 516 loss: 16.874410209755357 loss_input: 82.49655936259903
step: 10000 epoch: 516 loss: 16.87464192189904 loss_input: 82.34830430531166
step: 11000 epoch: 516 loss: 16.891018596457066 loss_input: 82.29581670221032
step: 12000 epoch: 516 loss: 16.894787321487236 loss_input: 82.26623591100481
step: 13000 epoch: 516 loss: 16.89680513152067 loss_input: 82.20333431570468
step: 14000 epoch: 516 loss: 16.892912183810232 loss_input: 82.21626444232778
step: 15000 epoch: 516 loss: 16.914372035022037 loss_input: 82.23562705276028
Save loss: 16.931042822420597 Name: 516_train_model.pth
step: 0 epoch: 517 loss: 15.853832244873047 loss_input: 72.86505126953125
step: 1000 epoch: 517 loss: 16.943146993349362 loss_input: 82.71567214523758
step: 2000 epoch: 517 loss: 16.817486704617128 loss_input: 82.65900951501789
step: 3000 epoch: 517 loss: 16.91157220721602 loss_input: 82.94637903901824
step: 4000 epoch: 517 loss: 16.90251661949949 loss_input: 82.53961640815382
step: 5000 epoch: 517 loss: 16.910615706295996 loss_input: 82.545983015311
step: 6000 epoch: 517 loss: 16.894179320816118 loss_input: 82.40211464186308
step: 7000 epoch: 517 loss: 16.919160874157935 loss_input: 82.41049756215072
step: 8000 epoch: 517 loss: 16.923998789196684 loss_input: 82.53784124762726
step: 9000 epoch: 517 loss: 16.91054597958871 loss_input: 82.46530780976593
step: 10000 epoch: 517 loss: 16.93480958453227 loss_input: 82.39728850601269
step: 11000 epoch: 517 loss: 16.907867509444706 loss_input: 82.18348153874328
step: 12000 epoch: 517 loss: 16.93019630777648 loss_input: 82.28636633389274
step: 13000 epoch: 517 loss: 16.909481293219308 loss_input: 82.27997851014165
step: 14000 epoch: 517 loss: 16.909036369649318 loss_input: 82.17678614386779
step: 15000 epoch: 517 loss: 16.92190547062869 loss_input: 82.17889307867756
Save loss: 16.925642376303674 Name: 517_train_model.pth
step: 0 epoch: 518 loss: 17.828712463378906 loss_input: 74.9786376953125
step: 1000 epoch: 518 loss: 17.032136473622355 loss_input: 83.5428687523414
step: 2000 epoch: 518 loss: 16.798222752942376 loss_input: 82.84811430344398
step: 3000 epoch: 518 loss: 16.886770386014213 loss_input: 83.06728348037633
step: 4000 epoch: 518 loss: 16.92103061137334 loss_input: 83.01671962075399
step: 5000 epoch: 518 loss: 16.86491087450311 loss_input: 82.35754451892825
step: 6000 epoch: 518 loss: 16.85836146422534 loss_input: 82.38161016539561
step: 7000 epoch: 518 loss: 16.862550243652304 loss_input: 82.19024770666132
step: 8000 epoch: 518 loss: 16.87515413264992 loss_input: 82.02706446514742
step: 9000 epoch: 518 loss: 16.913822820962448 loss_input: 82.13047850369057
step: 10000 epoch: 518 loss: 16.894850813952722 loss_input: 82.18749339055353
step: 11000 epoch: 518 loss: 16.901767945226762 loss_input: 82.22847600568805
step: 12000 epoch: 518 loss: 16.91908033791507 loss_input: 82.34752680013482
step: 13000 epoch: 518 loss: 16.91608619513892 loss_input: 82.32442745070541
step: 14000 epoch: 518 loss: 16.946827042027035 loss_input: 82.306483178622
step: 15000 epoch: 518 loss: 16.941242646506673 loss_input: 82.24088634976418
Save loss: 16.92997026245296 Name: 518_train_model.pth
step: 0 epoch: 519 loss: 18.329429626464844 loss_input: 71.73828125
step: 1000 epoch: 519 loss: 16.90492048749438 loss_input: 81.73970993606004
step: 2000 epoch: 519 loss: 16.923536123602705 loss_input: 81.53363988781976
step: 3000 epoch: 519 loss: 16.891542782905855 loss_input: 81.68669511285951
step: 4000 epoch: 519 loss: 16.883075800099334 loss_input: 81.49822538872118
step: 5000 epoch: 519 loss: 16.903501416845003 loss_input: 81.94557412980295
step: 6000 epoch: 519 loss: 16.91483079777263 loss_input: 81.94318390782526
step: 7000 epoch: 519 loss: 16.903195091186806 loss_input: 81.92562386281591
step: 8000 epoch: 519 loss: 16.922668533703042 loss_input: 82.14304517629877
step: 9000 epoch: 519 loss: 16.920708671594618 loss_input: 82.04212977936263
step: 10000 epoch: 519 loss: 16.929322311227626 loss_input: 82.05149782873275
step: 11000 epoch: 519 loss: 16.918909053479137 loss_input: 82.02690679465042
step: 12000 epoch: 519 loss: 16.93057358345383 loss_input: 82.09718965476836
step: 13000 epoch: 519 loss: 16.920846592544656 loss_input: 82.20087570355916
step: 14000 epoch: 519 loss: 16.922925787511513 loss_input: 82.14252712505049
step: 15000 epoch: 519 loss: 16.92469635890902 loss_input: 82.21488417699237
Save loss: 16.930380860686302 Name: 519_train_model.pth
step: 0 epoch: 520 loss: 18.091651916503906 loss_input: 82.86358642578125
step: 1000 epoch: 520 loss: 16.921118815581163 loss_input: 82.55128977467845
step: 2000 epoch: 520 loss: 16.84415866624469 loss_input: 82.2402029271009
step: 3000 epoch: 520 loss: 16.889060641161326 loss_input: 82.33082563787332
step: 4000 epoch: 520 loss: 16.93913475837269 loss_input: 82.2394682091196
step: 5000 epoch: 520 loss: 16.92089820704301 loss_input: 82.28497847793723
step: 6000 epoch: 520 loss: 16.906967975719436 loss_input: 82.3686185460789
step: 7000 epoch: 520 loss: 16.899017815997883 loss_input: 82.15668844880419
step: 8000 epoch: 520 loss: 16.908213658506252 loss_input: 82.22766466478068
step: 9000 epoch: 520 loss: 16.903071593369475 loss_input: 82.23030392592013
step: 10000 epoch: 520 loss: 16.92090388720375 loss_input: 82.3555619962906
step: 11000 epoch: 520 loss: 16.906003152313108 loss_input: 82.21103873147541
step: 12000 epoch: 520 loss: 16.89053809377573 loss_input: 82.17257757308472
step: 13000 epoch: 520 loss: 16.900339048868435 loss_input: 82.16478288009546
step: 14000 epoch: 520 loss: 16.925129697813443 loss_input: 82.25890514743506
step: 15000 epoch: 520 loss: 16.938384329618657 loss_input: 82.30072908876387
Save loss: 16.92999091911316 Name: 520_train_model.pth
step: 0 epoch: 521 loss: 15.400858879089355 loss_input: 59.5589599609375
step: 1000 epoch: 521 loss: 16.969232475364603 loss_input: 81.84021892414226
step: 2000 epoch: 521 loss: 16.929273749517833 loss_input: 82.16699413964892
step: 3000 epoch: 521 loss: 16.830181459473277 loss_input: 81.92210504620205
step: 4000 epoch: 521 loss: 16.89645939616971 loss_input: 82.28967816029392
step: 5000 epoch: 521 loss: 16.921142804338988 loss_input: 82.180456535646
step: 6000 epoch: 521 loss: 16.92554180349157 loss_input: 82.31706023442707
step: 7000 epoch: 521 loss: 16.910810173111223 loss_input: 82.23335606771577
step: 8000 epoch: 521 loss: 16.93251881818744 loss_input: 82.28011222565685
step: 9000 epoch: 521 loss: 16.948870351587107 loss_input: 82.32109899979646
step: 10000 epoch: 521 loss: 16.939521566389466 loss_input: 82.30798781694097
step: 11000 epoch: 521 loss: 16.94444186940387 loss_input: 82.2707506437625
step: 12000 epoch: 521 loss: 16.947336466707156 loss_input: 82.30656525286304
step: 13000 epoch: 521 loss: 16.932854696912276 loss_input: 82.25220850931389
step: 14000 epoch: 521 loss: 16.933468598330432 loss_input: 82.18855072799627
step: 15000 epoch: 521 loss: 16.938280950537617 loss_input: 82.26719686786505
Save loss: 16.92704522995651 Name: 521_train_model.pth
step: 0 epoch: 522 loss: 8.195219039916992 loss_input: 64.34527587890625
step: 1000 epoch: 522 loss: 16.835137079050252 loss_input: 82.26376626303384
step: 2000 epoch: 522 loss: 16.843322054497424 loss_input: 82.2974780005017
step: 3000 epoch: 522 loss: 16.851001961634026 loss_input: 82.41981558718709
step: 4000 epoch: 522 loss: 16.89209401884606 loss_input: 82.46990371852837
step: 5000 epoch: 522 loss: 16.87865945144406 loss_input: 82.38352967834167
step: 6000 epoch: 522 loss: 16.893451702076284 loss_input: 82.40062810373075
step: 7000 epoch: 522 loss: 16.897831141276114 loss_input: 82.35723414024001
step: 8000 epoch: 522 loss: 16.905786789055096 loss_input: 82.37191376926273
step: 9000 epoch: 522 loss: 16.91733832139993 loss_input: 82.42298598766804
step: 10000 epoch: 522 loss: 16.927294010449 loss_input: 82.53232850543071
step: 11000 epoch: 522 loss: 16.918419350495004 loss_input: 82.39452759377166
step: 12000 epoch: 522 loss: 16.936802692189236 loss_input: 82.44747757226286
step: 13000 epoch: 522 loss: 16.9258164679873 loss_input: 82.36500883927648
step: 14000 epoch: 522 loss: 16.91321934656487 loss_input: 82.27532379627739
step: 15000 epoch: 522 loss: 16.927427666289482 loss_input: 82.3054689217008
Save loss: 16.925265847593547 Name: 522_train_model.pth
step: 0 epoch: 523 loss: 15.60542106628418 loss_input: 82.42584228515625
step: 1000 epoch: 523 loss: 16.83121801827933 loss_input: 81.33779050539304
step: 2000 epoch: 523 loss: 16.870095621401642 loss_input: 82.05950686765992
step: 3000 epoch: 523 loss: 16.84792179427358 loss_input: 81.78522173923797
step: 4000 epoch: 523 loss: 16.799793787343418 loss_input: 81.8012072909507
step: 5000 epoch: 523 loss: 16.863863396563545 loss_input: 81.96253604503708
step: 6000 epoch: 523 loss: 16.86992125295834 loss_input: 81.923088003806
step: 7000 epoch: 523 loss: 16.879364487580307 loss_input: 81.98764973098152
step: 8000 epoch: 523 loss: 16.85958114401607 loss_input: 81.7785899778885
step: 9000 epoch: 523 loss: 16.896492720100564 loss_input: 81.89926870647184
step: 10000 epoch: 523 loss: 16.896898082751843 loss_input: 81.97118928854185
step: 11000 epoch: 523 loss: 16.895801351348027 loss_input: 82.01861231054635
step: 12000 epoch: 523 loss: 16.88864904534329 loss_input: 81.98414940743056
step: 13000 epoch: 523 loss: 16.902389714263254 loss_input: 82.12184003048444
step: 14000 epoch: 523 loss: 16.901557297410644 loss_input: 82.13323191671438
step: 15000 epoch: 523 loss: 16.925345611861527 loss_input: 82.22793586221856
Save loss: 16.922443887010218 Name: 523_train_model.pth
step: 0 epoch: 524 loss: 16.052295684814453 loss_input: 85.21795654296875
step: 1000 epoch: 524 loss: 16.92509651850987 loss_input: 82.21199698738761
step: 2000 epoch: 524 loss: 16.84171346805502 loss_input: 82.24678966058963
step: 3000 epoch: 524 loss: 16.925698305916207 loss_input: 82.86221784307574
step: 4000 epoch: 524 loss: 16.93777001312273 loss_input: 83.04989772759447
step: 5000 epoch: 524 loss: 16.932207755436064 loss_input: 82.77841920629545
step: 6000 epoch: 524 loss: 16.91841066314705 loss_input: 82.6977024621873
step: 7000 epoch: 524 loss: 16.936992694847653 loss_input: 82.55433039071985
step: 8000 epoch: 524 loss: 16.932501788855106 loss_input: 82.47884898787066
step: 9000 epoch: 524 loss: 16.948122517848834 loss_input: 82.53614856460176
step: 10000 epoch: 524 loss: 16.918041544167497 loss_input: 82.3497146257054
step: 11000 epoch: 524 loss: 16.92450456521303 loss_input: 82.37587225180346
step: 12000 epoch: 524 loss: 16.934370375208015 loss_input: 82.31777616656132
step: 13000 epoch: 524 loss: 16.94135343869845 loss_input: 82.26562071847839
step: 14000 epoch: 524 loss: 16.93884283084119 loss_input: 82.20866622581507
step: 15000 epoch: 524 loss: 16.934248295301405 loss_input: 82.18565106320386
Save loss: 16.94027920769155 Name: 524_train_model.pth
step: 0 epoch: 525 loss: 22.635894775390625 loss_input: 169.6983642578125
step: 1000 epoch: 525 loss: 16.903770619458133 loss_input: 83.03679462627217
step: 2000 epoch: 525 loss: 16.852951949027585 loss_input: 82.52113933267741
step: 3000 epoch: 525 loss: 16.816269013930146 loss_input: 82.22767583738363
step: 4000 epoch: 525 loss: 16.772259996641104 loss_input: 81.96869688217593
step: 5000 epoch: 525 loss: 16.826193518172357 loss_input: 82.12429783330444
step: 6000 epoch: 525 loss: 16.84213016021651 loss_input: 82.21941578259093
step: 7000 epoch: 525 loss: 16.87907009035532 loss_input: 82.38073874316783
step: 8000 epoch: 525 loss: 16.866625264083993 loss_input: 82.55608969899271
step: 9000 epoch: 525 loss: 16.859396841244994 loss_input: 82.37788462143529
step: 10000 epoch: 525 loss: 16.859617034955782 loss_input: 82.40973612599677
step: 11000 epoch: 525 loss: 16.886072572583817 loss_input: 82.3708137683506
step: 12000 epoch: 525 loss: 16.90874828850068 loss_input: 82.43669108201757
step: 13000 epoch: 525 loss: 16.925146584363727 loss_input: 82.50155917806502
step: 14000 epoch: 525 loss: 16.928015911632773 loss_input: 82.47017798986735
step: 15000 epoch: 525 loss: 16.920914399799493 loss_input: 82.37812469809955
Save loss: 16.92059969007969 Name: 525_train_model.pth
step: 0 epoch: 526 loss: 24.836956024169922 loss_input: 104.51983642578125
step: 1000 epoch: 526 loss: 16.906057357788086 loss_input: 82.01172015240618
step: 2000 epoch: 526 loss: 16.891249060213774 loss_input: 82.3608288629123
step: 3000 epoch: 526 loss: 16.985148660582887 loss_input: 82.7963577163732
step: 4000 epoch: 526 loss: 16.998501625218353 loss_input: 82.87946014289885
step: 5000 epoch: 526 loss: 16.9149648749907 loss_input: 82.50694721895465
step: 6000 epoch: 526 loss: 16.937054207833125 loss_input: 82.76256482073951
step: 7000 epoch: 526 loss: 16.932944297688362 loss_input: 82.54736424895495
step: 8000 epoch: 526 loss: 16.936603349203647 loss_input: 82.41722742573438
step: 9000 epoch: 526 loss: 16.95265787008828 loss_input: 82.44317759859365
step: 10000 epoch: 526 loss: 16.92789438771863 loss_input: 82.27022616341881
step: 11000 epoch: 526 loss: 16.922992972848242 loss_input: 82.35027084163336
step: 12000 epoch: 526 loss: 16.917956295475125 loss_input: 82.3706870608683
step: 13000 epoch: 526 loss: 16.913171145065263 loss_input: 82.2449490767462
step: 14000 epoch: 526 loss: 16.925415853561397 loss_input: 82.2605461129869
step: 15000 epoch: 526 loss: 16.939233149586418 loss_input: 82.24862405006333
Save loss: 16.922866083517672 Name: 526_train_model.pth
step: 0 epoch: 527 loss: 22.14922523498535 loss_input: 78.5150146484375
step: 1000 epoch: 527 loss: 16.967828813013615 loss_input: 81.41830063247299
step: 2000 epoch: 527 loss: 17.007443269332132 loss_input: 81.99116515350723
step: 3000 epoch: 527 loss: 16.991998098405826 loss_input: 82.79853167433772
step: 4000 epoch: 527 loss: 16.950727955694944 loss_input: 82.51284176026306
step: 5000 epoch: 527 loss: 16.90484144219969 loss_input: 82.49063468556288
step: 6000 epoch: 527 loss: 16.9034774881505 loss_input: 82.3195386116474
step: 7000 epoch: 527 loss: 16.866685728569777 loss_input: 82.07043555545357
step: 8000 epoch: 527 loss: 16.84811543101237 loss_input: 81.99382284703425
step: 9000 epoch: 527 loss: 16.8692035685644 loss_input: 82.09080995123806
step: 10000 epoch: 527 loss: 16.87298491210678 loss_input: 82.09893497954891
step: 11000 epoch: 527 loss: 16.86753893319005 loss_input: 82.13941489938237
step: 12000 epoch: 527 loss: 16.885570875575826 loss_input: 82.09765042671411
step: 13000 epoch: 527 loss: 16.907346858637908 loss_input: 82.15574145663675
step: 14000 epoch: 527 loss: 16.908229923684225 loss_input: 82.16081826849143
step: 15000 epoch: 527 loss: 16.92481097875615 loss_input: 82.23828379703072
Save loss: 16.929856030374765 Name: 527_train_model.pth
step: 0 epoch: 528 loss: 24.069278717041016 loss_input: 213.502685546875
step: 1000 epoch: 528 loss: 16.870276847919385 loss_input: 83.09844763390906
step: 2000 epoch: 528 loss: 16.932337826934234 loss_input: 82.24611311897762
step: 3000 epoch: 528 loss: 16.83226071735574 loss_input: 81.77595230707603
step: 4000 epoch: 528 loss: 16.838683182941143 loss_input: 81.81586896976927
step: 5000 epoch: 528 loss: 16.82527900113985 loss_input: 82.13311829821535
step: 6000 epoch: 528 loss: 16.832360701369478 loss_input: 82.09677356426924
step: 7000 epoch: 528 loss: 16.843311344107768 loss_input: 82.20010858521327
step: 8000 epoch: 528 loss: 16.830575351103622 loss_input: 82.16763432677605
step: 9000 epoch: 528 loss: 16.85478770572627 loss_input: 82.23113998753827
step: 10000 epoch: 528 loss: 16.86583232672235 loss_input: 82.23625572013111
step: 11000 epoch: 528 loss: 16.868469089239838 loss_input: 82.18137815884813
step: 12000 epoch: 528 loss: 16.882949642614168 loss_input: 82.20411723705642
step: 13000 epoch: 528 loss: 16.890570909717837 loss_input: 82.23422061640541
step: 14000 epoch: 528 loss: 16.882500360526492 loss_input: 82.16845618553752
step: 15000 epoch: 528 loss: 16.899676544572042 loss_input: 82.1999858985638
Save loss: 16.91786274732649 Name: 528_train_model.pth
step: 0 epoch: 529 loss: 16.424449920654297 loss_input: 100.97283935546875
step: 1000 epoch: 529 loss: 16.893887180667537 loss_input: 82.51782287536682
step: 2000 epoch: 529 loss: 16.765545265487525 loss_input: 81.36545643194027
step: 3000 epoch: 529 loss: 16.81369748738399 loss_input: 81.69165935980324
step: 4000 epoch: 529 loss: 16.894238522576558 loss_input: 82.34544171574294
step: 5000 epoch: 529 loss: 16.93336760385159 loss_input: 82.43249316445305
step: 6000 epoch: 529 loss: 16.93578454375843 loss_input: 82.19540977382675
step: 7000 epoch: 529 loss: 16.91470615197345 loss_input: 82.23493224112515
step: 8000 epoch: 529 loss: 16.92141935590237 loss_input: 82.27199761650887
step: 9000 epoch: 529 loss: 16.923704310081202 loss_input: 82.22072302703128
step: 10000 epoch: 529 loss: 16.8972821613274 loss_input: 82.09358167576796
step: 11000 epoch: 529 loss: 16.907041203313586 loss_input: 82.15006159773827
step: 12000 epoch: 529 loss: 16.89625636776947 loss_input: 82.2324853157582
step: 13000 epoch: 529 loss: 16.89243538588178 loss_input: 82.22028884553201
step: 14000 epoch: 529 loss: 16.90206708447285 loss_input: 82.2721836179182
step: 15000 epoch: 529 loss: 16.90867944461776 loss_input: 82.21712455090885
Save loss: 16.917526899442077 Name: 529_train_model.pth
step: 0 epoch: 530 loss: 14.731239318847656 loss_input: 71.86065673828125
step: 1000 epoch: 530 loss: 16.82687370879548 loss_input: 82.56742366520199
step: 2000 epoch: 530 loss: 16.748091975788306 loss_input: 81.78913948787326
step: 3000 epoch: 530 loss: 16.82718763952055 loss_input: 81.93971569210002
step: 4000 epoch: 530 loss: 16.91195293689662 loss_input: 82.57652867361773
step: 5000 epoch: 530 loss: 16.90610250218633 loss_input: 82.51947585336448
step: 6000 epoch: 530 loss: 16.896104285010217 loss_input: 82.4863346750151
step: 7000 epoch: 530 loss: 16.881187473564655 loss_input: 82.45680038698434
step: 8000 epoch: 530 loss: 16.873435885917246 loss_input: 82.50394088312203
step: 9000 epoch: 530 loss: 16.878426251656716 loss_input: 82.55053582777911
step: 10000 epoch: 530 loss: 16.889761730165866 loss_input: 82.52639651196013
step: 11000 epoch: 530 loss: 16.897149832224198 loss_input: 82.45493427266555
step: 12000 epoch: 530 loss: 16.904181282457316 loss_input: 82.4482789428599
step: 13000 epoch: 530 loss: 16.902557293641255 loss_input: 82.42680331305645
step: 14000 epoch: 530 loss: 16.903128989738494 loss_input: 82.3340936244858
step: 15000 epoch: 530 loss: 16.905053000650074 loss_input: 82.27292347176855
Save loss: 16.917806014612317 Name: 530_train_model.pth
step: 0 epoch: 531 loss: 20.6751766204834 loss_input: 66.30780029296875
step: 1000 epoch: 531 loss: 16.89407258124261 loss_input: 81.59956022980926
step: 2000 epoch: 531 loss: 16.993508945400272 loss_input: 82.15964661712113
step: 3000 epoch: 531 loss: 16.936737597524942 loss_input: 82.3210312342175
step: 4000 epoch: 531 loss: 16.873381449800227 loss_input: 82.12173886765125
step: 5000 epoch: 531 loss: 16.8961372638173 loss_input: 82.25467300338761
step: 6000 epoch: 531 loss: 16.862989360859864 loss_input: 82.21101014007431
step: 7000 epoch: 531 loss: 16.90473206805052 loss_input: 82.35845417207419
step: 8000 epoch: 531 loss: 16.882660641966424 loss_input: 82.26451987538422
step: 9000 epoch: 531 loss: 16.890378018774307 loss_input: 82.18584458497666
step: 10000 epoch: 531 loss: 16.875611981538853 loss_input: 82.09834620785968
step: 11000 epoch: 531 loss: 16.869373930073817 loss_input: 81.9331728509855
step: 12000 epoch: 531 loss: 16.891634503540104 loss_input: 82.02357980178084
step: 13000 epoch: 531 loss: 16.882079544493568 loss_input: 82.0942238076413
step: 14000 epoch: 531 loss: 16.89427402983154 loss_input: 82.20228733051573
step: 15000 epoch: 531 loss: 16.905977042895397 loss_input: 82.22393045297632
Save loss: 16.913550320908428 Name: 531_train_model.pth
step: 0 epoch: 532 loss: 15.603872299194336 loss_input: 113.04034423828125
step: 1000 epoch: 532 loss: 16.85676011219844 loss_input: 83.55727054927495
step: 2000 epoch: 532 loss: 16.780107193860573 loss_input: 82.0846036637443
step: 3000 epoch: 532 loss: 16.79058990999684 loss_input: 81.9884884969984
step: 4000 epoch: 532 loss: 16.8626101599786 loss_input: 82.0335680392646
step: 5000 epoch: 532 loss: 16.918626616559393 loss_input: 82.06605030312296
step: 6000 epoch: 532 loss: 16.940743494502627 loss_input: 82.09280969651218
step: 7000 epoch: 532 loss: 16.96486380971716 loss_input: 82.38789356124349
step: 8000 epoch: 532 loss: 16.931847580908656 loss_input: 82.31991148811477
step: 9000 epoch: 532 loss: 16.922063332347786 loss_input: 82.17823425164131
step: 10000 epoch: 532 loss: 16.912941495271554 loss_input: 82.19436109977869
step: 11000 epoch: 532 loss: 16.928527532604736 loss_input: 82.28612179942981
step: 12000 epoch: 532 loss: 16.93482964281817 loss_input: 82.26043997167716
step: 13000 epoch: 532 loss: 16.9367100682848 loss_input: 82.37219681456294
step: 14000 epoch: 532 loss: 16.932730980612842 loss_input: 82.34940567599664
step: 15000 epoch: 532 loss: 16.931809434540455 loss_input: 82.25016559361538
Save loss: 16.926250794887544 Name: 532_train_model.pth
step: 0 epoch: 533 loss: 19.817310333251953 loss_input: 50.144775390625
step: 1000 epoch: 533 loss: 16.77684235596633 loss_input: 82.52693436934159
step: 2000 epoch: 533 loss: 16.721528653798252 loss_input: 82.13960677132137
step: 3000 epoch: 533 loss: 16.777899049671838 loss_input: 82.46803860789893
step: 4000 epoch: 533 loss: 16.811590139283684 loss_input: 82.31256628286776
step: 5000 epoch: 533 loss: 16.8115839778936 loss_input: 82.4387230952247
step: 6000 epoch: 533 loss: 16.842096024007724 loss_input: 82.54002782373185
step: 7000 epoch: 533 loss: 16.838939456288568 loss_input: 82.38950235764311
step: 8000 epoch: 533 loss: 16.863519222255707 loss_input: 82.58301063502316
step: 9000 epoch: 533 loss: 16.854198019049747 loss_input: 82.36121360186324
step: 10000 epoch: 533 loss: 16.858722077502144 loss_input: 82.46318914251152
step: 11000 epoch: 533 loss: 16.865920897993476 loss_input: 82.33224616853902
step: 12000 epoch: 533 loss: 16.88094963460016 loss_input: 82.32231362515037
step: 13000 epoch: 533 loss: 16.888265511501974 loss_input: 82.24203472667067
step: 14000 epoch: 533 loss: 16.90818801323658 loss_input: 82.20443210965539
step: 15000 epoch: 533 loss: 16.91224778813447 loss_input: 82.25488537580567
Save loss: 16.914008970797063 Name: 533_train_model.pth
step: 0 epoch: 534 loss: 15.072854042053223 loss_input: 64.71563720703125
step: 1000 epoch: 534 loss: 17.22770745890005 loss_input: 83.91072190748704
step: 2000 epoch: 534 loss: 16.906691137758987 loss_input: 82.44540080423656
step: 3000 epoch: 534 loss: 16.866921076731696 loss_input: 82.24005505244996
step: 4000 epoch: 534 loss: 16.875648941644517 loss_input: 81.93340056099257
step: 5000 epoch: 534 loss: 16.898428099986386 loss_input: 82.13086131552986
step: 6000 epoch: 534 loss: 16.891903603361 loss_input: 82.34634298214296
step: 7000 epoch: 534 loss: 16.87482148094733 loss_input: 82.31574250902214
step: 8000 epoch: 534 loss: 16.885891175213462 loss_input: 82.28964149104165
step: 9000 epoch: 534 loss: 16.870309121554964 loss_input: 82.21368010162499
step: 10000 epoch: 534 loss: 16.90105277389875 loss_input: 82.09908570300196
step: 11000 epoch: 534 loss: 16.901466171044284 loss_input: 82.05640667631955
step: 12000 epoch: 534 loss: 16.912543845071404 loss_input: 82.13176266581463
step: 13000 epoch: 534 loss: 16.907593045085406 loss_input: 82.13978532882537
step: 14000 epoch: 534 loss: 16.914523336889232 loss_input: 82.14269386990634
step: 15000 epoch: 534 loss: 16.919028915792186 loss_input: 82.23833310201195
Save loss: 16.924123069807887 Name: 534_train_model.pth
step: 0 epoch: 535 loss: 14.751690864562988 loss_input: 70.5838623046875
step: 1000 epoch: 535 loss: 16.880293042986068 loss_input: 82.52627249459525
step: 2000 epoch: 535 loss: 16.81122906478508 loss_input: 82.200231976297
step: 3000 epoch: 535 loss: 16.851515010292232 loss_input: 82.03601645724211
step: 4000 epoch: 535 loss: 16.858313432010345 loss_input: 81.70366807712135
step: 5000 epoch: 535 loss: 16.86885278622071 loss_input: 81.80148302614867
step: 6000 epoch: 535 loss: 16.859743867351938 loss_input: 81.64915085156706
step: 7000 epoch: 535 loss: 16.858286925782956 loss_input: 81.74662150663744
step: 8000 epoch: 535 loss: 16.877847710664145 loss_input: 81.91256716537141
step: 9000 epoch: 535 loss: 16.878141783089283 loss_input: 81.99587549218707
step: 10000 epoch: 535 loss: 16.885154480290954 loss_input: 81.96205990362878
step: 11000 epoch: 535 loss: 16.91271908180723 loss_input: 82.1618248050337
step: 12000 epoch: 535 loss: 16.913485909867173 loss_input: 82.20156208601269
step: 13000 epoch: 535 loss: 16.92033893806587 loss_input: 82.18845490138078
step: 14000 epoch: 535 loss: 16.91374924696988 loss_input: 82.20773945572665
step: 15000 epoch: 535 loss: 16.920379638449365 loss_input: 82.22225197600758
Save loss: 16.91643324238062 Name: 535_train_model.pth
step: 0 epoch: 536 loss: 14.037676811218262 loss_input: 111.66204833984375
step: 1000 epoch: 536 loss: 16.63476500430188 loss_input: 81.67229718523664
step: 2000 epoch: 536 loss: 16.740915733596673 loss_input: 82.08789562738163
step: 3000 epoch: 536 loss: 16.788509097030982 loss_input: 81.80805149097436
step: 4000 epoch: 536 loss: 16.8297725176698 loss_input: 81.8909102375285
step: 5000 epoch: 536 loss: 16.897172469135477 loss_input: 82.03912680598646
step: 6000 epoch: 536 loss: 16.876739146132962 loss_input: 81.97741801944142
step: 7000 epoch: 536 loss: 16.877197492567067 loss_input: 82.05700860570421
step: 8000 epoch: 536 loss: 16.906163476196145 loss_input: 82.19473767408712
step: 9000 epoch: 536 loss: 16.91732479601274 loss_input: 82.19394703756875
step: 10000 epoch: 536 loss: 16.898947849498725 loss_input: 82.19753373173425
step: 11000 epoch: 536 loss: 16.88983913417643 loss_input: 82.20706385645516
step: 12000 epoch: 536 loss: 16.894075478665897 loss_input: 82.10904482223324
step: 13000 epoch: 536 loss: 16.903555280033896 loss_input: 82.13729988205534
step: 14000 epoch: 536 loss: 16.89971606651891 loss_input: 82.12874345906793
step: 15000 epoch: 536 loss: 16.907066201330878 loss_input: 82.16326990554145
Save loss: 16.913135196760297 Name: 536_train_model.pth
step: 0 epoch: 537 loss: 12.402473449707031 loss_input: 48.51605224609375
step: 1000 epoch: 537 loss: 17.00824282719539 loss_input: 81.35365812166349
step: 2000 epoch: 537 loss: 16.839586990228717 loss_input: 81.27052227060298
step: 3000 epoch: 537 loss: 16.81670825420241 loss_input: 81.24646286931049
step: 4000 epoch: 537 loss: 16.80380240138606 loss_input: 81.39734586922117
step: 5000 epoch: 537 loss: 16.827855744950178 loss_input: 81.67725294052518
step: 6000 epoch: 537 loss: 16.850949468463288 loss_input: 81.85923464253354
step: 7000 epoch: 537 loss: 16.871927865521087 loss_input: 81.96846696809774
step: 8000 epoch: 537 loss: 16.86265485508116 loss_input: 82.00050436963203
step: 9000 epoch: 537 loss: 16.894053406006044 loss_input: 82.1120239501926
step: 10000 epoch: 537 loss: 16.88551474830983 loss_input: 82.22294121295879
step: 11000 epoch: 537 loss: 16.895998529797954 loss_input: 82.26643815849837
step: 12000 epoch: 537 loss: 16.901316310254625 loss_input: 82.27082531635547
step: 13000 epoch: 537 loss: 16.913240989697236 loss_input: 82.23934060266849
step: 14000 epoch: 537 loss: 16.917812611305596 loss_input: 82.21207136937903
step: 15000 epoch: 537 loss: 16.91047620053339 loss_input: 82.17660670359606
Save loss: 16.91310708387196 Name: 537_train_model.pth
step: 0 epoch: 538 loss: 18.446426391601562 loss_input: 89.03265380859375
step: 1000 epoch: 538 loss: 16.93432305909537 loss_input: 83.17637026059877
step: 2000 epoch: 538 loss: 16.94778716224602 loss_input: 83.0418362901069
step: 3000 epoch: 538 loss: 16.940146945866296 loss_input: 82.64534205557783
step: 4000 epoch: 538 loss: 16.931453843499327 loss_input: 82.79410397949829
step: 5000 epoch: 538 loss: 16.89158550578817 loss_input: 82.66711141414295
step: 6000 epoch: 538 loss: 16.92171451179728 loss_input: 82.49864624206512
step: 7000 epoch: 538 loss: 16.892662035637354 loss_input: 82.31049893437377
step: 8000 epoch: 538 loss: 16.85142524646768 loss_input: 82.10455031622024
step: 9000 epoch: 538 loss: 16.864561179838848 loss_input: 82.16020129572192
step: 10000 epoch: 538 loss: 16.85150843364646 loss_input: 82.18462699628475
step: 11000 epoch: 538 loss: 16.84756124566852 loss_input: 82.00362377723296
step: 12000 epoch: 538 loss: 16.87123815970226 loss_input: 82.0055046580591
step: 13000 epoch: 538 loss: 16.882618978641354 loss_input: 82.0292776691832
step: 14000 epoch: 538 loss: 16.88843522886831 loss_input: 82.06517947867005
step: 15000 epoch: 538 loss: 16.903637538805906 loss_input: 82.10126496937964
Save loss: 16.91445663046837 Name: 538_train_model.pth
step: 0 epoch: 539 loss: 16.30352020263672 loss_input: 75.23321533203125
step: 1000 epoch: 539 loss: 16.88473602584549 loss_input: 82.7876333261465
step: 2000 epoch: 539 loss: 16.89575281386254 loss_input: 82.28286427464978
step: 3000 epoch: 539 loss: 16.85542369278142 loss_input: 82.26980006094973
step: 4000 epoch: 539 loss: 16.851255066244043 loss_input: 82.43030445696681
step: 5000 epoch: 539 loss: 16.8478285750969 loss_input: 82.31593760593584
step: 6000 epoch: 539 loss: 16.872144051262428 loss_input: 82.14511686610174
step: 7000 epoch: 539 loss: 16.84262065774389 loss_input: 82.05658383554024
step: 8000 epoch: 539 loss: 16.878434320253636 loss_input: 82.27225210129984
step: 9000 epoch: 539 loss: 16.893702812743655 loss_input: 82.18813029427938
step: 10000 epoch: 539 loss: 16.9002603936727 loss_input: 82.28271676006227
step: 11000 epoch: 539 loss: 16.88664235683823 loss_input: 82.29310254909355
step: 12000 epoch: 539 loss: 16.909980158261106 loss_input: 82.40703842205045
step: 13000 epoch: 539 loss: 16.90835063966015 loss_input: 82.390521332719
step: 14000 epoch: 539 loss: 16.912073981871767 loss_input: 82.36357856152713
step: 15000 epoch: 539 loss: 16.913284010366155 loss_input: 82.34385199922218
Save loss: 16.909361534953117 Name: 539_train_model.pth
step: 0 epoch: 540 loss: 14.275861740112305 loss_input: 48.93487548828125
step: 1000 epoch: 540 loss: 16.800870430457604 loss_input: 82.44812170251623
step: 2000 epoch: 540 loss: 16.78213311206812 loss_input: 82.43990211925288
step: 3000 epoch: 540 loss: 16.803667363942523 loss_input: 82.54276664858459
step: 4000 epoch: 540 loss: 16.86454183028597 loss_input: 82.5578326487714
step: 5000 epoch: 540 loss: 16.85503029136795 loss_input: 82.46823200057254
step: 6000 epoch: 540 loss: 16.86859423958089 loss_input: 82.19152740493772
step: 7000 epoch: 540 loss: 16.912295829941588 loss_input: 82.20388398581174
step: 8000 epoch: 540 loss: 16.894291509465713 loss_input: 82.19947260163036
step: 9000 epoch: 540 loss: 16.91295052228961 loss_input: 82.26064628479865
step: 10000 epoch: 540 loss: 16.93959402842541 loss_input: 82.37782851353536
step: 11000 epoch: 540 loss: 16.924041983171243 loss_input: 82.21354745893129
step: 12000 epoch: 540 loss: 16.91528292648316 loss_input: 82.16139554695312
step: 13000 epoch: 540 loss: 16.913350641594494 loss_input: 82.21385185477018
step: 14000 epoch: 540 loss: 16.92090087946411 loss_input: 82.31543873749531
step: 15000 epoch: 540 loss: 16.91630809768487 loss_input: 82.23427704510455
Save loss: 16.916605853691696 Name: 540_train_model.pth
step: 0 epoch: 541 loss: 15.15051555633545 loss_input: 120.2286376953125
step: 1000 epoch: 541 loss: 16.753048586678673 loss_input: 81.60271686321491
step: 2000 epoch: 541 loss: 16.819542160396395 loss_input: 82.18090188914331
step: 3000 epoch: 541 loss: 16.831913984445524 loss_input: 82.17064595818321
step: 4000 epoch: 541 loss: 16.798663863120332 loss_input: 82.15333498778625
step: 5000 epoch: 541 loss: 16.815622967020367 loss_input: 82.2294285649706
step: 6000 epoch: 541 loss: 16.846425194955632 loss_input: 82.13503084975746
step: 7000 epoch: 541 loss: 16.852074599644062 loss_input: 81.9093775613668
step: 8000 epoch: 541 loss: 16.852922817630002 loss_input: 81.84265901088536
step: 9000 epoch: 541 loss: 16.873237122377944 loss_input: 81.9620601847733
step: 10000 epoch: 541 loss: 16.864251622199156 loss_input: 82.02455842062278
step: 11000 epoch: 541 loss: 16.886789732830664 loss_input: 82.10438751617396
step: 12000 epoch: 541 loss: 16.881667234512005 loss_input: 82.06715785566047
step: 13000 epoch: 541 loss: 16.890144918746337 loss_input: 82.01247397848904
step: 14000 epoch: 541 loss: 16.90771560419986 loss_input: 82.03391191487857
step: 15000 epoch: 541 loss: 16.90894798480402 loss_input: 82.1519407861416
Save loss: 16.911979019090534 Name: 541_train_model.pth
step: 0 epoch: 542 loss: 26.46067237854004 loss_input: 133.387451171875
step: 1000 epoch: 542 loss: 16.76636007472828 loss_input: 82.09699266797655
step: 2000 epoch: 542 loss: 16.851478698192864 loss_input: 82.83906850285794
step: 3000 epoch: 542 loss: 16.835010528485007 loss_input: 82.56175191765506
step: 4000 epoch: 542 loss: 16.81341425897598 loss_input: 82.3942672016322
step: 5000 epoch: 542 loss: 16.839145978244154 loss_input: 82.15771218314931
step: 6000 epoch: 542 loss: 16.851265930291156 loss_input: 82.19979075753
step: 7000 epoch: 542 loss: 16.891754108332783 loss_input: 82.3995050616374
step: 8000 epoch: 542 loss: 16.884289244088123 loss_input: 82.3145429498925
step: 9000 epoch: 542 loss: 16.889718532747672 loss_input: 82.18514601350718
step: 10000 epoch: 542 loss: 16.90833639216511 loss_input: 82.38258783374592
step: 11000 epoch: 542 loss: 16.89401608423237 loss_input: 82.22289497563432
step: 12000 epoch: 542 loss: 16.904270648618567 loss_input: 82.2206628807544
step: 13000 epoch: 542 loss: 16.89901481902688 loss_input: 82.2794579296935
step: 14000 epoch: 542 loss: 16.903520505042273 loss_input: 82.29014679493457
step: 15000 epoch: 542 loss: 16.907377719259305 loss_input: 82.23297233472195
Save loss: 16.904832377925516 Name: 542_train_model.pth
step: 0 epoch: 543 loss: 18.554780960083008 loss_input: 124.26739501953125
step: 1000 epoch: 543 loss: 16.742908126228933 loss_input: 81.69856376533623
step: 2000 epoch: 543 loss: 16.76908432585427 loss_input: 82.14686781975223
step: 3000 epoch: 543 loss: 16.812208472629738 loss_input: 81.90893528247746
step: 4000 epoch: 543 loss: 16.816401448854055 loss_input: 81.70057870053971
step: 5000 epoch: 543 loss: 16.829620717931 loss_input: 81.87873117515169
step: 6000 epoch: 543 loss: 16.848019704483406 loss_input: 82.19403980358743
step: 7000 epoch: 543 loss: 16.887333163805067 loss_input: 82.04500106116258
step: 8000 epoch: 543 loss: 16.864846124274777 loss_input: 82.12571138263479
step: 9000 epoch: 543 loss: 16.873091314384876 loss_input: 81.99348351214014
step: 10000 epoch: 543 loss: 16.84863535991467 loss_input: 81.94688188651838
step: 11000 epoch: 543 loss: 16.855477489413875 loss_input: 82.0634952763971
step: 12000 epoch: 543 loss: 16.84805901147318 loss_input: 82.03375757301801
step: 13000 epoch: 543 loss: 16.897269532218566 loss_input: 82.10885322241589
step: 14000 epoch: 543 loss: 16.905331096651896 loss_input: 82.18634911610667
step: 15000 epoch: 543 loss: 16.903120755974463 loss_input: 82.16445494616765
Save loss: 16.913536532342434 Name: 543_train_model.pth
step: 0 epoch: 544 loss: 19.70125389099121 loss_input: 76.6370849609375
step: 1000 epoch: 544 loss: 16.582219154327422 loss_input: 81.53480717280766
step: 2000 epoch: 544 loss: 16.67107356661025 loss_input: 82.12631989669228
step: 3000 epoch: 544 loss: 16.767466480753416 loss_input: 82.26507273454422
step: 4000 epoch: 544 loss: 16.746947175590613 loss_input: 82.04565650884642
step: 5000 epoch: 544 loss: 16.80397993425111 loss_input: 81.98285218444592
step: 6000 epoch: 544 loss: 16.812063322208697 loss_input: 82.0465767200421
step: 7000 epoch: 544 loss: 16.828353759851172 loss_input: 82.03542823611694
step: 8000 epoch: 544 loss: 16.84242706301212 loss_input: 82.062318412591
step: 9000 epoch: 544 loss: 16.85881449129698 loss_input: 82.2402077734093
step: 10000 epoch: 544 loss: 16.879757383444012 loss_input: 82.35998828705996
step: 11000 epoch: 544 loss: 16.887283161351622 loss_input: 82.21983187186285
step: 12000 epoch: 544 loss: 16.879271898554936 loss_input: 82.19207428006487
step: 13000 epoch: 544 loss: 16.908498077078622 loss_input: 82.2789898119691
step: 14000 epoch: 544 loss: 16.903768473902343 loss_input: 82.19659484422988
step: 15000 epoch: 544 loss: 16.9049796454152 loss_input: 82.20056124676324
Save loss: 16.90258766427636 Name: 544_train_model.pth
step: 0 epoch: 545 loss: 20.07611083984375 loss_input: 158.15289306640625
step: 1000 epoch: 545 loss: 16.726183688128508 loss_input: 81.47171413791287
step: 2000 epoch: 545 loss: 16.641181546291786 loss_input: 81.74799660716516
step: 3000 epoch: 545 loss: 16.73882235204804 loss_input: 81.57258054281385
step: 4000 epoch: 545 loss: 16.78359673500538 loss_input: 81.78732903138962
step: 5000 epoch: 545 loss: 16.81805170366607 loss_input: 82.01138159819207
step: 6000 epoch: 545 loss: 16.810058632208456 loss_input: 82.07305311175351
step: 7000 epoch: 545 loss: 16.811762217231657 loss_input: 81.89860132983235
step: 8000 epoch: 545 loss: 16.81555495535101 loss_input: 81.80410490833421
step: 9000 epoch: 545 loss: 16.861138705081004 loss_input: 81.99269811085443
step: 10000 epoch: 545 loss: 16.85917637090947 loss_input: 81.95550300926939
step: 11000 epoch: 545 loss: 16.869909557122856 loss_input: 82.06053938500264
step: 12000 epoch: 545 loss: 16.87579154799396 loss_input: 82.00828313847381
step: 13000 epoch: 545 loss: 16.890915017872167 loss_input: 82.08927672067594
step: 14000 epoch: 545 loss: 16.88049793207648 loss_input: 82.13576951621899
step: 15000 epoch: 545 loss: 16.890337739147494 loss_input: 82.18759786945193
Save loss: 16.90179121775925 Name: 545_train_model.pth
step: 0 epoch: 546 loss: 16.12993621826172 loss_input: 93.33056640625
step: 1000 epoch: 546 loss: 16.966242423900713 loss_input: 82.63381668380448
step: 2000 epoch: 546 loss: 16.772356615729002 loss_input: 82.03945628504107
step: 3000 epoch: 546 loss: 16.837142950214968 loss_input: 82.22829733431955
step: 4000 epoch: 546 loss: 16.797379192189258 loss_input: 81.94162706576773
step: 5000 epoch: 546 loss: 16.799455954584687 loss_input: 82.0074630703742
step: 6000 epoch: 546 loss: 16.856514158218708 loss_input: 82.17439094826969
step: 7000 epoch: 546 loss: 16.823501598663015 loss_input: 82.03743844699083
step: 8000 epoch: 546 loss: 16.85450736622023 loss_input: 82.15815556167051
step: 9000 epoch: 546 loss: 16.833221580383313 loss_input: 82.08802045947803
step: 10000 epoch: 546 loss: 16.848286272704442 loss_input: 82.30106858698896
step: 11000 epoch: 546 loss: 16.849324577429677 loss_input: 82.19258370532543
step: 12000 epoch: 546 loss: 16.862158295313783 loss_input: 82.19063451967382
step: 13000 epoch: 546 loss: 16.863764500601476 loss_input: 82.1766164800349
step: 14000 epoch: 546 loss: 16.862997376793633 loss_input: 82.16212766244236
step: 15000 epoch: 546 loss: 16.884415607771533 loss_input: 82.15319999174534
Save loss: 16.899832629442216 Name: 546_train_model.pth
step: 0 epoch: 547 loss: 19.11090087890625 loss_input: 94.7178955078125
step: 1000 epoch: 547 loss: 16.9803024843618 loss_input: 81.7681626235093
step: 2000 epoch: 547 loss: 17.004740352573425 loss_input: 82.32658353440468
step: 3000 epoch: 547 loss: 16.92767717877216 loss_input: 82.09016519639938
step: 4000 epoch: 547 loss: 16.900587063615127 loss_input: 82.03808932410452
step: 5000 epoch: 547 loss: 16.904225353859967 loss_input: 81.95311970320779
step: 6000 epoch: 547 loss: 16.855776460662202 loss_input: 81.78233931633139
step: 7000 epoch: 547 loss: 16.874294107189485 loss_input: 82.06988075538595
step: 8000 epoch: 547 loss: 16.873235329495802 loss_input: 82.2362857718957
step: 9000 epoch: 547 loss: 16.871227442085868 loss_input: 82.27975445897721
step: 10000 epoch: 547 loss: 16.878427770159004 loss_input: 82.23894235332327
step: 11000 epoch: 547 loss: 16.881677697262063 loss_input: 82.15768279210945
step: 12000 epoch: 547 loss: 16.88536655047607 loss_input: 82.23280401478588
step: 13000 epoch: 547 loss: 16.889986351099154 loss_input: 82.21257785796826
step: 14000 epoch: 547 loss: 16.903242974151553 loss_input: 82.2447810560267
step: 15000 epoch: 547 loss: 16.9052464890008 loss_input: 82.31803085686215
Save loss: 16.89994279691577 Name: 547_train_model.pth
step: 0 epoch: 548 loss: 11.28297233581543 loss_input: 41.82281494140625
step: 1000 epoch: 548 loss: 16.743438129539374 loss_input: 82.30678488991477
step: 2000 epoch: 548 loss: 16.69197686966034 loss_input: 81.40996728808447
step: 3000 epoch: 548 loss: 16.822089406896616 loss_input: 82.08578360029078
step: 4000 epoch: 548 loss: 16.86555537388522 loss_input: 82.37368416684892
step: 5000 epoch: 548 loss: 16.895657177520643 loss_input: 82.50150680313156
step: 6000 epoch: 548 loss: 16.88669278831844 loss_input: 82.62810027267591
step: 7000 epoch: 548 loss: 16.893416958797864 loss_input: 82.44734839428631
step: 8000 epoch: 548 loss: 16.896497389895067 loss_input: 82.39892706282808
step: 9000 epoch: 548 loss: 16.901229628270606 loss_input: 82.39568193323572
step: 10000 epoch: 548 loss: 16.9194814478227 loss_input: 82.41032005808208
step: 11000 epoch: 548 loss: 16.898767443572744 loss_input: 82.29587642245592
step: 12000 epoch: 548 loss: 16.902866639352702 loss_input: 82.24280242373194
step: 13000 epoch: 548 loss: 16.886106449827288 loss_input: 82.27267275545361
step: 14000 epoch: 548 loss: 16.901806958836577 loss_input: 82.19578122523893
step: 15000 epoch: 548 loss: 16.899317077140587 loss_input: 82.22937211086969
Save loss: 16.90155827873945 Name: 548_train_model.pth
step: 0 epoch: 549 loss: 11.129450798034668 loss_input: 50.1107177734375
step: 1000 epoch: 549 loss: 16.99909408990439 loss_input: 84.0020163552268
step: 2000 epoch: 549 loss: 16.983446692657854 loss_input: 83.95090848740669
step: 3000 epoch: 549 loss: 16.906535175473483 loss_input: 83.2786159292931
step: 4000 epoch: 549 loss: 16.883637885217873 loss_input: 82.79220940309357
step: 5000 epoch: 549 loss: 16.90702203549616 loss_input: 82.76637078590142
step: 6000 epoch: 549 loss: 16.92897768760399 loss_input: 82.7129360784433
step: 7000 epoch: 549 loss: 16.90720550807233 loss_input: 82.57893533520044
step: 8000 epoch: 549 loss: 16.91042961127876 loss_input: 82.62510735504986
step: 9000 epoch: 549 loss: 16.88913362948792 loss_input: 82.43003395808668
step: 10000 epoch: 549 loss: 16.878926817053213 loss_input: 82.33635109999742
step: 11000 epoch: 549 loss: 16.88246103536323 loss_input: 82.23979245405957
step: 12000 epoch: 549 loss: 16.88989461112962 loss_input: 82.32400255606142
step: 13000 epoch: 549 loss: 16.894814209776303 loss_input: 82.35245945530482
step: 14000 epoch: 549 loss: 16.885673669395885 loss_input: 82.2973724838223
step: 15000 epoch: 549 loss: 16.888345346316665 loss_input: 82.20510409576211
Save loss: 16.897349582627417 Name: 549_train_model.pth
step: 0 epoch: 550 loss: 17.417226791381836 loss_input: 76.46087646484375
step: 1000 epoch: 550 loss: 17.097143624331448 loss_input: 83.16849026217923
step: 2000 epoch: 550 loss: 16.946040376789984 loss_input: 82.69056764952485
step: 3000 epoch: 550 loss: 16.825647371048056 loss_input: 82.62290810696565
step: 4000 epoch: 550 loss: 16.821476096422842 loss_input: 82.42547367918762
step: 5000 epoch: 550 loss: 16.81805595453442 loss_input: 82.2434600970431
step: 6000 epoch: 550 loss: 16.84413216424334 loss_input: 82.42282160940299
step: 7000 epoch: 550 loss: 16.865017007579294 loss_input: 82.33242920340342
step: 8000 epoch: 550 loss: 16.86018055612602 loss_input: 82.19060665207377
step: 9000 epoch: 550 loss: 16.885312472061507 loss_input: 82.30511086740145
step: 10000 epoch: 550 loss: 16.893435654765593 loss_input: 82.32704070901504
step: 11000 epoch: 550 loss: 16.90346071523121 loss_input: 82.37529634868066
step: 12000 epoch: 550 loss: 16.877072199912142 loss_input: 82.29732883486109
step: 13000 epoch: 550 loss: 16.90175296321171 loss_input: 82.32449227444494
step: 14000 epoch: 550 loss: 16.89007534114695 loss_input: 82.2073394162903
step: 15000 epoch: 550 loss: 16.900339001934096 loss_input: 82.20454940455141
Save loss: 16.89319130052626 Name: 550_train_model.pth
step: 0 epoch: 551 loss: 18.31046485900879 loss_input: 83.99896240234375
step: 1000 epoch: 551 loss: 16.85516706999246 loss_input: 82.57866138987966
step: 2000 epoch: 551 loss: 16.793914089674715 loss_input: 82.12396838079984
step: 3000 epoch: 551 loss: 16.839760062457003 loss_input: 81.83922861894342
step: 4000 epoch: 551 loss: 16.866041665135608 loss_input: 82.02991772198551
step: 5000 epoch: 551 loss: 16.8781822116774 loss_input: 82.1907027285949
step: 6000 epoch: 551 loss: 16.84767990572376 loss_input: 82.16406783968623
step: 7000 epoch: 551 loss: 16.871437974187955 loss_input: 82.28151665938069
step: 8000 epoch: 551 loss: 16.879532060836528 loss_input: 82.2566883805662
step: 9000 epoch: 551 loss: 16.87511743141431 loss_input: 82.21434003535198
step: 10000 epoch: 551 loss: 16.864317771542396 loss_input: 82.14358930928196
step: 11000 epoch: 551 loss: 16.868923394271064 loss_input: 82.14170713262573
step: 12000 epoch: 551 loss: 16.878924969265576 loss_input: 82.15273814665835
step: 13000 epoch: 551 loss: 16.88085697161382 loss_input: 82.09459406067322
step: 14000 epoch: 551 loss: 16.891458380112486 loss_input: 82.1472314486256
step: 15000 epoch: 551 loss: 16.888513231061314 loss_input: 82.16355197088423
Save loss: 16.892487742468713 Name: 551_train_model.pth
step: 0 epoch: 552 loss: 12.289130210876465 loss_input: 70.890380859375
step: 1000 epoch: 552 loss: 16.791548074899495 loss_input: 82.18527639352834
step: 2000 epoch: 552 loss: 16.934892231199157 loss_input: 82.18975223081819
step: 3000 epoch: 552 loss: 16.97620623559008 loss_input: 82.60202364999428
step: 4000 epoch: 552 loss: 16.97396121445789 loss_input: 82.90428470152969
step: 5000 epoch: 552 loss: 16.925644494800228 loss_input: 82.54968144554682
step: 6000 epoch: 552 loss: 16.894789321405494 loss_input: 82.6873268517747
step: 7000 epoch: 552 loss: 16.911346920352887 loss_input: 82.748146452711
step: 8000 epoch: 552 loss: 16.888320525070082 loss_input: 82.6277286246812
step: 9000 epoch: 552 loss: 16.88231688207447 loss_input: 82.65102748322018
step: 10000 epoch: 552 loss: 16.866141540338344 loss_input: 82.51171334892878
step: 11000 epoch: 552 loss: 16.87419582288836 loss_input: 82.58076850857131
step: 12000 epoch: 552 loss: 16.872502855912476 loss_input: 82.44766503120997
step: 13000 epoch: 552 loss: 16.873112764553643 loss_input: 82.40775572007531
step: 14000 epoch: 552 loss: 16.876109477409543 loss_input: 82.40320247074986
step: 15000 epoch: 552 loss: 16.875198928048377 loss_input: 82.29549936742498
Save loss: 16.890523324862123 Name: 552_train_model.pth
step: 0 epoch: 553 loss: 18.67620849609375 loss_input: 77.203369140625
step: 1000 epoch: 553 loss: 16.931975095064846 loss_input: 81.812812797554
step: 2000 epoch: 553 loss: 16.998128472060813 loss_input: 81.74320822986944
step: 3000 epoch: 553 loss: 16.936624120212088 loss_input: 81.5218505045844
step: 4000 epoch: 553 loss: 16.891370996538623 loss_input: 81.8395530664453
step: 5000 epoch: 553 loss: 16.84497863868312 loss_input: 81.79558331497763
step: 6000 epoch: 553 loss: 16.8672726797553 loss_input: 82.05145111109411
step: 7000 epoch: 553 loss: 16.887701491631198 loss_input: 81.92616992738482
step: 8000 epoch: 553 loss: 16.904482283900343 loss_input: 82.19144125691564
step: 9000 epoch: 553 loss: 16.92492034331704 loss_input: 82.34467615984292
step: 10000 epoch: 553 loss: 16.933013784993875 loss_input: 82.4329371811354
step: 11000 epoch: 553 loss: 16.89505546427393 loss_input: 82.35797581856018
step: 12000 epoch: 553 loss: 16.887393565587566 loss_input: 82.28062910549284
step: 13000 epoch: 553 loss: 16.89506262911713 loss_input: 82.21598475673146
step: 14000 epoch: 553 loss: 16.896772818653577 loss_input: 82.24945138544791
step: 15000 epoch: 553 loss: 16.88901504681322 loss_input: 82.20378468095616
Save loss: 16.89529611928761 Name: 553_train_model.pth
step: 0 epoch: 554 loss: 12.159858703613281 loss_input: 64.65399169921875
step: 1000 epoch: 554 loss: 16.882647143734562 loss_input: 83.14160765991821
step: 2000 epoch: 554 loss: 16.878910713110013 loss_input: 82.69107655034787
step: 3000 epoch: 554 loss: 16.855678279016146 loss_input: 82.45636677829395
step: 4000 epoch: 554 loss: 16.859872211250355 loss_input: 82.51065399848768
step: 5000 epoch: 554 loss: 16.88319983965777 loss_input: 82.41826142828074
step: 6000 epoch: 554 loss: 16.923790539012234 loss_input: 82.51623129340096
step: 7000 epoch: 554 loss: 16.931936950789844 loss_input: 82.52540946473327
step: 8000 epoch: 554 loss: 16.911835767555022 loss_input: 82.46141644403825
step: 9000 epoch: 554 loss: 16.902585106629186 loss_input: 82.30110663973534
step: 10000 epoch: 554 loss: 16.904517439052757 loss_input: 82.27919543250754
step: 11000 epoch: 554 loss: 16.8914814210352 loss_input: 82.21993373027878
step: 12000 epoch: 554 loss: 16.902719230515174 loss_input: 82.16068502470172
step: 13000 epoch: 554 loss: 16.880448332713865 loss_input: 82.141906714808
step: 14000 epoch: 554 loss: 16.894900991800693 loss_input: 82.14220096775314
step: 15000 epoch: 554 loss: 16.893002352152862 loss_input: 82.19815745600684
Save loss: 16.899215128719806 Name: 554_train_model.pth
step: 0 epoch: 555 loss: 10.837291717529297 loss_input: 54.48760986328125
step: 1000 epoch: 555 loss: 16.905658802905165 loss_input: 82.04502742130916
step: 2000 epoch: 555 loss: 16.8675321608052 loss_input: 81.97226362404736
step: 3000 epoch: 555 loss: 16.882338779364296 loss_input: 81.80236944537367
step: 4000 epoch: 555 loss: 16.85477307569203 loss_input: 81.75455568480926
step: 5000 epoch: 555 loss: 16.83542735022179 loss_input: 81.61541938438484
step: 6000 epoch: 555 loss: 16.885771943855 loss_input: 82.01728899878991
step: 7000 epoch: 555 loss: 16.901085411918245 loss_input: 82.01981619987602
step: 8000 epoch: 555 loss: 16.895916776766764 loss_input: 82.15474941700403
step: 9000 epoch: 555 loss: 16.885702689029603 loss_input: 82.15392155727271
step: 10000 epoch: 555 loss: 16.880528464292052 loss_input: 82.04726070361714
step: 11000 epoch: 555 loss: 16.896674515799862 loss_input: 82.18240641897434
step: 12000 epoch: 555 loss: 16.890733976839343 loss_input: 82.22342371839294
step: 13000 epoch: 555 loss: 16.887438538991233 loss_input: 82.20341026107216
step: 14000 epoch: 555 loss: 16.885583581519835 loss_input: 82.14151123256123
step: 15000 epoch: 555 loss: 16.888925471867523 loss_input: 82.1842793935808
Save loss: 16.89006154729426 Name: 555_train_model.pth
step: 0 epoch: 556 loss: 23.93039321899414 loss_input: 89.84393310546875
step: 1000 epoch: 556 loss: 16.911497316636762 loss_input: 82.06566029185657
step: 2000 epoch: 556 loss: 16.93878612787589 loss_input: 82.1622713728585
step: 3000 epoch: 556 loss: 16.92126282172694 loss_input: 82.44098132246933
step: 4000 epoch: 556 loss: 16.95953307590375 loss_input: 82.52384476207608
step: 5000 epoch: 556 loss: 16.89800618477188 loss_input: 82.11003033768246
step: 6000 epoch: 556 loss: 16.928876837856112 loss_input: 81.99588293918787
step: 7000 epoch: 556 loss: 16.943264980892373 loss_input: 82.01228649639059
step: 8000 epoch: 556 loss: 16.9271974258759 loss_input: 82.11438075245478
step: 9000 epoch: 556 loss: 16.91642623204097 loss_input: 82.01525537147349
step: 10000 epoch: 556 loss: 16.935013309071962 loss_input: 82.12580944664323
step: 11000 epoch: 556 loss: 16.923825850281734 loss_input: 82.14353761585417
step: 12000 epoch: 556 loss: 16.915168099200343 loss_input: 82.00847192482495
step: 13000 epoch: 556 loss: 16.9025006666705 loss_input: 82.0783702485626
step: 14000 epoch: 556 loss: 16.892539771656743 loss_input: 82.10519034058254
step: 15000 epoch: 556 loss: 16.892228390183355 loss_input: 82.14331883082795
Save loss: 16.892246397152544 Name: 556_train_model.pth
step: 0 epoch: 557 loss: 12.309144973754883 loss_input: 62.7689208984375
step: 1000 epoch: 557 loss: 16.710888828788246 loss_input: 81.60811191981847
step: 2000 epoch: 557 loss: 16.800447194234305 loss_input: 81.79311345303911
step: 3000 epoch: 557 loss: 16.712226590566818 loss_input: 81.3310430133315
step: 4000 epoch: 557 loss: 16.73114536810267 loss_input: 81.33277561652068
step: 5000 epoch: 557 loss: 16.727453424176844 loss_input: 81.5413986294538
step: 6000 epoch: 557 loss: 16.71309176585492 loss_input: 81.82872849642247
step: 7000 epoch: 557 loss: 16.769785581325024 loss_input: 81.8078263425398
step: 8000 epoch: 557 loss: 16.811453110157675 loss_input: 81.96101678238706
step: 9000 epoch: 557 loss: 16.807717358029002 loss_input: 82.0072896834705
step: 10000 epoch: 557 loss: 16.837726461232965 loss_input: 82.08132292849817
step: 11000 epoch: 557 loss: 16.837175222280166 loss_input: 82.19976269025258
step: 12000 epoch: 557 loss: 16.842511393131687 loss_input: 82.26184018966843
step: 13000 epoch: 557 loss: 16.852562749507786 loss_input: 82.21216567700922
step: 14000 epoch: 557 loss: 16.86406505445899 loss_input: 82.20449700282988
step: 15000 epoch: 557 loss: 16.879748509968213 loss_input: 82.24955671493645
Save loss: 16.8834141715765 Name: 557_train_model.pth
step: 0 epoch: 558 loss: 18.478614807128906 loss_input: 72.62396240234375
step: 1000 epoch: 558 loss: 16.868252232119993 loss_input: 81.81722397475572
step: 2000 epoch: 558 loss: 16.931932496285807 loss_input: 82.99692567046554
step: 3000 epoch: 558 loss: 16.897413684224652 loss_input: 82.54379991418875
step: 4000 epoch: 558 loss: 16.944044441498686 loss_input: 82.29408335477166
step: 5000 epoch: 558 loss: 16.904558037882015 loss_input: 82.13409861972917
step: 6000 epoch: 558 loss: 16.90544303245017 loss_input: 82.19679520372907
step: 7000 epoch: 558 loss: 16.886175960085254 loss_input: 81.98451999881713
step: 8000 epoch: 558 loss: 16.893355265362175 loss_input: 82.13860730787557
step: 9000 epoch: 558 loss: 16.89580306823009 loss_input: 82.20246175039796
step: 10000 epoch: 558 loss: 16.87658485349757 loss_input: 82.11497349410567
step: 11000 epoch: 558 loss: 16.893437495460056 loss_input: 82.09450945799139
step: 12000 epoch: 558 loss: 16.901243796875832 loss_input: 82.09664194040145
step: 13000 epoch: 558 loss: 16.898166007440317 loss_input: 82.16675242698429
step: 14000 epoch: 558 loss: 16.915787942953923 loss_input: 82.21836646067553
step: 15000 epoch: 558 loss: 16.923605962353033 loss_input: 82.2953903984526
Save loss: 16.90288540135324 Name: 558_train_model.pth
step: 0 epoch: 559 loss: 19.711837768554688 loss_input: 104.07257080078125
step: 1000 epoch: 559 loss: 16.55549916592273 loss_input: 82.3921170382352
step: 2000 epoch: 559 loss: 16.633224846660227 loss_input: 81.93501094911528
step: 3000 epoch: 559 loss: 16.69436080755293 loss_input: 82.00874661652496
step: 4000 epoch: 559 loss: 16.7341445056655 loss_input: 82.07873767031874
step: 5000 epoch: 559 loss: 16.735534323904186 loss_input: 81.76784697992042
step: 6000 epoch: 559 loss: 16.759220702035133 loss_input: 81.93771054637052
step: 7000 epoch: 559 loss: 16.80524484087205 loss_input: 82.08098894458995
step: 8000 epoch: 559 loss: 16.828430173546355 loss_input: 82.18072053584855
step: 9000 epoch: 559 loss: 16.83839052455344 loss_input: 82.0379708718888
step: 10000 epoch: 559 loss: 16.854287041412473 loss_input: 82.07126488889197
step: 11000 epoch: 559 loss: 16.876397274893073 loss_input: 82.18214024111266
step: 12000 epoch: 559 loss: 16.89321364888389 loss_input: 82.28677457929125
step: 13000 epoch: 559 loss: 16.907158657309 loss_input: 82.26933683511724
step: 14000 epoch: 559 loss: 16.895680499855054 loss_input: 82.30540709668897
step: 15000 epoch: 559 loss: 16.887264962053628 loss_input: 82.23655865162375
Save loss: 16.88718950226903 Name: 559_train_model.pth
step: 0 epoch: 560 loss: 17.435739517211914 loss_input: 177.0465087890625
step: 1000 epoch: 560 loss: 16.67520186808202 loss_input: 82.63740964845701
step: 2000 epoch: 560 loss: 16.63365896102013 loss_input: 81.79078142813358
step: 3000 epoch: 560 loss: 16.74427253808311 loss_input: 81.65354745032906
step: 4000 epoch: 560 loss: 16.786925150793333 loss_input: 81.8315787569221
step: 5000 epoch: 560 loss: 16.81420529916081 loss_input: 81.895824912166
step: 6000 epoch: 560 loss: 16.849273504922436 loss_input: 82.00075520453622
step: 7000 epoch: 560 loss: 16.84543648724419 loss_input: 81.82059460778761
step: 8000 epoch: 560 loss: 16.832590688721655 loss_input: 81.72096715297063
step: 9000 epoch: 560 loss: 16.83649220589518 loss_input: 81.79790979713192
step: 10000 epoch: 560 loss: 16.859482032348485 loss_input: 81.9966288283281
step: 11000 epoch: 560 loss: 16.87106437497156 loss_input: 82.05050720486703
step: 12000 epoch: 560 loss: 16.861075204964948 loss_input: 82.06839409602422
step: 13000 epoch: 560 loss: 16.87599838925237 loss_input: 82.08805203648697
step: 14000 epoch: 560 loss: 16.874644292489894 loss_input: 82.05203409019892
step: 15000 epoch: 560 loss: 16.8806706671635 loss_input: 82.1700212783666
Save loss: 16.893753747314214 Name: 560_train_model.pth
step: 0 epoch: 561 loss: 14.03814697265625 loss_input: 56.95172119140625
step: 1000 epoch: 561 loss: 16.777203031591363 loss_input: 82.12966050063218
step: 2000 epoch: 561 loss: 16.924697844044438 loss_input: 83.11320889848045
step: 3000 epoch: 561 loss: 16.862918091869005 loss_input: 82.9753418782281
step: 4000 epoch: 561 loss: 16.86073335854002 loss_input: 82.56490882162272
step: 5000 epoch: 561 loss: 16.908967182698714 loss_input: 82.45363130742992
step: 6000 epoch: 561 loss: 16.9100669148246 loss_input: 82.43571441909747
step: 7000 epoch: 561 loss: 16.910711515598546 loss_input: 82.53757960114235
step: 8000 epoch: 561 loss: 16.904449874826437 loss_input: 82.45558411186151
step: 9000 epoch: 561 loss: 16.891189882933013 loss_input: 82.44422391652664
step: 10000 epoch: 561 loss: 16.889687852541954 loss_input: 82.36911803204445
step: 11000 epoch: 561 loss: 16.8888777654915 loss_input: 82.31731675627145
step: 12000 epoch: 561 loss: 16.909637511168885 loss_input: 82.42973477796122
step: 13000 epoch: 561 loss: 16.90868894676641 loss_input: 82.45356150394531
step: 14000 epoch: 561 loss: 16.908269521636832 loss_input: 82.36381849102715
step: 15000 epoch: 561 loss: 16.89788677722452 loss_input: 82.33302295227844
Save loss: 16.894249742373823 Name: 561_train_model.pth
step: 0 epoch: 562 loss: 9.538349151611328 loss_input: 66.175048828125
step: 1000 epoch: 562 loss: 16.910385436706846 loss_input: 82.27610249857564
step: 2000 epoch: 562 loss: 16.91953618999483 loss_input: 82.02346242040113
step: 3000 epoch: 562 loss: 16.939757421786528 loss_input: 82.03946110980584
step: 4000 epoch: 562 loss: 16.875392723488705 loss_input: 81.91936061394807
step: 5000 epoch: 562 loss: 16.87454783060722 loss_input: 81.89474343900751
step: 6000 epoch: 562 loss: 16.88419725854801 loss_input: 81.94402728329459
step: 7000 epoch: 562 loss: 16.853957508652744 loss_input: 81.94783461807353
step: 8000 epoch: 562 loss: 16.87089440870577 loss_input: 81.96639735066076
step: 9000 epoch: 562 loss: 16.85194385235819 loss_input: 82.06574479056364
step: 10000 epoch: 562 loss: 16.843602211925223 loss_input: 82.01817212687911
step: 11000 epoch: 562 loss: 16.872062684730903 loss_input: 82.19548439266529
step: 12000 epoch: 562 loss: 16.87532753346811 loss_input: 82.21400828902279
step: 13000 epoch: 562 loss: 16.881648011193278 loss_input: 82.20090932664885
step: 14000 epoch: 562 loss: 16.875610420375406 loss_input: 82.19127818559137
step: 15000 epoch: 562 loss: 16.87663788641622 loss_input: 82.21821926552553
Save loss: 16.883107935294507 Name: 562_train_model.pth
step: 0 epoch: 563 loss: 17.357410430908203 loss_input: 73.3118896484375
step: 1000 epoch: 563 loss: 16.865477778694846 loss_input: 81.8394007115931
step: 2000 epoch: 563 loss: 16.8698631922404 loss_input: 82.99572888116488
step: 3000 epoch: 563 loss: 16.920014578753495 loss_input: 83.09231159600445
step: 4000 epoch: 563 loss: 16.873353363662563 loss_input: 82.51867639169309
step: 5000 epoch: 563 loss: 16.84949768459623 loss_input: 82.28000420423727
step: 6000 epoch: 563 loss: 16.881503713983154 loss_input: 82.40072367902876
step: 7000 epoch: 563 loss: 16.870579931978124 loss_input: 82.36359945928483
step: 8000 epoch: 563 loss: 16.863909805257563 loss_input: 82.2910586494071
step: 9000 epoch: 563 loss: 16.871093057365023 loss_input: 82.35378489575378
step: 10000 epoch: 563 loss: 16.86312333010111 loss_input: 82.32182720448658
step: 11000 epoch: 563 loss: 16.871950184363754 loss_input: 82.32367441582903
step: 12000 epoch: 563 loss: 16.873845799169324 loss_input: 82.2833786220533
step: 13000 epoch: 563 loss: 16.880177028086926 loss_input: 82.27486116696703
step: 14000 epoch: 563 loss: 16.87807340234035 loss_input: 82.22470051141715
step: 15000 epoch: 563 loss: 16.873674864737513 loss_input: 82.25316235021181
Save loss: 16.885968189001083 Name: 563_train_model.pth
step: 0 epoch: 564 loss: 13.765742301940918 loss_input: 112.59857177734375
step: 1000 epoch: 564 loss: 16.813989400149108 loss_input: 82.93257493239183
step: 2000 epoch: 564 loss: 16.834970916050306 loss_input: 82.6656152514563
step: 3000 epoch: 564 loss: 16.804781807616965 loss_input: 82.50361272804898
step: 4000 epoch: 564 loss: 16.791298852089614 loss_input: 82.69415501349272
step: 5000 epoch: 564 loss: 16.778719663286275 loss_input: 82.53350338233135
step: 6000 epoch: 564 loss: 16.799744732676697 loss_input: 82.41353221321837
step: 7000 epoch: 564 loss: 16.829601017105087 loss_input: 82.29502458249547
step: 8000 epoch: 564 loss: 16.830473809760147 loss_input: 82.36818751232757
step: 9000 epoch: 564 loss: 16.8677066148036 loss_input: 82.55461263852628
step: 10000 epoch: 564 loss: 16.858769500223403 loss_input: 82.32697249073921
step: 11000 epoch: 564 loss: 16.880244842757637 loss_input: 82.38719663402404
step: 12000 epoch: 564 loss: 16.86539633917159 loss_input: 82.3346226986781
step: 13000 epoch: 564 loss: 16.8607277585565 loss_input: 82.20640757032362
step: 14000 epoch: 564 loss: 16.875490310728342 loss_input: 82.17977397627988
step: 15000 epoch: 564 loss: 16.889473851319433 loss_input: 82.22398735836231
Save loss: 16.888468369767068 Name: 564_train_model.pth
step: 0 epoch: 565 loss: 11.713034629821777 loss_input: 43.98016357421875
step: 1000 epoch: 565 loss: 16.963961062016903 loss_input: 81.91230619965971
step: 2000 epoch: 565 loss: 16.975161145175473 loss_input: 82.08925295042908
step: 3000 epoch: 565 loss: 16.842520463550063 loss_input: 81.96068681045121
step: 4000 epoch: 565 loss: 16.772562855692154 loss_input: 81.83146670912838
step: 5000 epoch: 565 loss: 16.820350206892293 loss_input: 82.13461178613886
step: 6000 epoch: 565 loss: 16.84662832444637 loss_input: 82.05865880263445
step: 7000 epoch: 565 loss: 16.805914089417836 loss_input: 81.87438383348974
step: 8000 epoch: 565 loss: 16.831622666797703 loss_input: 81.93330964397272
step: 9000 epoch: 565 loss: 16.822244336268408 loss_input: 82.03649530748754
step: 10000 epoch: 565 loss: 16.865023432940365 loss_input: 82.18265876659405
step: 11000 epoch: 565 loss: 16.86835672035942 loss_input: 82.19658771920948
step: 12000 epoch: 565 loss: 16.874694094639224 loss_input: 82.19161761747719
step: 13000 epoch: 565 loss: 16.884951971703405 loss_input: 82.25600031311444
step: 14000 epoch: 565 loss: 16.88358462172724 loss_input: 82.2437772563539
step: 15000 epoch: 565 loss: 16.878276527233897 loss_input: 82.20621890995028
Save loss: 16.883627987891437 Name: 565_train_model.pth
step: 0 epoch: 566 loss: 22.579782485961914 loss_input: 108.21044921875
step: 1000 epoch: 566 loss: 17.093888974451758 loss_input: 83.09303855324363
step: 2000 epoch: 566 loss: 16.977793043461638 loss_input: 82.58965707575899
step: 3000 epoch: 566 loss: 16.89595102254886 loss_input: 82.63052393586904
step: 4000 epoch: 566 loss: 16.88621155579845 loss_input: 82.69552063888325
step: 5000 epoch: 566 loss: 16.92856616684018 loss_input: 82.57863805656838
step: 6000 epoch: 566 loss: 16.895800475378152 loss_input: 82.44545801407773
step: 7000 epoch: 566 loss: 16.899660162475513 loss_input: 82.37465568011768
step: 8000 epoch: 566 loss: 16.882180657778335 loss_input: 82.40952558389918
step: 9000 epoch: 566 loss: 16.914630443066336 loss_input: 82.48149720018088
step: 10000 epoch: 566 loss: 16.89826056244683 loss_input: 82.30355090711632
step: 11000 epoch: 566 loss: 16.897339684650493 loss_input: 82.314262229934
step: 12000 epoch: 566 loss: 16.891496950860283 loss_input: 82.21447645069291
step: 13000 epoch: 566 loss: 16.881478527785905 loss_input: 82.20177722206465
step: 14000 epoch: 566 loss: 16.882880250961982 loss_input: 82.23904909153391
step: 15000 epoch: 566 loss: 16.882850920086774 loss_input: 82.18768745088218
Save loss: 16.88638311895728 Name: 566_train_model.pth
step: 0 epoch: 567 loss: 22.12775421142578 loss_input: 91.38372802734375
step: 1000 epoch: 567 loss: 16.714956670374303 loss_input: 82.34352238337834
step: 2000 epoch: 567 loss: 16.80379011010242 loss_input: 82.23846020715229
step: 3000 epoch: 567 loss: 16.712002891097534 loss_input: 81.57962342223816
step: 4000 epoch: 567 loss: 16.776268674981086 loss_input: 81.97718369784995
step: 5000 epoch: 567 loss: 16.802125352450645 loss_input: 81.96286597172752
step: 6000 epoch: 567 loss: 16.824604749917945 loss_input: 82.05859914054038
step: 7000 epoch: 567 loss: 16.80865925081354 loss_input: 81.95966991985242
step: 8000 epoch: 567 loss: 16.833283275920827 loss_input: 82.13370095400673
step: 9000 epoch: 567 loss: 16.85882893462616 loss_input: 82.25357091263947
step: 10000 epoch: 567 loss: 16.867795880800866 loss_input: 82.09520596402274
step: 11000 epoch: 567 loss: 16.86862855594317 loss_input: 82.06466099739335
step: 12000 epoch: 567 loss: 16.845714988633798 loss_input: 82.00860643510013
step: 13000 epoch: 567 loss: 16.849165604945448 loss_input: 82.08438917127319
step: 14000 epoch: 567 loss: 16.8624067905247 loss_input: 82.16659268426278
step: 15000 epoch: 567 loss: 16.881114569531068 loss_input: 82.16338199523678
Save loss: 16.887509258300067 Name: 567_train_model.pth
step: 0 epoch: 568 loss: 17.262619018554688 loss_input: 94.80743408203125
step: 1000 epoch: 568 loss: 16.568324779773448 loss_input: 81.14531470726538
step: 2000 epoch: 568 loss: 16.8416380126854 loss_input: 82.09295880014095
step: 3000 epoch: 568 loss: 16.831189459063776 loss_input: 82.15317237190467
step: 4000 epoch: 568 loss: 16.835800072813715 loss_input: 82.17591534123305
step: 5000 epoch: 568 loss: 16.821172550329564 loss_input: 82.19386725682207
step: 6000 epoch: 568 loss: 16.82288469789108 loss_input: 82.2078353558912
step: 7000 epoch: 568 loss: 16.858466247101713 loss_input: 82.29065375565767
step: 8000 epoch: 568 loss: 16.83737760951349 loss_input: 82.16876560168748
step: 9000 epoch: 568 loss: 16.834603423344376 loss_input: 82.1978748441471
step: 10000 epoch: 568 loss: 16.811777757079753 loss_input: 82.09276586379448
step: 11000 epoch: 568 loss: 16.809163553391183 loss_input: 82.13755705824506
step: 12000 epoch: 568 loss: 16.81792993573743 loss_input: 82.09160886169641
step: 13000 epoch: 568 loss: 16.850300078795108 loss_input: 82.16996304708833
step: 14000 epoch: 568 loss: 16.851687132975908 loss_input: 82.18451920792627
step: 15000 epoch: 568 loss: 16.857701308456598 loss_input: 82.19276253978902
Save loss: 16.88419411189854 Name: 568_train_model.pth
step: 0 epoch: 569 loss: 17.252166748046875 loss_input: 93.7127685546875
step: 1000 epoch: 569 loss: 16.776244617961385 loss_input: 81.24835650189654
step: 2000 epoch: 569 loss: 16.784781499721596 loss_input: 82.08260591383996
step: 3000 epoch: 569 loss: 16.79886352145962 loss_input: 82.39506114335586
step: 4000 epoch: 569 loss: 16.83130559507712 loss_input: 82.2397067206587
step: 5000 epoch: 569 loss: 16.899269975154215 loss_input: 82.3727846837859
step: 6000 epoch: 569 loss: 16.89324460798771 loss_input: 82.32574514761862
step: 7000 epoch: 569 loss: 16.848816057525863 loss_input: 82.19985699064475
step: 8000 epoch: 569 loss: 16.85567129357072 loss_input: 82.23721786821325
step: 9000 epoch: 569 loss: 16.872338555042194 loss_input: 82.33064781132174
step: 10000 epoch: 569 loss: 16.86017223353768 loss_input: 82.26523404178137
step: 11000 epoch: 569 loss: 16.870476166905732 loss_input: 82.40740024721651
step: 12000 epoch: 569 loss: 16.859805643394605 loss_input: 82.2015907192874
step: 13000 epoch: 569 loss: 16.85234816233366 loss_input: 82.115624838504
step: 14000 epoch: 569 loss: 16.874008531374944 loss_input: 82.20372278357767
step: 15000 epoch: 569 loss: 16.87292965799401 loss_input: 82.19555511077907
Save loss: 16.87822995007038 Name: 569_train_model.pth
step: 0 epoch: 570 loss: 13.307599067687988 loss_input: 47.492919921875
step: 1000 epoch: 570 loss: 16.738433645679045 loss_input: 82.02560561996597
step: 2000 epoch: 570 loss: 16.89893877679023 loss_input: 82.77667922630482
step: 3000 epoch: 570 loss: 16.93554824394053 loss_input: 82.5477592470804
step: 4000 epoch: 570 loss: 16.911891877248983 loss_input: 82.5411410361521
step: 5000 epoch: 570 loss: 16.878989194541234 loss_input: 82.1329887098752
step: 6000 epoch: 570 loss: 16.871757186506017 loss_input: 82.29198558663074
step: 7000 epoch: 570 loss: 16.87129249856772 loss_input: 82.16014763655416
step: 8000 epoch: 570 loss: 16.896719039939164 loss_input: 82.36682083898448
step: 9000 epoch: 570 loss: 16.887853092914714 loss_input: 82.29688121811336
step: 10000 epoch: 570 loss: 16.869022332695817 loss_input: 82.32303333387364
step: 11000 epoch: 570 loss: 16.88274961076599 loss_input: 82.28127080554822
step: 12000 epoch: 570 loss: 16.891857225570508 loss_input: 82.3098516611246
step: 13000 epoch: 570 loss: 16.885063194713265 loss_input: 82.2440469197682
step: 14000 epoch: 570 loss: 16.873255217043845 loss_input: 82.13286661252015
step: 15000 epoch: 570 loss: 16.864304078832387 loss_input: 82.13633182918014
Save loss: 16.88125564339757 Name: 570_train_model.pth
step: 0 epoch: 571 loss: 12.363969802856445 loss_input: 47.7449951171875
step: 1000 epoch: 571 loss: 16.93755215817279 loss_input: 82.6644449910441
step: 2000 epoch: 571 loss: 16.786668730282532 loss_input: 81.87387245098154
step: 3000 epoch: 571 loss: 16.73945092106215 loss_input: 81.85769701115254
step: 4000 epoch: 571 loss: 16.76206561190818 loss_input: 81.9211891820746
step: 5000 epoch: 571 loss: 16.805255226601698 loss_input: 82.14423340917755
step: 6000 epoch: 571 loss: 16.799364979397037 loss_input: 82.21915364976606
step: 7000 epoch: 571 loss: 16.816433542882557 loss_input: 82.39712768083912
step: 8000 epoch: 571 loss: 16.82015373018172 loss_input: 82.29452076394503
step: 9000 epoch: 571 loss: 16.838755445577824 loss_input: 82.23553080961925
step: 10000 epoch: 571 loss: 16.880822708673232 loss_input: 82.34857421240298
step: 11000 epoch: 571 loss: 16.8814371269775 loss_input: 82.30206529631006
step: 12000 epoch: 571 loss: 16.86434543171522 loss_input: 82.1745627134425
step: 13000 epoch: 571 loss: 16.8739709402816 loss_input: 82.22214854228461
step: 14000 epoch: 571 loss: 16.864389473723904 loss_input: 82.17059670124009
step: 15000 epoch: 571 loss: 16.886541366370533 loss_input: 82.257917200865
Save loss: 16.875539875328542 Name: 571_train_model.pth
step: 0 epoch: 572 loss: 21.343534469604492 loss_input: 64.27972412109375
step: 1000 epoch: 572 loss: 16.93195234836041 loss_input: 82.49710579947396
step: 2000 epoch: 572 loss: 16.921994035330968 loss_input: 82.85074369553504
step: 3000 epoch: 572 loss: 16.91732755155414 loss_input: 83.0560035692855
step: 4000 epoch: 572 loss: 16.93976025359686 loss_input: 82.65391748376382
step: 5000 epoch: 572 loss: 16.939289767321196 loss_input: 82.6461369713362
step: 6000 epoch: 572 loss: 16.91884608801912 loss_input: 82.45680478627911
step: 7000 epoch: 572 loss: 16.914209350349733 loss_input: 82.47725144489682
step: 8000 epoch: 572 loss: 16.90000142638139 loss_input: 82.41642933061102
step: 9000 epoch: 572 loss: 16.902593249705696 loss_input: 82.3287867984829
step: 10000 epoch: 572 loss: 16.89393660686288 loss_input: 82.25124698833828
step: 11000 epoch: 572 loss: 16.88433285327773 loss_input: 82.15996253647573
step: 12000 epoch: 572 loss: 16.86783812085983 loss_input: 82.07853256389605
step: 13000 epoch: 572 loss: 16.882328842082543 loss_input: 82.18532662545338
step: 14000 epoch: 572 loss: 16.872520835491073 loss_input: 82.15421332383903
step: 15000 epoch: 572 loss: 16.88098478693937 loss_input: 82.23150762117162
Save loss: 16.88794986627996 Name: 572_train_model.pth
step: 0 epoch: 573 loss: 14.97457504272461 loss_input: 68.506591796875
step: 1000 epoch: 573 loss: 17.06342596202702 loss_input: 83.140448174872
step: 2000 epoch: 573 loss: 17.042562283616494 loss_input: 82.86139431504951
step: 3000 epoch: 573 loss: 16.98472858563696 loss_input: 82.48750998202422
step: 4000 epoch: 573 loss: 16.923192439690677 loss_input: 82.22100201573141
step: 5000 epoch: 573 loss: 16.94882211082579 loss_input: 82.24997633529935
step: 6000 epoch: 573 loss: 16.913891712798016 loss_input: 82.35719081406354
step: 7000 epoch: 573 loss: 16.916523488346737 loss_input: 82.33166901449904
step: 8000 epoch: 573 loss: 16.887015568734764 loss_input: 82.20772023553208
step: 9000 epoch: 573 loss: 16.910843634708712 loss_input: 82.1813651632566
step: 10000 epoch: 573 loss: 16.897702224659163 loss_input: 82.1455485371873
step: 11000 epoch: 573 loss: 16.897336296466964 loss_input: 82.13892361658529
step: 12000 epoch: 573 loss: 16.886540015512043 loss_input: 82.18873317868857
step: 13000 epoch: 573 loss: 16.87747159452404 loss_input: 82.23330861943913
step: 14000 epoch: 573 loss: 16.879181317283155 loss_input: 82.271395339786
step: 15000 epoch: 573 loss: 16.888985850589926 loss_input: 82.23446210763683
Save loss: 16.889195389166474 Name: 573_train_model.pth
step: 0 epoch: 574 loss: 19.92254638671875 loss_input: 81.38177490234375
step: 1000 epoch: 574 loss: 16.75845811512325 loss_input: 82.04546466716877
step: 2000 epoch: 574 loss: 16.754628856798103 loss_input: 81.97978934116927
step: 3000 epoch: 574 loss: 16.837452185070543 loss_input: 82.24845530819154
step: 4000 epoch: 574 loss: 16.787373771252735 loss_input: 82.3067510100014
step: 5000 epoch: 574 loss: 16.84098611243175 loss_input: 82.17765538689137
step: 6000 epoch: 574 loss: 16.85555452379539 loss_input: 82.11694353227912
step: 7000 epoch: 574 loss: 16.85940436958637 loss_input: 82.23289582375509
step: 8000 epoch: 574 loss: 16.852603449849482 loss_input: 82.09475380366273
step: 9000 epoch: 574 loss: 16.86007347958258 loss_input: 82.12647140090988
step: 10000 epoch: 574 loss: 16.850405804122307 loss_input: 81.99836424438611
step: 11000 epoch: 574 loss: 16.88594045507703 loss_input: 82.11226458549933
step: 12000 epoch: 574 loss: 16.891427308040385 loss_input: 82.17425303160772
step: 13000 epoch: 574 loss: 16.896941200236835 loss_input: 82.23596249150457
step: 14000 epoch: 574 loss: 16.909052530124132 loss_input: 82.33779898725844
step: 15000 epoch: 574 loss: 16.878435296667757 loss_input: 82.178725117056
Save loss: 16.87762985868752 Name: 574_train_model.pth
step: 0 epoch: 575 loss: 17.8452091217041 loss_input: 48.20806884765625
step: 1000 epoch: 575 loss: 16.635068580939933 loss_input: 81.87735098153799
step: 2000 epoch: 575 loss: 16.7873165568133 loss_input: 81.91352776370604
step: 3000 epoch: 575 loss: 16.826902076825426 loss_input: 81.95583902045672
step: 4000 epoch: 575 loss: 16.8485479217802 loss_input: 82.06504678761951
step: 5000 epoch: 575 loss: 16.818982642833006 loss_input: 82.23041481791532
step: 6000 epoch: 575 loss: 16.79935889612772 loss_input: 82.10189636769523
step: 7000 epoch: 575 loss: 16.806309230769298 loss_input: 82.30375535602894
step: 8000 epoch: 575 loss: 16.824680010984277 loss_input: 82.3615541328208
step: 9000 epoch: 575 loss: 16.830428624706737 loss_input: 82.43031941492178
step: 10000 epoch: 575 loss: 16.831474936016797 loss_input: 82.24875014939913
step: 11000 epoch: 575 loss: 16.839800262784927 loss_input: 82.31958885529228
step: 12000 epoch: 575 loss: 16.85717951002821 loss_input: 82.35030515340355
step: 13000 epoch: 575 loss: 16.864864650648123 loss_input: 82.3748056461477
step: 14000 epoch: 575 loss: 16.863284441992416 loss_input: 82.31064505611485
step: 15000 epoch: 575 loss: 16.850092183859395 loss_input: 82.13835882620846
Save loss: 16.87648185968399 Name: 575_train_model.pth
step: 0 epoch: 576 loss: 18.262636184692383 loss_input: 118.59136962890625
step: 1000 epoch: 576 loss: 17.04138887416828 loss_input: 81.8000216336398
step: 2000 epoch: 576 loss: 16.883727639868876 loss_input: 82.11386918235219
step: 3000 epoch: 576 loss: 16.921585513289394 loss_input: 82.50434006535582
step: 4000 epoch: 576 loss: 16.89893489883888 loss_input: 82.04280233347424
step: 5000 epoch: 576 loss: 16.921320386229457 loss_input: 82.08986234979567
step: 6000 epoch: 576 loss: 16.916020123288344 loss_input: 82.01788509084749
step: 7000 epoch: 576 loss: 16.92086433965876 loss_input: 82.09469346001632
step: 8000 epoch: 576 loss: 16.88980669421623 loss_input: 82.08290659834513
step: 9000 epoch: 576 loss: 16.870392600293556 loss_input: 82.09138295343486
step: 10000 epoch: 576 loss: 16.887687823424613 loss_input: 82.14649027650945
step: 11000 epoch: 576 loss: 16.910195199546678 loss_input: 82.25853772038124
step: 12000 epoch: 576 loss: 16.90587093051141 loss_input: 82.17555827155192
step: 13000 epoch: 576 loss: 16.890192651480916 loss_input: 82.17441910173827
step: 14000 epoch: 576 loss: 16.879208729595945 loss_input: 82.15469578100728
step: 15000 epoch: 576 loss: 16.876218256255196 loss_input: 82.20232803164684
Save loss: 16.88999697583914 Name: 576_train_model.pth
step: 0 epoch: 577 loss: 14.879545211791992 loss_input: 73.6629638671875
step: 1000 epoch: 577 loss: 16.547361454406342 loss_input: 81.60693432544018
step: 2000 epoch: 577 loss: 16.747007935598813 loss_input: 82.36287105470703
step: 3000 epoch: 577 loss: 16.727494343881883 loss_input: 82.34842115209644
step: 4000 epoch: 577 loss: 16.72113783053832 loss_input: 82.48543117821619
step: 5000 epoch: 577 loss: 16.74058217922227 loss_input: 82.3504366021327
step: 6000 epoch: 577 loss: 16.76184392376674 loss_input: 82.4640627827491
step: 7000 epoch: 577 loss: 16.771975577754507 loss_input: 82.26482040128475
step: 8000 epoch: 577 loss: 16.80523652932656 loss_input: 82.36535023576154
step: 9000 epoch: 577 loss: 16.823670289686238 loss_input: 82.41037176831698
step: 10000 epoch: 577 loss: 16.819041711856645 loss_input: 82.40549463730385
step: 11000 epoch: 577 loss: 16.821831181270188 loss_input: 82.34568597013198
step: 12000 epoch: 577 loss: 16.847704173902525 loss_input: 82.29889533436504
step: 13000 epoch: 577 loss: 16.863162369922843 loss_input: 82.33896290673704
step: 14000 epoch: 577 loss: 16.87153463865653 loss_input: 82.30767193212482
step: 15000 epoch: 577 loss: 16.874879152788637 loss_input: 82.21561570781348
Save loss: 16.879769967973232 Name: 577_train_model.pth
step: 0 epoch: 578 loss: 23.589609146118164 loss_input: 69.13787841796875
step: 1000 epoch: 578 loss: 17.01415842324942 loss_input: 81.7898565057989
step: 2000 epoch: 578 loss: 16.94208676573159 loss_input: 82.12583497069825
step: 3000 epoch: 578 loss: 16.882079484501666 loss_input: 82.41432785344338
step: 4000 epoch: 578 loss: 16.85686042230745 loss_input: 82.29588588265531
step: 5000 epoch: 578 loss: 16.88474556904415 loss_input: 82.43187512399864
step: 6000 epoch: 578 loss: 16.881927104696317 loss_input: 82.43034828871295
step: 7000 epoch: 578 loss: 16.911045424411373 loss_input: 82.52273319387687
step: 8000 epoch: 578 loss: 16.89322694023465 loss_input: 82.42225021249786
step: 9000 epoch: 578 loss: 16.856792204990054 loss_input: 82.21125591893869
step: 10000 epoch: 578 loss: 16.894573298684 loss_input: 82.30545919407082
step: 11000 epoch: 578 loss: 16.90081908856508 loss_input: 82.28033981552103
step: 12000 epoch: 578 loss: 16.890246273765822 loss_input: 82.19987990909611
step: 13000 epoch: 578 loss: 16.881213298569037 loss_input: 82.13119038075045
step: 14000 epoch: 578 loss: 16.874266421775307 loss_input: 82.1869719964199
step: 15000 epoch: 578 loss: 16.882642707350445 loss_input: 82.19309134273861
Save loss: 16.877363021433354 Name: 578_train_model.pth
step: 0 epoch: 579 loss: 10.14682388305664 loss_input: 42.4453125
step: 1000 epoch: 579 loss: 16.84421989562866 loss_input: 81.13337888917722
step: 2000 epoch: 579 loss: 16.765675131765857 loss_input: 81.63514177969608
step: 3000 epoch: 579 loss: 16.775598456405948 loss_input: 81.73864131877916
step: 4000 epoch: 579 loss: 16.766227119834802 loss_input: 81.85655202105235
step: 5000 epoch: 579 loss: 16.77749089559682 loss_input: 82.04877746374554
step: 6000 epoch: 579 loss: 16.80466211892986 loss_input: 82.25455019581698
step: 7000 epoch: 579 loss: 16.78881602075471 loss_input: 82.2197173300717
step: 8000 epoch: 579 loss: 16.80949837072926 loss_input: 82.2587070186173
step: 9000 epoch: 579 loss: 16.820660202466915 loss_input: 82.31826050225105
step: 10000 epoch: 579 loss: 16.84196673103743 loss_input: 82.33190686058347
step: 11000 epoch: 579 loss: 16.8487531432086 loss_input: 82.36951970763667
step: 12000 epoch: 579 loss: 16.870174823527673 loss_input: 82.36995582907967
step: 13000 epoch: 579 loss: 16.873149346245334 loss_input: 82.34938001783065
step: 14000 epoch: 579 loss: 16.883110523428222 loss_input: 82.3944201303562
step: 15000 epoch: 579 loss: 16.878297239977094 loss_input: 82.28681237738448
Save loss: 16.874884302318097 Name: 579_train_model.pth
step: 0 epoch: 580 loss: 21.496074676513672 loss_input: 142.945068359375
step: 1000 epoch: 580 loss: 16.82437878388625 loss_input: 82.30806156733891
step: 2000 epoch: 580 loss: 16.82136070603195 loss_input: 82.64321507459161
step: 3000 epoch: 580 loss: 16.909564363364577 loss_input: 82.94241596388443
step: 4000 epoch: 580 loss: 16.89071497336771 loss_input: 82.83123397922492
step: 5000 epoch: 580 loss: 16.84648351987775 loss_input: 82.69876387271374
step: 6000 epoch: 580 loss: 16.867056682693462 loss_input: 82.65257199361629
step: 7000 epoch: 580 loss: 16.87444810447753 loss_input: 82.4321772043168
step: 8000 epoch: 580 loss: 16.894456233520565 loss_input: 82.56619049480388
step: 9000 epoch: 580 loss: 16.897610759671537 loss_input: 82.62134352557833
step: 10000 epoch: 580 loss: 16.869479371623843 loss_input: 82.40468367103719
step: 11000 epoch: 580 loss: 16.881442339894296 loss_input: 82.3343994706536
step: 12000 epoch: 580 loss: 16.881438474338875 loss_input: 82.35383649059519
step: 13000 epoch: 580 loss: 16.886471814572303 loss_input: 82.30449426347462
step: 14000 epoch: 580 loss: 16.874250741219846 loss_input: 82.38127005123239
step: 15000 epoch: 580 loss: 16.86312318746189 loss_input: 82.28991896145726
Save loss: 16.876478242635727 Name: 580_train_model.pth
step: 0 epoch: 581 loss: 22.810997009277344 loss_input: 99.306396484375
step: 1000 epoch: 581 loss: 17.0041220247686 loss_input: 83.22957797817416
step: 2000 epoch: 581 loss: 16.84116199241764 loss_input: 82.51134625558315
step: 3000 epoch: 581 loss: 16.87438614874512 loss_input: 82.41323835251531
step: 4000 epoch: 581 loss: 16.846276069515735 loss_input: 82.41860129755159
step: 5000 epoch: 581 loss: 16.918723835465528 loss_input: 82.55907497699678
step: 6000 epoch: 581 loss: 16.931620639754303 loss_input: 82.57528368684972
step: 7000 epoch: 581 loss: 16.928168024408563 loss_input: 82.56253099271254
step: 8000 epoch: 581 loss: 16.94461790628246 loss_input: 82.46175191235861
step: 9000 epoch: 581 loss: 16.96827213979538 loss_input: 82.64785332282958
step: 10000 epoch: 581 loss: 16.930177499813837 loss_input: 82.42995367211326
step: 11000 epoch: 581 loss: 16.92604133480517 loss_input: 82.42896626873586
step: 12000 epoch: 581 loss: 16.923468832651402 loss_input: 82.40516463105614
step: 13000 epoch: 581 loss: 16.91042611898069 loss_input: 82.3098268141408
step: 14000 epoch: 581 loss: 16.88979671798547 loss_input: 82.24919179095328
step: 15000 epoch: 581 loss: 16.8848044002432 loss_input: 82.19538343032991
Save loss: 16.88751077029109 Name: 581_train_model.pth
step: 0 epoch: 582 loss: 28.121328353881836 loss_input: 93.2041015625
step: 1000 epoch: 582 loss: 16.831979655838392 loss_input: 81.15748326380651
step: 2000 epoch: 582 loss: 16.81811797481844 loss_input: 81.88145621403166
step: 3000 epoch: 582 loss: 16.854043563339403 loss_input: 81.9177019932158
step: 4000 epoch: 582 loss: 16.913265487248527 loss_input: 82.20124436664271
step: 5000 epoch: 582 loss: 16.90597810585054 loss_input: 82.08781144161793
step: 6000 epoch: 582 loss: 16.88749846004085 loss_input: 82.05842128322037
step: 7000 epoch: 582 loss: 16.882923992169516 loss_input: 82.23090895113477
step: 8000 epoch: 582 loss: 16.87186308178868 loss_input: 82.20629724140927
step: 9000 epoch: 582 loss: 16.866809653568342 loss_input: 82.13447300750221
step: 10000 epoch: 582 loss: 16.84539361858759 loss_input: 82.11249309029058
step: 11000 epoch: 582 loss: 16.85492462704869 loss_input: 82.19765357357427
step: 12000 epoch: 582 loss: 16.876604791105155 loss_input: 82.3991598163125
step: 13000 epoch: 582 loss: 16.882865337067113 loss_input: 82.31280897844847
step: 14000 epoch: 582 loss: 16.880081701326368 loss_input: 82.32872825019335
step: 15000 epoch: 582 loss: 16.879592945111465 loss_input: 82.32225756075897
Save loss: 16.869937214925883 Name: 582_train_model.pth
step: 0 epoch: 583 loss: 25.351036071777344 loss_input: 104.51983642578125
step: 1000 epoch: 583 loss: 16.851418934382878 loss_input: 82.98876623864417
step: 2000 epoch: 583 loss: 16.823094598535654 loss_input: 82.48320151447713
step: 3000 epoch: 583 loss: 16.884913574731655 loss_input: 82.79865105999824
step: 4000 epoch: 583 loss: 16.883576607114225 loss_input: 82.59934506729763
step: 5000 epoch: 583 loss: 16.83697589491158 loss_input: 82.41021702885985
step: 6000 epoch: 583 loss: 16.828181127293313 loss_input: 82.16325577003443
step: 7000 epoch: 583 loss: 16.812747502289504 loss_input: 82.03821601099396
step: 8000 epoch: 583 loss: 16.828329659479735 loss_input: 82.25850900717488
step: 9000 epoch: 583 loss: 16.847264823245016 loss_input: 82.27854411033323
step: 10000 epoch: 583 loss: 16.865971931397063 loss_input: 82.25260458573284
step: 11000 epoch: 583 loss: 16.86366254506832 loss_input: 82.222183414789
step: 12000 epoch: 583 loss: 16.87342755282564 loss_input: 82.19119983104436
step: 13000 epoch: 583 loss: 16.880212271931484 loss_input: 82.18864310993506
step: 14000 epoch: 583 loss: 16.864288292206336 loss_input: 82.14210525838556
step: 15000 epoch: 583 loss: 16.85989689739551 loss_input: 82.16145541875984
Save loss: 16.869576302170753 Name: 583_train_model.pth
step: 0 epoch: 584 loss: 17.05672264099121 loss_input: 60.919189453125
step: 1000 epoch: 584 loss: 16.871559151640902 loss_input: 81.46866890433785
step: 2000 epoch: 584 loss: 16.84308971660486 loss_input: 81.64249933748945
step: 3000 epoch: 584 loss: 16.882563225550083 loss_input: 81.62017777521424
read
Begin
step: 0 epoch: 584 loss: 11.51194953918457 loss_input: 53.43975830078125
step: 1000 epoch: 584 loss: 16.748488820158876 loss_input: 81.58251465331543
step: 2000 epoch: 584 loss: 16.81821217577437 loss_input: 82.0842569132914
step: 3000 epoch: 584 loss: 16.863989814763386 loss_input: 82.47152843176623
step: 4000 epoch: 584 loss: 16.843197557992323 loss_input: 82.44090234765527
step: 5000 epoch: 584 loss: 16.854322834792935 loss_input: 82.24813136737887
step: 6000 epoch: 584 loss: 16.864193237735677 loss_input: 82.38587871056322
step: 7000 epoch: 584 loss: 16.858756598668887 loss_input: 82.42497690410856
step: 8000 epoch: 584 loss: 16.83400033524805 loss_input: 82.26544641819675
step: 9000 epoch: 584 loss: 16.85811500943458 loss_input: 82.25824746279277
step: 10000 epoch: 584 loss: 16.876493737526197 loss_input: 82.20156694047392
step: 11000 epoch: 584 loss: 16.893055768893337 loss_input: 82.33256291220506
step: 12000 epoch: 584 loss: 16.905177073005476 loss_input: 82.28341420767019
step: 13000 epoch: 584 loss: 16.900791193373284 loss_input: 82.2342857875491
step: 14000 epoch: 584 loss: 16.881562330068736 loss_input: 82.1971026466162
step: 15000 epoch: 584 loss: 16.886248458394146 loss_input: 82.22978274430142
Save loss: 16.870727628678083 Name: 584_train_model.pth
step: 0 epoch: 585 loss: 11.77043628692627 loss_input: 59.44635009765625
step: 1000 epoch: 585 loss: 16.712901768984494 loss_input: 80.88196062970233
step: 2000 epoch: 585 loss: 16.61801529740882 loss_input: 81.42839307275074
step: 3000 epoch: 585 loss: 16.71322523614082 loss_input: 81.69333106412446
step: 4000 epoch: 585 loss: 16.8096084865264 loss_input: 82.00967583421527
step: 5000 epoch: 585 loss: 16.813361656472722 loss_input: 81.89637237347452
step: 6000 epoch: 585 loss: 16.80060935429664 loss_input: 81.90124100817181
step: 7000 epoch: 585 loss: 16.78339416499547 loss_input: 81.90916014160365
step: 8000 epoch: 585 loss: 16.789470173600108 loss_input: 82.19854434241863
step: 9000 epoch: 585 loss: 16.773948962126106 loss_input: 82.10837586378206
step: 10000 epoch: 585 loss: 16.791482043783613 loss_input: 82.06995335622688
step: 11000 epoch: 585 loss: 16.807614988353553 loss_input: 82.16655265226244
step: 12000 epoch: 585 loss: 16.81629694649482 loss_input: 82.1264579676954
step: 13000 epoch: 585 loss: 16.83341166881605 loss_input: 82.20597300996377
step: 14000 epoch: 585 loss: 16.849193051750085 loss_input: 82.26158783894063
step: 15000 epoch: 585 loss: 16.864698349098454 loss_input: 82.22383909350705
Save loss: 16.876312027037145 Name: 585_train_model.pth
step: 0 epoch: 586 loss: 11.93431568145752 loss_input: 61.99700927734375
step: 1000 epoch: 586 loss: 16.758386419488716 loss_input: 82.20510306344046
step: 2000 epoch: 586 loss: 16.806546036569195 loss_input: 82.03693779941084
step: 3000 epoch: 586 loss: 16.762787984951622 loss_input: 81.81632178515365
step: 4000 epoch: 586 loss: 16.778329884877834 loss_input: 81.98739798692905
step: 5000 epoch: 586 loss: 16.79678916926385 loss_input: 81.82849185778079
step: 6000 epoch: 586 loss: 16.796313393773527 loss_input: 81.9236743928909
step: 7000 epoch: 586 loss: 16.79442612594612 loss_input: 81.91314610956805
step: 8000 epoch: 586 loss: 16.814659023624618 loss_input: 82.10028977853598
step: 9000 epoch: 586 loss: 16.81024656689388 loss_input: 82.20170713453713
step: 10000 epoch: 586 loss: 16.842772277542714 loss_input: 82.30568675920495
step: 11000 epoch: 586 loss: 16.853641636815247 loss_input: 82.41356849861128
step: 12000 epoch: 586 loss: 16.86278733656055 loss_input: 82.33124566584783
step: 13000 epoch: 586 loss: 16.871182273675934 loss_input: 82.35255247981225
step: 14000 epoch: 586 loss: 16.868958317001464 loss_input: 82.29249811972834
step: 15000 epoch: 586 loss: 16.87129360686524 loss_input: 82.2556730959147
Save loss: 16.868369216665627 Name: 586_train_model.pth
step: 0 epoch: 587 loss: 18.171676635742188 loss_input: 83.262939453125
step: 1000 epoch: 587 loss: 16.802303118424696 loss_input: 81.71527477649303
step: 2000 epoch: 587 loss: 16.951129400748005 loss_input: 82.38893581163519
step: 3000 epoch: 587 loss: 16.83774963389711 loss_input: 82.6189221512751
step: 4000 epoch: 587 loss: 16.805822067098656 loss_input: 82.45111401287862
step: 5000 epoch: 587 loss: 16.817888404721856 loss_input: 82.19532742621398
step: 6000 epoch: 587 loss: 16.84153100304396 loss_input: 81.95932266700667
step: 7000 epoch: 587 loss: 16.830770912995902 loss_input: 81.9487218204319
step: 8000 epoch: 587 loss: 16.826106116825635 loss_input: 82.06534149509388
step: 9000 epoch: 587 loss: 16.839078717093482 loss_input: 82.12398585080173
step: 10000 epoch: 587 loss: 16.86962071719998 loss_input: 82.12247212168431
step: 11000 epoch: 587 loss: 16.840107725681083 loss_input: 82.09000536749855
step: 12000 epoch: 587 loss: 16.841880339660722 loss_input: 82.0656920303435
step: 13000 epoch: 587 loss: 16.84838645585307 loss_input: 82.28566835190863
step: 14000 epoch: 587 loss: 16.859802971175036 loss_input: 82.2913126575292
step: 15000 epoch: 587 loss: 16.863062863572107 loss_input: 82.36416741325318
Save loss: 16.866027930110693 Name: 587_train_model.pth
step: 0 epoch: 588 loss: 20.02962875366211 loss_input: 83.53106689453125
step: 1000 epoch: 588 loss: 16.519479421945242 loss_input: 81.02601603718547
step: 2000 epoch: 588 loss: 16.69933456018649 loss_input: 81.11563551110187
step: 3000 epoch: 588 loss: 16.765895662209225 loss_input: 81.51887833043283
step: 4000 epoch: 588 loss: 16.775028078474424 loss_input: 81.81793606468989
step: 5000 epoch: 588 loss: 16.799646257901284 loss_input: 82.19927912610837
step: 6000 epoch: 588 loss: 16.830903547760883 loss_input: 82.25287951479116
step: 7000 epoch: 588 loss: 16.842174656304984 loss_input: 82.10892441565268
step: 8000 epoch: 588 loss: 16.86252061093901 loss_input: 82.01300183088686
step: 9000 epoch: 588 loss: 16.870293841549003 loss_input: 82.05537942615751
step: 10000 epoch: 588 loss: 16.89086604034909 loss_input: 82.26123214094606
step: 11000 epoch: 588 loss: 16.873375882149695 loss_input: 82.2165886253327
step: 12000 epoch: 588 loss: 16.855414119941692 loss_input: 82.19869768034467
step: 13000 epoch: 588 loss: 16.85101125266183 loss_input: 82.24140415055211
step: 14000 epoch: 588 loss: 16.866304115519917 loss_input: 82.24832264701517
step: 15000 epoch: 588 loss: 16.879254954106475 loss_input: 82.26924773211734
Save loss: 16.870716978535057 Name: 588_train_model.pth
step: 0 epoch: 589 loss: 17.412437438964844 loss_input: 70.4755859375
step: 1000 epoch: 589 loss: 16.698147624641745 loss_input: 81.70128571379792
step: 2000 epoch: 589 loss: 16.732653418998012 loss_input: 81.79704983933814
step: 3000 epoch: 589 loss: 16.782721288042918 loss_input: 81.6716236392882
step: 4000 epoch: 589 loss: 16.851824003110913 loss_input: 81.91658393385052
step: 5000 epoch: 589 loss: 16.856941837092634 loss_input: 81.94957644213345
step: 6000 epoch: 589 loss: 16.857374366015875 loss_input: 81.96265720443077
step: 7000 epoch: 589 loss: 16.835442021376338 loss_input: 81.85868049297925
step: 8000 epoch: 589 loss: 16.81729666654236 loss_input: 81.9158287663383
step: 9000 epoch: 589 loss: 16.821881132408745 loss_input: 81.96975186215204
step: 10000 epoch: 589 loss: 16.834398108593835 loss_input: 81.95786403415323
step: 11000 epoch: 589 loss: 16.84473670347096 loss_input: 81.95355269549358
step: 12000 epoch: 589 loss: 16.844201878362593 loss_input: 82.04061828364075
step: 13000 epoch: 589 loss: 16.847883334518183 loss_input: 82.13750084597609
step: 14000 epoch: 589 loss: 16.845885126226825 loss_input: 82.18938150535983
step: 15000 epoch: 589 loss: 16.861505710763158 loss_input: 82.29107458509928
Save loss: 16.867040095344187 Name: 589_train_model.pth
step: 0 epoch: 590 loss: 26.792171478271484 loss_input: 70.3941650390625
step: 1000 epoch: 590 loss: 16.907439581997746 loss_input: 80.72810075666521
step: 2000 epoch: 590 loss: 16.96837469698607 loss_input: 82.08920051692904
step: 3000 epoch: 590 loss: 16.91844315562237 loss_input: 82.15394817531526
step: 4000 epoch: 590 loss: 16.86695718228951 loss_input: 82.21995941444327
step: 5000 epoch: 590 loss: 16.894773398034932 loss_input: 82.38492229532609
step: 6000 epoch: 590 loss: 16.862542492253564 loss_input: 82.27082280313307
step: 7000 epoch: 590 loss: 16.883265714341615 loss_input: 82.50868859554662
step: 8000 epoch: 590 loss: 16.895877269458328 loss_input: 82.5354174630759
step: 9000 epoch: 590 loss: 16.88580465533974 loss_input: 82.48015495565596
step: 10000 epoch: 590 loss: 16.883618547372635 loss_input: 82.40119069758063
step: 11000 epoch: 590 loss: 16.90523491732782 loss_input: 82.41712943199234
step: 12000 epoch: 590 loss: 16.90396575043275 loss_input: 82.35346727901256
step: 13000 epoch: 590 loss: 16.90261960233159 loss_input: 82.49507202965307
step: 14000 epoch: 590 loss: 16.88344757870754 loss_input: 82.37958160807989
step: 15000 epoch: 590 loss: 16.87620705145612 loss_input: 82.27511569622573
Save loss: 16.857970359891652 Name: 590_train_model.pth
step: 0 epoch: 591 loss: 18.187864303588867 loss_input: 87.07257080078125
step: 1000 epoch: 591 loss: 16.855534715966865 loss_input: 82.59943846436767
step: 2000 epoch: 591 loss: 16.778127892383154 loss_input: 81.99225521516586
step: 3000 epoch: 591 loss: 16.692449102478 loss_input: 81.84349819184621
step: 4000 epoch: 591 loss: 16.704393850508644 loss_input: 81.77283193849975
step: 5000 epoch: 591 loss: 16.725237865110465 loss_input: 81.71179642245379
step: 6000 epoch: 591 loss: 16.73234353739308 loss_input: 81.84830487439919
step: 7000 epoch: 591 loss: 16.77031887733771 loss_input: 82.06954985260197
step: 8000 epoch: 591 loss: 16.771532938281023 loss_input: 82.04265236041051
step: 9000 epoch: 591 loss: 16.782619646238945 loss_input: 82.18189072606299
step: 10000 epoch: 591 loss: 16.779983203132897 loss_input: 82.01102641811015
step: 11000 epoch: 591 loss: 16.812705606647473 loss_input: 82.19015412772312
step: 12000 epoch: 591 loss: 16.818421353122492 loss_input: 82.13979085116931
step: 13000 epoch: 591 loss: 16.82883854371182 loss_input: 82.1923941406926
step: 14000 epoch: 591 loss: 16.835991691482143 loss_input: 82.24841693523705
step: 15000 epoch: 591 loss: 16.853697044945868 loss_input: 82.26576321100077
Save loss: 16.864815680101515 Name: 591_train_model.pth
step: 0 epoch: 592 loss: 17.637493133544922 loss_input: 138.531982421875
step: 1000 epoch: 592 loss: 16.963985426918967 loss_input: 82.12644533201174
step: 2000 epoch: 592 loss: 16.889073987176335 loss_input: 81.7726634424487
step: 3000 epoch: 592 loss: 16.87727626027047 loss_input: 82.1782225748969
step: 4000 epoch: 592 loss: 16.830145344261048 loss_input: 81.92098371281203
step: 5000 epoch: 592 loss: 16.84174649159066 loss_input: 82.01063705715887
step: 6000 epoch: 592 loss: 16.84564703699947 loss_input: 82.14029680762842
step: 7000 epoch: 592 loss: 16.832702478703865 loss_input: 82.1987663174234
step: 8000 epoch: 592 loss: 16.839712342326752 loss_input: 82.27729736907887
step: 9000 epoch: 592 loss: 16.865840242857562 loss_input: 82.36362464988699
step: 10000 epoch: 592 loss: 16.8729601326662 loss_input: 82.44545314755634
step: 11000 epoch: 592 loss: 16.88800410625772 loss_input: 82.47408905875389
step: 12000 epoch: 592 loss: 16.87932257284354 loss_input: 82.39189500525022
step: 13000 epoch: 592 loss: 16.88684109599487 loss_input: 82.44718904585464
step: 14000 epoch: 592 loss: 16.89333162063548 loss_input: 82.44368684881883
step: 15000 epoch: 592 loss: 16.862889097369628 loss_input: 82.24390849529303
Save loss: 16.8599239218086 Name: 592_train_model.pth
step: 0 epoch: 593 loss: 10.162787437438965 loss_input: 54.0528564453125
step: 1000 epoch: 593 loss: 16.73943851187036 loss_input: 81.26446575885052
step: 2000 epoch: 593 loss: 16.793775500803218 loss_input: 81.39273332083958
step: 3000 epoch: 593 loss: 16.783593566288197 loss_input: 81.71868314809777
step: 4000 epoch: 593 loss: 16.75395000281855 loss_input: 81.52878891483184
step: 5000 epoch: 593 loss: 16.783814924999465 loss_input: 81.68033095041147
step: 6000 epoch: 593 loss: 16.81404571377462 loss_input: 81.80458627989086
step: 7000 epoch: 593 loss: 16.803488573573723 loss_input: 81.90214579767064
step: 8000 epoch: 593 loss: 16.82190017726418 loss_input: 81.96998607443356
step: 9000 epoch: 593 loss: 16.826860022272033 loss_input: 82.00146236871034
step: 10000 epoch: 593 loss: 16.841991019527885 loss_input: 82.15203194036066
step: 11000 epoch: 593 loss: 16.85069485883606 loss_input: 82.17837645086242
step: 12000 epoch: 593 loss: 16.857500680952548 loss_input: 82.18524420145164
step: 13000 epoch: 593 loss: 16.86160410368372 loss_input: 82.19224748449338
step: 14000 epoch: 593 loss: 16.854656165160925 loss_input: 82.16755357928535
step: 15000 epoch: 593 loss: 16.84991193315155 loss_input: 82.16083750747997
Save loss: 16.861561600118876 Name: 593_train_model.pth
step: 0 epoch: 594 loss: 13.110013961791992 loss_input: 44.99884033203125
step: 1000 epoch: 594 loss: 17.00386053222519 loss_input: 83.29389811848309
step: 2000 epoch: 594 loss: 16.823578756967226 loss_input: 82.5126704226012
step: 3000 epoch: 594 loss: 16.84718659932277 loss_input: 82.21543506493929
step: 4000 epoch: 594 loss: 16.885974553012158 loss_input: 82.45171705730795
step: 5000 epoch: 594 loss: 16.829871594631726 loss_input: 82.26116286752415
step: 6000 epoch: 594 loss: 16.829042075614375 loss_input: 82.30976934548994
step: 7000 epoch: 594 loss: 16.79631933990912 loss_input: 82.25718344819323
step: 8000 epoch: 594 loss: 16.82208079800667 loss_input: 82.28193437795373
step: 9000 epoch: 594 loss: 16.82887574749885 loss_input: 82.35188050530193
step: 10000 epoch: 594 loss: 16.85376459979353 loss_input: 82.48210775846243
step: 11000 epoch: 594 loss: 16.860928150645474 loss_input: 82.34616686125125
step: 12000 epoch: 594 loss: 16.84843262147232 loss_input: 82.33230910465306
step: 13000 epoch: 594 loss: 16.853630993624996 loss_input: 82.36991511564165
step: 14000 epoch: 594 loss: 16.846489621659583 loss_input: 82.31103605863393
step: 15000 epoch: 594 loss: 16.84173353986179 loss_input: 82.2280149503705
Save loss: 16.859267540246247 Name: 594_train_model.pth
step: 0 epoch: 595 loss: 16.41881561279297 loss_input: 72.4688720703125
step: 1000 epoch: 595 loss: 17.030077495060482 loss_input: 82.98056008932474
step: 2000 epoch: 595 loss: 16.964803216577707 loss_input: 82.49603408744846
step: 3000 epoch: 595 loss: 16.834875611375466 loss_input: 81.81905474054221
step: 4000 epoch: 595 loss: 16.850327930162024 loss_input: 82.17066878910155
step: 5000 epoch: 595 loss: 16.84959778879147 loss_input: 82.11026011350464
step: 6000 epoch: 595 loss: 16.842726536183292 loss_input: 82.08711779441383
step: 7000 epoch: 595 loss: 16.818067597552822 loss_input: 81.93190251560046
step: 8000 epoch: 595 loss: 16.83354072933748 loss_input: 82.00983196192273
step: 9000 epoch: 595 loss: 16.82502194627525 loss_input: 81.95030756402501
step: 10000 epoch: 595 loss: 16.82377074108328 loss_input: 81.98124301949687
step: 11000 epoch: 595 loss: 16.827448417550706 loss_input: 82.03046766145371
step: 12000 epoch: 595 loss: 16.842821734993173 loss_input: 82.17478819920932
step: 13000 epoch: 595 loss: 16.83659182604565 loss_input: 82.12425644241308
step: 14000 epoch: 595 loss: 16.83519700136996 loss_input: 82.19021209091231
step: 15000 epoch: 595 loss: 16.844235851116064 loss_input: 82.13169198143952
Save loss: 16.86489669881761 Name: 595_train_model.pth
step: 0 epoch: 596 loss: 25.42861557006836 loss_input: 88.0772705078125
step: 1000 epoch: 596 loss: 16.73893769756778 loss_input: 81.80985939109718
step: 2000 epoch: 596 loss: 16.784376836430724 loss_input: 82.19807872552981
step: 3000 epoch: 596 loss: 16.81868493243163 loss_input: 82.5934928428805
step: 4000 epoch: 596 loss: 16.84835999752694 loss_input: 82.71522420908445
step: 5000 epoch: 596 loss: 16.824696429274937 loss_input: 82.52210401425575
step: 6000 epoch: 596 loss: 16.83109052172106 loss_input: 82.44284049989065
step: 7000 epoch: 596 loss: 16.806673173205613 loss_input: 82.51800229361766
step: 8000 epoch: 596 loss: 16.821502275279784 loss_input: 82.36445455762122
step: 9000 epoch: 596 loss: 16.80487542374374 loss_input: 82.34328048158494
step: 10000 epoch: 596 loss: 16.809310582766187 loss_input: 82.4260414046486
step: 11000 epoch: 596 loss: 16.814950202749095 loss_input: 82.31987518402092
step: 12000 epoch: 596 loss: 16.83410765649716 loss_input: 82.26901965526311
step: 13000 epoch: 596 loss: 16.838132086739833 loss_input: 82.2160326186973
step: 14000 epoch: 596 loss: 16.84589468555002 loss_input: 82.22864565111281
step: 15000 epoch: 596 loss: 16.859159286415867 loss_input: 82.23593839268138
Save loss: 16.861134334415198 Name: 596_train_model.pth
step: 0 epoch: 597 loss: 17.795421600341797 loss_input: 71.0689697265625
step: 1000 epoch: 597 loss: 17.153022619870516 loss_input: 83.50906808035714
step: 2000 epoch: 597 loss: 17.03779922897133 loss_input: 82.78587930766062
step: 3000 epoch: 597 loss: 16.99855843308527 loss_input: 83.21465673958289
step: 4000 epoch: 597 loss: 16.956867851575772 loss_input: 82.81775562884181
step: 5000 epoch: 597 loss: 16.914857394598503 loss_input: 82.4718081896316
step: 6000 epoch: 597 loss: 16.89247109432694 loss_input: 82.52264276144406
step: 7000 epoch: 597 loss: 16.894331339954768 loss_input: 82.56950958387083
step: 8000 epoch: 597 loss: 16.89426604209431 loss_input: 82.50307459736479
step: 9000 epoch: 597 loss: 16.89114887014308 loss_input: 82.28968197191415
step: 10000 epoch: 597 loss: 16.895297614255792 loss_input: 82.25833417634799
step: 11000 epoch: 597 loss: 16.878294428070397 loss_input: 82.29213461950903
step: 12000 epoch: 597 loss: 16.885537123264903 loss_input: 82.3911253601181
step: 13000 epoch: 597 loss: 16.893421096349165 loss_input: 82.40485009735579
step: 14000 epoch: 597 loss: 16.875862223737094 loss_input: 82.37048828961994
step: 15000 epoch: 597 loss: 16.862436328551887 loss_input: 82.25257634598448
Save loss: 16.863699746593834 Name: 597_train_model.pth
step: 0 epoch: 598 loss: 15.200096130371094 loss_input: 102.44732666015625
step: 1000 epoch: 598 loss: 16.81034974832754 loss_input: 82.90185540777581
step: 2000 epoch: 598 loss: 16.921963586144777 loss_input: 83.75289875814046
step: 3000 epoch: 598 loss: 16.93803527568905 loss_input: 83.14365434503603
step: 4000 epoch: 598 loss: 16.902280350680115 loss_input: 83.15009327502109
step: 5000 epoch: 598 loss: 16.92029368908399 loss_input: 83.0100169663333
step: 6000 epoch: 598 loss: 16.915365019831494 loss_input: 82.82440952427525
step: 7000 epoch: 598 loss: 16.90558190778943 loss_input: 82.52412207714747
step: 8000 epoch: 598 loss: 16.891320492175055 loss_input: 82.4269130207601
step: 9000 epoch: 598 loss: 16.887351192086157 loss_input: 82.51050603481124
step: 10000 epoch: 598 loss: 16.89754545158201 loss_input: 82.53459181059434
step: 11000 epoch: 598 loss: 16.879526328199635 loss_input: 82.44542885017378
step: 12000 epoch: 598 loss: 16.863303343878815 loss_input: 82.45328046901854
step: 13000 epoch: 598 loss: 16.863141434787373 loss_input: 82.44949733997325
step: 14000 epoch: 598 loss: 16.861640927893937 loss_input: 82.31548590558263
step: 15000 epoch: 598 loss: 16.84994364137626 loss_input: 82.20473570804916
Save loss: 16.859694697976114 Name: 598_train_model.pth
step: 0 epoch: 599 loss: 13.744829177856445 loss_input: 66.33349609375
step: 1000 epoch: 599 loss: 16.645530770708632 loss_input: 82.9684276294994
step: 2000 epoch: 599 loss: 16.725385105651597 loss_input: 82.02643072384706
step: 3000 epoch: 599 loss: 16.788823596162423 loss_input: 82.32064891536805
step: 4000 epoch: 599 loss: 16.785193346882128 loss_input: 82.3177673141529
step: 5000 epoch: 599 loss: 16.776819598743902 loss_input: 82.16049136364133
step: 6000 epoch: 599 loss: 16.782359597405243 loss_input: 81.90506212831716
step: 7000 epoch: 599 loss: 16.794039874498445 loss_input: 82.01250766492063
step: 8000 epoch: 599 loss: 16.7835471299332 loss_input: 81.80478165784697
step: 9000 epoch: 599 loss: 16.803990417765903 loss_input: 81.97695512266215
step: 10000 epoch: 599 loss: 16.813725712799737 loss_input: 82.10618119706584
step: 11000 epoch: 599 loss: 16.824532581582826 loss_input: 82.21202504206826
step: 12000 epoch: 599 loss: 16.82696918499151 loss_input: 82.28425690071074
step: 13000 epoch: 599 loss: 16.83871965846248 loss_input: 82.35549439218318
step: 14000 epoch: 599 loss: 16.841012650375305 loss_input: 82.3182513112554
step: 15000 epoch: 599 loss: 16.8494177033731 loss_input: 82.27571650659488
Save loss: 16.85699800091982 Name: 599_train_model.pth
step: 0 epoch: 600 loss: 17.261022567749023 loss_input: 54.03271484375
step: 1000 epoch: 600 loss: 16.766121181217464 loss_input: 82.05966226156656
step: 2000 epoch: 600 loss: 16.76265365895124 loss_input: 82.2916505309357
step: 3000 epoch: 600 loss: 16.802557104867685 loss_input: 82.21643849429748
step: 4000 epoch: 600 loss: 16.855912276131903 loss_input: 82.4449511096347
step: 5000 epoch: 600 loss: 16.8768865425714 loss_input: 82.30337404198848
step: 6000 epoch: 600 loss: 16.85754045616128 loss_input: 82.13012211180948
step: 7000 epoch: 600 loss: 16.894554206838336 loss_input: 82.28431765084062
step: 8000 epoch: 600 loss: 16.890745035545063 loss_input: 82.27378824808764
step: 9000 epoch: 600 loss: 16.874958560885542 loss_input: 82.40142325901506
step: 10000 epoch: 600 loss: 16.86214103251502 loss_input: 82.28361389935225
step: 11000 epoch: 600 loss: 16.852600750256947 loss_input: 82.17801288639066
step: 12000 epoch: 600 loss: 16.851970017527492 loss_input: 82.1434003780528
step: 13000 epoch: 600 loss: 16.83545389724469 loss_input: 81.97970883624937
step: 14000 epoch: 600 loss: 16.835945959696044 loss_input: 82.09315448328049
step: 15000 epoch: 600 loss: 16.848221868588507 loss_input: 82.1261241356696
Save loss: 16.857156704172493 Name: 600_train_model.pth
step: 0 epoch: 601 loss: 12.601179122924805 loss_input: 76.1141357421875
step: 1000 epoch: 601 loss: 16.846045411669174 loss_input: 82.9387502756033
step: 2000 epoch: 601 loss: 16.825357258885816 loss_input: 82.62501851491247
step: 3000 epoch: 601 loss: 16.741155214525786 loss_input: 82.47362538903047
step: 4000 epoch: 601 loss: 16.78486328898475 loss_input: 82.64938358305932
step: 5000 epoch: 601 loss: 16.746437996822557 loss_input: 82.48938463039813
step: 6000 epoch: 601 loss: 16.74110569844264 loss_input: 82.4585052068145
step: 7000 epoch: 601 loss: 16.78156201683816 loss_input: 82.25705058491923
step: 8000 epoch: 601 loss: 16.804612750605394 loss_input: 82.23218291408642
step: 9000 epoch: 601 loss: 16.802004430945907 loss_input: 82.16317930413331
step: 10000 epoch: 601 loss: 16.816397594149237 loss_input: 82.26094945437097
step: 11000 epoch: 601 loss: 16.81448856896351 loss_input: 82.16429403523685
step: 12000 epoch: 601 loss: 16.837460899062975 loss_input: 82.14320411551805
step: 13000 epoch: 601 loss: 16.845072847743666 loss_input: 82.14285491692196
step: 14000 epoch: 601 loss: 16.845856638586817 loss_input: 82.17812558240817
step: 15000 epoch: 601 loss: 16.85400369478682 loss_input: 82.22131342773763
Save loss: 16.854600169047714 Name: 601_train_model.pth
step: 0 epoch: 602 loss: 16.47026824951172 loss_input: 60.11993408203125
step: 1000 epoch: 602 loss: 16.70415034470382 loss_input: 81.23244169160917
step: 2000 epoch: 602 loss: 16.728832490559757 loss_input: 82.1653531144584
step: 3000 epoch: 602 loss: 16.775827215894466 loss_input: 82.19987284918699
step: 4000 epoch: 602 loss: 16.851486479809985 loss_input: 82.43927084329992
step: 5000 epoch: 602 loss: 16.84862313302034 loss_input: 82.47956801461973
step: 6000 epoch: 602 loss: 16.80911305681345 loss_input: 82.5801216358146
step: 7000 epoch: 602 loss: 16.848023916717736 loss_input: 82.33988697415437
step: 8000 epoch: 602 loss: 16.856480403984058 loss_input: 82.34765100926104
step: 9000 epoch: 602 loss: 16.873986980912687 loss_input: 82.38634982669026
step: 10000 epoch: 602 loss: 16.866733789849242 loss_input: 82.39064681178367
step: 11000 epoch: 602 loss: 16.85464467266323 loss_input: 82.36916132513343
step: 12000 epoch: 602 loss: 16.870185225995897 loss_input: 82.43487825497778
step: 13000 epoch: 602 loss: 16.891279430501637 loss_input: 82.44904632016738
step: 14000 epoch: 602 loss: 16.889279324142823 loss_input: 82.33083691217927
step: 15000 epoch: 602 loss: 16.886405902913026 loss_input: 82.31537792906906
Save loss: 16.87336199067533 Name: 602_train_model.pth
step: 0 epoch: 603 loss: 12.133871078491211 loss_input: 43.98016357421875
step: 1000 epoch: 603 loss: 16.817018160215028 loss_input: 81.32241275570134
step: 2000 epoch: 603 loss: 16.770320360926256 loss_input: 82.29524736533101
step: 3000 epoch: 603 loss: 16.798750468867098 loss_input: 82.2692824315723
step: 4000 epoch: 603 loss: 16.82821951112697 loss_input: 82.44061613380835
step: 5000 epoch: 603 loss: 16.825493845265523 loss_input: 82.19852239268943
step: 6000 epoch: 603 loss: 16.80460310161084 loss_input: 82.1844866167325
step: 7000 epoch: 603 loss: 16.81850299932602 loss_input: 81.9418177512727
step: 8000 epoch: 603 loss: 16.81348693202457 loss_input: 81.96703679709714
step: 9000 epoch: 603 loss: 16.825761869316327 loss_input: 81.99016527053635
step: 10000 epoch: 603 loss: 16.83643464014156 loss_input: 82.07981129987587
step: 11000 epoch: 603 loss: 16.851500184522326 loss_input: 82.23717334068706
step: 12000 epoch: 603 loss: 16.86846335849487 loss_input: 82.30611675304728
step: 13000 epoch: 603 loss: 16.859256431713682 loss_input: 82.20880289388046
step: 14000 epoch: 603 loss: 16.84700568418283 loss_input: 82.16621457231989
step: 15000 epoch: 603 loss: 16.854122664958474 loss_input: 82.22557273851014
Save loss: 16.85457795009017 Name: 603_train_model.pth
step: 0 epoch: 604 loss: 16.931053161621094 loss_input: 58.857177734375
step: 1000 epoch: 604 loss: 16.617382276308287 loss_input: 82.53333385364635
step: 2000 epoch: 604 loss: 16.755368895914362 loss_input: 82.62295518500515
step: 3000 epoch: 604 loss: 16.70492246539463 loss_input: 82.15782599241008
step: 4000 epoch: 604 loss: 16.714079539259206 loss_input: 82.07054277385244
step: 5000 epoch: 604 loss: 16.792533889290716 loss_input: 82.32181329847312
step: 6000 epoch: 604 loss: 16.817902147521618 loss_input: 82.21822267293017
step: 7000 epoch: 604 loss: 16.796387612555748 loss_input: 82.16786269476849
step: 8000 epoch: 604 loss: 16.801259860472744 loss_input: 82.1180585611613
step: 9000 epoch: 604 loss: 16.808928155353396 loss_input: 81.99638328217438
step: 10000 epoch: 604 loss: 16.829313412223765 loss_input: 82.12991580480625
step: 11000 epoch: 604 loss: 16.833946520171917 loss_input: 82.09731796288229
step: 12000 epoch: 604 loss: 16.838934785017877 loss_input: 82.09465910901469
step: 13000 epoch: 604 loss: 16.84665629434802 loss_input: 82.11586335493725
step: 14000 epoch: 604 loss: 16.84024791160351 loss_input: 82.13523432844222
step: 15000 epoch: 604 loss: 16.85701996198885 loss_input: 82.16697023258345
Save loss: 16.853619236022233 Name: 604_train_model.pth
step: 0 epoch: 605 loss: 18.736892700195312 loss_input: 110.78948974609375
step: 1000 epoch: 605 loss: 16.450323730796487 loss_input: 81.16939286299638
step: 2000 epoch: 605 loss: 16.57548979221136 loss_input: 81.5899704566662
step: 3000 epoch: 605 loss: 16.751152922971293 loss_input: 81.846026930639
step: 4000 epoch: 605 loss: 16.776074426706302 loss_input: 82.15568899828266
step: 5000 epoch: 605 loss: 16.820696052659205 loss_input: 82.10388516779066
step: 6000 epoch: 605 loss: 16.855922766555015 loss_input: 82.29692709499588
step: 7000 epoch: 605 loss: 16.854831977633644 loss_input: 82.22697007814453
step: 8000 epoch: 605 loss: 16.861113209885936 loss_input: 82.35533569473249
step: 9000 epoch: 605 loss: 16.845215533020998 loss_input: 82.2905116052101
step: 10000 epoch: 605 loss: 16.846508561080842 loss_input: 82.2946582966215
step: 11000 epoch: 605 loss: 16.83989321115981 loss_input: 82.2785155073793
step: 12000 epoch: 605 loss: 16.857077265349975 loss_input: 82.32486365475403
step: 13000 epoch: 605 loss: 16.847823212086425 loss_input: 82.2046542966121
step: 14000 epoch: 605 loss: 16.85826155872194 loss_input: 82.26888162947903
step: 15000 epoch: 605 loss: 16.867310385697365 loss_input: 82.27866198487158
Save loss: 16.855991902157665 Name: 605_train_model.pth
step: 0 epoch: 606 loss: 19.74781036376953 loss_input: 62.75830078125
step: 1000 epoch: 606 loss: 17.01464119157591 loss_input: 83.06461098715738
step: 2000 epoch: 606 loss: 16.949278530628902 loss_input: 82.53992409922968
step: 3000 epoch: 606 loss: 16.875559874670937 loss_input: 82.51129784944732
step: 4000 epoch: 606 loss: 16.8520582666757 loss_input: 82.40379454391773
step: 5000 epoch: 606 loss: 16.821105483197567 loss_input: 82.33188136542613
step: 6000 epoch: 606 loss: 16.82714474385946 loss_input: 82.43854293734248
step: 7000 epoch: 606 loss: 16.847833415437233 loss_input: 82.26098279730965
step: 8000 epoch: 606 loss: 16.85810529454144 loss_input: 82.3138950511435
step: 9000 epoch: 606 loss: 16.838799699837995 loss_input: 82.24733973921518
step: 10000 epoch: 606 loss: 16.822098802011165 loss_input: 82.1828146592079
step: 11000 epoch: 606 loss: 16.84190770080226 loss_input: 82.13887313400309
step: 12000 epoch: 606 loss: 16.85779756254459 loss_input: 82.18414392082724
step: 13000 epoch: 606 loss: 16.85765336858393 loss_input: 82.15573581836102
step: 14000 epoch: 606 loss: 16.850551130550095 loss_input: 82.12845229857327
step: 15000 epoch: 606 loss: 16.85696955887717 loss_input: 82.2058521822829
Save loss: 16.85577110759914 Name: 606_train_model.pth
step: 0 epoch: 607 loss: 17.1122989654541 loss_input: 115.8912353515625
step: 1000 epoch: 607 loss: 17.019244410774924 loss_input: 81.77530440750655
step: 2000 epoch: 607 loss: 16.938900211702162 loss_input: 81.64632011996932
step: 3000 epoch: 607 loss: 16.8793134865702 loss_input: 81.73923200323199
step: 4000 epoch: 607 loss: 16.866323199697625 loss_input: 81.86498132546942
step: 5000 epoch: 607 loss: 16.833069103809624 loss_input: 81.98983694472044
step: 6000 epoch: 607 loss: 16.823533963251265 loss_input: 81.87530018190327
step: 7000 epoch: 607 loss: 16.82320525775278 loss_input: 81.9036594625712
step: 8000 epoch: 607 loss: 16.827608994343418 loss_input: 81.91869289501653
step: 9000 epoch: 607 loss: 16.810956195942126 loss_input: 81.89022512174212
step: 10000 epoch: 607 loss: 16.815049880505228 loss_input: 81.9998596148686
step: 11000 epoch: 607 loss: 16.83071347969509 loss_input: 82.1534345876671
step: 12000 epoch: 607 loss: 16.8432190921662 loss_input: 82.17033856289157
step: 13000 epoch: 607 loss: 16.856462284304822 loss_input: 82.26269393227264
step: 14000 epoch: 607 loss: 16.85670833897228 loss_input: 82.25420631227233
step: 15000 epoch: 607 loss: 16.84968912834311 loss_input: 82.22387328312227
Save loss: 16.852462612494826 Name: 607_train_model.pth
step: 0 epoch: 608 loss: 18.075729370117188 loss_input: 105.30108642578125
step: 1000 epoch: 608 loss: 16.596253071155225 loss_input: 81.79117043844828
step: 2000 epoch: 608 loss: 16.73114514434296 loss_input: 81.93905888242402
step: 3000 epoch: 608 loss: 16.81269515414748 loss_input: 82.13512998515469
step: 4000 epoch: 608 loss: 16.77089457343859 loss_input: 82.02347980204655
step: 5000 epoch: 608 loss: 16.789286915718662 loss_input: 81.88135215974383
step: 6000 epoch: 608 loss: 16.79193572413224 loss_input: 82.30857022486812
step: 7000 epoch: 608 loss: 16.79859919902887 loss_input: 82.43179218851827
step: 8000 epoch: 608 loss: 16.806047163371993 loss_input: 82.47397049306825
step: 9000 epoch: 608 loss: 16.797003133126648 loss_input: 82.36300738177529
step: 10000 epoch: 608 loss: 16.779844663367488 loss_input: 82.27628941207442
step: 11000 epoch: 608 loss: 16.794710900000947 loss_input: 82.26310269623039
step: 12000 epoch: 608 loss: 16.81746689077914 loss_input: 82.35366651676325
step: 13000 epoch: 608 loss: 16.837492837272105 loss_input: 82.29406767388086
step: 14000 epoch: 608 loss: 16.83340004832138 loss_input: 82.26450389041686
step: 15000 epoch: 608 loss: 16.835895518447295 loss_input: 82.25367183366161
Save loss: 16.845100283935665 Name: 608_train_model.pth
step: 0 epoch: 609 loss: 15.969947814941406 loss_input: 119.51824951171875
step: 1000 epoch: 609 loss: 16.814589811967206 loss_input: 82.33190674072021
step: 2000 epoch: 609 loss: 16.71449559703581 loss_input: 82.21797697952782
step: 3000 epoch: 609 loss: 16.802345558151885 loss_input: 82.19033924320148
step: 4000 epoch: 609 loss: 16.81322607467306 loss_input: 82.18869949870931
step: 5000 epoch: 609 loss: 16.79733863207751 loss_input: 82.13816763014584
step: 6000 epoch: 609 loss: 16.824381105463974 loss_input: 82.12326552752276
step: 7000 epoch: 609 loss: 16.835908386574832 loss_input: 82.27568717861735
step: 8000 epoch: 609 loss: 16.842785377142473 loss_input: 82.23768479745696
step: 9000 epoch: 609 loss: 16.83897942440362 loss_input: 82.2641573421638
step: 10000 epoch: 609 loss: 16.81277008746078 loss_input: 82.16569187807782
step: 11000 epoch: 609 loss: 16.832757559768417 loss_input: 82.20632161990956
step: 12000 epoch: 609 loss: 16.84148701369072 loss_input: 82.10829838887123
step: 13000 epoch: 609 loss: 16.83844461834217 loss_input: 82.08117826949008
step: 14000 epoch: 609 loss: 16.853026401382728 loss_input: 82.11677678889286
step: 15000 epoch: 609 loss: 16.852385006462065 loss_input: 82.17790680409718
Save loss: 16.85158656129241 Name: 609_train_model.pth
step: 0 epoch: 610 loss: 11.785506248474121 loss_input: 53.6934814453125
step: 1000 epoch: 610 loss: 16.775929978796533 loss_input: 83.06724116947505
step: 2000 epoch: 610 loss: 16.81216597986007 loss_input: 82.94198884444496
step: 3000 epoch: 610 loss: 16.764190778220666 loss_input: 82.59919103706967
step: 4000 epoch: 610 loss: 16.783899222752716 loss_input: 82.4273618637577
step: 5000 epoch: 610 loss: 16.758933383496945 loss_input: 82.30197302824591
step: 6000 epoch: 610 loss: 16.803064223151072 loss_input: 82.27941333101384
step: 7000 epoch: 610 loss: 16.784038813824893 loss_input: 82.0629322676978
step: 8000 epoch: 610 loss: 16.79471148069077 loss_input: 82.04229376265711
step: 9000 epoch: 610 loss: 16.80470337490018 loss_input: 82.0260631102295
step: 10000 epoch: 610 loss: 16.816275644869748 loss_input: 82.13584184184121
step: 11000 epoch: 610 loss: 16.828255386463503 loss_input: 82.17379077487983
step: 12000 epoch: 610 loss: 16.827002505741163 loss_input: 82.16003840601988
step: 13000 epoch: 610 loss: 16.83084310822171 loss_input: 82.15255307039934
step: 14000 epoch: 610 loss: 16.84682661469907 loss_input: 82.26463799687438
step: 15000 epoch: 610 loss: 16.851330161054932 loss_input: 82.20954693922098
Save loss: 16.85208100168407 Name: 610_train_model.pth
step: 0 epoch: 611 loss: 16.960041046142578 loss_input: 68.09454345703125
step: 1000 epoch: 611 loss: 16.587139810358252 loss_input: 82.07344614613902
step: 2000 epoch: 611 loss: 16.70136309492177 loss_input: 82.12518786383175
step: 3000 epoch: 611 loss: 16.723251529075192 loss_input: 82.30297646145947
step: 4000 epoch: 611 loss: 16.780459235173947 loss_input: 82.37899742898747
step: 5000 epoch: 611 loss: 16.72453678457576 loss_input: 82.17496035704968
step: 6000 epoch: 611 loss: 16.73269645143918 loss_input: 82.22227875961103
step: 7000 epoch: 611 loss: 16.76566849046393 loss_input: 82.30013303589163
step: 8000 epoch: 611 loss: 16.78780255569784 loss_input: 82.18160653349727
step: 9000 epoch: 611 loss: 16.777149485079608 loss_input: 82.27900187061464
step: 10000 epoch: 611 loss: 16.808117620731135 loss_input: 82.38280635437432
step: 11000 epoch: 611 loss: 16.82200566379279 loss_input: 82.50116986546145
step: 12000 epoch: 611 loss: 16.823201750548776 loss_input: 82.53032044082589
step: 13000 epoch: 611 loss: 16.835168751026426 loss_input: 82.48989092373003
step: 14000 epoch: 611 loss: 16.843707195614655 loss_input: 82.4271991436298
step: 15000 epoch: 611 loss: 16.85187261478558 loss_input: 82.31715706257603
Save loss: 16.849681951880456 Name: 611_train_model.pth
step: 0 epoch: 612 loss: 24.251535415649414 loss_input: 128.220703125
step: 1000 epoch: 612 loss: 16.823870486431904 loss_input: 82.35877147754589
step: 2000 epoch: 612 loss: 16.778889842893648 loss_input: 82.09750337233727
step: 3000 epoch: 612 loss: 16.801319864502513 loss_input: 82.1996763814731
step: 4000 epoch: 612 loss: 16.779976714881947 loss_input: 82.46936415005135
step: 5000 epoch: 612 loss: 16.778645834000773 loss_input: 82.2152559112201
step: 6000 epoch: 612 loss: 16.823090540093396 loss_input: 82.52731663456481
step: 7000 epoch: 612 loss: 16.826078800316658 loss_input: 82.34254219088959
step: 8000 epoch: 612 loss: 16.844580835945052 loss_input: 82.22219301292172
step: 9000 epoch: 612 loss: 16.823492931506458 loss_input: 82.09593951502133
step: 10000 epoch: 612 loss: 16.83159397022925 loss_input: 82.16241362313964
step: 11000 epoch: 612 loss: 16.83417478081053 loss_input: 82.11911882633362
step: 12000 epoch: 612 loss: 16.818222365452044 loss_input: 82.08953433262886
step: 13000 epoch: 612 loss: 16.828390434772892 loss_input: 82.09038886469371
step: 14000 epoch: 612 loss: 16.834758753487403 loss_input: 82.10772141851056
step: 15000 epoch: 612 loss: 16.84220639370845 loss_input: 82.23196779964017
Save loss: 16.854212050274015 Name: 612_train_model.pth
step: 0 epoch: 613 loss: 16.258710861206055 loss_input: 82.50836181640625
step: 1000 epoch: 613 loss: 16.783521734155737 loss_input: 81.68931806718672
step: 2000 epoch: 613 loss: 16.76975256320776 loss_input: 81.9656991572573
step: 3000 epoch: 613 loss: 16.80486727396435 loss_input: 82.32650403966908
step: 4000 epoch: 613 loss: 16.82925082385257 loss_input: 82.16978424461267
step: 5000 epoch: 613 loss: 16.8116034779685 loss_input: 82.13672767155553
step: 6000 epoch: 613 loss: 16.817411913432352 loss_input: 82.1598195954971
step: 7000 epoch: 613 loss: 16.795183530314926 loss_input: 81.94432981742005
step: 8000 epoch: 613 loss: 16.762235718330075 loss_input: 81.8665287078373
step: 9000 epoch: 613 loss: 16.800722337778296 loss_input: 81.96130507390455
step: 10000 epoch: 613 loss: 16.807013017632297 loss_input: 81.95978718168222
step: 11000 epoch: 613 loss: 16.83171480055907 loss_input: 82.27124851220911
step: 12000 epoch: 613 loss: 16.828406912815012 loss_input: 82.27274380679052
step: 13000 epoch: 613 loss: 16.8340164697791 loss_input: 82.16326615575845
step: 14000 epoch: 613 loss: 16.841744488847656 loss_input: 82.1736560879022
step: 15000 epoch: 613 loss: 16.846290779418926 loss_input: 82.14118392676816
Save loss: 16.854752902522684 Name: 613_train_model.pth
step: 0 epoch: 614 loss: 14.817188262939453 loss_input: 46.86260986328125
step: 1000 epoch: 614 loss: 16.58722706250735 loss_input: 82.13700904808083
step: 2000 epoch: 614 loss: 16.702878849557614 loss_input: 82.50361415971702
step: 3000 epoch: 614 loss: 16.762303709467425 loss_input: 82.58731741112338
step: 4000 epoch: 614 loss: 16.798874716137803 loss_input: 82.70405764342605
step: 5000 epoch: 614 loss: 16.843847301811532 loss_input: 82.62002538457152
step: 6000 epoch: 614 loss: 16.833331751398315 loss_input: 82.3746477434425
step: 7000 epoch: 614 loss: 16.815838310654172 loss_input: 82.26122487175381
step: 8000 epoch: 614 loss: 16.819946952796464 loss_input: 82.26494763258353
step: 9000 epoch: 614 loss: 16.812266280923655 loss_input: 82.37417857163007
step: 10000 epoch: 614 loss: 16.823957745426476 loss_input: 82.3877736677016
step: 11000 epoch: 614 loss: 16.834397268841435 loss_input: 82.40238764693267
step: 12000 epoch: 614 loss: 16.833690904615402 loss_input: 82.35761588959383
step: 13000 epoch: 614 loss: 16.83836068192699 loss_input: 82.42238551511663
step: 14000 epoch: 614 loss: 16.851070438161184 loss_input: 82.38536915855062
step: 15000 epoch: 614 loss: 16.85222674091612 loss_input: 82.34391082098433
Save loss: 16.84749552355707 Name: 614_train_model.pth
step: 0 epoch: 615 loss: 19.540199279785156 loss_input: 62.188232421875
step: 1000 epoch: 615 loss: 16.53107712199757 loss_input: 81.86986652263752
step: 2000 epoch: 615 loss: 16.691013656694373 loss_input: 81.84000550383988
step: 3000 epoch: 615 loss: 16.739890035650248 loss_input: 82.1299414274018
step: 4000 epoch: 615 loss: 16.740129405157294 loss_input: 82.15225865774767
step: 5000 epoch: 615 loss: 16.75159673513448 loss_input: 82.22310006992743
step: 6000 epoch: 615 loss: 16.791777029690632 loss_input: 82.23946866399545
step: 7000 epoch: 615 loss: 16.81633981577483 loss_input: 82.43669387562379
step: 8000 epoch: 615 loss: 16.799577765070847 loss_input: 82.35064883399585
step: 9000 epoch: 615 loss: 16.812229591745123 loss_input: 82.3541369955747
step: 10000 epoch: 615 loss: 16.826776975155497 loss_input: 82.3398586288832
step: 11000 epoch: 615 loss: 16.84582486333397 loss_input: 82.20546807645505
step: 12000 epoch: 615 loss: 16.86135968501623 loss_input: 82.30765207635692
step: 13000 epoch: 615 loss: 16.8516195621979 loss_input: 82.26243950566753
step: 14000 epoch: 615 loss: 16.8405861818793 loss_input: 82.26972532357482
step: 15000 epoch: 615 loss: 16.848002529074357 loss_input: 82.2875968669146
Save loss: 16.85437095147371 Name: 615_train_model.pth
step: 0 epoch: 616 loss: 18.059350967407227 loss_input: 85.558837890625
step: 1000 epoch: 616 loss: 16.618846754213195 loss_input: 82.87757890303057
step: 2000 epoch: 616 loss: 16.781697650005793 loss_input: 82.42976610449658
step: 3000 epoch: 616 loss: 16.74215371034337 loss_input: 82.35216223498418
step: 4000 epoch: 616 loss: 16.774768887147758 loss_input: 82.40382183506857
step: 5000 epoch: 616 loss: 16.73798485036803 loss_input: 82.17936179609781
step: 6000 epoch: 616 loss: 16.731683887852924 loss_input: 82.1388843233159
step: 7000 epoch: 616 loss: 16.78302916648166 loss_input: 82.21994549920603
step: 8000 epoch: 616 loss: 16.783371608952496 loss_input: 82.14819677691656
step: 9000 epoch: 616 loss: 16.78738794638281 loss_input: 82.14928859494127
step: 10000 epoch: 616 loss: 16.804857975267765 loss_input: 82.26528226083641
step: 11000 epoch: 616 loss: 16.81079491761888 loss_input: 82.2245393130099
step: 12000 epoch: 616 loss: 16.818387899048357 loss_input: 82.25480545179991
step: 13000 epoch: 616 loss: 16.797357457278828 loss_input: 82.10535156212443
step: 14000 epoch: 616 loss: 16.815391450309384 loss_input: 82.101787429002
step: 15000 epoch: 616 loss: 16.83068000109273 loss_input: 82.20742757774482
Save loss: 16.843547812461853 Name: 616_train_model.pth
step: 0 epoch: 617 loss: 17.440189361572266 loss_input: 136.6971435546875
step: 1000 epoch: 617 loss: 16.68657593007807 loss_input: 82.05693903264704
step: 2000 epoch: 617 loss: 16.79445292626781 loss_input: 82.22215390717727
step: 3000 epoch: 617 loss: 16.777127496483562 loss_input: 81.9373177487387
step: 4000 epoch: 617 loss: 16.786460161626234 loss_input: 81.85473605353545
step: 5000 epoch: 617 loss: 16.780500211565048 loss_input: 82.04191013603919
step: 6000 epoch: 617 loss: 16.773561143215606 loss_input: 82.07171621705628
step: 7000 epoch: 617 loss: 16.78881857184372 loss_input: 82.07771886905387
step: 8000 epoch: 617 loss: 16.81283066690214 loss_input: 82.06298875421334
step: 9000 epoch: 617 loss: 16.82893140513875 loss_input: 82.07241285934592
step: 10000 epoch: 617 loss: 16.815602390399732 loss_input: 82.1646292524068
step: 11000 epoch: 617 loss: 16.840186834682086 loss_input: 82.1427575021145
step: 12000 epoch: 617 loss: 16.826400646954 loss_input: 82.09910840458119
step: 13000 epoch: 617 loss: 16.830532409183686 loss_input: 82.13693988214098
step: 14000 epoch: 617 loss: 16.83833683987071 loss_input: 82.18102611462463
step: 15000 epoch: 617 loss: 16.835185049549832 loss_input: 82.2365875884967
Save loss: 16.838958418294787 Name: 617_train_model.pth
step: 0 epoch: 618 loss: 19.729808807373047 loss_input: 97.3489990234375
step: 1000 epoch: 618 loss: 16.82007555671029 loss_input: 82.59154401506696
step: 2000 epoch: 618 loss: 16.821453298943332 loss_input: 82.67808490774144
step: 3000 epoch: 618 loss: 16.759489845887934 loss_input: 82.61336459624135
step: 4000 epoch: 618 loss: 16.77936162015433 loss_input: 82.7297447336164
step: 5000 epoch: 618 loss: 16.74382444787707 loss_input: 82.59828593363359
step: 6000 epoch: 618 loss: 16.726219920074318 loss_input: 82.45314222938755
step: 7000 epoch: 618 loss: 16.756535436405215 loss_input: 82.61213250576368
step: 8000 epoch: 618 loss: 16.76583975965478 loss_input: 82.53995280008944
step: 9000 epoch: 618 loss: 16.792481209275298 loss_input: 82.4799108218119
step: 10000 epoch: 618 loss: 16.795519342924067 loss_input: 82.45519697297848
step: 11000 epoch: 618 loss: 16.773400297880976 loss_input: 82.38151606467949
step: 12000 epoch: 618 loss: 16.79735334671872 loss_input: 82.30938227885306
step: 13000 epoch: 618 loss: 16.812684733944117 loss_input: 82.28074073835516
step: 14000 epoch: 618 loss: 16.825400661769983 loss_input: 82.27893984483809
step: 15000 epoch: 618 loss: 16.8354586672047 loss_input: 82.30085392988775
Save loss: 16.84182277534902 Name: 618_train_model.pth
step: 0 epoch: 619 loss: 11.793954849243164 loss_input: 51.70269775390625
step: 1000 epoch: 619 loss: 16.757528707816764 loss_input: 82.2411721579202
step: 2000 epoch: 619 loss: 16.815071468053013 loss_input: 81.69988373123009
step: 3000 epoch: 619 loss: 16.678906898902124 loss_input: 81.60102105450527
step: 4000 epoch: 619 loss: 16.757702042120332 loss_input: 81.75077980382834
step: 5000 epoch: 619 loss: 16.780333663482377 loss_input: 81.9372438744673
step: 6000 epoch: 619 loss: 16.79078448384906 loss_input: 82.0961304524604
step: 7000 epoch: 619 loss: 16.822181908543868 loss_input: 82.22923421130966
step: 8000 epoch: 619 loss: 16.794641206091963 loss_input: 82.04429281087178
step: 9000 epoch: 619 loss: 16.77564609944721 loss_input: 81.96529828255845
step: 10000 epoch: 619 loss: 16.795332227798834 loss_input: 82.17374095720311
step: 11000 epoch: 619 loss: 16.808995234512242 loss_input: 82.15971143847541
step: 12000 epoch: 619 loss: 16.815383388781367 loss_input: 82.30819215687917
step: 13000 epoch: 619 loss: 16.831573718731683 loss_input: 82.39599062917637
step: 14000 epoch: 619 loss: 16.844926540174363 loss_input: 82.38787718427479
step: 15000 epoch: 619 loss: 16.839019974647588 loss_input: 82.26168602513087
Save loss: 16.83898035545647 Name: 619_train_model.pth
step: 0 epoch: 620 loss: 19.495037078857422 loss_input: 65.76678466796875
step: 1000 epoch: 620 loss: 16.77842940698256 loss_input: 81.10390756704233
step: 2000 epoch: 620 loss: 16.858777727620833 loss_input: 81.80278921818388
step: 3000 epoch: 620 loss: 16.739728447676736 loss_input: 82.0030286331965
step: 4000 epoch: 620 loss: 16.823429006899755 loss_input: 82.08209968381928
step: 5000 epoch: 620 loss: 16.787052648969947 loss_input: 82.14386151090095
step: 6000 epoch: 620 loss: 16.790641049626785 loss_input: 81.95986854567208
step: 7000 epoch: 620 loss: 16.79219775992689 loss_input: 81.94369429681083
step: 8000 epoch: 620 loss: 16.786618276858775 loss_input: 81.84012463682518
step: 9000 epoch: 620 loss: 16.79310126360781 loss_input: 81.63867464161969
step: 10000 epoch: 620 loss: 16.80967199384016 loss_input: 81.8607953413643
step: 11000 epoch: 620 loss: 16.819743747462383 loss_input: 81.94447522925827
step: 12000 epoch: 620 loss: 16.828434256869212 loss_input: 81.99261847401388
step: 13000 epoch: 620 loss: 16.836455847793207 loss_input: 81.99561696595003
step: 14000 epoch: 620 loss: 16.842234619702026 loss_input: 82.06007911317579
step: 15000 epoch: 620 loss: 16.829257993632638 loss_input: 82.11288785988486
Save loss: 16.847531791359184 Name: 620_train_model.pth
step: 0 epoch: 621 loss: 14.161972999572754 loss_input: 56.9903564453125
step: 1000 epoch: 621 loss: 16.744391520182926 loss_input: 81.57894016384006
step: 2000 epoch: 621 loss: 16.78050292854843 loss_input: 82.20034507892538
step: 3000 epoch: 621 loss: 16.795859885430264 loss_input: 81.94443089689504
step: 4000 epoch: 621 loss: 16.809057256872133 loss_input: 82.21522587187675
step: 5000 epoch: 621 loss: 16.778363417110736 loss_input: 82.22140693931526
step: 6000 epoch: 621 loss: 16.76189134073663 loss_input: 82.08733030375471
step: 7000 epoch: 621 loss: 16.79102652230581 loss_input: 82.19782998878229
step: 8000 epoch: 621 loss: 16.798529185767233 loss_input: 82.2475567858095
step: 9000 epoch: 621 loss: 16.80272688297229 loss_input: 82.12959468397825
step: 10000 epoch: 621 loss: 16.79906233388559 loss_input: 82.16101776663155
step: 11000 epoch: 621 loss: 16.82522329672089 loss_input: 82.2653877779133
step: 12000 epoch: 621 loss: 16.811411893464122 loss_input: 82.28018385552832
step: 13000 epoch: 621 loss: 16.80650648217267 loss_input: 82.24387809542746
step: 14000 epoch: 621 loss: 16.806712058380445 loss_input: 82.19493863841618
step: 15000 epoch: 621 loss: 16.815114508850655 loss_input: 82.2034802090668
Save loss: 16.83723585408926 Name: 621_train_model.pth
step: 0 epoch: 622 loss: 20.05206298828125 loss_input: 86.26971435546875
step: 1000 epoch: 622 loss: 16.845504593539548 loss_input: 83.53319891778143
step: 2000 epoch: 622 loss: 16.812028938624216 loss_input: 83.02628739341267
step: 3000 epoch: 622 loss: 16.862597802606118 loss_input: 82.89559894829979
step: 4000 epoch: 622 loss: 16.869524625443542 loss_input: 82.468141814644
step: 5000 epoch: 622 loss: 16.863295835105212 loss_input: 82.39679260192884
step: 6000 epoch: 622 loss: 16.849429567383442 loss_input: 82.28937671764993
step: 7000 epoch: 622 loss: 16.83669532879542 loss_input: 82.35005118897635
step: 8000 epoch: 622 loss: 16.867752281967782 loss_input: 82.43607477837243
step: 9000 epoch: 622 loss: 16.86637705792958 loss_input: 82.51121902932009
step: 10000 epoch: 622 loss: 16.85203842007748 loss_input: 82.43118725847143
step: 11000 epoch: 622 loss: 16.855739873882293 loss_input: 82.33342840636647
step: 12000 epoch: 622 loss: 16.867833873429166 loss_input: 82.41642184114865
step: 13000 epoch: 622 loss: 16.867192500224984 loss_input: 82.35225193295678
step: 14000 epoch: 622 loss: 16.862215660953595 loss_input: 82.38778099102018
step: 15000 epoch: 622 loss: 16.856536391560027 loss_input: 82.2737455889985
Save loss: 16.833170226469637 Name: 622_train_model.pth
step: 0 epoch: 623 loss: 18.84571075439453 loss_input: 51.3299560546875
step: 1000 epoch: 623 loss: 16.721134196747315 loss_input: 81.74904172975462
step: 2000 epoch: 623 loss: 16.70944181506125 loss_input: 82.15683842038942
step: 3000 epoch: 623 loss: 16.792408104381416 loss_input: 82.45417427182475
step: 4000 epoch: 623 loss: 16.82094919821585 loss_input: 82.53120713351936
step: 5000 epoch: 623 loss: 16.791626959794808 loss_input: 82.49157899133064
step: 6000 epoch: 623 loss: 16.789699041610994 loss_input: 82.29978246423448
step: 7000 epoch: 623 loss: 16.81699402559452 loss_input: 82.25717740657585
step: 8000 epoch: 623 loss: 16.82179123847846 loss_input: 82.45307679588147
step: 9000 epoch: 623 loss: 16.85143036231003 loss_input: 82.40619792245306
step: 10000 epoch: 623 loss: 16.858052723623015 loss_input: 82.39206164222445
step: 11000 epoch: 623 loss: 16.83018538265074 loss_input: 82.2978794530309
step: 12000 epoch: 623 loss: 16.853523372521092 loss_input: 82.41266345683758
step: 13000 epoch: 623 loss: 16.858436419793104 loss_input: 82.33374562383422
step: 14000 epoch: 623 loss: 16.85552932592675 loss_input: 82.39508355255596
step: 15000 epoch: 623 loss: 16.846666612733834 loss_input: 82.34896744595282
Save loss: 16.843577798187734 Name: 623_train_model.pth
step: 0 epoch: 624 loss: 15.78144645690918 loss_input: 66.35498046875
step: 1000 epoch: 624 loss: 17.044356493325857 loss_input: 82.62233274537962
step: 2000 epoch: 624 loss: 16.879957847032827 loss_input: 82.24377493260207
step: 3000 epoch: 624 loss: 16.792813072757536 loss_input: 81.75569591630264
step: 4000 epoch: 624 loss: 16.759867604986724 loss_input: 82.17109499785698
step: 5000 epoch: 624 loss: 16.77650610767968 loss_input: 82.30961142976483
step: 6000 epoch: 624 loss: 16.810358249710074 loss_input: 82.44052120463647
step: 7000 epoch: 624 loss: 16.78829649110774 loss_input: 82.27280309530279
step: 8000 epoch: 624 loss: 16.799091069732007 loss_input: 82.20766761454622
step: 9000 epoch: 624 loss: 16.783768021865708 loss_input: 82.10880595786666
step: 10000 epoch: 624 loss: 16.798689339926405 loss_input: 82.10968584538031
step: 11000 epoch: 624 loss: 16.80476274738289 loss_input: 82.15346880863281
step: 12000 epoch: 624 loss: 16.81931183360217 loss_input: 82.21522784783397
step: 13000 epoch: 624 loss: 16.813912179119615 loss_input: 82.23998324873448
step: 14000 epoch: 624 loss: 16.827432302430427 loss_input: 82.21402579782247
step: 15000 epoch: 624 loss: 16.83375301911635 loss_input: 82.18103078609863
Save loss: 16.849835899055005 Name: 624_train_model.pth
step: 0 epoch: 625 loss: 18.285274505615234 loss_input: 108.34014892578125
step: 1000 epoch: 625 loss: 16.871439922820556 loss_input: 83.52463893528346
step: 2000 epoch: 625 loss: 16.774565639047847 loss_input: 81.90584154333966
step: 3000 epoch: 625 loss: 16.835577285357292 loss_input: 81.80485329759871
step: 4000 epoch: 625 loss: 16.840976770267996 loss_input: 81.9151003112259
step: 5000 epoch: 625 loss: 16.863171488112197 loss_input: 82.09164055860703
step: 6000 epoch: 625 loss: 16.851478861800988 loss_input: 82.11411532932988
step: 7000 epoch: 625 loss: 16.8289731069218 loss_input: 82.18838932956564
step: 8000 epoch: 625 loss: 16.831768693871506 loss_input: 82.35218685738013
step: 9000 epoch: 625 loss: 16.839060026279437 loss_input: 82.41977063990502
step: 10000 epoch: 625 loss: 16.836703660762044 loss_input: 82.2388795971477
step: 11000 epoch: 625 loss: 16.833848909230333 loss_input: 82.22440101991793
step: 12000 epoch: 625 loss: 16.841050730914972 loss_input: 82.26088481477456
step: 13000 epoch: 625 loss: 16.83431265291989 loss_input: 82.18894450182768
step: 14000 epoch: 625 loss: 16.843996117672027 loss_input: 82.17142192916047
step: 15000 epoch: 625 loss: 16.840285864522254 loss_input: 82.26592771826279
Save loss: 16.84326444770396 Name: 625_train_model.pth
step: 0 epoch: 626 loss: 13.016368865966797 loss_input: 54.70721435546875
step: 1000 epoch: 626 loss: 16.878808169693617 loss_input: 82.63675600522524
step: 2000 epoch: 626 loss: 16.815470600652432 loss_input: 82.63197621770169
step: 3000 epoch: 626 loss: 16.847740127419836 loss_input: 82.78929364446877
step: 4000 epoch: 626 loss: 16.79591269899505 loss_input: 82.47144646943732
step: 5000 epoch: 626 loss: 16.785228979489823 loss_input: 82.43919514436956
step: 6000 epoch: 626 loss: 16.845758909425065 loss_input: 82.47313562160213
step: 7000 epoch: 626 loss: 16.86507978423666 loss_input: 82.38589459667034
step: 8000 epoch: 626 loss: 16.866500227976672 loss_input: 82.32714292213717
step: 9000 epoch: 626 loss: 16.886216309239845 loss_input: 82.42649385121489
step: 10000 epoch: 626 loss: 16.883496954350242 loss_input: 82.44776354884043
step: 11000 epoch: 626 loss: 16.879890681310044 loss_input: 82.40355455523219
step: 12000 epoch: 626 loss: 16.845861744437652 loss_input: 82.3565175365025
step: 13000 epoch: 626 loss: 16.840371720543843 loss_input: 82.27444774662015
step: 14000 epoch: 626 loss: 16.837113983281537 loss_input: 82.28884675177359
step: 15000 epoch: 626 loss: 16.84036508082358 loss_input: 82.26857573508771
Save loss: 16.83580876210332 Name: 626_train_model.pth
step: 0 epoch: 627 loss: 22.687347412109375 loss_input: 118.53424072265625
step: 1000 epoch: 627 loss: 16.850466359030833 loss_input: 82.4821824903612
step: 2000 epoch: 627 loss: 16.820053853612134 loss_input: 82.23001493637946
step: 3000 epoch: 627 loss: 16.86653518581422 loss_input: 82.11012696695502
step: 4000 epoch: 627 loss: 16.852635454636935 loss_input: 82.37432884210588
step: 5000 epoch: 627 loss: 16.86075450973114 loss_input: 82.3188732858897
step: 6000 epoch: 627 loss: 16.83744286020842 loss_input: 82.3494957973313
step: 7000 epoch: 627 loss: 16.8136226506663 loss_input: 82.29875310607135
step: 8000 epoch: 627 loss: 16.826403589252234 loss_input: 82.31029737158099
step: 9000 epoch: 627 loss: 16.845279275622715 loss_input: 82.32498879247792
step: 10000 epoch: 627 loss: 16.806424156902622 loss_input: 82.2887049795997
step: 11000 epoch: 627 loss: 16.80145008127729 loss_input: 82.21751154835533
step: 12000 epoch: 627 loss: 16.808686728199742 loss_input: 82.19882256312626
step: 13000 epoch: 627 loss: 16.814962714940307 loss_input: 82.19745862679284
step: 14000 epoch: 627 loss: 16.831098374107313 loss_input: 82.16726906242886
step: 15000 epoch: 627 loss: 16.83817504461698 loss_input: 82.16659034254488
Save loss: 16.8377388702631 Name: 627_train_model.pth
step: 0 epoch: 628 loss: 17.220415115356445 loss_input: 124.88165283203125
step: 1000 epoch: 628 loss: 16.814031698844293 loss_input: 83.75296761344124
step: 2000 epoch: 628 loss: 16.75418512097959 loss_input: 83.1618885381528
step: 3000 epoch: 628 loss: 16.86406330973337 loss_input: 83.41601359117273
step: 4000 epoch: 628 loss: 16.831587792217537 loss_input: 83.03037971891304
step: 5000 epoch: 628 loss: 16.85526926840241 loss_input: 82.81561157250972
step: 6000 epoch: 628 loss: 16.81446064477999 loss_input: 82.53697090930808
step: 7000 epoch: 628 loss: 16.83423471011497 loss_input: 82.35251635194693
step: 8000 epoch: 628 loss: 16.84137374114251 loss_input: 82.40068496076663
step: 9000 epoch: 628 loss: 16.864611374909394 loss_input: 82.55994436219326
step: 10000 epoch: 628 loss: 16.853170547446254 loss_input: 82.47379889696577
step: 11000 epoch: 628 loss: 16.864829781488684 loss_input: 82.50330577413771
step: 12000 epoch: 628 loss: 16.85112665047418 loss_input: 82.4436745342836
step: 13000 epoch: 628 loss: 16.843377373858733 loss_input: 82.3502739216124
step: 14000 epoch: 628 loss: 16.82033497230912 loss_input: 82.31266358433243
step: 15000 epoch: 628 loss: 16.828512255950464 loss_input: 82.28471224070088
Save loss: 16.83048205396533 Name: 628_train_model.pth
step: 0 epoch: 629 loss: 16.229148864746094 loss_input: 79.8623046875
step: 1000 epoch: 629 loss: 16.740290849001614 loss_input: 81.37820730295095
step: 2000 epoch: 629 loss: 16.816414931009913 loss_input: 82.2394751415796
step: 3000 epoch: 629 loss: 16.740107274778442 loss_input: 82.2750475386785
step: 4000 epoch: 629 loss: 16.717771720123483 loss_input: 82.33444952970741
step: 5000 epoch: 629 loss: 16.736166083748355 loss_input: 82.1032819681181
step: 6000 epoch: 629 loss: 16.78744799910814 loss_input: 82.2818564357926
step: 7000 epoch: 629 loss: 16.81890738979269 loss_input: 82.35473230038008
step: 8000 epoch: 629 loss: 16.810741681513854 loss_input: 82.32738106680756
step: 9000 epoch: 629 loss: 16.805159515784112 loss_input: 82.305238111564
step: 10000 epoch: 629 loss: 16.823840333514067 loss_input: 82.43857957343914
step: 11000 epoch: 629 loss: 16.80164906040407 loss_input: 82.33027794148508
step: 12000 epoch: 629 loss: 16.808565459602246 loss_input: 82.29969832225895
step: 13000 epoch: 629 loss: 16.81066582800929 loss_input: 82.21588708451964
step: 14000 epoch: 629 loss: 16.817348416009448 loss_input: 82.15785752939382
step: 15000 epoch: 629 loss: 16.8229062114904 loss_input: 82.20886303299181
Save loss: 16.83124907374382 Name: 629_train_model.pth
step: 0 epoch: 630 loss: 12.028812408447266 loss_input: 83.48626708984375
step: 1000 epoch: 630 loss: 16.540533841311277 loss_input: 81.6575084461437
step: 2000 epoch: 630 loss: 16.77366635991239 loss_input: 81.74531786230908
step: 3000 epoch: 630 loss: 16.702104751367006 loss_input: 81.7891784688307
step: 4000 epoch: 630 loss: 16.74838374156232 loss_input: 81.91647385394862
step: 5000 epoch: 630 loss: 16.80768877233274 loss_input: 82.2341510701766
step: 6000 epoch: 630 loss: 16.822958495016596 loss_input: 82.24672518569496
step: 7000 epoch: 630 loss: 16.819321650468424 loss_input: 82.24402000188589
step: 8000 epoch: 630 loss: 16.82424506147628 loss_input: 82.18043853527098
step: 9000 epoch: 630 loss: 16.796901325877013 loss_input: 82.10984730537753
step: 10000 epoch: 630 loss: 16.818873077973688 loss_input: 82.1929472885231
step: 11000 epoch: 630 loss: 16.85264871725331 loss_input: 82.35861138644711
step: 12000 epoch: 630 loss: 16.840503316235118 loss_input: 82.33615394823055
step: 13000 epoch: 630 loss: 16.8525372413789 loss_input: 82.28095595523601
step: 14000 epoch: 630 loss: 16.84117549627596 loss_input: 82.23048450030494
step: 15000 epoch: 630 loss: 16.85483927793816 loss_input: 82.23397516093328
Save loss: 16.848285311490297 Name: 630_train_model.pth
step: 0 epoch: 631 loss: 21.07994270324707 loss_input: 77.69561767578125
step: 1000 epoch: 631 loss: 16.735048966212467 loss_input: 82.43371746661542
step: 2000 epoch: 631 loss: 16.77620295439286 loss_input: 82.20797666962417
step: 3000 epoch: 631 loss: 16.808526277303773 loss_input: 82.12453773140074
step: 4000 epoch: 631 loss: 16.79892463202597 loss_input: 82.02170840956752
step: 5000 epoch: 631 loss: 16.84439014420703 loss_input: 82.2664022371307
step: 6000 epoch: 631 loss: 16.788375902009037 loss_input: 82.08088435659486
step: 7000 epoch: 631 loss: 16.777935047419373 loss_input: 82.05969975829346
step: 8000 epoch: 631 loss: 16.806347687085587 loss_input: 82.14861396109666
step: 9000 epoch: 631 loss: 16.82625870039272 loss_input: 82.31973540890205
step: 10000 epoch: 631 loss: 16.803882667653834 loss_input: 82.32833186407922
step: 11000 epoch: 631 loss: 16.78691197592457 loss_input: 82.34411844129227
step: 12000 epoch: 631 loss: 16.795343896049566 loss_input: 82.24118957697536
step: 13000 epoch: 631 loss: 16.80869518609315 loss_input: 82.25846608460256
step: 14000 epoch: 631 loss: 16.82169343197263 loss_input: 82.25717362525863
step: 15000 epoch: 631 loss: 16.820992625432766 loss_input: 82.231440430013
Save loss: 16.83459399475157 Name: 631_train_model.pth
step: 0 epoch: 632 loss: 21.59284210205078 loss_input: 113.11322021484375
step: 1000 epoch: 632 loss: 16.94370251888043 loss_input: 82.51646388279688
step: 2000 epoch: 632 loss: 16.854737219484015 loss_input: 82.54327179061836
step: 3000 epoch: 632 loss: 16.7795016468465 loss_input: 82.31378924310386
step: 4000 epoch: 632 loss: 16.791802170395226 loss_input: 82.36240424146892
step: 5000 epoch: 632 loss: 16.757305500245625 loss_input: 82.41126718503955
step: 6000 epoch: 632 loss: 16.791215308525825 loss_input: 82.56436697806225
step: 7000 epoch: 632 loss: 16.795407873411687 loss_input: 82.50214806087969
step: 8000 epoch: 632 loss: 16.799160800297816 loss_input: 82.25695427750486
step: 9000 epoch: 632 loss: 16.793521544705786 loss_input: 82.1637559612517
step: 10000 epoch: 632 loss: 16.814369558918514 loss_input: 82.27507890690423
step: 11000 epoch: 632 loss: 16.82575455748724 loss_input: 82.27071154042814
step: 12000 epoch: 632 loss: 16.829339789764134 loss_input: 82.21109339622903
step: 13000 epoch: 632 loss: 16.823542295497305 loss_input: 82.25787372763693
step: 14000 epoch: 632 loss: 16.84609776476044 loss_input: 82.26847405273925
step: 15000 epoch: 632 loss: 16.84473980648439 loss_input: 82.2921665817982
Save loss: 16.82880398003757 Name: 632_train_model.pth
step: 0 epoch: 633 loss: 14.798221588134766 loss_input: 75.63671875
step: 1000 epoch: 633 loss: 16.871182498398362 loss_input: 83.0008572360257
step: 2000 epoch: 633 loss: 16.76629079442689 loss_input: 82.28420893923156
step: 3000 epoch: 633 loss: 16.755745858678974 loss_input: 82.4578645497074
step: 4000 epoch: 633 loss: 16.80562950414111 loss_input: 82.36068216832511
step: 5000 epoch: 633 loss: 16.85429168896445 loss_input: 82.18133670629156
step: 6000 epoch: 633 loss: 16.824528086564875 loss_input: 82.24541664727428
step: 7000 epoch: 633 loss: 16.807421087316232 loss_input: 82.25120173262079
step: 8000 epoch: 633 loss: 16.834753710692294 loss_input: 82.18904182997633
step: 9000 epoch: 633 loss: 16.82809158556065 loss_input: 82.24274356343219
step: 10000 epoch: 633 loss: 16.821721628920674 loss_input: 82.28614248560496
step: 11000 epoch: 633 loss: 16.852234278274832 loss_input: 82.39430792047136
step: 12000 epoch: 633 loss: 16.84003437887519 loss_input: 82.3678717894928
step: 13000 epoch: 633 loss: 16.829530390122606 loss_input: 82.31980959491247
step: 14000 epoch: 633 loss: 16.829617982063624 loss_input: 82.35881896584675
step: 15000 epoch: 633 loss: 16.836544274838793 loss_input: 82.28260042673587
Save loss: 16.83571294821799 Name: 633_train_model.pth
step: 0 epoch: 634 loss: 7.6073384284973145 loss_input: 57.5576171875
step: 1000 epoch: 634 loss: 16.548418563562674 loss_input: 81.72258125175605
step: 2000 epoch: 634 loss: 16.607274159022058 loss_input: 81.47961727754287
step: 3000 epoch: 634 loss: 16.748332921603012 loss_input: 82.60672309262797
step: 4000 epoch: 634 loss: 16.748737928301832 loss_input: 82.58176909324915
step: 5000 epoch: 634 loss: 16.785274583371823 loss_input: 82.57998615120727
step: 6000 epoch: 634 loss: 16.764523738146902 loss_input: 82.4495773938393
step: 7000 epoch: 634 loss: 16.73944894188967 loss_input: 82.2380258979465
step: 8000 epoch: 634 loss: 16.74625943249694 loss_input: 82.23968119860261
step: 9000 epoch: 634 loss: 16.774073719038007 loss_input: 82.33170596875955
step: 10000 epoch: 634 loss: 16.795331365429703 loss_input: 82.4013565379302
step: 11000 epoch: 634 loss: 16.803622323527637 loss_input: 82.37527383985156
step: 12000 epoch: 634 loss: 16.826293346156934 loss_input: 82.39143186838922
step: 13000 epoch: 634 loss: 16.820789242513307 loss_input: 82.3284788405324
step: 14000 epoch: 634 loss: 16.823556662525384 loss_input: 82.33635118417881
step: 15000 epoch: 634 loss: 16.815525478477852 loss_input: 82.26636375283314
Save loss: 16.827932846084238 Name: 634_train_model.pth
step: 0 epoch: 635 loss: 20.433666229248047 loss_input: 88.9669189453125
step: 1000 epoch: 635 loss: 16.606826832244447 loss_input: 79.90661175982221
step: 2000 epoch: 635 loss: 16.681090117096602 loss_input: 80.49521827221155
step: 3000 epoch: 635 loss: 16.720842925519477 loss_input: 81.12664992203119
step: 4000 epoch: 635 loss: 16.767926153198715 loss_input: 81.39343890223734
step: 5000 epoch: 635 loss: 16.730578647377826 loss_input: 81.45978340952904
step: 6000 epoch: 635 loss: 16.754563894575387 loss_input: 81.61747530400207
step: 7000 epoch: 635 loss: 16.75686509097377 loss_input: 81.70866662777249
step: 8000 epoch: 635 loss: 16.77195399758995 loss_input: 81.77082977085139
step: 9000 epoch: 635 loss: 16.769571161259545 loss_input: 81.84280054087957
step: 10000 epoch: 635 loss: 16.787899511359402 loss_input: 81.9679984699284
step: 11000 epoch: 635 loss: 16.78824493035697 loss_input: 81.95593732201547
step: 12000 epoch: 635 loss: 16.80763507290648 loss_input: 81.94978440212176
step: 13000 epoch: 635 loss: 16.798004093669707 loss_input: 82.00673032663353
step: 14000 epoch: 635 loss: 16.803863304324274 loss_input: 82.06606471294182
step: 15000 epoch: 635 loss: 16.821820151558732 loss_input: 82.25388795287996
Save loss: 16.832281835496424 Name: 635_train_model.pth
step: 0 epoch: 636 loss: 19.681289672851562 loss_input: 91.8212890625
step: 1000 epoch: 636 loss: 16.780038056673703 loss_input: 80.7289311030766
step: 2000 epoch: 636 loss: 16.804764410187637 loss_input: 81.46852901064116
step: 3000 epoch: 636 loss: 16.83890426893784 loss_input: 82.00242745419496
step: 4000 epoch: 636 loss: 16.842207344315465 loss_input: 81.86616549292853
step: 5000 epoch: 636 loss: 16.826550654949273 loss_input: 81.9487458831476
step: 6000 epoch: 636 loss: 16.794333744954912 loss_input: 81.82117037907419
step: 7000 epoch: 636 loss: 16.827465919303375 loss_input: 82.05999616370373
step: 8000 epoch: 636 loss: 16.84739610472704 loss_input: 82.11352426161574
step: 9000 epoch: 636 loss: 16.834808046639516 loss_input: 82.07314509135486
step: 10000 epoch: 636 loss: 16.833303263396957 loss_input: 82.15029206918176
step: 11000 epoch: 636 loss: 16.83643063466426 loss_input: 82.13783804965706
step: 12000 epoch: 636 loss: 16.81783380425778 loss_input: 82.16884758301391
step: 13000 epoch: 636 loss: 16.835359848514813 loss_input: 82.20155092111304
step: 14000 epoch: 636 loss: 16.825743479947008 loss_input: 82.21558771105495
step: 15000 epoch: 636 loss: 16.834471228217915 loss_input: 82.19224982169158
Save loss: 16.830502717643977 Name: 636_train_model.pth
step: 0 epoch: 637 loss: 18.028003692626953 loss_input: 160.9432373046875
step: 1000 epoch: 637 loss: 16.778318303210156 loss_input: 82.31069461544315
step: 2000 epoch: 637 loss: 16.820146037839997 loss_input: 82.80553140788004
step: 3000 epoch: 637 loss: 16.770952339452016 loss_input: 82.46393804604195
step: 4000 epoch: 637 loss: 16.842434440127494 loss_input: 82.61272995044696
step: 5000 epoch: 637 loss: 16.817582877677243 loss_input: 82.37642977996198
step: 6000 epoch: 637 loss: 16.772034293233386 loss_input: 82.43199787543547
step: 7000 epoch: 637 loss: 16.787808438706886 loss_input: 82.46692243253754
step: 8000 epoch: 637 loss: 16.81488988405763 loss_input: 82.53832963567542
step: 9000 epoch: 637 loss: 16.82156316419003 loss_input: 82.43203425123988
step: 10000 epoch: 637 loss: 16.814308652924534 loss_input: 82.42659452316487
step: 11000 epoch: 637 loss: 16.81217724630458 loss_input: 82.35947286375067
step: 12000 epoch: 637 loss: 16.811037718897094 loss_input: 82.32151920689552
step: 13000 epoch: 637 loss: 16.813049350204068 loss_input: 82.26500896790992
step: 14000 epoch: 637 loss: 16.815407932047314 loss_input: 82.23275073073044
step: 15000 epoch: 637 loss: 16.83010169318752 loss_input: 82.30342311896004
Save loss: 16.82305556900799 Name: 637_train_model.pth
step: 0 epoch: 638 loss: 6.211946487426758 loss_input: 40.5606689453125
step: 1000 epoch: 638 loss: 16.75595004122693 loss_input: 81.66574861905673
step: 2000 epoch: 638 loss: 16.803469025451264 loss_input: 81.42201918449955
step: 3000 epoch: 638 loss: 16.844586045691983 loss_input: 81.64981074016994
step: 4000 epoch: 638 loss: 16.77362099447062 loss_input: 81.515584208196
step: 5000 epoch: 638 loss: 16.778500340695714 loss_input: 81.59846742809641
step: 6000 epoch: 638 loss: 16.768709117026155 loss_input: 81.30152495132647
step: 7000 epoch: 638 loss: 16.775328879866528 loss_input: 81.30656807328306
step: 8000 epoch: 638 loss: 16.800503228011987 loss_input: 81.60091937665507
step: 9000 epoch: 638 loss: 16.79884593691538 loss_input: 81.70644451641874
step: 10000 epoch: 638 loss: 16.792705506208527 loss_input: 81.77251405718803
step: 11000 epoch: 638 loss: 16.836957361078102 loss_input: 82.06701897616907
step: 12000 epoch: 638 loss: 16.83482911717682 loss_input: 82.1059231188742
step: 13000 epoch: 638 loss: 16.829989569635174 loss_input: 82.19750896753435
step: 14000 epoch: 638 loss: 16.819175840573568 loss_input: 82.21873589752657
step: 15000 epoch: 638 loss: 16.82739967171744 loss_input: 82.27707956676586
Save loss: 16.828507357597353 Name: 638_train_model.pth
step: 0 epoch: 639 loss: 18.73217010498047 loss_input: 66.61834716796875
step: 1000 epoch: 639 loss: 16.482111575482012 loss_input: 81.02535129665256
step: 2000 epoch: 639 loss: 16.690386507881694 loss_input: 81.63445785652095
step: 3000 epoch: 639 loss: 16.72507610403669 loss_input: 81.52686144820215
step: 4000 epoch: 639 loss: 16.80330467998788 loss_input: 81.87445631267964
step: 5000 epoch: 639 loss: 16.784438898315003 loss_input: 81.99727780274023
step: 6000 epoch: 639 loss: 16.834928690205214 loss_input: 82.32967626530018
step: 7000 epoch: 639 loss: 16.831016442278866 loss_input: 82.45778696653827
step: 8000 epoch: 639 loss: 16.813416845842298 loss_input: 82.284744932091
step: 9000 epoch: 639 loss: 16.808071135865386 loss_input: 82.22948332482372
step: 10000 epoch: 639 loss: 16.811082138727695 loss_input: 82.14344132603341
step: 11000 epoch: 639 loss: 16.839506193178696 loss_input: 82.11319753957027
step: 12000 epoch: 639 loss: 16.858234859210196 loss_input: 82.2394994102054
step: 13000 epoch: 639 loss: 16.856328188020555 loss_input: 82.26870802418745
step: 14000 epoch: 639 loss: 16.84527780629764 loss_input: 82.27369949649244
step: 15000 epoch: 639 loss: 16.851169133105284 loss_input: 82.31104441670034
Save loss: 16.828841215327383 Name: 639_train_model.pth
step: 0 epoch: 640 loss: 19.499889373779297 loss_input: 115.7479248046875
step: 1000 epoch: 640 loss: 16.749772641089532 loss_input: 82.14380590113012
step: 2000 epoch: 640 loss: 16.717380467204677 loss_input: 82.10207417653479
step: 3000 epoch: 640 loss: 16.750100591030968 loss_input: 82.07034380060162
step: 4000 epoch: 640 loss: 16.804329816116987 loss_input: 82.3602689873186
step: 5000 epoch: 640 loss: 16.812226437635598 loss_input: 82.17167817657172
step: 6000 epoch: 640 loss: 16.802681050684388 loss_input: 82.24057974590498
step: 7000 epoch: 640 loss: 16.810439747719233 loss_input: 82.32533362456311
step: 8000 epoch: 640 loss: 16.838278447966353 loss_input: 82.29806225241131
step: 9000 epoch: 640 loss: 16.816850018943633 loss_input: 82.29677822933954
step: 10000 epoch: 640 loss: 16.814891718349603 loss_input: 82.28731937597256
step: 11000 epoch: 640 loss: 16.798146817242273 loss_input: 82.2382511624032
step: 12000 epoch: 640 loss: 16.801044864104636 loss_input: 82.26402112466377
step: 13000 epoch: 640 loss: 16.80376082480426 loss_input: 82.25059148845521
step: 14000 epoch: 640 loss: 16.822940699671534 loss_input: 82.31259568321153
step: 15000 epoch: 640 loss: 16.827728134402832 loss_input: 82.26839008665698
Save loss: 16.83890599949658 Name: 640_train_model.pth
step: 0 epoch: 641 loss: 15.82447338104248 loss_input: 69.954833984375
step: 1000 epoch: 641 loss: 16.517257890501224 loss_input: 80.2786110374001
step: 2000 epoch: 641 loss: 16.68711973535365 loss_input: 81.70598282401768
step: 3000 epoch: 641 loss: 16.70371444636049 loss_input: 81.76170681143395
step: 4000 epoch: 641 loss: 16.747868025431004 loss_input: 81.86349073132108
step: 5000 epoch: 641 loss: 16.746185063028594 loss_input: 81.73968785344493
step: 6000 epoch: 641 loss: 16.795289827374454 loss_input: 82.17519420998192
step: 7000 epoch: 641 loss: 16.79488856464501 loss_input: 82.09328322621043
step: 8000 epoch: 641 loss: 16.78862863072096 loss_input: 81.94725814912785
step: 9000 epoch: 641 loss: 16.80150446292096 loss_input: 82.0676676726132
step: 10000 epoch: 641 loss: 16.8096790001662 loss_input: 81.99045374393225
step: 11000 epoch: 641 loss: 16.80978159297218 loss_input: 81.99854064891473
step: 12000 epoch: 641 loss: 16.826132637690964 loss_input: 82.12362681324407
step: 13000 epoch: 641 loss: 16.832183613702707 loss_input: 82.23637600574445
step: 14000 epoch: 641 loss: 16.842115748499932 loss_input: 82.30042097212782
step: 15000 epoch: 641 loss: 16.83327989342705 loss_input: 82.31392892898937
Save loss: 16.822005187258124 Name: 641_train_model.pth
step: 0 epoch: 642 loss: 12.07638931274414 loss_input: 47.87054443359375
step: 1000 epoch: 642 loss: 16.9384899544311 loss_input: 82.63322194163258
step: 2000 epoch: 642 loss: 16.919421944005794 loss_input: 82.94556820100692
step: 3000 epoch: 642 loss: 16.94837664540948 loss_input: 82.59426453796
step: 4000 epoch: 642 loss: 16.88934511579415 loss_input: 82.41088019010873
step: 5000 epoch: 642 loss: 16.914281643049595 loss_input: 82.62890866650889
step: 6000 epoch: 642 loss: 16.86433087792164 loss_input: 82.49945204648509
step: 7000 epoch: 642 loss: 16.865674084926432 loss_input: 82.47853416831914
step: 8000 epoch: 642 loss: 16.87123290322271 loss_input: 82.43158461698636
step: 9000 epoch: 642 loss: 16.839530591977436 loss_input: 82.18121346027742
step: 10000 epoch: 642 loss: 16.84494159977599 loss_input: 82.21933847874978
step: 11000 epoch: 642 loss: 16.818361819500208 loss_input: 82.09748081206929
step: 12000 epoch: 642 loss: 16.807034811172155 loss_input: 82.10175170338995
step: 13000 epoch: 642 loss: 16.803722394006947 loss_input: 82.0653281234226
step: 14000 epoch: 642 loss: 16.79479948943551 loss_input: 82.00526976518977
step: 15000 epoch: 642 loss: 16.811741759861146 loss_input: 82.1676770539516
Save loss: 16.82748017309606 Name: 642_train_model.pth
step: 0 epoch: 643 loss: 18.03289794921875 loss_input: 63.916748046875
step: 1000 epoch: 643 loss: 16.798581220053293 loss_input: 81.35934280563187
step: 2000 epoch: 643 loss: 16.649075842451776 loss_input: 81.76641616935673
step: 3000 epoch: 643 loss: 16.75298232104293 loss_input: 81.68610251660904
step: 4000 epoch: 643 loss: 16.74582320587303 loss_input: 81.52179949702516
step: 5000 epoch: 643 loss: 16.734836631001436 loss_input: 81.62186472617586
step: 6000 epoch: 643 loss: 16.751929450960798 loss_input: 81.74476004894366
step: 7000 epoch: 643 loss: 16.772583653459957 loss_input: 81.95175789444978
step: 8000 epoch: 643 loss: 16.807201324321525 loss_input: 81.98529198437598
step: 9000 epoch: 643 loss: 16.81947479117196 loss_input: 82.00083338315056
step: 10000 epoch: 643 loss: 16.801545239605794 loss_input: 82.00064803090004
step: 11000 epoch: 643 loss: 16.78906360982429 loss_input: 82.00077123670576
step: 12000 epoch: 643 loss: 16.80542858645634 loss_input: 82.1548544101224
step: 13000 epoch: 643 loss: 16.79263895453421 loss_input: 82.16082733156644
step: 14000 epoch: 643 loss: 16.812768937059747 loss_input: 82.21963223504403
step: 15000 epoch: 643 loss: 16.811895191442982 loss_input: 82.21570980147547
Save loss: 16.814902961537243 Name: 643_train_model.pth
step: 0 epoch: 644 loss: 15.30709457397461 loss_input: 140.24322509765625
step: 1000 epoch: 644 loss: 16.6850768388449 loss_input: 81.70398369940607
step: 2000 epoch: 644 loss: 16.774002533445117 loss_input: 81.77310853240373
step: 3000 epoch: 644 loss: 16.805527818794847 loss_input: 81.58298800302244
step: 4000 epoch: 644 loss: 16.76036760599069 loss_input: 81.87497095452699
step: 5000 epoch: 644 loss: 16.759567181889093 loss_input: 81.79737461931443
step: 6000 epoch: 644 loss: 16.781136296228894 loss_input: 81.81706505540092
step: 7000 epoch: 644 loss: 16.76623197133806 loss_input: 81.8404046875837
step: 8000 epoch: 644 loss: 16.76295084277476 loss_input: 81.83015289135001
step: 9000 epoch: 644 loss: 16.767964152068167 loss_input: 81.78962821269641
step: 10000 epoch: 644 loss: 16.761731402395533 loss_input: 81.95760617419704
step: 11000 epoch: 644 loss: 16.775040463744137 loss_input: 81.89291713337673
step: 12000 epoch: 644 loss: 16.817258660827672 loss_input: 82.09002816761526
step: 13000 epoch: 644 loss: 16.820296907689002 loss_input: 82.16757408478344
step: 14000 epoch: 644 loss: 16.814932164426583 loss_input: 82.13918434624229
step: 15000 epoch: 644 loss: 16.812671652460693 loss_input: 82.14559394363944
Save loss: 16.82958499278128 Name: 644_train_model.pth
step: 0 epoch: 645 loss: 19.42366600036621 loss_input: 77.06878662109375
step: 1000 epoch: 645 loss: 16.957660546431413 loss_input: 82.86321143456153
step: 2000 epoch: 645 loss: 16.734992779355714 loss_input: 82.07104010250734
step: 3000 epoch: 645 loss: 16.728214627939316 loss_input: 81.89872819493111
step: 4000 epoch: 645 loss: 16.66620935758511 loss_input: 81.82274133615034
step: 5000 epoch: 645 loss: 16.694355327447543 loss_input: 82.13355726071583
step: 6000 epoch: 645 loss: 16.720651202034183 loss_input: 81.93989718347723
step: 7000 epoch: 645 loss: 16.725768404506884 loss_input: 81.94699727431517
step: 8000 epoch: 645 loss: 16.72889062637479 loss_input: 81.97113471167667
step: 9000 epoch: 645 loss: 16.747206230876632 loss_input: 81.99256203792027
step: 10000 epoch: 645 loss: 16.762927076838253 loss_input: 82.08094481689527
step: 11000 epoch: 645 loss: 16.77787966860846 loss_input: 82.13036494640401
step: 12000 epoch: 645 loss: 16.810256238262234 loss_input: 82.20155157358306
step: 13000 epoch: 645 loss: 16.807376016480088 loss_input: 82.1543703322025
step: 14000 epoch: 645 loss: 16.797137136689578 loss_input: 82.181823529939
step: 15000 epoch: 645 loss: 16.805113604891883 loss_input: 82.22979034470619
Save loss: 16.81917981763184 Name: 645_train_model.pth
step: 0 epoch: 646 loss: 16.169986724853516 loss_input: 94.083251953125
step: 1000 epoch: 646 loss: 16.814549614261317 loss_input: 83.47403652255947
step: 2000 epoch: 646 loss: 16.841179096835784 loss_input: 82.40210372718914
step: 3000 epoch: 646 loss: 16.814911326898095 loss_input: 82.37270553776996
step: 4000 epoch: 646 loss: 16.863784042664452 loss_input: 82.40051190205378
step: 5000 epoch: 646 loss: 16.818225986979957 loss_input: 82.25658908745595
step: 6000 epoch: 646 loss: 16.82846195605214 loss_input: 82.44433445255869
step: 7000 epoch: 646 loss: 16.823888867161237 loss_input: 82.35354680079185
step: 8000 epoch: 646 loss: 16.801239530409354 loss_input: 82.27610705766271
step: 9000 epoch: 646 loss: 16.83043703157734 loss_input: 82.29192097416693
step: 10000 epoch: 646 loss: 16.830749640714142 loss_input: 82.30138209323599
step: 11000 epoch: 646 loss: 16.816993680328427 loss_input: 82.11958836039763
step: 12000 epoch: 646 loss: 16.828509630357093 loss_input: 82.21559073771847
step: 13000 epoch: 646 loss: 16.850231895299338 loss_input: 82.31616311872494
step: 14000 epoch: 646 loss: 16.84023917502109 loss_input: 82.27826191910539
step: 15000 epoch: 646 loss: 16.83407648488145 loss_input: 82.22913156701011
Save loss: 16.830304143443705 Name: 646_train_model.pth
step: 0 epoch: 647 loss: 16.952775955200195 loss_input: 74.94287109375
step: 1000 epoch: 647 loss: 16.702880122921208 loss_input: 81.69346364085133
step: 2000 epoch: 647 loss: 16.84437567373921 loss_input: 82.22600693061672
step: 3000 epoch: 647 loss: 16.73145266669228 loss_input: 82.12324395286207
step: 4000 epoch: 647 loss: 16.707436238369684 loss_input: 81.90928848598398
step: 5000 epoch: 647 loss: 16.720445761750206 loss_input: 81.9692593585775
step: 6000 epoch: 647 loss: 16.719142896814002 loss_input: 81.7952932527196
step: 7000 epoch: 647 loss: 16.755318432156656 loss_input: 81.9539854379062
step: 8000 epoch: 647 loss: 16.796536627538114 loss_input: 82.15230865154307
step: 9000 epoch: 647 loss: 16.81661976807489 loss_input: 82.21095506939116
step: 10000 epoch: 647 loss: 16.80079197030153 loss_input: 82.15353652623018
step: 11000 epoch: 647 loss: 16.80549792670822 loss_input: 82.18908433417452
step: 12000 epoch: 647 loss: 16.82898435470671 loss_input: 82.16713266346457
step: 13000 epoch: 647 loss: 16.819089050375858 loss_input: 82.14561616034794
step: 14000 epoch: 647 loss: 16.836814005454432 loss_input: 82.13031702918262
step: 15000 epoch: 647 loss: 16.830142552801295 loss_input: 82.17463345461213
Save loss: 16.827360052213074 Name: 647_train_model.pth
step: 0 epoch: 648 loss: 17.89710235595703 loss_input: 66.8707275390625
step: 1000 epoch: 648 loss: 16.71842811395834 loss_input: 81.79558027421797
step: 2000 epoch: 648 loss: 16.702446266271544 loss_input: 82.14530549056526
step: 3000 epoch: 648 loss: 16.706317237121826 loss_input: 81.90397786919334
step: 4000 epoch: 648 loss: 16.79708360058461 loss_input: 81.99956768925445
step: 5000 epoch: 648 loss: 16.77011913348379 loss_input: 81.82342668429204
step: 6000 epoch: 648 loss: 16.795717344425494 loss_input: 82.06269813795683
step: 7000 epoch: 648 loss: 16.832295362548546 loss_input: 82.03071206066544
step: 8000 epoch: 648 loss: 16.827505157166996 loss_input: 82.12136497921891
step: 9000 epoch: 648 loss: 16.80939803967382 loss_input: 81.97409044394372
step: 10000 epoch: 648 loss: 16.82717163688409 loss_input: 82.15973393505377
step: 11000 epoch: 648 loss: 16.827209937096768 loss_input: 82.16486809637512
step: 12000 epoch: 648 loss: 16.820740703065916 loss_input: 82.1545865796584
step: 13000 epoch: 648 loss: 16.832703662133202 loss_input: 82.29463754346871
step: 14000 epoch: 648 loss: 16.825942403937603 loss_input: 82.23747641072512
step: 15000 epoch: 648 loss: 16.82666152379138 loss_input: 82.23862944862921
Save loss: 16.831143309503794 Name: 648_train_model.pth
step: 0 epoch: 649 loss: 9.700401306152344 loss_input: 64.84698486328125
step: 1000 epoch: 649 loss: 16.654121664258746 loss_input: 81.26678003393091
step: 2000 epoch: 649 loss: 16.64170478928512 loss_input: 81.37409261677755
step: 3000 epoch: 649 loss: 16.733131845646483 loss_input: 81.8746806484427
step: 4000 epoch: 649 loss: 16.748550982452638 loss_input: 82.00698764822954
step: 5000 epoch: 649 loss: 16.775910393139572 loss_input: 82.33567081749666
step: 6000 epoch: 649 loss: 16.802802011502266 loss_input: 82.33812513384814
step: 7000 epoch: 649 loss: 16.838441411830104 loss_input: 82.39950345751389
step: 8000 epoch: 649 loss: 16.80581343801241 loss_input: 82.25071756167198
step: 9000 epoch: 649 loss: 16.822959455433534 loss_input: 82.27385016709297
step: 10000 epoch: 649 loss: 16.79201024895298 loss_input: 82.26535613650549
step: 11000 epoch: 649 loss: 16.80297484726009 loss_input: 82.29274422207179
step: 12000 epoch: 649 loss: 16.821953753035977 loss_input: 82.4149742587369
step: 13000 epoch: 649 loss: 16.80646526247765 loss_input: 82.33426797416575
step: 14000 epoch: 649 loss: 16.824449025974147 loss_input: 82.31492456175005
step: 15000 epoch: 649 loss: 16.815871154568242 loss_input: 82.40290919488831
Save loss: 16.818705342739822 Name: 649_train_model.pth
step: 0 epoch: 650 loss: 12.87756061553955 loss_input: 56.43890380859375
step: 1000 epoch: 650 loss: 16.75289978109278 loss_input: 82.5602071878317
step: 2000 epoch: 650 loss: 16.822271228611083 loss_input: 82.33312979130552
step: 3000 epoch: 650 loss: 16.85312372666524 loss_input: 82.21522200119492
step: 4000 epoch: 650 loss: 16.829734063452406 loss_input: 82.41618861642011
step: 5000 epoch: 650 loss: 16.79710910301689 loss_input: 82.1775911053141
step: 6000 epoch: 650 loss: 16.78131381536877 loss_input: 82.08402891306852
step: 7000 epoch: 650 loss: 16.80726041509124 loss_input: 82.19803239604164
step: 8000 epoch: 650 loss: 16.806167767146516 loss_input: 82.28355069965619
step: 9000 epoch: 650 loss: 16.820454640304256 loss_input: 82.39775378554944
step: 10000 epoch: 650 loss: 16.811118790345507 loss_input: 82.3964547392917
step: 11000 epoch: 650 loss: 16.805560414785255 loss_input: 82.31855090810282
step: 12000 epoch: 650 loss: 16.794777289339628 loss_input: 82.30133476730943
step: 13000 epoch: 650 loss: 16.807159081760457 loss_input: 82.18566479524111
step: 14000 epoch: 650 loss: 16.80453117594839 loss_input: 82.12762074960605
step: 15000 epoch: 650 loss: 16.817912302385306 loss_input: 82.19861187720949
Save loss: 16.81707236842811 Name: 650_train_model.pth
step: 0 epoch: 651 loss: 16.647729873657227 loss_input: 68.90545654296875
step: 1000 epoch: 651 loss: 16.725002867358548 loss_input: 80.87056309169347
step: 2000 epoch: 651 loss: 16.79167180618961 loss_input: 81.24363812966564
step: 3000 epoch: 651 loss: 16.879185845080475 loss_input: 81.33141154283207
step: 4000 epoch: 651 loss: 16.82159943873809 loss_input: 81.36115165544551
step: 5000 epoch: 651 loss: 16.768600026551926 loss_input: 81.56033250137082
step: 6000 epoch: 651 loss: 16.794373717989014 loss_input: 81.70511926601101
step: 7000 epoch: 651 loss: 16.787678039171002 loss_input: 81.69565866811159
step: 8000 epoch: 651 loss: 16.783896253788683 loss_input: 81.76675877704008
step: 9000 epoch: 651 loss: 16.807879486662483 loss_input: 81.9496562692036
step: 10000 epoch: 651 loss: 16.808582820268693 loss_input: 82.14472138601569
step: 11000 epoch: 651 loss: 16.830125386298175 loss_input: 82.07143001315399
step: 12000 epoch: 651 loss: 16.82788792203619 loss_input: 82.13848832706324
step: 13000 epoch: 651 loss: 16.828994251894827 loss_input: 82.2250973698763
step: 14000 epoch: 651 loss: 16.832693445164 loss_input: 82.28336873642675
step: 15000 epoch: 651 loss: 16.829272093927692 loss_input: 82.29024336203108
Save loss: 16.82458194564283 Name: 651_train_model.pth
step: 0 epoch: 652 loss: 17.319955825805664 loss_input: 65.79119873046875
step: 1000 epoch: 652 loss: 16.744451819123565 loss_input: 82.3531052077805
step: 2000 epoch: 652 loss: 16.86021076518854 loss_input: 82.70912846799257
step: 3000 epoch: 652 loss: 16.831429840286187 loss_input: 82.51671657543189
step: 4000 epoch: 652 loss: 16.81601648287784 loss_input: 82.76631562324263
step: 5000 epoch: 652 loss: 16.791081388100128 loss_input: 82.53915008014022
step: 6000 epoch: 652 loss: 16.79561879384162 loss_input: 82.38077445873279
step: 7000 epoch: 652 loss: 16.769390395804315 loss_input: 82.24125159854395
step: 8000 epoch: 652 loss: 16.782192619990266 loss_input: 82.27683628322094
step: 9000 epoch: 652 loss: 16.797396422094696 loss_input: 82.21530167980467
step: 10000 epoch: 652 loss: 16.825757634674307 loss_input: 82.4125442253138
step: 11000 epoch: 652 loss: 16.820188715917155 loss_input: 82.3394982225515
step: 12000 epoch: 652 loss: 16.80382782363383 loss_input: 82.30050604506916
step: 13000 epoch: 652 loss: 16.81703494724957 loss_input: 82.36718743896954
step: 14000 epoch: 652 loss: 16.82216141425016 loss_input: 82.24499023472374
step: 15000 epoch: 652 loss: 16.824560131472623 loss_input: 82.28883745396791
Save loss: 16.819612306565045 Name: 652_train_model.pth
step: 0 epoch: 653 loss: 15.877634048461914 loss_input: 166.10888671875
step: 1000 epoch: 653 loss: 16.489001511812926 loss_input: 81.34822117579687
step: 2000 epoch: 653 loss: 16.701450488735354 loss_input: 82.20110843015992
step: 3000 epoch: 653 loss: 16.670244088692492 loss_input: 82.04456835514463
step: 4000 epoch: 653 loss: 16.71619078803736 loss_input: 82.44052027154345
step: 5000 epoch: 653 loss: 16.71863776115245 loss_input: 82.09437729153387
step: 6000 epoch: 653 loss: 16.768482809205032 loss_input: 82.10335332905127
step: 7000 epoch: 653 loss: 16.804115245894284 loss_input: 82.29933868089994
step: 8000 epoch: 653 loss: 16.810942265260607 loss_input: 82.34605993006977
step: 9000 epoch: 653 loss: 16.81978187475214 loss_input: 82.32049214041322
step: 10000 epoch: 653 loss: 16.82082271699893 loss_input: 82.24987029495293
step: 11000 epoch: 653 loss: 16.81520522357052 loss_input: 82.20043773874782
step: 12000 epoch: 653 loss: 16.794200816618165 loss_input: 82.1469331901378
step: 13000 epoch: 653 loss: 16.814382196802256 loss_input: 82.14423006936666
step: 14000 epoch: 653 loss: 16.819005683254968 loss_input: 82.2092725013922
step: 15000 epoch: 653 loss: 16.809977531194704 loss_input: 82.21728922498917
Save loss: 16.816098097622394 Name: 653_train_model.pth
step: 0 epoch: 654 loss: 16.214374542236328 loss_input: 50.30322265625
step: 1000 epoch: 654 loss: 16.696313188268945 loss_input: 82.18989122449815
step: 2000 epoch: 654 loss: 16.770613575982548 loss_input: 82.82959106384308
step: 3000 epoch: 654 loss: 16.793384282519842 loss_input: 83.00976131328619
step: 4000 epoch: 654 loss: 16.748727133440333 loss_input: 82.5895906547045
step: 5000 epoch: 654 loss: 16.770797106772036 loss_input: 82.47346351042292
step: 6000 epoch: 654 loss: 16.75917762828815 loss_input: 82.49573559436058
step: 7000 epoch: 654 loss: 16.744782009527967 loss_input: 82.27066052680253
step: 8000 epoch: 654 loss: 16.775663766990885 loss_input: 82.35021553091997
step: 9000 epoch: 654 loss: 16.787707460070965 loss_input: 82.42096228674245
step: 10000 epoch: 654 loss: 16.78868392786614 loss_input: 82.2461392715709
step: 11000 epoch: 654 loss: 16.783231786354794 loss_input: 82.2023249795118
step: 12000 epoch: 654 loss: 16.808504147824024 loss_input: 82.21155609570785
step: 13000 epoch: 654 loss: 16.803762841908696 loss_input: 82.1770117039149
step: 14000 epoch: 654 loss: 16.795430361990434 loss_input: 82.16464599365251
step: 15000 epoch: 654 loss: 16.81265873052654 loss_input: 82.23392346760215
Save loss: 16.815183117046953 Name: 654_train_model.pth
step: 0 epoch: 655 loss: 16.960586547851562 loss_input: 98.53955078125
step: 1000 epoch: 655 loss: 16.659368142024142 loss_input: 81.96594110235468
step: 2000 epoch: 655 loss: 16.73250407448177 loss_input: 82.76077506900846
step: 3000 epoch: 655 loss: 16.712351131979446 loss_input: 82.63229826711408
step: 4000 epoch: 655 loss: 16.68905121396405 loss_input: 82.63156095131491
step: 5000 epoch: 655 loss: 16.69506116705736 loss_input: 82.2964190243006
step: 6000 epoch: 655 loss: 16.739553542360426 loss_input: 82.22962385907687
step: 7000 epoch: 655 loss: 16.747378110749402 loss_input: 82.35891838275336
step: 8000 epoch: 655 loss: 16.746033947075357 loss_input: 82.18908912631038
step: 9000 epoch: 655 loss: 16.77615989821313 loss_input: 82.20056649385963
step: 10000 epoch: 655 loss: 16.784941037146286 loss_input: 82.12710706468415
step: 11000 epoch: 655 loss: 16.792032404538013 loss_input: 82.10314247887976
step: 12000 epoch: 655 loss: 16.807709327450137 loss_input: 82.17738406748168
step: 13000 epoch: 655 loss: 16.814746720214483 loss_input: 82.09959892001247
step: 14000 epoch: 655 loss: 16.8148183607219 loss_input: 82.13295914828629
step: 15000 epoch: 655 loss: 16.810577566850487 loss_input: 82.21500418445412
Save loss: 16.816555195853113 Name: 655_train_model.pth
step: 0 epoch: 656 loss: 17.55238151550293 loss_input: 72.42193603515625
step: 1000 epoch: 656 loss: 17.038089225342222 loss_input: 83.45184563971185
step: 2000 epoch: 656 loss: 16.745663619053357 loss_input: 82.61200875392382
step: 3000 epoch: 656 loss: 16.7925355724397 loss_input: 82.27167836144463
step: 4000 epoch: 656 loss: 16.8259528642653 loss_input: 82.4517041058339
step: 5000 epoch: 656 loss: 16.873927698448117 loss_input: 82.42845962057588
step: 6000 epoch: 656 loss: 16.85394326628615 loss_input: 82.27727865668221
step: 7000 epoch: 656 loss: 16.834951166799453 loss_input: 82.30028622968761
step: 8000 epoch: 656 loss: 16.846031155769207 loss_input: 82.4179972345986
step: 9000 epoch: 656 loss: 16.842561961889082 loss_input: 82.4518007791886
step: 10000 epoch: 656 loss: 16.84500035995508 loss_input: 82.44133407704152
step: 11000 epoch: 656 loss: 16.86594435458031 loss_input: 82.49545686165426
step: 12000 epoch: 656 loss: 16.86965369109402 loss_input: 82.36635653490464
step: 13000 epoch: 656 loss: 16.85529025222107 loss_input: 82.31518049543064
step: 14000 epoch: 656 loss: 16.85390323881132 loss_input: 82.24204806206643
step: 15000 epoch: 656 loss: 16.838126901101848 loss_input: 82.19786602849524
Save loss: 16.82740137130022 Name: 656_train_model.pth
step: 0 epoch: 657 loss: 13.387825012207031 loss_input: 94.27850341796875
step: 1000 epoch: 657 loss: 16.700864841888002 loss_input: 81.32195831512237
step: 2000 epoch: 657 loss: 16.767664088659558 loss_input: 81.8317101825064
step: 3000 epoch: 657 loss: 16.73886908089785 loss_input: 81.84966626114863
step: 4000 epoch: 657 loss: 16.768972298050784 loss_input: 81.97570848071673
step: 5000 epoch: 657 loss: 16.777791620754904 loss_input: 82.1224892838815
step: 6000 epoch: 657 loss: 16.76857952327852 loss_input: 82.10904429543318
step: 7000 epoch: 657 loss: 16.809619510127824 loss_input: 82.24928730710066
step: 8000 epoch: 657 loss: 16.82311445596054 loss_input: 82.3762127542895
step: 9000 epoch: 657 loss: 16.811296585890574 loss_input: 82.39733901636798
step: 10000 epoch: 657 loss: 16.798751126955825 loss_input: 82.21830886979662
step: 11000 epoch: 657 loss: 16.80497385974191 loss_input: 82.20642620246525
step: 12000 epoch: 657 loss: 16.804359436968884 loss_input: 82.22835260351026
step: 13000 epoch: 657 loss: 16.818503823704688 loss_input: 82.32643557823526
step: 14000 epoch: 657 loss: 16.804317809268326 loss_input: 82.20261917239588
step: 15000 epoch: 657 loss: 16.821358762115775 loss_input: 82.24210309807471
Save loss: 16.8125414493531 Name: 657_train_model.pth
step: 0 epoch: 658 loss: 13.49289321899414 loss_input: 60.617431640625
step: 1000 epoch: 658 loss: 16.829832170393082 loss_input: 82.74064875554133
step: 2000 epoch: 658 loss: 16.77208959835878 loss_input: 82.68628768501492
step: 3000 epoch: 658 loss: 16.725246550996317 loss_input: 82.39102383352684
step: 4000 epoch: 658 loss: 16.679098206619955 loss_input: 82.03514169675354
step: 5000 epoch: 658 loss: 16.723374958110796 loss_input: 82.18367061450991
step: 6000 epoch: 658 loss: 16.722515882760003 loss_input: 82.07641128618863
step: 7000 epoch: 658 loss: 16.708727060360765 loss_input: 82.0977038244676
step: 8000 epoch: 658 loss: 16.699994146756836 loss_input: 82.15372619895902
step: 9000 epoch: 658 loss: 16.7324880636635 loss_input: 82.18627420439613
step: 10000 epoch: 658 loss: 16.76692013952234 loss_input: 82.29990508029275
step: 11000 epoch: 658 loss: 16.793364465177063 loss_input: 82.41328433365965
step: 12000 epoch: 658 loss: 16.808987157641425 loss_input: 82.39285966180064
step: 13000 epoch: 658 loss: 16.796871784585925 loss_input: 82.24111820875558
step: 14000 epoch: 658 loss: 16.79344038464037 loss_input: 82.20586599771333
step: 15000 epoch: 658 loss: 16.807228625722665 loss_input: 82.22709293338778
Save loss: 16.81820107360184 Name: 658_train_model.pth
step: 0 epoch: 659 loss: 14.633041381835938 loss_input: 159.7506103515625
step: 1000 epoch: 659 loss: 16.659007509747944 loss_input: 82.47175767347886
step: 2000 epoch: 659 loss: 16.644871173412547 loss_input: 82.16574894315538
step: 3000 epoch: 659 loss: 16.66682491521762 loss_input: 81.8002044565874
step: 4000 epoch: 659 loss: 16.71172482548461 loss_input: 81.97199319249778
step: 5000 epoch: 659 loss: 16.732970189771706 loss_input: 82.11601105071955
step: 6000 epoch: 659 loss: 16.754503953419135 loss_input: 82.00238464284273
step: 7000 epoch: 659 loss: 16.773992819745885 loss_input: 82.10591086039048
step: 8000 epoch: 659 loss: 16.791072190724197 loss_input: 82.23319401053872
step: 9000 epoch: 659 loss: 16.782980621821245 loss_input: 82.19305240905308
step: 10000 epoch: 659 loss: 16.77243935393162 loss_input: 82.14748362810204
step: 11000 epoch: 659 loss: 16.764778440707968 loss_input: 82.16899051324702
step: 12000 epoch: 659 loss: 16.76597139980582 loss_input: 82.16321512290155
step: 13000 epoch: 659 loss: 16.801683440977552 loss_input: 82.27899347379679
step: 14000 epoch: 659 loss: 16.794829023419922 loss_input: 82.21117286526827
step: 15000 epoch: 659 loss: 16.80579231179688 loss_input: 82.19842413744682
Save loss: 16.808895858585835 Name: 659_train_model.pth
step: 0 epoch: 660 loss: 16.47999382019043 loss_input: 80.8355712890625
step: 1000 epoch: 660 loss: 16.79184267308924 loss_input: 82.06056236244224
step: 2000 epoch: 660 loss: 16.776676040718044 loss_input: 82.77470956904361
step: 3000 epoch: 660 loss: 16.82340391514024 loss_input: 82.4858897538711
step: 4000 epoch: 660 loss: 16.806340132079043 loss_input: 82.26016442056478
step: 5000 epoch: 660 loss: 16.819134068856165 loss_input: 82.1815677880049
step: 6000 epoch: 660 loss: 16.830309236035745 loss_input: 82.40549357496407
step: 7000 epoch: 660 loss: 16.813952473874878 loss_input: 82.28204525295214
step: 8000 epoch: 660 loss: 16.803051186388632 loss_input: 82.14892829863552
step: 9000 epoch: 660 loss: 16.80451690415623 loss_input: 82.29788522982038
step: 10000 epoch: 660 loss: 16.834319267376888 loss_input: 82.43137315907082
step: 11000 epoch: 660 loss: 16.828314217597004 loss_input: 82.35573280846982
step: 12000 epoch: 660 loss: 16.835425142367356 loss_input: 82.34470635646086
step: 13000 epoch: 660 loss: 16.815907434301828 loss_input: 82.27603561369678
step: 14000 epoch: 660 loss: 16.81303180661272 loss_input: 82.21661012076146
step: 15000 epoch: 660 loss: 16.806366693273432 loss_input: 82.19957290933209
Save loss: 16.80980192382634 Name: 660_train_model.pth
step: 0 epoch: 661 loss: 18.183673858642578 loss_input: 94.6871337890625
step: 1000 epoch: 661 loss: 16.726556242048204 loss_input: 82.63529439310689
step: 2000 epoch: 661 loss: 16.645755678936577 loss_input: 82.93460773885518
step: 3000 epoch: 661 loss: 16.705882339229667 loss_input: 82.9286911483607
step: 4000 epoch: 661 loss: 16.7076896626006 loss_input: 82.48119685471698
step: 5000 epoch: 661 loss: 16.71292851944252 loss_input: 82.29653787260126
step: 6000 epoch: 661 loss: 16.684797577134095 loss_input: 82.02229827540137
step: 7000 epoch: 661 loss: 16.750405163547683 loss_input: 82.16913604627352
step: 8000 epoch: 661 loss: 16.749855914811405 loss_input: 82.16996555545303
step: 9000 epoch: 661 loss: 16.74000523678661 loss_input: 82.13326059745532
step: 10000 epoch: 661 loss: 16.743054596856407 loss_input: 82.09986385394663
step: 11000 epoch: 661 loss: 16.76989536063908 loss_input: 82.1118992844578
step: 12000 epoch: 661 loss: 16.782068952680895 loss_input: 82.14178416447066
step: 13000 epoch: 661 loss: 16.795180752225843 loss_input: 82.19462154128168
step: 14000 epoch: 661 loss: 16.797728187296613 loss_input: 82.17604605826844
step: 15000 epoch: 661 loss: 16.80824656278624 loss_input: 82.25875896603638
Save loss: 16.811324153468014 Name: 661_train_model.pth
step: 0 epoch: 662 loss: 16.4036922454834 loss_input: 90.722412109375
step: 1000 epoch: 662 loss: 16.688780957525903 loss_input: 81.44469202672327
step: 2000 epoch: 662 loss: 16.723154273645573 loss_input: 81.74227565172492
step: 3000 epoch: 662 loss: 16.769368177888712 loss_input: 82.08848255008748
step: 4000 epoch: 662 loss: 16.71306119540786 loss_input: 82.13389297939187
step: 5000 epoch: 662 loss: 16.729417387568173 loss_input: 82.28309271007127
step: 6000 epoch: 662 loss: 16.734382423355747 loss_input: 82.109918376641
step: 7000 epoch: 662 loss: 16.72930481559531 loss_input: 82.2389096653882
step: 8000 epoch: 662 loss: 16.78045935467502 loss_input: 82.34181194879937
step: 9000 epoch: 662 loss: 16.789912103268986 loss_input: 82.28486288659984
step: 10000 epoch: 662 loss: 16.79195927651021 loss_input: 82.3191757728524
step: 11000 epoch: 662 loss: 16.78906047759235 loss_input: 82.25191234057735
step: 12000 epoch: 662 loss: 16.785854940662364 loss_input: 82.33787598795477
step: 13000 epoch: 662 loss: 16.785931765670988 loss_input: 82.32616016654067
step: 14000 epoch: 662 loss: 16.796444268731012 loss_input: 82.31105490407214
step: 15000 epoch: 662 loss: 16.80393753656665 loss_input: 82.30847946318552
Save loss: 16.806465793922545 Name: 662_train_model.pth
step: 0 epoch: 663 loss: 19.929027557373047 loss_input: 66.8843994140625
step: 1000 epoch: 663 loss: 16.922726262461293 loss_input: 82.58742697731955
step: 2000 epoch: 663 loss: 16.914066815602666 loss_input: 83.48028366188
step: 3000 epoch: 663 loss: 16.833154900794582 loss_input: 83.0103622075519
step: 4000 epoch: 663 loss: 16.832882180031582 loss_input: 82.93026265833444
step: 5000 epoch: 663 loss: 16.830971760169145 loss_input: 82.79812694687625
step: 6000 epoch: 663 loss: 16.818143355371635 loss_input: 82.55715879832202
step: 7000 epoch: 663 loss: 16.791306609818637 loss_input: 82.36741160651715
step: 8000 epoch: 663 loss: 16.795462255164423 loss_input: 82.34783442749588
step: 9000 epoch: 663 loss: 16.774784282344857 loss_input: 82.26139430959493
step: 10000 epoch: 663 loss: 16.76097894389562 loss_input: 82.16477098628027
step: 11000 epoch: 663 loss: 16.77155237520362 loss_input: 82.23822010942897
step: 12000 epoch: 663 loss: 16.796612228038818 loss_input: 82.29952183855285
step: 13000 epoch: 663 loss: 16.817802637817767 loss_input: 82.29571883424082
step: 14000 epoch: 663 loss: 16.808706593968154 loss_input: 82.22276726501957
step: 15000 epoch: 663 loss: 16.816811545150582 loss_input: 82.22979278188095
Save loss: 16.8082316339314 Name: 663_train_model.pth
step: 0 epoch: 664 loss: 17.611522674560547 loss_input: 70.50579833984375
step: 1000 epoch: 664 loss: 16.961746905590747 loss_input: 82.98764937455122
step: 2000 epoch: 664 loss: 16.757444170580573 loss_input: 82.02022618451517
step: 3000 epoch: 664 loss: 16.765687052069882 loss_input: 81.77682640535836
step: 4000 epoch: 664 loss: 16.7250582785107 loss_input: 82.02465731833644
step: 5000 epoch: 664 loss: 16.703117450412048 loss_input: 82.05826789964273
step: 6000 epoch: 664 loss: 16.73797241780504 loss_input: 82.16167389053322
step: 7000 epoch: 664 loss: 16.760976466906445 loss_input: 82.26754637381602
step: 8000 epoch: 664 loss: 16.7708059606396 loss_input: 82.1641395930245
step: 9000 epoch: 664 loss: 16.764186046107877 loss_input: 82.1219339683286
step: 10000 epoch: 664 loss: 16.743651612927085 loss_input: 82.04356454613328
step: 11000 epoch: 664 loss: 16.75510120023414 loss_input: 81.98322738815293
step: 12000 epoch: 664 loss: 16.763774171172354 loss_input: 82.09799639092576
step: 13000 epoch: 664 loss: 16.775275223255708 loss_input: 82.21177369243833
step: 14000 epoch: 664 loss: 16.786287476391326 loss_input: 82.19742097017485
step: 15000 epoch: 664 loss: 16.79302433288683 loss_input: 82.18785921677487
Save loss: 16.807600599750877 Name: 664_train_model.pth
step: 0 epoch: 665 loss: 21.221811294555664 loss_input: 84.86627197265625
step: 1000 epoch: 665 loss: 16.79259035804055 loss_input: 80.39658175076876
step: 2000 epoch: 665 loss: 16.725721683340154 loss_input: 81.68035467739763
step: 3000 epoch: 665 loss: 16.81323196156269 loss_input: 81.73301973886309
step: 4000 epoch: 665 loss: 16.808719741913773 loss_input: 82.2354723971595
step: 5000 epoch: 665 loss: 16.799369574594298 loss_input: 82.17748299705293
step: 6000 epoch: 665 loss: 16.821414290299277 loss_input: 82.24391487323727
step: 7000 epoch: 665 loss: 16.810219523260006 loss_input: 82.43907921601323
step: 8000 epoch: 665 loss: 16.765912290096818 loss_input: 82.3998063550772
step: 9000 epoch: 665 loss: 16.786053100886207 loss_input: 82.29332533879699
step: 10000 epoch: 665 loss: 16.788897282170623 loss_input: 82.26249942754748
step: 11000 epoch: 665 loss: 16.778020923760312 loss_input: 82.05567416567422
step: 12000 epoch: 665 loss: 16.79445139341241 loss_input: 82.21535714003814
step: 13000 epoch: 665 loss: 16.78110423224512 loss_input: 82.2080838705391
step: 14000 epoch: 665 loss: 16.803797854431085 loss_input: 82.27224046374617
step: 15000 epoch: 665 loss: 16.80867810473745 loss_input: 82.23379799989246
Save loss: 16.808533346638082 Name: 665_train_model.pth
step: 0 epoch: 666 loss: 21.32286834716797 loss_input: 95.63629150390625
step: 1000 epoch: 666 loss: 16.854355018455667 loss_input: 83.29159189199473
step: 2000 epoch: 666 loss: 16.802220808274146 loss_input: 82.92750652261759
step: 3000 epoch: 666 loss: 16.81586605769243 loss_input: 82.17819383691764
step: 4000 epoch: 666 loss: 16.817966129505105 loss_input: 82.2308601163918
step: 5000 epoch: 666 loss: 16.815312685715725 loss_input: 82.58429125942389
step: 6000 epoch: 666 loss: 16.862178403761877 loss_input: 82.51503927201912
step: 7000 epoch: 666 loss: 16.84378909090726 loss_input: 82.54990339340473
step: 8000 epoch: 666 loss: 16.83008029609602 loss_input: 82.57013561171064
step: 9000 epoch: 666 loss: 16.840006257465 loss_input: 82.60469128918402
step: 10000 epoch: 666 loss: 16.829085223901963 loss_input: 82.58748619766929
step: 11000 epoch: 666 loss: 16.81160437370493 loss_input: 82.51499692765857
step: 12000 epoch: 666 loss: 16.80305652292199 loss_input: 82.38558022895064
step: 13000 epoch: 666 loss: 16.80404182898339 loss_input: 82.32500074269377
step: 14000 epoch: 666 loss: 16.814135262663214 loss_input: 82.28007016571222
step: 15000 epoch: 666 loss: 16.80646757839219 loss_input: 82.30131615900086
Save loss: 16.80918381150067 Name: 666_train_model.pth
step: 0 epoch: 667 loss: 13.122413635253906 loss_input: 87.9012451171875
step: 1000 epoch: 667 loss: 16.64900673257483 loss_input: 82.31132557627919
step: 2000 epoch: 667 loss: 16.666151669191038 loss_input: 81.64463126713011
step: 3000 epoch: 667 loss: 16.660991486768967 loss_input: 82.2022567388019
step: 4000 epoch: 667 loss: 16.67466418321834 loss_input: 82.0859317488743
step: 5000 epoch: 667 loss: 16.69907122925505 loss_input: 82.11930480114914
step: 6000 epoch: 667 loss: 16.756548130041438 loss_input: 82.25318237162296
step: 7000 epoch: 667 loss: 16.772663855617377 loss_input: 82.17923318432189
step: 8000 epoch: 667 loss: 16.76917918132791 loss_input: 82.123055662964
step: 9000 epoch: 667 loss: 16.785782986542287 loss_input: 81.90064597450326
step: 10000 epoch: 667 loss: 16.78574589118637 loss_input: 81.9724708010156
step: 11000 epoch: 667 loss: 16.794802053940643 loss_input: 82.02289225593306
step: 12000 epoch: 667 loss: 16.801627910829527 loss_input: 82.03479430099576
step: 13000 epoch: 667 loss: 16.81325979715457 loss_input: 82.09577415976266
step: 14000 epoch: 667 loss: 16.808966497124693 loss_input: 82.12505599575616
step: 15000 epoch: 667 loss: 16.80529830223894 loss_input: 82.17126143413356
Save loss: 16.802565212219953 Name: 667_train_model.pth
step: 0 epoch: 668 loss: 21.27387237548828 loss_input: 92.54193115234375
step: 1000 epoch: 668 loss: 16.78108709937447 loss_input: 83.62930264315763
step: 2000 epoch: 668 loss: 16.630906474882217 loss_input: 82.47772351007113
step: 3000 epoch: 668 loss: 16.708824253527176 loss_input: 82.5936307567074
step: 4000 epoch: 668 loss: 16.704451939726557 loss_input: 82.48971221244982
step: 5000 epoch: 668 loss: 16.75930188618 loss_input: 82.46384060590226
step: 6000 epoch: 668 loss: 16.826301879077093 loss_input: 82.56719838482323
step: 7000 epoch: 668 loss: 16.82596430692004 loss_input: 82.50660619911442
step: 8000 epoch: 668 loss: 16.82681743128123 loss_input: 82.5395109607881
step: 9000 epoch: 668 loss: 16.82642824090119 loss_input: 82.45107273101382
step: 10000 epoch: 668 loss: 16.81701662649859 loss_input: 82.33014874782053
step: 11000 epoch: 668 loss: 16.813587792168292 loss_input: 82.38694800494876
step: 12000 epoch: 668 loss: 16.797878610105396 loss_input: 82.24824488197761
step: 13000 epoch: 668 loss: 16.794275229421544 loss_input: 82.26911514900746
step: 14000 epoch: 668 loss: 16.796904392833667 loss_input: 82.2383637810748
step: 15000 epoch: 668 loss: 16.79928466960769 loss_input: 82.18949217675853
Save loss: 16.81030530564487 Name: 668_train_model.pth
step: 0 epoch: 669 loss: 15.778646469116211 loss_input: 64.47100830078125
step: 1000 epoch: 669 loss: 16.790547577651232 loss_input: 82.5800247116165
step: 2000 epoch: 669 loss: 16.69575402499556 loss_input: 82.36211102822612
step: 3000 epoch: 669 loss: 16.764803559412282 loss_input: 82.40022800017182
step: 4000 epoch: 669 loss: 16.801715461709026 loss_input: 82.46966813594453
step: 5000 epoch: 669 loss: 16.804378545658512 loss_input: 82.49785724007542
step: 6000 epoch: 669 loss: 16.775210770979026 loss_input: 82.20544166100898
step: 7000 epoch: 669 loss: 16.77529817399868 loss_input: 82.24334967005271
step: 8000 epoch: 669 loss: 16.775758600282664 loss_input: 82.25102671949942
step: 9000 epoch: 669 loss: 16.776245065297488 loss_input: 82.32857242614318
step: 10000 epoch: 669 loss: 16.7734579053739 loss_input: 82.28011193021322
step: 11000 epoch: 669 loss: 16.78577353754712 loss_input: 82.2433508852267
step: 12000 epoch: 669 loss: 16.784902617848125 loss_input: 82.27910153808797
step: 13000 epoch: 669 loss: 16.78383716738469 loss_input: 82.2036488526198
step: 14000 epoch: 669 loss: 16.790442568363016 loss_input: 82.22277967606806
step: 15000 epoch: 669 loss: 16.799630576138114 loss_input: 82.22277359650633
Save loss: 16.804386202543974 Name: 669_train_model.pth
step: 0 epoch: 670 loss: 19.258548736572266 loss_input: 70.549072265625
step: 1000 epoch: 670 loss: 16.771067370901573 loss_input: 81.96757264951844
step: 2000 epoch: 670 loss: 16.845335174595814 loss_input: 82.14375487427185
step: 3000 epoch: 670 loss: 16.845948516667107 loss_input: 82.29220171238494
step: 4000 epoch: 670 loss: 16.82287336802131 loss_input: 82.38244737216574
step: 5000 epoch: 670 loss: 16.815350091783934 loss_input: 82.4423361055328
step: 6000 epoch: 670 loss: 16.79152807321693 loss_input: 82.34218979453945
step: 7000 epoch: 670 loss: 16.796455303169527 loss_input: 82.49100226195365
step: 8000 epoch: 670 loss: 16.81665803578299 loss_input: 82.54605335587934
step: 9000 epoch: 670 loss: 16.80188774797681 loss_input: 82.39961793117821
step: 10000 epoch: 670 loss: 16.81055200680436 loss_input: 82.46728142127193
step: 11000 epoch: 670 loss: 16.786019606282956 loss_input: 82.29932057845593
step: 12000 epoch: 670 loss: 16.78673402033948 loss_input: 82.23480355315442
step: 13000 epoch: 670 loss: 16.79078357498991 loss_input: 82.28618231759366
step: 14000 epoch: 670 loss: 16.79732308158891 loss_input: 82.28891797035423
step: 15000 epoch: 670 loss: 16.80419425903642 loss_input: 82.26927969206349
Save loss: 16.801507788702846 Name: 670_train_model.pth
step: 0 epoch: 671 loss: 14.444708824157715 loss_input: 86.58026123046875
step: 1000 epoch: 671 loss: 16.669188434189255 loss_input: 80.82355004614526
step: 2000 epoch: 671 loss: 16.765863702393723 loss_input: 81.54371559947565
step: 3000 epoch: 671 loss: 16.770524485672922 loss_input: 81.9706855730707
step: 4000 epoch: 671 loss: 16.74399229342626 loss_input: 81.92664895299613
step: 5000 epoch: 671 loss: 16.77787140054098 loss_input: 82.2130101689623
step: 6000 epoch: 671 loss: 16.761015989565486 loss_input: 82.02495499814158
step: 7000 epoch: 671 loss: 16.790575647912625 loss_input: 82.11159588285658
step: 8000 epoch: 671 loss: 16.81238833574515 loss_input: 82.11811189922061
step: 9000 epoch: 671 loss: 16.80714006249659 loss_input: 82.12337036607478
step: 10000 epoch: 671 loss: 16.80760345169573 loss_input: 82.12385744457781
step: 11000 epoch: 671 loss: 16.806475882681486 loss_input: 82.14519181783368
step: 12000 epoch: 671 loss: 16.788628830471474 loss_input: 82.11520112886915
step: 13000 epoch: 671 loss: 16.807185208923073 loss_input: 82.24838199326464
step: 14000 epoch: 671 loss: 16.81046641319958 loss_input: 82.21813436491422
step: 15000 epoch: 671 loss: 16.796224184715225 loss_input: 82.16989389021201
Save loss: 16.803642444863915 Name: 671_train_model.pth
step: 0 epoch: 672 loss: 12.438260078430176 loss_input: 71.08758544921875
step: 1000 epoch: 672 loss: 16.647344687840082 loss_input: 82.2332736233493
step: 2000 epoch: 672 loss: 16.71439766562146 loss_input: 81.91396922388415
step: 3000 epoch: 672 loss: 16.707137528914924 loss_input: 81.99444964471478
step: 4000 epoch: 672 loss: 16.720319364286013 loss_input: 82.08593559312808
step: 5000 epoch: 672 loss: 16.736873747324662 loss_input: 82.19698444344334
step: 6000 epoch: 672 loss: 16.783966119677558 loss_input: 82.11817949165048
step: 7000 epoch: 672 loss: 16.8220051771708 loss_input: 82.2313728653998
step: 8000 epoch: 672 loss: 16.838725023099204 loss_input: 82.38946958870132
step: 9000 epoch: 672 loss: 16.81436534836986 loss_input: 82.35578171028956
step: 10000 epoch: 672 loss: 16.7943959853349 loss_input: 82.32985465491537
step: 11000 epoch: 672 loss: 16.787601745168725 loss_input: 82.19841993900768
step: 12000 epoch: 672 loss: 16.798184880275805 loss_input: 82.16576308256865
step: 13000 epoch: 672 loss: 16.812044139770663 loss_input: 82.2008090489058
step: 14000 epoch: 672 loss: 16.805456124802213 loss_input: 82.19963092584625
step: 15000 epoch: 672 loss: 16.806029107219242 loss_input: 82.15208628832296
Save loss: 16.79929977954924 Name: 672_train_model.pth
step: 0 epoch: 673 loss: 20.264135360717773 loss_input: 151.34039306640625
step: 1000 epoch: 673 loss: 16.906040857602786 loss_input: 82.52613572950487
step: 2000 epoch: 673 loss: 16.80819950027504 loss_input: 82.06307579242605
step: 3000 epoch: 673 loss: 16.87366169716906 loss_input: 82.25411130046496
step: 4000 epoch: 673 loss: 16.83365712395849 loss_input: 82.25220486260271
step: 5000 epoch: 673 loss: 16.764551846081435 loss_input: 82.10714500166372
step: 6000 epoch: 673 loss: 16.80930655790436 loss_input: 82.22865606383311
step: 7000 epoch: 673 loss: 16.794155077259298 loss_input: 82.19449307186026
step: 8000 epoch: 673 loss: 16.78299701102092 loss_input: 82.06462276627043
step: 9000 epoch: 673 loss: 16.783897143339477 loss_input: 82.09920550955493
step: 10000 epoch: 673 loss: 16.761655401508875 loss_input: 82.01043187307246
step: 11000 epoch: 673 loss: 16.767669860389752 loss_input: 82.0630587981881
step: 12000 epoch: 673 loss: 16.7657668222021 loss_input: 82.02013989780637
step: 13000 epoch: 673 loss: 16.760843212095995 loss_input: 82.1174319269987
step: 14000 epoch: 673 loss: 16.785611693037126 loss_input: 82.10621208766896
step: 15000 epoch: 673 loss: 16.789262932798447 loss_input: 82.1490541987956
Save loss: 16.801436099678277 Name: 673_train_model.pth
step: 0 epoch: 674 loss: 13.283821105957031 loss_input: 68.10748291015625
step: 1000 epoch: 674 loss: 16.51058680051333 loss_input: 81.2648194700807
step: 2000 epoch: 674 loss: 16.629736018145103 loss_input: 81.74529699871744
step: 3000 epoch: 674 loss: 16.660930584764845 loss_input: 81.93836433591147
step: 4000 epoch: 674 loss: 16.732595870268998 loss_input: 82.13630381467134
step: 5000 epoch: 674 loss: 16.738351299008997 loss_input: 82.07154212468053
step: 6000 epoch: 674 loss: 16.72584333008199 loss_input: 81.89815210648486
step: 7000 epoch: 674 loss: 16.736410681102434 loss_input: 81.94201138816106
step: 8000 epoch: 674 loss: 16.71320318838996 loss_input: 82.00424031468722
step: 9000 epoch: 674 loss: 16.719571128790545 loss_input: 82.03268348194602
step: 10000 epoch: 674 loss: 16.745321697264288 loss_input: 82.05634264454415
step: 11000 epoch: 674 loss: 16.756302360209233 loss_input: 82.28797689423476
step: 12000 epoch: 674 loss: 16.771950690252226 loss_input: 82.34996370541712
step: 13000 epoch: 674 loss: 16.770522543415915 loss_input: 82.31889584214676
step: 14000 epoch: 674 loss: 16.761415548949877 loss_input: 82.17406427930929
step: 15000 epoch: 674 loss: 16.787458625541323 loss_input: 82.2164629291641
Save loss: 16.79919033794105 Name: 674_train_model.pth
step: 0 epoch: 675 loss: 19.682329177856445 loss_input: 119.453369140625
step: 1000 epoch: 675 loss: 16.98989784753287 loss_input: 83.6094810950768
step: 2000 epoch: 675 loss: 16.9144522656446 loss_input: 82.92164946579445
step: 3000 epoch: 675 loss: 16.884921102355378 loss_input: 82.81338689105584
step: 4000 epoch: 675 loss: 16.874142195575985 loss_input: 82.73435085137407
step: 5000 epoch: 675 loss: 16.80604846666775 loss_input: 82.48883408132826
step: 6000 epoch: 675 loss: 16.809090721907168 loss_input: 82.39097191687128
step: 7000 epoch: 675 loss: 16.7831054074375 loss_input: 82.16050133707182
step: 8000 epoch: 675 loss: 16.79763222032749 loss_input: 82.1631965575226
step: 9000 epoch: 675 loss: 16.7980929880245 loss_input: 82.27810364142482
step: 10000 epoch: 675 loss: 16.79992488796336 loss_input: 82.257727724542
step: 11000 epoch: 675 loss: 16.813257231819403 loss_input: 82.33622148741095
step: 12000 epoch: 675 loss: 16.819095946979388 loss_input: 82.31715341572284
step: 13000 epoch: 675 loss: 16.787612345996028 loss_input: 82.20407700008653
step: 14000 epoch: 675 loss: 16.805493966501686 loss_input: 82.20860428391555
step: 15000 epoch: 675 loss: 16.800743505451077 loss_input: 82.1951164030471
Save loss: 16.80318685734272 Name: 675_train_model.pth
step: 0 epoch: 676 loss: 16.673786163330078 loss_input: 86.8992919921875
step: 1000 epoch: 676 loss: 16.467413412107454 loss_input: 80.23085069227648
step: 2000 epoch: 676 loss: 16.525375480594665 loss_input: 80.46211010095539
step: 3000 epoch: 676 loss: 16.665760252563924 loss_input: 81.0764660070992
step: 4000 epoch: 676 loss: 16.71744324320169 loss_input: 81.42966478534176
step: 5000 epoch: 676 loss: 16.76355650872618 loss_input: 81.73415177804283
step: 6000 epoch: 676 loss: 16.782002405253397 loss_input: 81.8579051216887
step: 7000 epoch: 676 loss: 16.794654994875238 loss_input: 82.02289363238287
step: 8000 epoch: 676 loss: 16.809434068365373 loss_input: 81.93173965742075
step: 9000 epoch: 676 loss: 16.793254679857977 loss_input: 81.87753806163464
step: 10000 epoch: 676 loss: 16.824701507929955 loss_input: 82.07279341645913
step: 11000 epoch: 676 loss: 16.822627708810252 loss_input: 82.15858152521469
step: 12000 epoch: 676 loss: 16.82060110989178 loss_input: 82.14573897406336
step: 13000 epoch: 676 loss: 16.79523008888056 loss_input: 82.05055515091838
step: 14000 epoch: 676 loss: 16.792274719595476 loss_input: 82.02484190582369
step: 15000 epoch: 676 loss: 16.80533646688518 loss_input: 82.11823215284997
Save loss: 16.81272394013405 Name: 676_train_model.pth
step: 0 epoch: 677 loss: 10.856405258178711 loss_input: 70.8243408203125
step: 1000 epoch: 677 loss: 16.586201702559983 loss_input: 82.77325506524726
step: 2000 epoch: 677 loss: 16.88008505365123 loss_input: 82.85740281104565
step: 3000 epoch: 677 loss: 16.861985123741434 loss_input: 82.6371955808025
step: 4000 epoch: 677 loss: 16.777058138784184 loss_input: 82.33633582266054
step: 5000 epoch: 677 loss: 16.752010715172258 loss_input: 82.30884665352086
step: 6000 epoch: 677 loss: 16.78545923782098 loss_input: 82.46728046749695
step: 7000 epoch: 677 loss: 16.78155587046781 loss_input: 82.37662225708827
step: 8000 epoch: 677 loss: 16.800805108396133 loss_input: 82.4306929742913
step: 9000 epoch: 677 loss: 16.797671098627525 loss_input: 82.36924620408931
step: 10000 epoch: 677 loss: 16.81710481677052 loss_input: 82.34103944952184
step: 11000 epoch: 677 loss: 16.79723574449643 loss_input: 82.21179876092586
step: 12000 epoch: 677 loss: 16.805262621914622 loss_input: 82.1888862624485
step: 13000 epoch: 677 loss: 16.813576420915375 loss_input: 82.35458501952674
step: 14000 epoch: 677 loss: 16.814123416132574 loss_input: 82.32545444054499
step: 15000 epoch: 677 loss: 16.807626155191848 loss_input: 82.27253664046094
Save loss: 16.80069587060809 Name: 677_train_model.pth
step: 0 epoch: 678 loss: 13.151395797729492 loss_input: 87.4034423828125
step: 1000 epoch: 678 loss: 16.934532841483314 loss_input: 82.83701363333931
step: 2000 epoch: 678 loss: 16.804909608889556 loss_input: 82.71437041489021
step: 3000 epoch: 678 loss: 16.758201176307473 loss_input: 82.86535691309277
step: 4000 epoch: 678 loss: 16.77126324829296 loss_input: 82.78388352740916
step: 5000 epoch: 678 loss: 16.703866183149938 loss_input: 82.41199750772502
step: 6000 epoch: 678 loss: 16.744374588516788 loss_input: 82.58527606035388
step: 7000 epoch: 678 loss: 16.74753913040281 loss_input: 82.3812708762727
step: 8000 epoch: 678 loss: 16.768819237065276 loss_input: 82.49375464877193
step: 9000 epoch: 678 loss: 16.77280632759012 loss_input: 82.38028706445812
step: 10000 epoch: 678 loss: 16.772841921807192 loss_input: 82.28907411382885
step: 11000 epoch: 678 loss: 16.79235810807007 loss_input: 82.33013622518692
step: 12000 epoch: 678 loss: 16.797435207273967 loss_input: 82.27554759942693
step: 13000 epoch: 678 loss: 16.801778486843943 loss_input: 82.27599053565892
step: 14000 epoch: 678 loss: 16.806525211931596 loss_input: 82.26010387511383
step: 15000 epoch: 678 loss: 16.79065007521672 loss_input: 82.24074189731752
Save loss: 16.796622444584965 Name: 678_train_model.pth
step: 0 epoch: 679 loss: 24.384855270385742 loss_input: 66.0657958984375
step: 1000 epoch: 679 loss: 16.69873160534686 loss_input: 81.87140898652129
step: 2000 epoch: 679 loss: 16.599535495027908 loss_input: 81.4387030422777
step: 3000 epoch: 679 loss: 16.618279465195815 loss_input: 81.39918184010278
step: 4000 epoch: 679 loss: 16.671176347396457 loss_input: 81.37556155089645
step: 5000 epoch: 679 loss: 16.700116377071343 loss_input: 81.3916932067688
step: 6000 epoch: 679 loss: 16.69044177112252 loss_input: 81.60689597184648
step: 7000 epoch: 679 loss: 16.69374672502573 loss_input: 81.59728040560333
step: 8000 epoch: 679 loss: 16.71856790121727 loss_input: 81.74587068675741
step: 9000 epoch: 679 loss: 16.73697332003 loss_input: 81.9836214962118
step: 10000 epoch: 679 loss: 16.766564357591836 loss_input: 82.18765477578219
step: 11000 epoch: 679 loss: 16.761859234458 loss_input: 82.1576098173426
step: 12000 epoch: 679 loss: 16.759781772638796 loss_input: 82.02473098148397
step: 13000 epoch: 679 loss: 16.76118129186012 loss_input: 81.99009828645994
step: 14000 epoch: 679 loss: 16.78417827723563 loss_input: 82.08918954780992
step: 15000 epoch: 679 loss: 16.787754606925663 loss_input: 82.19575053232128
Save loss: 16.797615578368305 Name: 679_train_model.pth
step: 0 epoch: 680 loss: 13.149652481079102 loss_input: 48.594482421875
step: 1000 epoch: 680 loss: 16.704325913192033 loss_input: 82.282981422874
step: 2000 epoch: 680 loss: 16.716959689034038 loss_input: 81.6283662673058
step: 3000 epoch: 680 loss: 16.763053843197287 loss_input: 81.6621493193675
step: 4000 epoch: 680 loss: 16.74717736846058 loss_input: 81.53020047295306
step: 5000 epoch: 680 loss: 16.767672167280107 loss_input: 81.67290714220437
step: 6000 epoch: 680 loss: 16.783415972679936 loss_input: 81.9402426459197
step: 7000 epoch: 680 loss: 16.763133788616244 loss_input: 81.86551195811313
step: 8000 epoch: 680 loss: 16.751313031218885 loss_input: 81.92335621438434
step: 9000 epoch: 680 loss: 16.74016773805342 loss_input: 81.94083379027764
step: 10000 epoch: 680 loss: 16.7463093859567 loss_input: 81.89060328586282
step: 11000 epoch: 680 loss: 16.753104277604624 loss_input: 81.92023161135742
step: 12000 epoch: 680 loss: 16.765536331611916 loss_input: 82.07036198521543
step: 13000 epoch: 680 loss: 16.77048993057475 loss_input: 82.01673351505262
step: 14000 epoch: 680 loss: 16.781246249466808 loss_input: 82.1135963781605
step: 15000 epoch: 680 loss: 16.786911641380865 loss_input: 82.17731768728439
Save loss: 16.796094365745784 Name: 680_train_model.pth
step: 0 epoch: 681 loss: 18.267004013061523 loss_input: 103.05706787109375
step: 1000 epoch: 681 loss: 16.556956293580534 loss_input: 82.04973846953827
step: 2000 epoch: 681 loss: 16.709715206345457 loss_input: 82.01458517567389
step: 3000 epoch: 681 loss: 16.67784738659819 loss_input: 82.10469892604158
step: 4000 epoch: 681 loss: 16.75419132693414 loss_input: 82.6009640778282
step: 5000 epoch: 681 loss: 16.719475434604014 loss_input: 82.31605012005411
step: 6000 epoch: 681 loss: 16.693718731790717 loss_input: 82.18724182362836
step: 7000 epoch: 681 loss: 16.6989066244585 loss_input: 82.23630316518678
step: 8000 epoch: 681 loss: 16.721192140457646 loss_input: 82.24758051788936
step: 9000 epoch: 681 loss: 16.735006615845656 loss_input: 82.35302238688973
step: 10000 epoch: 681 loss: 16.740061615123928 loss_input: 82.24551672640354
step: 11000 epoch: 681 loss: 16.74750297018186 loss_input: 82.20571024748816
step: 12000 epoch: 681 loss: 16.770500172059663 loss_input: 82.30088576399558
step: 13000 epoch: 681 loss: 16.78528729230383 loss_input: 82.30141495745069
step: 14000 epoch: 681 loss: 16.798263921881393 loss_input: 82.29337346701101
step: 15000 epoch: 681 loss: 16.794141742708078 loss_input: 82.26388549194377
Save loss: 16.79094631293416 Name: 681_train_model.pth
step: 0 epoch: 682 loss: 18.611968994140625 loss_input: 88.208740234375
step: 1000 epoch: 682 loss: 16.750828647232435 loss_input: 80.9308330829327
step: 2000 epoch: 682 loss: 16.663796906945468 loss_input: 81.08458978518553
step: 3000 epoch: 682 loss: 16.666622899365958 loss_input: 81.43420570828604
step: 4000 epoch: 682 loss: 16.666216894198406 loss_input: 81.81841057069717
step: 5000 epoch: 682 loss: 16.730217349884438 loss_input: 81.94652166978713
step: 6000 epoch: 682 loss: 16.757851714353684 loss_input: 82.19679372895858
step: 7000 epoch: 682 loss: 16.752523724036017 loss_input: 82.22107792322916
step: 8000 epoch: 682 loss: 16.768124490987507 loss_input: 82.11551915995852
step: 9000 epoch: 682 loss: 16.78094970881972 loss_input: 82.16660163302168
step: 10000 epoch: 682 loss: 16.7585679692586 loss_input: 82.23782429496308
step: 11000 epoch: 682 loss: 16.78506122156876 loss_input: 82.29593489991677
step: 12000 epoch: 682 loss: 16.770993841358965 loss_input: 82.15291407600799
step: 13000 epoch: 682 loss: 16.777159801364615 loss_input: 82.17734290214605
step: 14000 epoch: 682 loss: 16.774090870168667 loss_input: 82.151707704472
step: 15000 epoch: 682 loss: 16.781241575770913 loss_input: 82.22314662665067
Save loss: 16.795373718768356 Name: 682_train_model.pth
step: 0 epoch: 683 loss: 16.726985931396484 loss_input: 81.31298828125
step: 1000 epoch: 683 loss: 16.872312262341694 loss_input: 83.0955640428907
step: 2000 epoch: 683 loss: 16.811077370993917 loss_input: 82.97196915458287
step: 3000 epoch: 683 loss: 16.818403382573038 loss_input: 82.86302854696221
step: 4000 epoch: 683 loss: 16.830601478868413 loss_input: 82.85648608904903
step: 5000 epoch: 683 loss: 16.807156958929944 loss_input: 82.50352971397908
step: 6000 epoch: 683 loss: 16.80952020029965 loss_input: 82.29674928853004
step: 7000 epoch: 683 loss: 16.79043246292928 loss_input: 82.22136622084122
step: 8000 epoch: 683 loss: 16.799123620617436 loss_input: 82.20827288556704
step: 9000 epoch: 683 loss: 16.80196545217027 loss_input: 82.24218583189108
step: 10000 epoch: 683 loss: 16.81045659000117 loss_input: 82.30130656978247
step: 11000 epoch: 683 loss: 16.80184601961553 loss_input: 82.2692238105751
step: 12000 epoch: 683 loss: 16.79797861542506 loss_input: 82.28070689848926
step: 13000 epoch: 683 loss: 16.791529263709933 loss_input: 82.25507680016952
step: 14000 epoch: 683 loss: 16.789560234416392 loss_input: 82.28921876499614
step: 15000 epoch: 683 loss: 16.79948155770786 loss_input: 82.33431246592023
Save loss: 16.790206983417274 Name: 683_train_model.pth
step: 0 epoch: 684 loss: 18.36186981201172 loss_input: 77.77947998046875
step: 1000 epoch: 684 loss: 16.74348722471224 loss_input: 81.97273545546251
step: 2000 epoch: 684 loss: 16.782893092676378 loss_input: 82.49399583045391
step: 3000 epoch: 684 loss: 16.753386091208785 loss_input: 82.23294017554045
step: 4000 epoch: 684 loss: 16.77613349772489 loss_input: 82.22372168798621
step: 5000 epoch: 684 loss: 16.784375520735544 loss_input: 82.20257773396493
step: 6000 epoch: 684 loss: 16.78256903567169 loss_input: 82.04824518382678
step: 7000 epoch: 684 loss: 16.813258751922465 loss_input: 82.12419456435074
step: 8000 epoch: 684 loss: 16.791639418233082 loss_input: 82.16735877985955
step: 9000 epoch: 684 loss: 16.76705887857748 loss_input: 81.98721606073717
step: 10000 epoch: 684 loss: 16.77434896988912 loss_input: 82.0454595714745
step: 11000 epoch: 684 loss: 16.770631018210278 loss_input: 82.09922364013606
step: 12000 epoch: 684 loss: 16.771416570174576 loss_input: 82.0646108216161
step: 13000 epoch: 684 loss: 16.782901125315124 loss_input: 82.16375139487468
step: 14000 epoch: 684 loss: 16.774634311424137 loss_input: 82.12186848365462
step: 15000 epoch: 684 loss: 16.789701641817807 loss_input: 82.25472096212467
Save loss: 16.791804384961726 Name: 684_train_model.pth
step: 0 epoch: 685 loss: 20.558975219726562 loss_input: 57.540771484375
step: 1000 epoch: 685 loss: 16.738893126393414 loss_input: 81.79246668858485
step: 2000 epoch: 685 loss: 16.763037112043953 loss_input: 82.10221921509948
step: 3000 epoch: 685 loss: 16.784166668304323 loss_input: 82.14014168128853
step: 4000 epoch: 685 loss: 16.800761002953188 loss_input: 82.5498684075319
step: 5000 epoch: 685 loss: 16.830974505820578 loss_input: 82.67665939468357
step: 6000 epoch: 685 loss: 16.796110515613552 loss_input: 82.46533688273634
step: 7000 epoch: 685 loss: 16.812693866384556 loss_input: 82.35718011440609
step: 8000 epoch: 685 loss: 16.80680438077922 loss_input: 82.2044551331108
step: 9000 epoch: 685 loss: 16.826613374583363 loss_input: 82.39229764536796
step: 10000 epoch: 685 loss: 16.827072173663943 loss_input: 82.47671646409577
step: 11000 epoch: 685 loss: 16.825039668491065 loss_input: 82.34921972869299
step: 12000 epoch: 685 loss: 16.826701655466152 loss_input: 82.26377953713005
step: 13000 epoch: 685 loss: 16.816569886568114 loss_input: 82.24799369399469
step: 14000 epoch: 685 loss: 16.820752963058677 loss_input: 82.30071498799192
step: 15000 epoch: 685 loss: 16.819228978707276 loss_input: 82.27853443396742
Save loss: 16.795701940834523 Name: 685_train_model.pth
step: 0 epoch: 686 loss: 9.772035598754883 loss_input: 42.45489501953125
step: 1000 epoch: 686 loss: 16.88254294028649 loss_input: 82.83061183201563
step: 2000 epoch: 686 loss: 16.713151664152438 loss_input: 82.63891162328991
step: 3000 epoch: 686 loss: 16.752894061837583 loss_input: 82.3995011713218
step: 4000 epoch: 686 loss: 16.802598553638223 loss_input: 82.5583185197353
step: 5000 epoch: 686 loss: 16.765548904760674 loss_input: 82.3077306779855
step: 6000 epoch: 686 loss: 16.79630982798351 loss_input: 82.49010541903498
step: 7000 epoch: 686 loss: 16.808152137492627 loss_input: 82.37004137522764
step: 8000 epoch: 686 loss: 16.809614787741225 loss_input: 82.35852796842778
step: 9000 epoch: 686 loss: 16.804020050459073 loss_input: 82.30296298051302
step: 10000 epoch: 686 loss: 16.795100087702792 loss_input: 82.21393716341257
step: 11000 epoch: 686 loss: 16.776831606650024 loss_input: 82.14979393852157
step: 12000 epoch: 686 loss: 16.77874790292096 loss_input: 82.26027390376974
step: 13000 epoch: 686 loss: 16.77145662051734 loss_input: 82.20819898807363
step: 14000 epoch: 686 loss: 16.77981925090716 loss_input: 82.25562973790795
step: 15000 epoch: 686 loss: 16.782187934653233 loss_input: 82.14875415776318
Save loss: 16.79228166602552 Name: 686_train_model.pth
step: 0 epoch: 687 loss: 20.514671325683594 loss_input: 78.2689208984375
step: 1000 epoch: 687 loss: 16.914279688607444 loss_input: 81.2718665611732
step: 2000 epoch: 687 loss: 16.84627503147726 loss_input: 81.68478950222155
step: 3000 epoch: 687 loss: 16.883544489210028 loss_input: 81.93873473653314
step: 4000 epoch: 687 loss: 16.873677926610572 loss_input: 82.37801932573497
step: 5000 epoch: 687 loss: 16.79473183856347 loss_input: 82.22566164257383
step: 6000 epoch: 687 loss: 16.784188970687527 loss_input: 82.31250704838581
step: 7000 epoch: 687 loss: 16.7623458037358 loss_input: 82.12416098237361
step: 8000 epoch: 687 loss: 16.762084843769415 loss_input: 82.1097216821286
step: 9000 epoch: 687 loss: 16.783956272736376 loss_input: 82.3116672949425
step: 10000 epoch: 687 loss: 16.788563146697033 loss_input: 82.2317800629605
step: 11000 epoch: 687 loss: 16.79104879953332 loss_input: 82.31134898713931
step: 12000 epoch: 687 loss: 16.783453861004848 loss_input: 82.30907989519754
step: 13000 epoch: 687 loss: 16.786790911999677 loss_input: 82.2791971559204
step: 14000 epoch: 687 loss: 16.7872228846534 loss_input: 82.28048083322592
step: 15000 epoch: 687 loss: 16.796130282729 loss_input: 82.26879474717961
Save loss: 16.793585687324406 Name: 687_train_model.pth
step: 0 epoch: 688 loss: 29.55195426940918 loss_input: 74.8775634765625
step: 1000 epoch: 688 loss: 16.87935009631482 loss_input: 82.30902172778393
step: 2000 epoch: 688 loss: 16.870477203843834 loss_input: 82.17053661889759
step: 3000 epoch: 688 loss: 16.820755201115364 loss_input: 81.59571363988697
step: 4000 epoch: 688 loss: 16.76450452778346 loss_input: 81.81142637009205
step: 5000 epoch: 688 loss: 16.803386614718836 loss_input: 82.22805607335565
step: 6000 epoch: 688 loss: 16.780166911236584 loss_input: 82.13014893229058
step: 7000 epoch: 688 loss: 16.77613456472978 loss_input: 82.07002289467854
step: 8000 epoch: 688 loss: 16.81730119485644 loss_input: 82.20371760780878
step: 9000 epoch: 688 loss: 16.82044381277387 loss_input: 82.23876883281415
step: 10000 epoch: 688 loss: 16.819643974351877 loss_input: 82.1304739460136
step: 11000 epoch: 688 loss: 16.811220237378933 loss_input: 82.11190338453784
step: 12000 epoch: 688 loss: 16.80296848968768 loss_input: 82.15492734105653
step: 13000 epoch: 688 loss: 16.79196698529364 loss_input: 82.16650782158884
step: 14000 epoch: 688 loss: 16.792371886277675 loss_input: 82.29042222692101
step: 15000 epoch: 688 loss: 16.81574994215575 loss_input: 82.29303919614995
Save loss: 16.79659984087944 Name: 688_train_model.pth
step: 0 epoch: 689 loss: 14.443474769592285 loss_input: 82.0791015625
step: 1000 epoch: 689 loss: 16.47674698500962 loss_input: 80.87201098462084
step: 2000 epoch: 689 loss: 16.621464139994593 loss_input: 81.63142992102581
step: 3000 epoch: 689 loss: 16.57798983835451 loss_input: 81.52137768042164
step: 4000 epoch: 689 loss: 16.62384147532014 loss_input: 81.72005942606145
step: 5000 epoch: 689 loss: 16.635950835174 loss_input: 81.62678644690983
step: 6000 epoch: 689 loss: 16.670633252790186 loss_input: 81.81679569111529
step: 7000 epoch: 689 loss: 16.726303927167248 loss_input: 82.02453324713377
step: 8000 epoch: 689 loss: 16.752619511335286 loss_input: 82.08317315895816
step: 9000 epoch: 689 loss: 16.755525608060626 loss_input: 82.07330296498088
step: 10000 epoch: 689 loss: 16.761182052804738 loss_input: 82.11918407768599
step: 11000 epoch: 689 loss: 16.742050746800345 loss_input: 82.0570570300161
step: 12000 epoch: 689 loss: 16.754126285356378 loss_input: 82.05935303019182
step: 13000 epoch: 689 loss: 16.76559327316416 loss_input: 82.02104750956657
step: 14000 epoch: 689 loss: 16.778533271240956 loss_input: 82.0958817054711
step: 15000 epoch: 689 loss: 16.784150035053433 loss_input: 82.17370880922391
Save loss: 16.793851302936673 Name: 689_train_model.pth
step: 0 epoch: 690 loss: 11.307581901550293 loss_input: 63.18499755859375
step: 1000 epoch: 690 loss: 16.818033490385805 loss_input: 82.85945005921812
step: 2000 epoch: 690 loss: 16.802772479793656 loss_input: 82.36762484236398
step: 3000 epoch: 690 loss: 16.747669178023017 loss_input: 82.37084607051555
step: 4000 epoch: 690 loss: 16.733454396086493 loss_input: 82.58572968147124
step: 5000 epoch: 690 loss: 16.71667713872005 loss_input: 82.4993089760954
step: 6000 epoch: 690 loss: 16.75224857659285 loss_input: 82.35359016701969
step: 7000 epoch: 690 loss: 16.73136846100734 loss_input: 82.11412102609783
step: 8000 epoch: 690 loss: 16.7444507356078 loss_input: 82.28707358998338
step: 9000 epoch: 690 loss: 16.760037626031902 loss_input: 82.25468408376717
step: 10000 epoch: 690 loss: 16.754738401882697 loss_input: 82.23654322800142
step: 11000 epoch: 690 loss: 16.759528519727784 loss_input: 82.25101916116645
step: 12000 epoch: 690 loss: 16.78456870489007 loss_input: 82.28117571623264
step: 13000 epoch: 690 loss: 16.79634062413683 loss_input: 82.36612088940123
step: 14000 epoch: 690 loss: 16.79338285839734 loss_input: 82.33017685975136
step: 15000 epoch: 690 loss: 16.787407788934093 loss_input: 82.27233761808458
Save loss: 16.787899307772516 Name: 690_train_model.pth
step: 0 epoch: 691 loss: 10.789712905883789 loss_input: 67.801025390625
step: 1000 epoch: 691 loss: 16.76580473545429 loss_input: 82.75678228021978
step: 2000 epoch: 691 loss: 16.59751807148012 loss_input: 81.87255285931253
step: 3000 epoch: 691 loss: 16.63696744854313 loss_input: 81.61132875548645
step: 4000 epoch: 691 loss: 16.701339080553595 loss_input: 81.9234595037764
step: 5000 epoch: 691 loss: 16.75193149536711 loss_input: 82.04240475147158
step: 6000 epoch: 691 loss: 16.746308162279515 loss_input: 81.94424745127114
step: 7000 epoch: 691 loss: 16.76270165819387 loss_input: 81.94644236963079
step: 8000 epoch: 691 loss: 16.754511526533552 loss_input: 81.78082681223685
step: 9000 epoch: 691 loss: 16.76089908941337 loss_input: 81.87045952278089
step: 10000 epoch: 691 loss: 16.765889154983846 loss_input: 81.84695623455232
step: 11000 epoch: 691 loss: 16.758777033750626 loss_input: 81.97412146547579
step: 12000 epoch: 691 loss: 16.782006704909595 loss_input: 82.10049482977655
step: 13000 epoch: 691 loss: 16.7741079892152 loss_input: 82.13478738470688
step: 14000 epoch: 691 loss: 16.779156132992927 loss_input: 82.13202068644092
step: 15000 epoch: 691 loss: 16.785815901124042 loss_input: 82.14338068380077
Save loss: 16.790137112542986 Name: 691_train_model.pth
step: 0 epoch: 692 loss: 19.975963592529297 loss_input: 65.4588623046875
step: 1000 epoch: 692 loss: 16.70270874450257 loss_input: 83.1412904112489
step: 2000 epoch: 692 loss: 16.642741993628164 loss_input: 83.25090219782687
step: 3000 epoch: 692 loss: 16.766252318528128 loss_input: 82.9815591254738
step: 4000 epoch: 692 loss: 16.804480597901957 loss_input: 82.83125226994032
step: 5000 epoch: 692 loss: 16.76973538907903 loss_input: 82.6675130428016
step: 6000 epoch: 692 loss: 16.751975848980297 loss_input: 82.44086016791798
step: 7000 epoch: 692 loss: 16.755533748516918 loss_input: 82.3755951995697
step: 8000 epoch: 692 loss: 16.76765724489531 loss_input: 82.2502535617496
step: 9000 epoch: 692 loss: 16.771743724060354 loss_input: 82.34285540538899
step: 10000 epoch: 692 loss: 16.768203101722182 loss_input: 82.31324372445567
step: 11000 epoch: 692 loss: 16.78852390152857 loss_input: 82.35823567708333
step: 12000 epoch: 692 loss: 16.769431279446817 loss_input: 82.27539668732032
step: 13000 epoch: 692 loss: 16.779036117914906 loss_input: 82.29747418768268
step: 14000 epoch: 692 loss: 16.782352625323266 loss_input: 82.25521197338443
step: 15000 epoch: 692 loss: 16.789676796062082 loss_input: 82.34996064959601
Save loss: 16.78736098150909 Name: 692_train_model.pth
step: 0 epoch: 693 loss: 22.839168548583984 loss_input: 107.33099365234375
step: 1000 epoch: 693 loss: 16.761211589142516 loss_input: 81.98289210789211
step: 2000 epoch: 693 loss: 16.656404680159614 loss_input: 82.39378577312905
step: 3000 epoch: 693 loss: 16.684549039779046 loss_input: 82.09749277096437
step: 4000 epoch: 693 loss: 16.718217671081383 loss_input: 82.2868198050585
step: 5000 epoch: 693 loss: 16.765065096493032 loss_input: 82.42977994977177
step: 6000 epoch: 693 loss: 16.79518137254669 loss_input: 82.405779883854
step: 7000 epoch: 693 loss: 16.780771961315686 loss_input: 81.99585381148117
step: 8000 epoch: 693 loss: 16.779140588686236 loss_input: 82.04185767520026
step: 9000 epoch: 693 loss: 16.773810655563995 loss_input: 82.03359502888351
step: 10000 epoch: 693 loss: 16.758941167450562 loss_input: 82.08814784415114
step: 11000 epoch: 693 loss: 16.769231905103673 loss_input: 82.18282965930653
step: 12000 epoch: 693 loss: 16.788499361872127 loss_input: 82.20499110629127
step: 13000 epoch: 693 loss: 16.79175502319958 loss_input: 82.16874926387874
step: 14000 epoch: 693 loss: 16.788188144348577 loss_input: 82.24084015797142
step: 15000 epoch: 693 loss: 16.77906366245213 loss_input: 82.28653233420506
Save loss: 16.79138005837798 Name: 693_train_model.pth
step: 0 epoch: 694 loss: 21.3008975982666 loss_input: 87.64373779296875
step: 1000 epoch: 694 loss: 16.914624023151685 loss_input: 83.34646877780423
step: 2000 epoch: 694 loss: 16.859781429923693 loss_input: 83.03913463800326
step: 3000 epoch: 694 loss: 16.7602064084387 loss_input: 82.4961100409564
step: 4000 epoch: 694 loss: 16.712527395695098 loss_input: 82.32673399033054
step: 5000 epoch: 694 loss: 16.73951071993014 loss_input: 82.11853073721193
step: 6000 epoch: 694 loss: 16.758215306143306 loss_input: 82.17712435907492
step: 7000 epoch: 694 loss: 16.745330187682168 loss_input: 82.12117089613594
step: 8000 epoch: 694 loss: 16.77320665300615 loss_input: 82.25806731281392
step: 9000 epoch: 694 loss: 16.773902670620096 loss_input: 82.16979985215401
step: 10000 epoch: 694 loss: 16.771604259328097 loss_input: 82.15967675083077
step: 11000 epoch: 694 loss: 16.780598811090734 loss_input: 82.24640786939291
step: 12000 epoch: 694 loss: 16.774643201528416 loss_input: 82.14954013536796
step: 13000 epoch: 694 loss: 16.768599172544775 loss_input: 82.12151389308696
step: 14000 epoch: 694 loss: 16.770746132006227 loss_input: 82.18330657394449
step: 15000 epoch: 694 loss: 16.781746876652214 loss_input: 82.24727366401484
Save loss: 16.791303176611663 Name: 694_train_model.pth
step: 0 epoch: 695 loss: 19.54410171508789 loss_input: 88.45806884765625
step: 1000 epoch: 695 loss: 16.915968239486038 loss_input: 82.54234577678181
step: 2000 epoch: 695 loss: 16.761846155836725 loss_input: 82.34472259719749
step: 3000 epoch: 695 loss: 16.795447374971182 loss_input: 82.4226814328492
step: 4000 epoch: 695 loss: 16.747492800352425 loss_input: 81.9360995627558
step: 5000 epoch: 695 loss: 16.74017333231123 loss_input: 81.7403943039517
step: 6000 epoch: 695 loss: 16.742502042480837 loss_input: 81.7913808493669
step: 7000 epoch: 695 loss: 16.7328436955982 loss_input: 81.86792900609623
step: 8000 epoch: 695 loss: 16.742471466152658 loss_input: 82.06531677894512
step: 9000 epoch: 695 loss: 16.76053846381715 loss_input: 82.1023756081681
step: 10000 epoch: 695 loss: 16.785216911305618 loss_input: 82.21913861470298
step: 11000 epoch: 695 loss: 16.77882502701226 loss_input: 82.15765009688394
step: 12000 epoch: 695 loss: 16.792476601207685 loss_input: 82.24844451642376
step: 13000 epoch: 695 loss: 16.776632122714357 loss_input: 82.26274149847548
step: 14000 epoch: 695 loss: 16.771184047625137 loss_input: 82.28923520843657
step: 15000 epoch: 695 loss: 16.77883339052192 loss_input: 82.32438446567755
Save loss: 16.783557752877474 Name: 695_train_model.pth
step: 0 epoch: 696 loss: 19.161592483520508 loss_input: 127.76373291015625
step: 1000 epoch: 696 loss: 16.99726286801425 loss_input: 83.38132844938265
step: 2000 epoch: 696 loss: 16.858887481427324 loss_input: 83.03560582379708
step: 3000 epoch: 696 loss: 16.804357758524894 loss_input: 82.77392582192654
step: 4000 epoch: 696 loss: 16.7955616527544 loss_input: 82.68642861257342
step: 5000 epoch: 696 loss: 16.78001657192098 loss_input: 82.51393517683182
step: 6000 epoch: 696 loss: 16.783708174970425 loss_input: 82.39763550964162
step: 7000 epoch: 696 loss: 16.807733842431265 loss_input: 82.42391323944514
step: 8000 epoch: 696 loss: 16.786481338774408 loss_input: 82.38740703371552
step: 9000 epoch: 696 loss: 16.806842632816046 loss_input: 82.38090530902383
step: 10000 epoch: 696 loss: 16.7977170892959 loss_input: 82.34101276761972
step: 11000 epoch: 696 loss: 16.802517024314074 loss_input: 82.34589698832421
step: 12000 epoch: 696 loss: 16.811830604804495 loss_input: 82.23410269908335
step: 13000 epoch: 696 loss: 16.83054370829146 loss_input: 82.17938546963485
step: 14000 epoch: 696 loss: 16.822005686747346 loss_input: 82.23825113130172
step: 15000 epoch: 696 loss: 16.80686229656922 loss_input: 82.24234478525
Save loss: 16.791754682436586 Name: 696_train_model.pth
step: 0 epoch: 697 loss: 11.665687561035156 loss_input: 46.30682373046875
step: 1000 epoch: 697 loss: 16.890541798346764 loss_input: 84.53482723331356
step: 2000 epoch: 697 loss: 16.76460766136974 loss_input: 83.31080669382106
step: 3000 epoch: 697 loss: 16.781595600004874 loss_input: 82.61399451322215
step: 4000 epoch: 697 loss: 16.742094493990866 loss_input: 82.17445722492717
step: 5000 epoch: 697 loss: 16.668555791367055 loss_input: 81.92982447211729
step: 6000 epoch: 697 loss: 16.685886667522066 loss_input: 82.16191702424597
step: 7000 epoch: 697 loss: 16.716388696364856 loss_input: 82.17933723439899
step: 8000 epoch: 697 loss: 16.703018898964523 loss_input: 82.15722927822499
step: 9000 epoch: 697 loss: 16.74132521829795 loss_input: 82.2347111985387
step: 10000 epoch: 697 loss: 16.756355823486903 loss_input: 82.26575007294193
step: 11000 epoch: 697 loss: 16.78379122641572 loss_input: 82.19393508393769
step: 12000 epoch: 697 loss: 16.800980988546925 loss_input: 82.27862063130482
step: 13000 epoch: 697 loss: 16.79073979181378 loss_input: 82.25614353752385
step: 14000 epoch: 697 loss: 16.796888903799793 loss_input: 82.2997890331006
step: 15000 epoch: 697 loss: 16.794092895920915 loss_input: 82.31603863941622
Save loss: 16.7856252104789 Name: 697_train_model.pth
step: 0 epoch: 698 loss: 17.469581604003906 loss_input: 113.13983154296875
step: 1000 epoch: 698 loss: 16.939330334429975 loss_input: 82.06538304225072
step: 2000 epoch: 698 loss: 16.859158735046503 loss_input: 82.09966196101168
step: 3000 epoch: 698 loss: 16.82768368307887 loss_input: 81.70257979192483
step: 4000 epoch: 698 loss: 16.759118442623592 loss_input: 81.77640489779898
step: 5000 epoch: 698 loss: 16.78581249449306 loss_input: 81.85147783675687
step: 6000 epoch: 698 loss: 16.786080300937076 loss_input: 81.76383035688713
step: 7000 epoch: 698 loss: 16.77660019321793 loss_input: 81.91038403553956
step: 8000 epoch: 698 loss: 16.788776801535672 loss_input: 82.14381989787897
step: 9000 epoch: 698 loss: 16.797569503334945 loss_input: 82.24277192806485
step: 10000 epoch: 698 loss: 16.81432133337913 loss_input: 82.31091510599917
step: 11000 epoch: 698 loss: 16.795703215833555 loss_input: 82.24766558420116
step: 12000 epoch: 698 loss: 16.78830830870683 loss_input: 82.22136613534158
step: 13000 epoch: 698 loss: 16.80773360011633 loss_input: 82.32901581000681
step: 14000 epoch: 698 loss: 16.79964256507994 loss_input: 82.29329101440439
step: 15000 epoch: 698 loss: 16.784822505375963 loss_input: 82.24515974651169
Save loss: 16.776420416653156 Name: 698_train_model.pth
step: 0 epoch: 699 loss: 12.809106826782227 loss_input: 71.76116943359375
step: 1000 epoch: 699 loss: 16.65265220552534 loss_input: 82.59051702691839
step: 2000 epoch: 699 loss: 16.681351981956563 loss_input: 82.63819561166682
step: 3000 epoch: 699 loss: 16.755289789757224 loss_input: 82.47839882230012
step: 4000 epoch: 699 loss: 16.700717144684624 loss_input: 82.22334606472893
step: 5000 epoch: 699 loss: 16.72533601208988 loss_input: 82.44559175323138
step: 6000 epoch: 699 loss: 16.722370498121986 loss_input: 82.3525858991763
step: 7000 epoch: 699 loss: 16.734507545882032 loss_input: 82.36378370681979
step: 8000 epoch: 699 loss: 16.758342347850114 loss_input: 82.3542089892572
step: 9000 epoch: 699 loss: 16.75310092252594 loss_input: 82.19044569727713
step: 10000 epoch: 699 loss: 16.78781474827409 loss_input: 82.42830134711139
step: 11000 epoch: 699 loss: 16.804240820180524 loss_input: 82.37843159499704
step: 12000 epoch: 699 loss: 16.801948171448245 loss_input: 82.36898673767746
step: 13000 epoch: 699 loss: 16.79900007645503 loss_input: 82.31204168470664
step: 14000 epoch: 699 loss: 16.79968180144551 loss_input: 82.31649299219977
step: 15000 epoch: 699 loss: 16.787736951824062 loss_input: 82.2696568316344
Save loss: 16.783043070524933 Name: 699_train_model.pth
step: 0 epoch: 700 loss: 10.674874305725098 loss_input: 69.2188720703125
step: 1000 epoch: 700 loss: 16.83314859950459 loss_input: 83.19451800640765
step: 2000 epoch: 700 loss: 16.73177175793512 loss_input: 82.67913641445878
step: 3000 epoch: 700 loss: 16.752072088641352 loss_input: 82.58533353465512
step: 4000 epoch: 700 loss: 16.73209630510444 loss_input: 82.17067066546352
step: 5000 epoch: 700 loss: 16.77110123457944 loss_input: 82.17167637029235
step: 6000 epoch: 700 loss: 16.767591199166098 loss_input: 82.05144380843753
step: 7000 epoch: 700 loss: 16.75530717185933 loss_input: 81.91853313121842
step: 8000 epoch: 700 loss: 16.759254669848122 loss_input: 81.89369024489615
step: 9000 epoch: 700 loss: 16.77505792173965 loss_input: 81.96734587948345
step: 10000 epoch: 700 loss: 16.772537197426384 loss_input: 82.01086997013189
step: 11000 epoch: 700 loss: 16.775921583609108 loss_input: 82.08086519611498
step: 12000 epoch: 700 loss: 16.77450752701325 loss_input: 82.0742082273987
step: 13000 epoch: 700 loss: 16.763710334735432 loss_input: 82.02162442113077
step: 14000 epoch: 700 loss: 16.77727322004223 loss_input: 82.14404436373964
step: 15000 epoch: 700 loss: 16.78171045814671 loss_input: 82.16856768061166
Save loss: 16.790420955255627 Name: 700_train_model.pth
step: 0 epoch: 701 loss: 21.577951431274414 loss_input: 74.34381103515625
step: 1000 epoch: 701 loss: 16.74876476739432 loss_input: 82.72109744503544
step: 2000 epoch: 701 loss: 16.90198674337796 loss_input: 81.91917008414738
step: 3000 epoch: 701 loss: 16.895543270371668 loss_input: 82.25796300321768
step: 4000 epoch: 701 loss: 16.916825499960076 loss_input: 82.47397580536507
step: 5000 epoch: 701 loss: 16.882223246789316 loss_input: 82.53077814612858
step: 6000 epoch: 701 loss: 16.836361349791094 loss_input: 82.4570034429483
step: 7000 epoch: 701 loss: 16.80767059928944 loss_input: 82.25189914275639
step: 8000 epoch: 701 loss: 16.812889183302133 loss_input: 82.28697921091162
step: 9000 epoch: 701 loss: 16.80725904202914 loss_input: 82.30370189851952
step: 10000 epoch: 701 loss: 16.784632823667746 loss_input: 82.24175289990532
step: 11000 epoch: 701 loss: 16.775127231397473 loss_input: 82.2174523884725
step: 12000 epoch: 701 loss: 16.776011125632756 loss_input: 82.16959026811243
step: 13000 epoch: 701 loss: 16.791516823068452 loss_input: 82.23661496347486
step: 14000 epoch: 701 loss: 16.79780929829851 loss_input: 82.27716137254147
step: 15000 epoch: 701 loss: 16.801451253411006 loss_input: 82.2996579036658
Save loss: 16.794009241849185 Name: 701_train_model.pth
step: 0 epoch: 702 loss: 18.15096664428711 loss_input: 63.92510986328125
step: 1000 epoch: 702 loss: 17.00912061961857 loss_input: 83.6481215927627
step: 2000 epoch: 702 loss: 16.87870620191842 loss_input: 82.61518444269076
step: 3000 epoch: 702 loss: 16.751589015101402 loss_input: 82.25366843457779
step: 4000 epoch: 702 loss: 16.753825511374615 loss_input: 82.40435397437261
step: 5000 epoch: 702 loss: 16.77210921631935 loss_input: 82.27210428275673
step: 6000 epoch: 702 loss: 16.80891654845417 loss_input: 82.27189269317743
step: 7000 epoch: 702 loss: 16.801465001008864 loss_input: 82.38678844219241
step: 8000 epoch: 702 loss: 16.798179773669915 loss_input: 82.50068718522002
step: 9000 epoch: 702 loss: 16.789017411632386 loss_input: 82.385181024808
step: 10000 epoch: 702 loss: 16.818356151783924 loss_input: 82.42822950370979
step: 11000 epoch: 702 loss: 16.80033404429862 loss_input: 82.21839983430128
step: 12000 epoch: 702 loss: 16.804587509242527 loss_input: 82.22170271109407
step: 13000 epoch: 702 loss: 16.78774719377287 loss_input: 82.16999438388304
step: 14000 epoch: 702 loss: 16.798221111689266 loss_input: 82.19422956518034
step: 15000 epoch: 702 loss: 16.782886251005582 loss_input: 82.22159338547353
Save loss: 16.785467873945834 Name: 702_train_model.pth
step: 0 epoch: 703 loss: 15.263214111328125 loss_input: 65.368408203125
step: 1000 epoch: 703 loss: 16.93916213857782 loss_input: 83.0740154011028
step: 2000 epoch: 703 loss: 16.773913205235914 loss_input: 81.84777603263798
step: 3000 epoch: 703 loss: 16.81408210263098 loss_input: 82.41381372224883
step: 4000 epoch: 703 loss: 16.805944006313712 loss_input: 82.48835409250715
step: 5000 epoch: 703 loss: 16.75093353950746 loss_input: 82.52634485309969
step: 6000 epoch: 703 loss: 16.719887228851178 loss_input: 82.24884915014164
step: 7000 epoch: 703 loss: 16.73154478167793 loss_input: 82.42210473838558
step: 8000 epoch: 703 loss: 16.711904862272398 loss_input: 82.18034209652016
step: 9000 epoch: 703 loss: 16.754372905140624 loss_input: 82.33239494521226
step: 10000 epoch: 703 loss: 16.76535735153196 loss_input: 82.38370498277321
step: 11000 epoch: 703 loss: 16.766578008733916 loss_input: 82.46358961374605
step: 12000 epoch: 703 loss: 16.788782526439313 loss_input: 82.45576449460104
step: 13000 epoch: 703 loss: 16.78409967275044 loss_input: 82.2999902060193
step: 14000 epoch: 703 loss: 16.788250492024563 loss_input: 82.274604779381
step: 15000 epoch: 703 loss: 16.776306548346504 loss_input: 82.25101092300227
Save loss: 16.78184986370802 Name: 703_train_model.pth
step: 0 epoch: 704 loss: 17.22539710998535 loss_input: 119.16131591796875
step: 1000 epoch: 704 loss: 16.57702546543651 loss_input: 81.77910767211304
step: 2000 epoch: 704 loss: 16.693518729998672 loss_input: 82.34958963999446
step: 3000 epoch: 704 loss: 16.7314799286373 loss_input: 82.51393181846285
step: 4000 epoch: 704 loss: 16.783034647562836 loss_input: 82.78613697710826
step: 5000 epoch: 704 loss: 16.753350429834306 loss_input: 82.4684737979162
step: 6000 epoch: 704 loss: 16.77401219926423 loss_input: 82.65933654205419
step: 7000 epoch: 704 loss: 16.74638510646011 loss_input: 82.55355573854963
step: 8000 epoch: 704 loss: 16.741109905503954 loss_input: 82.51105415259372
step: 9000 epoch: 704 loss: 16.73362264224204 loss_input: 82.49357739412417
step: 10000 epoch: 704 loss: 16.74394130077425 loss_input: 82.38403410025208
step: 11000 epoch: 704 loss: 16.779197211266 loss_input: 82.36521269284468
step: 12000 epoch: 704 loss: 16.759398429197685 loss_input: 82.2584046239148
step: 13000 epoch: 704 loss: 16.7634577074103 loss_input: 82.29142225688169
step: 14000 epoch: 704 loss: 16.76494892926976 loss_input: 82.16433273129068
step: 15000 epoch: 704 loss: 16.78438576335359 loss_input: 82.21688773401435
Save loss: 16.780111152887343 Name: 704_train_model.pth
step: 0 epoch: 705 loss: 14.155174255371094 loss_input: 52.43609619140625
step: 1000 epoch: 705 loss: 16.672156071924903 loss_input: 82.19719806560627
step: 2000 epoch: 705 loss: 16.65752279609516 loss_input: 82.24004480303793
step: 3000 epoch: 705 loss: 16.655576489520683 loss_input: 82.19041305079297
step: 4000 epoch: 705 loss: 16.67776926205594 loss_input: 81.97381695530707
step: 5000 epoch: 705 loss: 16.702140173180727 loss_input: 82.12598015064955
step: 6000 epoch: 705 loss: 16.709650666132468 loss_input: 82.23601534143724
step: 7000 epoch: 705 loss: 16.726710218784554 loss_input: 82.33813733745346
step: 8000 epoch: 705 loss: 16.731818853534918 loss_input: 82.16900107640353
step: 9000 epoch: 705 loss: 16.75865969357524 loss_input: 82.40308927941383
step: 10000 epoch: 705 loss: 16.76959888869054 loss_input: 82.38229503465669
step: 11000 epoch: 705 loss: 16.746868095748177 loss_input: 82.38346653766561
step: 12000 epoch: 705 loss: 16.748634523614705 loss_input: 82.34776073856638
step: 13000 epoch: 705 loss: 16.7522459794866 loss_input: 82.28244892933417
step: 14000 epoch: 705 loss: 16.762535995180695 loss_input: 82.39125699133935
step: 15000 epoch: 705 loss: 16.764834434570815 loss_input: 82.29304297600864
Save loss: 16.780676191031933 Name: 705_train_model.pth
step: 0 epoch: 706 loss: 14.791166305541992 loss_input: 69.5438232421875
step: 1000 epoch: 706 loss: 16.712078602759394 loss_input: 81.68351777616914
step: 2000 epoch: 706 loss: 16.713430367726673 loss_input: 82.52374200473005
step: 3000 epoch: 706 loss: 16.717449685090067 loss_input: 82.63445294439614
step: 4000 epoch: 706 loss: 16.717066540297374 loss_input: 82.60161987396218
step: 5000 epoch: 706 loss: 16.782538308832223 loss_input: 82.659170094692
step: 6000 epoch: 706 loss: 16.792851344364284 loss_input: 82.39163357010486
step: 7000 epoch: 706 loss: 16.814505837369246 loss_input: 82.56011068417402
step: 8000 epoch: 706 loss: 16.785143432043863 loss_input: 82.43825725245038
step: 9000 epoch: 706 loss: 16.800587375327463 loss_input: 82.5099465470091
step: 10000 epoch: 706 loss: 16.809875067967486 loss_input: 82.45593648642948
step: 11000 epoch: 706 loss: 16.789808032729606 loss_input: 82.39044441893776
step: 12000 epoch: 706 loss: 16.76969575188615 loss_input: 82.27056611147476
step: 13000 epoch: 706 loss: 16.779790711470746 loss_input: 82.31594051246358
step: 14000 epoch: 706 loss: 16.78935199207548 loss_input: 82.29935356964597
step: 15000 epoch: 706 loss: 16.781816666384202 loss_input: 82.25774138250809
Save loss: 16.777740233406426 Name: 706_train_model.pth
step: 0 epoch: 707 loss: 11.736061096191406 loss_input: 40.09014892578125
step: 1000 epoch: 707 loss: 16.776124484055526 loss_input: 81.93470317524273
step: 2000 epoch: 707 loss: 16.65773881095341 loss_input: 81.73813837710051
step: 3000 epoch: 707 loss: 16.553907875218655 loss_input: 81.57478547596764
step: 4000 epoch: 707 loss: 16.62191052044728 loss_input: 81.90827416742006
step: 5000 epoch: 707 loss: 16.63835318349309 loss_input: 81.95469289931076
step: 6000 epoch: 707 loss: 16.61824867388066 loss_input: 81.68464623775269
step: 7000 epoch: 707 loss: 16.6582257325369 loss_input: 81.79555962743325
step: 8000 epoch: 707 loss: 16.665281362316634 loss_input: 81.82968851000558
step: 9000 epoch: 707 loss: 16.684271352845503 loss_input: 81.78752733121884
step: 10000 epoch: 707 loss: 16.70549407764359 loss_input: 81.90054149492278
step: 11000 epoch: 707 loss: 16.72934850939035 loss_input: 81.96062139991889
step: 12000 epoch: 707 loss: 16.740129097353744 loss_input: 82.10033750951256
step: 13000 epoch: 707 loss: 16.771313074616028 loss_input: 82.20428222205564
step: 14000 epoch: 707 loss: 16.771763079711842 loss_input: 82.20935256507906
step: 15000 epoch: 707 loss: 16.78250923365579 loss_input: 82.24424182268056
Save loss: 16.78832182328403 Name: 707_train_model.pth
step: 0 epoch: 708 loss: 14.666848182678223 loss_input: 94.4052734375
step: 1000 epoch: 708 loss: 16.562497386684665 loss_input: 80.62193446202235
step: 2000 epoch: 708 loss: 16.67569975612284 loss_input: 81.65874359573144
step: 3000 epoch: 708 loss: 16.73524377156162 loss_input: 81.99548437467459
step: 4000 epoch: 708 loss: 16.752638123447195 loss_input: 81.9641593507873
step: 5000 epoch: 708 loss: 16.741167902207522 loss_input: 82.04934507483269
step: 6000 epoch: 708 loss: 16.749531677495916 loss_input: 82.05035718737632
step: 7000 epoch: 708 loss: 16.750964369233753 loss_input: 82.03907755609411
step: 8000 epoch: 708 loss: 16.753916878533982 loss_input: 82.1162930195249
step: 9000 epoch: 708 loss: 16.763865845929967 loss_input: 82.00040446216697
step: 10000 epoch: 708 loss: 16.771263410253365 loss_input: 82.05644519776538
step: 11000 epoch: 708 loss: 16.770735993925825 loss_input: 82.1169746731121
step: 12000 epoch: 708 loss: 16.77893719002859 loss_input: 82.22974680495375
step: 13000 epoch: 708 loss: 16.805224574425523 loss_input: 82.28398877952147
step: 14000 epoch: 708 loss: 16.799463274308454 loss_input: 82.30895952936258
step: 15000 epoch: 708 loss: 16.801070481155342 loss_input: 82.27647154252216
Save loss: 16.78624298915267 Name: 708_train_model.pth
step: 0 epoch: 709 loss: 14.040006637573242 loss_input: 42.9803466796875
step: 1000 epoch: 709 loss: 16.687923413771134 loss_input: 82.02174058851305
step: 2000 epoch: 709 loss: 16.769282722759105 loss_input: 81.84675521126155
step: 3000 epoch: 709 loss: 16.79110655280917 loss_input: 81.65569185678342
step: 4000 epoch: 709 loss: 16.80327138481245 loss_input: 81.94605714111023
step: 5000 epoch: 709 loss: 16.776247511670913 loss_input: 82.19187532122481
step: 6000 epoch: 709 loss: 16.777126565255276 loss_input: 82.20292959799669
step: 7000 epoch: 709 loss: 16.762533430201106 loss_input: 82.10923761205152
step: 8000 epoch: 709 loss: 16.73135587826712 loss_input: 82.0701274492788
step: 9000 epoch: 709 loss: 16.76306389178346 loss_input: 82.082266012598
step: 10000 epoch: 709 loss: 16.765141701724527 loss_input: 81.90747903359077
step: 11000 epoch: 709 loss: 16.751797158461724 loss_input: 81.91846265707024
step: 12000 epoch: 709 loss: 16.7658374706275 loss_input: 82.05310944038355
step: 13000 epoch: 709 loss: 16.77670715968523 loss_input: 82.16578589225858
step: 14000 epoch: 709 loss: 16.775809542654788 loss_input: 82.24011294115152
step: 15000 epoch: 709 loss: 16.78056143825908 loss_input: 82.22732485152032
Save loss: 16.77715452966094 Name: 709_train_model.pth
step: 0 epoch: 710 loss: 20.689762115478516 loss_input: 102.36590576171875
step: 1000 epoch: 710 loss: 16.691980968822133 loss_input: 81.12077589158888
step: 2000 epoch: 710 loss: 16.794435839007225 loss_input: 82.17151635363959
step: 3000 epoch: 710 loss: 16.7199719159534 loss_input: 82.25466740985426
step: 4000 epoch: 710 loss: 16.77409480435048 loss_input: 82.57380781147486
step: 5000 epoch: 710 loss: 16.760005651533877 loss_input: 82.417542822002
step: 6000 epoch: 710 loss: 16.735555348843658 loss_input: 82.18684708351017
step: 7000 epoch: 710 loss: 16.731644938629262 loss_input: 82.15423560445606
step: 8000 epoch: 710 loss: 16.746082239516927 loss_input: 82.0456316404053
step: 9000 epoch: 710 loss: 16.777436642127096 loss_input: 82.29037861081842
step: 10000 epoch: 710 loss: 16.783568123986324 loss_input: 82.28329985604955
step: 11000 epoch: 710 loss: 16.796519658010922 loss_input: 82.42129856980331
step: 12000 epoch: 710 loss: 16.7812588572671 loss_input: 82.35549276024686
step: 13000 epoch: 710 loss: 16.782101713651034 loss_input: 82.36019882672787
step: 14000 epoch: 710 loss: 16.777020531103922 loss_input: 82.31402033382241
step: 15000 epoch: 710 loss: 16.777204660914453 loss_input: 82.25350769665486
Save loss: 16.779944651305676 Name: 710_train_model.pth
step: 0 epoch: 711 loss: 17.441204071044922 loss_input: 81.69183349609375
step: 1000 epoch: 711 loss: 16.59834361719442 loss_input: 81.0778951273336
step: 2000 epoch: 711 loss: 16.62801210109381 loss_input: 81.55327734692224
step: 3000 epoch: 711 loss: 16.71417278172532 loss_input: 81.93486712925119
step: 4000 epoch: 711 loss: 16.645981441048256 loss_input: 82.13768327632448
step: 5000 epoch: 711 loss: 16.669821718744554 loss_input: 82.1340604681798
step: 6000 epoch: 711 loss: 16.708464059963205 loss_input: 82.16221062562164
step: 7000 epoch: 711 loss: 16.723854137376247 loss_input: 82.09544191440844
step: 8000 epoch: 711 loss: 16.733722862966687 loss_input: 82.173758462077
step: 9000 epoch: 711 loss: 16.761500824161295 loss_input: 82.36777787494098
step: 10000 epoch: 711 loss: 16.762584958287693 loss_input: 82.43507933142233
step: 11000 epoch: 711 loss: 16.768857766948887 loss_input: 82.34797369828063
step: 12000 epoch: 711 loss: 16.782188249442907 loss_input: 82.36053559359148
step: 13000 epoch: 711 loss: 16.782011119670734 loss_input: 82.3148975159588
step: 14000 epoch: 711 loss: 16.778988655052324 loss_input: 82.32434924256248
step: 15000 epoch: 711 loss: 16.785677214684736 loss_input: 82.32582210176874
Save loss: 16.773703291490673 Name: 711_train_model.pth
step: 0 epoch: 712 loss: 21.85424041748047 loss_input: 72.523193359375
step: 1000 epoch: 712 loss: 16.8663258519206 loss_input: 83.35726974441575
step: 2000 epoch: 712 loss: 16.81407895414666 loss_input: 83.14441772033905
step: 3000 epoch: 712 loss: 16.814540180751617 loss_input: 83.27017259709004
step: 4000 epoch: 712 loss: 16.827976097973607 loss_input: 83.0639383916228
step: 5000 epoch: 712 loss: 16.752714123351172 loss_input: 82.45491512049152
step: 6000 epoch: 712 loss: 16.742840634368097 loss_input: 82.23596707067377
step: 7000 epoch: 712 loss: 16.743403501364867 loss_input: 82.14117422922563
step: 8000 epoch: 712 loss: 16.739008760410552 loss_input: 82.06294804885229
step: 9000 epoch: 712 loss: 16.747024765571958 loss_input: 82.19702116567126
step: 10000 epoch: 712 loss: 16.74244442916777 loss_input: 82.19814473020472
step: 11000 epoch: 712 loss: 16.737195603750628 loss_input: 82.18380473490336
step: 12000 epoch: 712 loss: 16.733881746825652 loss_input: 82.19236951811632
step: 13000 epoch: 712 loss: 16.762837829704274 loss_input: 82.22120265463354
step: 14000 epoch: 712 loss: 16.766312368460515 loss_input: 82.24305044931221
step: 15000 epoch: 712 loss: 16.773799640815596 loss_input: 82.20246577474262
Save loss: 16.781523373678326 Name: 712_train_model.pth
step: 0 epoch: 713 loss: 11.5101318359375 loss_input: 71.01165771484375
step: 1000 epoch: 713 loss: 16.90532705905316 loss_input: 82.6782828377677
step: 2000 epoch: 713 loss: 16.837796211719276 loss_input: 82.9382843368355
step: 3000 epoch: 713 loss: 16.818256012879385 loss_input: 82.617431864346
step: 4000 epoch: 713 loss: 16.784901249620265 loss_input: 82.74368781156761
step: 5000 epoch: 713 loss: 16.751426388230044 loss_input: 82.66096578047672
step: 6000 epoch: 713 loss: 16.767247177366215 loss_input: 82.78080351069418
step: 7000 epoch: 713 loss: 16.759466400250012 loss_input: 82.52843094471437
step: 8000 epoch: 713 loss: 16.762822751864213 loss_input: 82.39654675276186
step: 9000 epoch: 713 loss: 16.733416038358918 loss_input: 82.09614577736805
step: 10000 epoch: 713 loss: 16.718507192049273 loss_input: 82.03751664018168
step: 11000 epoch: 713 loss: 16.72461538908211 loss_input: 82.10760529671309
step: 12000 epoch: 713 loss: 16.750505059135527 loss_input: 82.1186837793221
step: 13000 epoch: 713 loss: 16.755289679242598 loss_input: 82.13308701032162
step: 14000 epoch: 713 loss: 16.764085353557473 loss_input: 82.17098283873278
step: 15000 epoch: 713 loss: 16.764993445172834 loss_input: 82.2118380218337
Save loss: 16.778068975910543 Name: 713_train_model.pth
step: 0 epoch: 714 loss: 9.16597843170166 loss_input: 51.96649169921875
step: 1000 epoch: 714 loss: 16.61447166515278 loss_input: 82.42180152611061
step: 2000 epoch: 714 loss: 16.56031235976555 loss_input: 81.76483410516421
step: 3000 epoch: 714 loss: 16.644351425189967 loss_input: 81.93940404874807
step: 4000 epoch: 714 loss: 16.764320953820114 loss_input: 82.25263378674374
step: 5000 epoch: 714 loss: 16.7419687269974 loss_input: 82.12940988462464
step: 6000 epoch: 714 loss: 16.73434995647431 loss_input: 82.11189258577346
step: 7000 epoch: 714 loss: 16.725449875991117 loss_input: 82.07330441703083
step: 8000 epoch: 714 loss: 16.745793969552825 loss_input: 82.05967962043074
step: 9000 epoch: 714 loss: 16.715643386525084 loss_input: 82.03717131880094
step: 10000 epoch: 714 loss: 16.74609391768686 loss_input: 82.14549455088671
step: 11000 epoch: 714 loss: 16.743245912740342 loss_input: 82.14942301720795
step: 12000 epoch: 714 loss: 16.76091440430622 loss_input: 82.2053883408479
step: 13000 epoch: 714 loss: 16.762835571027043 loss_input: 82.24574938796775
step: 14000 epoch: 714 loss: 16.7633314590762 loss_input: 82.20131254226818
step: 15000 epoch: 714 loss: 16.761333219576386 loss_input: 82.25628193797496
Save loss: 16.77824925202131 Name: 714_train_model.pth
step: 0 epoch: 715 loss: 19.323209762573242 loss_input: 71.11907958984375
step: 1000 epoch: 715 loss: 16.606774832699802 loss_input: 82.39763459197053
step: 2000 epoch: 715 loss: 16.604490497718746 loss_input: 82.42145986333006
step: 3000 epoch: 715 loss: 16.66506257163648 loss_input: 82.4847621186818
step: 4000 epoch: 715 loss: 16.657958184442233 loss_input: 82.2331674008988
step: 5000 epoch: 715 loss: 16.689981264725752 loss_input: 82.13962501689116
step: 6000 epoch: 715 loss: 16.67874443751537 loss_input: 82.20551082265911
step: 7000 epoch: 715 loss: 16.69556746331645 loss_input: 82.31193053615108
step: 8000 epoch: 715 loss: 16.697031910218918 loss_input: 82.31963639890472
step: 9000 epoch: 715 loss: 16.724746398085582 loss_input: 82.23500985106473
step: 10000 epoch: 715 loss: 16.731983220740542 loss_input: 82.18395178304435
step: 11000 epoch: 715 loss: 16.744992541655684 loss_input: 82.09537893572175
step: 12000 epoch: 715 loss: 16.754850566034545 loss_input: 82.1332084224101
step: 13000 epoch: 715 loss: 16.76722814447925 loss_input: 82.16155326543391
step: 14000 epoch: 715 loss: 16.774884503872496 loss_input: 82.20102451178153
step: 15000 epoch: 715 loss: 16.772504567146363 loss_input: 82.24093814074504
Save loss: 16.769856623992325 Name: 715_train_model.pth
step: 0 epoch: 716 loss: 15.59659481048584 loss_input: 101.048095703125
step: 1000 epoch: 716 loss: 16.7935841309798 loss_input: 82.74160733065763
step: 2000 epoch: 716 loss: 16.83644380502734 loss_input: 82.44150968339072
step: 3000 epoch: 716 loss: 16.804879415595344 loss_input: 82.43080134584045
step: 4000 epoch: 716 loss: 16.760132040568692 loss_input: 82.30650424552066
step: 5000 epoch: 716 loss: 16.76641831972961 loss_input: 82.19043725434601
step: 6000 epoch: 716 loss: 16.725864127524154 loss_input: 82.12396366352678
step: 7000 epoch: 716 loss: 16.73907047795357 loss_input: 82.01992999674712
step: 8000 epoch: 716 loss: 16.73614993606146 loss_input: 81.88591905290403
step: 9000 epoch: 716 loss: 16.748928798647565 loss_input: 82.03264377281647
step: 10000 epoch: 716 loss: 16.74721126544 loss_input: 82.06842992458567
step: 11000 epoch: 716 loss: 16.747287694545953 loss_input: 82.12523260048093
step: 12000 epoch: 716 loss: 16.765559705135555 loss_input: 82.11979609812789
step: 13000 epoch: 716 loss: 16.77052340141838 loss_input: 82.24161827357932
step: 14000 epoch: 716 loss: 16.759726852478703 loss_input: 82.14669265131313
step: 15000 epoch: 716 loss: 16.755897443315664 loss_input: 82.15275401317646
Save loss: 16.782948444977404 Name: 716_train_model.pth
step: 0 epoch: 717 loss: 20.332857131958008 loss_input: 74.91375732421875
step: 1000 epoch: 717 loss: 16.617741994448117 loss_input: 82.56911362563218
step: 2000 epoch: 717 loss: 16.599287315942476 loss_input: 82.32309754546554
step: 3000 epoch: 717 loss: 16.687771491470514 loss_input: 82.46991882019145
step: 4000 epoch: 717 loss: 16.714403104674844 loss_input: 82.40589565743329
step: 5000 epoch: 717 loss: 16.758654631130508 loss_input: 82.60077552360622
step: 6000 epoch: 717 loss: 16.733496870006885 loss_input: 82.48476950618907
step: 7000 epoch: 717 loss: 16.71647704115869 loss_input: 82.31383737695676
step: 8000 epoch: 717 loss: 16.71850448905669 loss_input: 82.29557250473086
step: 9000 epoch: 717 loss: 16.726643928619588 loss_input: 82.25264463752075
step: 10000 epoch: 717 loss: 16.720047903375594 loss_input: 82.09517561427451
step: 11000 epoch: 717 loss: 16.73676404668227 loss_input: 82.20854447913294
step: 12000 epoch: 717 loss: 16.74421545054831 loss_input: 82.27411041755634
step: 13000 epoch: 717 loss: 16.772483024897184 loss_input: 82.36400788337045
step: 14000 epoch: 717 loss: 16.776172708182767 loss_input: 82.36110864517629
step: 15000 epoch: 717 loss: 16.765124406093644 loss_input: 82.2885655441981
Save loss: 16.76463824094832 Name: 717_train_model.pth
step: 0 epoch: 718 loss: 18.598241806030273 loss_input: 122.7069091796875
step: 1000 epoch: 718 loss: 16.705014990521715 loss_input: 82.0004750498525
step: 2000 epoch: 718 loss: 16.76615298050514 loss_input: 82.52182551302474
step: 3000 epoch: 718 loss: 16.759638096244682 loss_input: 82.72175927116092
step: 4000 epoch: 718 loss: 16.811232835583258 loss_input: 82.73361693451149
step: 5000 epoch: 718 loss: 16.82149484433596 loss_input: 82.66589829006855
step: 6000 epoch: 718 loss: 16.822658303657626 loss_input: 82.44257456317501
step: 7000 epoch: 718 loss: 16.809112710691217 loss_input: 82.66550761477434
step: 8000 epoch: 718 loss: 16.813328588534826 loss_input: 82.62276174676164
step: 9000 epoch: 718 loss: 16.808455711549527 loss_input: 82.51767872688625
step: 10000 epoch: 718 loss: 16.807458588867448 loss_input: 82.47440373150185
step: 11000 epoch: 718 loss: 16.79493582334294 loss_input: 82.37232126070037
step: 12000 epoch: 718 loss: 16.768664711009023 loss_input: 82.28009691819301
step: 13000 epoch: 718 loss: 16.78025908118422 loss_input: 82.30237224840877
step: 14000 epoch: 718 loss: 16.772491565287755 loss_input: 82.27591436514301
step: 15000 epoch: 718 loss: 16.772430198207378 loss_input: 82.24935753794799
Save loss: 16.776290939569474 Name: 718_train_model.pth
step: 0 epoch: 719 loss: 17.150360107421875 loss_input: 93.7127685546875
step: 1000 epoch: 719 loss: 16.574929781369754 loss_input: 82.70582914400053
step: 2000 epoch: 719 loss: 16.695517408436743 loss_input: 82.88072336512408
step: 3000 epoch: 719 loss: 16.710409781727066 loss_input: 82.73391458218354
step: 4000 epoch: 719 loss: 16.753878563590362 loss_input: 82.6608486885847
step: 5000 epoch: 719 loss: 16.75724189340103 loss_input: 82.70525279709683
step: 6000 epoch: 719 loss: 16.75039919899456 loss_input: 82.50398807433461
step: 7000 epoch: 719 loss: 16.716026878751972 loss_input: 82.55579057902443
step: 8000 epoch: 719 loss: 16.739494485536852 loss_input: 82.47538273108928
step: 9000 epoch: 719 loss: 16.741347552431833 loss_input: 82.38376058937244
step: 10000 epoch: 719 loss: 16.738099547007504 loss_input: 82.3599193242297
step: 11000 epoch: 719 loss: 16.74652527291258 loss_input: 82.34117939961779
step: 12000 epoch: 719 loss: 16.762157092242624 loss_input: 82.3092824489154
step: 13000 epoch: 719 loss: 16.765294171142518 loss_input: 82.19441744133414
step: 14000 epoch: 719 loss: 16.761507499661583 loss_input: 82.18262527408059
step: 15000 epoch: 719 loss: 16.775653390818917 loss_input: 82.2382669036257
Save loss: 16.769145919948816 Name: 719_train_model.pth
step: 0 epoch: 720 loss: 10.525229454040527 loss_input: 61.3646240234375
step: 1000 epoch: 720 loss: 16.70999845281824 loss_input: 82.35828539136645
step: 2000 epoch: 720 loss: 16.733037606410416 loss_input: 82.70231217399113
step: 3000 epoch: 720 loss: 16.746097814 loss_input: 82.50242104763907
step: 4000 epoch: 720 loss: 16.726448348092305 loss_input: 82.28352808648424
step: 5000 epoch: 720 loss: 16.73379195916417 loss_input: 82.09400093858181
step: 6000 epoch: 720 loss: 16.742751353543078 loss_input: 82.15156265052829
step: 7000 epoch: 720 loss: 16.754672015944372 loss_input: 82.08446852388833
step: 8000 epoch: 720 loss: 16.763631607976322 loss_input: 82.22314056446069
step: 9000 epoch: 720 loss: 16.752348769308394 loss_input: 82.27998909951846
step: 10000 epoch: 720 loss: 16.788087556104923 loss_input: 82.51674785524378
step: 11000 epoch: 720 loss: 16.804112359920683 loss_input: 82.52714711815084
step: 12000 epoch: 720 loss: 16.80663892891236 loss_input: 82.47028041038898
step: 13000 epoch: 720 loss: 16.78057562401731 loss_input: 82.30422752749487
step: 14000 epoch: 720 loss: 16.78120101193821 loss_input: 82.33835737545672
step: 15000 epoch: 720 loss: 16.792123086181817 loss_input: 82.27831591419296
Save loss: 16.78726617978513 Name: 720_train_model.pth
step: 0 epoch: 721 loss: 16.908954620361328 loss_input: 86.86981201171875
step: 1000 epoch: 721 loss: 16.910119584985786 loss_input: 83.3247182504995
step: 2000 epoch: 721 loss: 16.931472731137024 loss_input: 83.31976107893318
step: 3000 epoch: 721 loss: 16.970091190547873 loss_input: 83.38738696394822
step: 4000 epoch: 721 loss: 16.978636457990987 loss_input: 83.21804253526462
step: 5000 epoch: 721 loss: 16.930529113293552 loss_input: 82.96942837994901
step: 6000 epoch: 721 loss: 16.926926795054904 loss_input: 83.00897753816747
step: 7000 epoch: 721 loss: 16.852383920080815 loss_input: 82.78171961587798
step: 8000 epoch: 721 loss: 16.837019415441684 loss_input: 82.68252911714893
step: 9000 epoch: 721 loss: 16.837035609038587 loss_input: 82.58742505308655
step: 10000 epoch: 721 loss: 16.845454770414225 loss_input: 82.64298407805704
step: 11000 epoch: 721 loss: 16.838620893890862 loss_input: 82.61948342276057
step: 12000 epoch: 721 loss: 16.818328828655098 loss_input: 82.42312365998309
step: 13000 epoch: 721 loss: 16.8138188293609 loss_input: 82.37187765811133
step: 14000 epoch: 721 loss: 16.796960965078835 loss_input: 82.3151506503372
step: 15000 epoch: 721 loss: 16.80321983648661 loss_input: 82.2702471243128
Save loss: 16.796242998301985 Name: 721_train_model.pth
step: 0 epoch: 722 loss: 22.17186737060547 loss_input: 113.94671630859375
step: 1000 epoch: 722 loss: 16.665698276771295 loss_input: 81.84348275015999
step: 2000 epoch: 722 loss: 16.699682782614964 loss_input: 82.40376711058533
step: 3000 epoch: 722 loss: 16.663331222152838 loss_input: 81.9183014027876
step: 4000 epoch: 722 loss: 16.67402818983002 loss_input: 81.86655913231165
step: 5000 epoch: 722 loss: 16.738939084283974 loss_input: 82.00986771639813
step: 6000 epoch: 722 loss: 16.725992168432235 loss_input: 81.98655443615843
step: 7000 epoch: 722 loss: 16.725421256498684 loss_input: 81.89598290332128
step: 8000 epoch: 722 loss: 16.740844676143272 loss_input: 81.92534629155243
step: 9000 epoch: 722 loss: 16.746401831886793 loss_input: 82.03382944565404
step: 10000 epoch: 722 loss: 16.738065269443705 loss_input: 82.03572071992019
step: 11000 epoch: 722 loss: 16.739510928790207 loss_input: 82.03917907764777
step: 12000 epoch: 722 loss: 16.756290530435304 loss_input: 82.14425647227965
step: 13000 epoch: 722 loss: 16.763223695861367 loss_input: 82.16621047648528
step: 14000 epoch: 722 loss: 16.771047811356283 loss_input: 82.22031827351339
step: 15000 epoch: 722 loss: 16.782594795282996 loss_input: 82.20321848335111
Save loss: 16.77743404150009 Name: 722_train_model.pth
step: 0 epoch: 723 loss: 19.8699951171875 loss_input: 134.0809326171875
step: 1000 epoch: 723 loss: 16.65566000643072 loss_input: 81.19059590312031
step: 2000 epoch: 723 loss: 16.753104655043238 loss_input: 81.84534002529985
step: 3000 epoch: 723 loss: 16.812491184788836 loss_input: 82.50266524927889
step: 4000 epoch: 723 loss: 16.746587428055058 loss_input: 82.15335515486363
step: 5000 epoch: 723 loss: 16.771604897045798 loss_input: 82.29709089920297
step: 6000 epoch: 723 loss: 16.76164623173728 loss_input: 82.29292367287744
step: 7000 epoch: 723 loss: 16.781534203119474 loss_input: 82.32453924341723
step: 8000 epoch: 723 loss: 16.777551426855926 loss_input: 82.23471966822018
step: 9000 epoch: 723 loss: 16.743593314583734 loss_input: 82.20995720120725
step: 10000 epoch: 723 loss: 16.748922502883683 loss_input: 82.24615062297481
step: 11000 epoch: 723 loss: 16.765607966692727 loss_input: 82.24756680500636
step: 12000 epoch: 723 loss: 16.74777243336224 loss_input: 82.10453269620923
step: 13000 epoch: 723 loss: 16.758634078228496 loss_input: 82.10751301225777
step: 14000 epoch: 723 loss: 16.7622054848924 loss_input: 82.0844021787576
step: 15000 epoch: 723 loss: 16.778636617602036 loss_input: 82.21706521241683
Save loss: 16.771154040798546 Name: 723_train_model.pth
step: 0 epoch: 724 loss: 20.945125579833984 loss_input: 127.9969482421875
step: 1000 epoch: 724 loss: 16.754417933903255 loss_input: 81.60975523499937
step: 2000 epoch: 724 loss: 16.71201693398067 loss_input: 81.94978761839783
step: 3000 epoch: 724 loss: 16.750056757763282 loss_input: 82.00631564380883
step: 4000 epoch: 724 loss: 16.732794722090123 loss_input: 81.86815483323993
step: 5000 epoch: 724 loss: 16.73960303955139 loss_input: 81.92357972497297
step: 6000 epoch: 724 loss: 16.7362835607098 loss_input: 82.04689274809992
step: 7000 epoch: 724 loss: 16.74651342690834 loss_input: 82.1937692198408
step: 8000 epoch: 724 loss: 16.75045895406625 loss_input: 82.15946446483008
step: 9000 epoch: 724 loss: 16.753906819386266 loss_input: 82.1407628291952
step: 10000 epoch: 724 loss: 16.743142708434235 loss_input: 82.00020106021624
step: 11000 epoch: 724 loss: 16.742784913502305 loss_input: 81.96790152755112
step: 12000 epoch: 724 loss: 16.74342952002268 loss_input: 81.96992917216208
step: 13000 epoch: 724 loss: 16.751529500976122 loss_input: 81.9813140453664
step: 14000 epoch: 724 loss: 16.752017199844133 loss_input: 82.0581210098065
step: 15000 epoch: 724 loss: 16.750342603222816 loss_input: 82.12123019516798
Save loss: 16.7671639406085 Name: 724_train_model.pth
step: 0 epoch: 725 loss: 17.203174591064453 loss_input: 125.13238525390625
step: 1000 epoch: 725 loss: 16.85244013046051 loss_input: 82.80315899920392
step: 2000 epoch: 725 loss: 16.83448708528045 loss_input: 82.48340191476527
step: 3000 epoch: 725 loss: 16.806795663811375 loss_input: 82.40628544960924
step: 4000 epoch: 725 loss: 16.775238412524782 loss_input: 82.21896826818686
step: 5000 epoch: 725 loss: 16.77738524856293 loss_input: 82.4179758164578
step: 6000 epoch: 725 loss: 16.759633743053794 loss_input: 82.19763612802814
step: 7000 epoch: 725 loss: 16.76350749882438 loss_input: 82.09611994429088
step: 8000 epoch: 725 loss: 16.743114021446925 loss_input: 81.98896012021935
step: 9000 epoch: 725 loss: 16.727303620669964 loss_input: 82.00343464983925
step: 10000 epoch: 725 loss: 16.72918921505829 loss_input: 81.9888872562939
step: 11000 epoch: 725 loss: 16.73472501758922 loss_input: 81.97254504295775
step: 12000 epoch: 725 loss: 16.75730020937726 loss_input: 82.09773240027036
step: 13000 epoch: 725 loss: 16.757375320245025 loss_input: 82.14578208808908
step: 14000 epoch: 725 loss: 16.782920652368684 loss_input: 82.24966566457586
step: 15000 epoch: 725 loss: 16.776352243148185 loss_input: 82.22554498157155
Save loss: 16.7638983387053 Name: 725_train_model.pth
step: 0 epoch: 726 loss: 12.314685821533203 loss_input: 61.26318359375
step: 1000 epoch: 726 loss: 16.82107178052584 loss_input: 82.30159635286589
step: 2000 epoch: 726 loss: 16.818555548571158 loss_input: 81.7780000380669
step: 3000 epoch: 726 loss: 16.88757265348984 loss_input: 82.53430047650133
step: 4000 epoch: 726 loss: 16.85020139580278 loss_input: 82.377187380401
step: 5000 epoch: 726 loss: 16.82237632244593 loss_input: 82.40581290480185
step: 6000 epoch: 726 loss: 16.78566395674243 loss_input: 82.18231204223125
step: 7000 epoch: 726 loss: 16.772878824957882 loss_input: 82.23508179077096
step: 8000 epoch: 726 loss: 16.777124790620274 loss_input: 82.1256228240635
step: 9000 epoch: 726 loss: 16.767584927121 loss_input: 82.27457715744258
step: 10000 epoch: 726 loss: 16.757818631989494 loss_input: 82.33787033894266
step: 11000 epoch: 726 loss: 16.761539021705175 loss_input: 82.29769295764483
step: 12000 epoch: 726 loss: 16.76748942331556 loss_input: 82.31811298643407
step: 13000 epoch: 726 loss: 16.755120485766597 loss_input: 82.28422118351924
step: 14000 epoch: 726 loss: 16.746500632071715 loss_input: 82.21756847428728
step: 15000 epoch: 726 loss: 16.752200696112496 loss_input: 82.17905822880037
Save loss: 16.76511345103383 Name: 726_train_model.pth
step: 0 epoch: 727 loss: 17.18321990966797 loss_input: 107.08441162109375
step: 1000 epoch: 727 loss: 16.60674446160262 loss_input: 81.86592112721263
step: 2000 epoch: 727 loss: 16.668886600405738 loss_input: 81.94145371162075
step: 3000 epoch: 727 loss: 16.698059835580143 loss_input: 81.91140689106236
step: 4000 epoch: 727 loss: 16.715313539776734 loss_input: 82.24934923800788
step: 5000 epoch: 727 loss: 16.712559143797538 loss_input: 82.09139635695908
step: 6000 epoch: 727 loss: 16.70109342622908 loss_input: 81.85077032449344
step: 7000 epoch: 727 loss: 16.719632342481184 loss_input: 82.11152588553198
step: 8000 epoch: 727 loss: 16.71320177775415 loss_input: 81.95111662983045
step: 9000 epoch: 727 loss: 16.71911562324802 loss_input: 81.9279613124041
step: 10000 epoch: 727 loss: 16.743052352226897 loss_input: 82.02286357667944
step: 11000 epoch: 727 loss: 16.74227972403839 loss_input: 82.00980015111375
step: 12000 epoch: 727 loss: 16.746046136472735 loss_input: 82.02985548832031
step: 13000 epoch: 727 loss: 16.76440202028107 loss_input: 82.12117906589066
step: 14000 epoch: 727 loss: 16.772721915421815 loss_input: 82.25171891059819
step: 15000 epoch: 727 loss: 16.763676167217845 loss_input: 82.25640808109537
Save loss: 16.771408551886676 Name: 727_train_model.pth
step: 0 epoch: 728 loss: 11.893362998962402 loss_input: 65.090576171875
step: 1000 epoch: 728 loss: 16.682356808449956 loss_input: 82.56841602001514
step: 2000 epoch: 728 loss: 16.749889648538538 loss_input: 82.25787701242152
step: 3000 epoch: 728 loss: 16.774741635804016 loss_input: 82.28583896513662
step: 4000 epoch: 728 loss: 16.75155412921367 loss_input: 82.1424076659058
step: 5000 epoch: 728 loss: 16.713085190960467 loss_input: 81.8622506577786
step: 6000 epoch: 728 loss: 16.738206730705762 loss_input: 82.08337605760369
step: 7000 epoch: 728 loss: 16.73337879979156 loss_input: 82.04169956572889
step: 8000 epoch: 728 loss: 16.754974387553524 loss_input: 82.07820197096945
step: 9000 epoch: 728 loss: 16.776269086797083 loss_input: 82.10342466559705
step: 10000 epoch: 728 loss: 16.773848441538483 loss_input: 82.23668992963985
step: 11000 epoch: 728 loss: 16.776156360870512 loss_input: 82.3192220340587
step: 12000 epoch: 728 loss: 16.765211220552857 loss_input: 82.29383951687295
step: 13000 epoch: 728 loss: 16.765606216243317 loss_input: 82.23250027097525
step: 14000 epoch: 728 loss: 16.76563100630569 loss_input: 82.14935763080958
step: 15000 epoch: 728 loss: 16.771332814418333 loss_input: 82.18408674284996
Save loss: 16.773869689688087 Name: 728_train_model.pth
step: 0 epoch: 729 loss: 8.160609245300293 loss_input: 58.119384765625
step: 1000 epoch: 729 loss: 17.0029436177188 loss_input: 82.68796748905392
step: 2000 epoch: 729 loss: 16.853191315204366 loss_input: 82.38171859504818
step: 3000 epoch: 729 loss: 16.79881791884166 loss_input: 82.09042436700787
step: 4000 epoch: 729 loss: 16.75808335971427 loss_input: 82.18475381459811
step: 5000 epoch: 729 loss: 16.714391964193677 loss_input: 82.12415154908471
step: 6000 epoch: 729 loss: 16.744003970272203 loss_input: 82.32210673627964
step: 7000 epoch: 729 loss: 16.754785043242112 loss_input: 82.44810719701873
step: 8000 epoch: 729 loss: 16.761665726911872 loss_input: 82.4416674249337
step: 9000 epoch: 729 loss: 16.76520337423819 loss_input: 82.47153635218916
step: 10000 epoch: 729 loss: 16.756610226552493 loss_input: 82.42963174995977
step: 11000 epoch: 729 loss: 16.758348433584118 loss_input: 82.43159681683767
step: 12000 epoch: 729 loss: 16.7455775021573 loss_input: 82.4045898752822
step: 13000 epoch: 729 loss: 16.758180673668342 loss_input: 82.3224266139684
step: 14000 epoch: 729 loss: 16.75359310173033 loss_input: 82.18811533865048
step: 15000 epoch: 729 loss: 16.741784551558307 loss_input: 82.1554223818347
Save loss: 16.76556024193764 Name: 729_train_model.pth
step: 0 epoch: 730 loss: 13.877735137939453 loss_input: 72.4010009765625
step: 1000 epoch: 730 loss: 16.631137148126378 loss_input: 81.86263809432755
step: 2000 epoch: 730 loss: 16.703713595301195 loss_input: 82.62654774907467
step: 3000 epoch: 730 loss: 16.687910733958635 loss_input: 82.02495898583936
step: 4000 epoch: 730 loss: 16.67711394561943 loss_input: 82.00938188609585
step: 5000 epoch: 730 loss: 16.73250839162459 loss_input: 82.18383607214103
step: 6000 epoch: 730 loss: 16.75146162650959 loss_input: 82.32853074495722
step: 7000 epoch: 730 loss: 16.775178381177874 loss_input: 82.27881454818676
step: 8000 epoch: 730 loss: 16.77768499763917 loss_input: 82.4573099627195
step: 9000 epoch: 730 loss: 16.78317019163589 loss_input: 82.43322871894442
step: 10000 epoch: 730 loss: 16.779428684524316 loss_input: 82.37399845220556
step: 11000 epoch: 730 loss: 16.79001470845717 loss_input: 82.40577952275113
step: 12000 epoch: 730 loss: 16.78441428246175 loss_input: 82.39595353544756
step: 13000 epoch: 730 loss: 16.77288798185213 loss_input: 82.31788103232594
step: 14000 epoch: 730 loss: 16.77139145974968 loss_input: 82.26567939587741
step: 15000 epoch: 730 loss: 16.770871898522575 loss_input: 82.2447608351175
Save loss: 16.76507404525578 Name: 730_train_model.pth
step: 0 epoch: 731 loss: 17.567975997924805 loss_input: 50.82904052734375
step: 1000 epoch: 731 loss: 16.888894033003282 loss_input: 82.78809776649132
step: 2000 epoch: 731 loss: 16.873624270704614 loss_input: 82.51901255793003
step: 3000 epoch: 731 loss: 16.850018707524534 loss_input: 82.43665939885273
step: 4000 epoch: 731 loss: 16.812839711078908 loss_input: 82.39850935093375
step: 5000 epoch: 731 loss: 16.777172709340885 loss_input: 82.2428689310966
step: 6000 epoch: 731 loss: 16.7529377669538 loss_input: 82.08918420212842
step: 7000 epoch: 731 loss: 16.74849277686228 loss_input: 82.25848386928263
step: 8000 epoch: 731 loss: 16.738347848107438 loss_input: 82.22673379664212
step: 9000 epoch: 731 loss: 16.729864039615503 loss_input: 82.14302550367667
step: 10000 epoch: 731 loss: 16.743025890601515 loss_input: 82.21386914653262
step: 11000 epoch: 731 loss: 16.760275390644246 loss_input: 82.29895563249576
step: 12000 epoch: 731 loss: 16.78075285733079 loss_input: 82.3634599765612
step: 13000 epoch: 731 loss: 16.767144835038366 loss_input: 82.23347732641359
step: 14000 epoch: 731 loss: 16.78027531864149 loss_input: 82.25899335001462
step: 15000 epoch: 731 loss: 16.768059804323364 loss_input: 82.17949022311274
Save loss: 16.771873553320766 Name: 731_train_model.pth
step: 0 epoch: 732 loss: 16.665006637573242 loss_input: 75.07073974609375
step: 1000 epoch: 732 loss: 16.7404424446327 loss_input: 82.45887712141375
step: 2000 epoch: 732 loss: 16.815435949055807 loss_input: 82.8735212471889
step: 3000 epoch: 732 loss: 16.82010525983399 loss_input: 82.63817918034484
step: 4000 epoch: 732 loss: 16.81539354143188 loss_input: 82.60850075261946
step: 5000 epoch: 732 loss: 16.74987301848407 loss_input: 82.35468833967582
step: 6000 epoch: 732 loss: 16.76312392990781 loss_input: 82.26183420927241
step: 7000 epoch: 732 loss: 16.74163424691172 loss_input: 82.06632784846306
step: 8000 epoch: 732 loss: 16.725895775569825 loss_input: 81.97039721688841
step: 9000 epoch: 732 loss: 16.740717439171526 loss_input: 82.08299602724263
step: 10000 epoch: 732 loss: 16.75367997048104 loss_input: 82.08403731770377
step: 11000 epoch: 732 loss: 16.75586967270609 loss_input: 82.09073245205691
step: 12000 epoch: 732 loss: 16.74820876814864 loss_input: 82.15228176887766
step: 13000 epoch: 732 loss: 16.76078713071336 loss_input: 82.20647627184478
step: 14000 epoch: 732 loss: 16.75698585866494 loss_input: 82.18877755767166
step: 15000 epoch: 732 loss: 16.767605752819705 loss_input: 82.2156778700105
Save loss: 16.76452638579905 Name: 732_train_model.pth
step: 0 epoch: 733 loss: 23.705623626708984 loss_input: 98.46087646484375
step: 1000 epoch: 733 loss: 16.663562160629134 loss_input: 81.97250100973245
step: 2000 epoch: 733 loss: 16.696325758229133 loss_input: 81.84546066200298
step: 3000 epoch: 733 loss: 16.72226837721319 loss_input: 81.87835766577156
step: 4000 epoch: 733 loss: 16.7403501240321 loss_input: 82.21606997542607
step: 5000 epoch: 733 loss: 16.74116314909168 loss_input: 82.03206745125584
step: 6000 epoch: 733 loss: 16.72411998560619 loss_input: 82.13938646740843
step: 7000 epoch: 733 loss: 16.735661243612537 loss_input: 82.21927931717065
step: 8000 epoch: 733 loss: 16.743234684759045 loss_input: 82.07873015659419
step: 9000 epoch: 733 loss: 16.76304527614239 loss_input: 82.2055384755969
step: 10000 epoch: 733 loss: 16.745680123087812 loss_input: 82.23262344786029
step: 11000 epoch: 733 loss: 16.756915848554886 loss_input: 82.28743079020515
step: 12000 epoch: 733 loss: 16.773403798388536 loss_input: 82.43402041630809
step: 13000 epoch: 733 loss: 16.773464031251024 loss_input: 82.39104872041094
step: 14000 epoch: 733 loss: 16.781943225714148 loss_input: 82.34316818469439
step: 15000 epoch: 733 loss: 16.770793372762576 loss_input: 82.38815307006877
Save loss: 16.760342376738787 Name: 733_train_model.pth
step: 0 epoch: 734 loss: 17.385196685791016 loss_input: 78.96502685546875
step: 1000 epoch: 734 loss: 16.673407537001115 loss_input: 83.20211118227476
step: 2000 epoch: 734 loss: 16.679552334418958 loss_input: 83.07612993430043
step: 3000 epoch: 734 loss: 16.68627743163295 loss_input: 82.63873687611942
step: 4000 epoch: 734 loss: 16.67396935579986 loss_input: 82.30790814564425
step: 5000 epoch: 734 loss: 16.655088369571263 loss_input: 82.10480319707543
step: 6000 epoch: 734 loss: 16.68538355283033 loss_input: 81.88094432177971
step: 7000 epoch: 734 loss: 16.689907043154896 loss_input: 81.9406713347591
step: 8000 epoch: 734 loss: 16.72698705382026 loss_input: 82.13012132333556
step: 9000 epoch: 734 loss: 16.77082728452251 loss_input: 82.23675486252395
step: 10000 epoch: 734 loss: 16.755490568277537 loss_input: 82.14855058683585
step: 11000 epoch: 734 loss: 16.738239276756993 loss_input: 82.11327187363294
step: 12000 epoch: 734 loss: 16.744250384561916 loss_input: 82.257077041879
step: 13000 epoch: 734 loss: 16.75410482738763 loss_input: 82.20158539393454
step: 14000 epoch: 734 loss: 16.74941600045939 loss_input: 82.27972979695787
step: 15000 epoch: 734 loss: 16.74954730263949 loss_input: 82.20729216196784
Save loss: 16.7678336378932 Name: 734_train_model.pth
step: 0 epoch: 735 loss: 18.16060447692871 loss_input: 98.48828125
step: 1000 epoch: 735 loss: 16.684373773656763 loss_input: 81.6714925089559
step: 2000 epoch: 735 loss: 16.771274038817154 loss_input: 82.86750493771669
step: 3000 epoch: 735 loss: 16.816487256863322 loss_input: 82.984670759161
step: 4000 epoch: 735 loss: 16.736625893835246 loss_input: 82.61918768659886
step: 5000 epoch: 735 loss: 16.781441156541412 loss_input: 82.69410092590857
step: 6000 epoch: 735 loss: 16.794186106524336 loss_input: 82.61436722012803
step: 7000 epoch: 735 loss: 16.803669803501148 loss_input: 82.48886264015854
step: 8000 epoch: 735 loss: 16.802027080077227 loss_input: 82.63324181340126
step: 9000 epoch: 735 loss: 16.801760637022472 loss_input: 82.57031089291945
step: 10000 epoch: 735 loss: 16.805591763383017 loss_input: 82.58411935295729
step: 11000 epoch: 735 loss: 16.80991139198236 loss_input: 82.62304823984397
step: 12000 epoch: 735 loss: 16.793590563057403 loss_input: 82.5123954361632
step: 13000 epoch: 735 loss: 16.78416367037811 loss_input: 82.49458376248703
step: 14000 epoch: 735 loss: 16.77580285343083 loss_input: 82.40681048068274
step: 15000 epoch: 735 loss: 16.77825682049282 loss_input: 82.37049736235032
Save loss: 16.767613267630338 Name: 735_train_model.pth
step: 0 epoch: 736 loss: 15.618782043457031 loss_input: 85.3896484375
step: 1000 epoch: 736 loss: 16.61907298891218 loss_input: 83.4031999494646
step: 2000 epoch: 736 loss: 16.617114424050182 loss_input: 82.63546937468766
step: 3000 epoch: 736 loss: 16.65952767884084 loss_input: 82.37867394624969
step: 4000 epoch: 736 loss: 16.63175702935247 loss_input: 82.36138854995724
step: 5000 epoch: 736 loss: 16.647286262352022 loss_input: 82.2269942251784
step: 6000 epoch: 736 loss: 16.64195362450063 loss_input: 82.15073506018854
step: 7000 epoch: 736 loss: 16.622405833303034 loss_input: 82.13180203295455
step: 8000 epoch: 736 loss: 16.672099012536506 loss_input: 82.28698365066427
step: 9000 epoch: 736 loss: 16.68316425255783 loss_input: 82.11846623432899
step: 10000 epoch: 736 loss: 16.668965090895735 loss_input: 82.02966601533205
step: 11000 epoch: 736 loss: 16.71015726300914 loss_input: 82.08716486644857
step: 12000 epoch: 736 loss: 16.743087000215503 loss_input: 82.16540076229813
step: 13000 epoch: 736 loss: 16.7655627058154 loss_input: 82.2679767055554
step: 14000 epoch: 736 loss: 16.76079879163989 loss_input: 82.25526487400732
step: 15000 epoch: 736 loss: 16.759099069638822 loss_input: 82.22386225683913
Save loss: 16.76631366685033 Name: 736_train_model.pth
step: 0 epoch: 737 loss: 15.764057159423828 loss_input: 72.4688720703125
step: 1000 epoch: 737 loss: 16.614055026184904 loss_input: 81.63410338440856
step: 2000 epoch: 737 loss: 16.725697339147047 loss_input: 81.90051004161005
step: 3000 epoch: 737 loss: 16.701378348270126 loss_input: 82.0204574549369
step: 4000 epoch: 737 loss: 16.72669271462204 loss_input: 81.97336214347293
step: 5000 epoch: 737 loss: 16.735892783305903 loss_input: 82.04587153858292
step: 6000 epoch: 737 loss: 16.72607515323482 loss_input: 81.93664723685376
step: 7000 epoch: 737 loss: 16.77036652243524 loss_input: 82.18787234845357
step: 8000 epoch: 737 loss: 16.75662646766246 loss_input: 82.30331201580759
step: 9000 epoch: 737 loss: 16.7485508623156 loss_input: 82.24091408192058
step: 10000 epoch: 737 loss: 16.762784515901895 loss_input: 82.32013575986151
step: 11000 epoch: 737 loss: 16.763787450600553 loss_input: 82.35525299369091
step: 12000 epoch: 737 loss: 16.753732035889367 loss_input: 82.25957235293869
step: 13000 epoch: 737 loss: 16.760123038785235 loss_input: 82.2616811599303
step: 14000 epoch: 737 loss: 16.75898417785554 loss_input: 82.22906188550706
step: 15000 epoch: 737 loss: 16.761242833680118 loss_input: 82.1552613248635
Save loss: 16.763367670416834 Name: 737_train_model.pth
step: 0 epoch: 738 loss: 16.523218154907227 loss_input: 91.08905029296875
step: 1000 epoch: 738 loss: 16.663168797126183 loss_input: 82.63533323366087
step: 2000 epoch: 738 loss: 16.743362501107235 loss_input: 82.93786142207216
step: 3000 epoch: 738 loss: 16.736207263861367 loss_input: 82.69464032008861
step: 4000 epoch: 738 loss: 16.72898462586807 loss_input: 82.44142840022417
step: 5000 epoch: 738 loss: 16.727665428828296 loss_input: 82.36566112070555
step: 6000 epoch: 738 loss: 16.758933153217622 loss_input: 82.4155816153037
step: 7000 epoch: 738 loss: 16.74339649013546 loss_input: 82.34926676385795
step: 8000 epoch: 738 loss: 16.731125235453263 loss_input: 82.35345764884858
step: 9000 epoch: 738 loss: 16.73158443775353 loss_input: 82.34777668610823
step: 10000 epoch: 738 loss: 16.714999730462324 loss_input: 82.35279436533885
step: 11000 epoch: 738 loss: 16.734185317225613 loss_input: 82.35125061651
step: 12000 epoch: 738 loss: 16.747243185002013 loss_input: 82.35335217629977
step: 13000 epoch: 738 loss: 16.746090284192316 loss_input: 82.31736593053539
step: 14000 epoch: 738 loss: 16.766748674800844 loss_input: 82.35154010169754
step: 15000 epoch: 738 loss: 16.76643054444475 loss_input: 82.3167422627554
Save loss: 16.764843794092535 Name: 738_train_model.pth
step: 0 epoch: 739 loss: 17.880260467529297 loss_input: 62.82763671875
step: 1000 epoch: 739 loss: 16.64755310902705 loss_input: 83.23946000288774
step: 2000 epoch: 739 loss: 16.836321963482295 loss_input: 82.58333437041244
step: 3000 epoch: 739 loss: 16.842517277909216 loss_input: 82.75525534864029
step: 4000 epoch: 739 loss: 16.83314244599975 loss_input: 82.37542498835741
step: 5000 epoch: 739 loss: 16.83185311742507 loss_input: 82.2371351535357
step: 6000 epoch: 739 loss: 16.83630337275737 loss_input: 82.36434996022798
step: 7000 epoch: 739 loss: 16.8133703982246 loss_input: 82.32556025931842
step: 8000 epoch: 739 loss: 16.816574210033554 loss_input: 82.34302356645117
step: 9000 epoch: 739 loss: 16.80729076568583 loss_input: 82.3091188923887
step: 10000 epoch: 739 loss: 16.809207616120787 loss_input: 82.27668338512828
step: 11000 epoch: 739 loss: 16.799919296986253 loss_input: 82.22988406970993
step: 12000 epoch: 739 loss: 16.790684225320636 loss_input: 82.21771045955317
step: 13000 epoch: 739 loss: 16.782847009652507 loss_input: 82.2638631961078
step: 14000 epoch: 739 loss: 16.779933816543195 loss_input: 82.264384461866
step: 15000 epoch: 739 loss: 16.7733137667493 loss_input: 82.2653926180312
Save loss: 16.773940340459347 Name: 739_train_model.pth
step: 0 epoch: 740 loss: 14.308904647827148 loss_input: 57.644287109375
step: 1000 epoch: 740 loss: 16.558879733919262 loss_input: 82.45379132586164
step: 2000 epoch: 740 loss: 16.682992725119718 loss_input: 82.83162889380505
step: 3000 epoch: 740 loss: 16.626010610992928 loss_input: 82.61284928542699
step: 4000 epoch: 740 loss: 16.658554301920965 loss_input: 82.62946824329075
step: 5000 epoch: 740 loss: 16.658121938539537 loss_input: 82.54724931478547
step: 6000 epoch: 740 loss: 16.6816778138486 loss_input: 82.47781079658924
step: 7000 epoch: 740 loss: 16.701923095878577 loss_input: 82.672606584941
step: 8000 epoch: 740 loss: 16.736749232880637 loss_input: 82.54201361579071
step: 9000 epoch: 740 loss: 16.754175143034747 loss_input: 82.6266416226105
step: 10000 epoch: 740 loss: 16.756747571578824 loss_input: 82.49613175889442
step: 11000 epoch: 740 loss: 16.755877451705516 loss_input: 82.44394894320686
step: 12000 epoch: 740 loss: 16.76846004962802 loss_input: 82.39026841147027
step: 13000 epoch: 740 loss: 16.768656186127366 loss_input: 82.296772717641
step: 14000 epoch: 740 loss: 16.770409571324986 loss_input: 82.30098174052718
step: 15000 epoch: 740 loss: 16.771096561870674 loss_input: 82.25623594494742
Save loss: 16.764809469014406 Name: 740_train_model.pth
step: 0 epoch: 741 loss: 26.5551700592041 loss_input: 148.5933837890625
step: 1000 epoch: 741 loss: 16.711952704411523 loss_input: 82.43399233822818
step: 2000 epoch: 741 loss: 16.66156703683509 loss_input: 82.03992861357408
step: 3000 epoch: 741 loss: 16.66131264152069 loss_input: 81.90021465826496
step: 4000 epoch: 741 loss: 16.705002081927525 loss_input: 82.15583241030562
step: 5000 epoch: 741 loss: 16.665084211761013 loss_input: 82.0865568097318
step: 6000 epoch: 741 loss: 16.6961012525452 loss_input: 82.09455034268377
step: 7000 epoch: 741 loss: 16.714438423363248 loss_input: 82.08022485871159
step: 8000 epoch: 741 loss: 16.7240046540613 loss_input: 82.15533425143146
step: 9000 epoch: 741 loss: 16.755871114750967 loss_input: 82.1945659805703
step: 10000 epoch: 741 loss: 16.764274260030128 loss_input: 82.16190065563757
step: 11000 epoch: 741 loss: 16.751876619921717 loss_input: 82.31509890147593
step: 12000 epoch: 741 loss: 16.75537283783525 loss_input: 82.30934523868616
step: 13000 epoch: 741 loss: 16.756350992459645 loss_input: 82.37090714232168
step: 14000 epoch: 741 loss: 16.768115936317713 loss_input: 82.37532921319655
step: 15000 epoch: 741 loss: 16.766655675586595 loss_input: 82.28865126032613
Save loss: 16.757207128465176 Name: 741_train_model.pth
step: 0 epoch: 742 loss: 17.15358543395996 loss_input: 89.40362548828125
step: 1000 epoch: 742 loss: 16.406606290247534 loss_input: 80.8060516753754
step: 2000 epoch: 742 loss: 16.63448179870293 loss_input: 81.70576973476153
step: 3000 epoch: 742 loss: 16.64521590696498 loss_input: 81.30634521158963
step: 4000 epoch: 742 loss: 16.638787140520893 loss_input: 81.42501043135212
step: 5000 epoch: 742 loss: 16.663794890996623 loss_input: 81.56435232728845
step: 6000 epoch: 742 loss: 16.67178652267062 loss_input: 81.85210093361718
step: 7000 epoch: 742 loss: 16.679798988082922 loss_input: 81.79672158825245
step: 8000 epoch: 742 loss: 16.67214053178665 loss_input: 81.98196975256023
step: 9000 epoch: 742 loss: 16.671073507910343 loss_input: 82.03960887348238
step: 10000 epoch: 742 loss: 16.694582630116848 loss_input: 82.09868319808645
step: 11000 epoch: 742 loss: 16.697852993301886 loss_input: 82.10722593112096
step: 12000 epoch: 742 loss: 16.708970735355393 loss_input: 82.16183545678648
step: 13000 epoch: 742 loss: 16.7212546412353 loss_input: 82.16893512041291
step: 14000 epoch: 742 loss: 16.744132687846097 loss_input: 82.27952741447598
step: 15000 epoch: 742 loss: 16.756027340308865 loss_input: 82.25271763295207
Save loss: 16.75840202997625 Name: 742_train_model.pth
step: 0 epoch: 743 loss: 15.862051010131836 loss_input: 56.77239990234375
step: 1000 epoch: 743 loss: 16.583019390925543 loss_input: 82.25884046373548
step: 2000 epoch: 743 loss: 16.657922644784367 loss_input: 81.47781553559157
step: 3000 epoch: 743 loss: 16.702713465460217 loss_input: 82.00253585718824
step: 4000 epoch: 743 loss: 16.69731180437265 loss_input: 81.92337354199049
step: 5000 epoch: 743 loss: 16.715681673645662 loss_input: 82.04552206013875
step: 6000 epoch: 743 loss: 16.66324194672306 loss_input: 81.86887248121407
step: 7000 epoch: 743 loss: 16.657005442021454 loss_input: 81.90655461782525
step: 8000 epoch: 743 loss: 16.67452648293598 loss_input: 81.82721759694589
step: 9000 epoch: 743 loss: 16.705939686757937 loss_input: 82.00693474922058
step: 10000 epoch: 743 loss: 16.708974736891392 loss_input: 81.98601514970574
step: 11000 epoch: 743 loss: 16.739678012794847 loss_input: 82.16023574938677
step: 12000 epoch: 743 loss: 16.747512177918 loss_input: 82.29363392182324
step: 13000 epoch: 743 loss: 16.759899811619622 loss_input: 82.29215981470476
step: 14000 epoch: 743 loss: 16.747875538360017 loss_input: 82.30189903776404
step: 15000 epoch: 743 loss: 16.743626173206444 loss_input: 82.25169394224629
Save loss: 16.757769617795944 Name: 743_train_model.pth
step: 0 epoch: 744 loss: 23.9580020904541 loss_input: 115.40576171875
step: 1000 epoch: 744 loss: 16.632926934010737 loss_input: 82.61059149590643
step: 2000 epoch: 744 loss: 16.655570233720116 loss_input: 81.96120085709099
step: 3000 epoch: 744 loss: 16.686474492175705 loss_input: 82.131509894651
step: 4000 epoch: 744 loss: 16.72165311667002 loss_input: 82.27490582188437
step: 5000 epoch: 744 loss: 16.702368505524053 loss_input: 82.21859835796512
step: 6000 epoch: 744 loss: 16.709657226159482 loss_input: 82.02986628896811
step: 7000 epoch: 744 loss: 16.689083308326296 loss_input: 82.05590004301841
step: 8000 epoch: 744 loss: 16.711270769243463 loss_input: 82.04649394411636
step: 9000 epoch: 744 loss: 16.754526376167995 loss_input: 82.13279437457253
step: 10000 epoch: 744 loss: 16.754380372128193 loss_input: 82.16953841114912
step: 11000 epoch: 744 loss: 16.740574661877663 loss_input: 82.24059545390227
step: 12000 epoch: 744 loss: 16.74484715772841 loss_input: 82.19456773479325
step: 13000 epoch: 744 loss: 16.728781852306984 loss_input: 82.24077965870113
step: 14000 epoch: 744 loss: 16.743210806470966 loss_input: 82.17192503653477
step: 15000 epoch: 744 loss: 16.753114570817043 loss_input: 82.24133476550782
Save loss: 16.758605498716236 Name: 744_train_model.pth
step: 0 epoch: 745 loss: 23.879117965698242 loss_input: 153.763916015625
step: 1000 epoch: 745 loss: 16.648683332658553 loss_input: 83.67979274143825
step: 2000 epoch: 745 loss: 16.55041409110737 loss_input: 82.38771840276151
step: 3000 epoch: 745 loss: 16.586113267245192 loss_input: 82.12600182263465
step: 4000 epoch: 745 loss: 16.64371397488715 loss_input: 82.18449559363088
step: 5000 epoch: 745 loss: 16.666632142407348 loss_input: 82.3428390859914
step: 6000 epoch: 745 loss: 16.65884204360569 loss_input: 82.23840173366288
step: 7000 epoch: 745 loss: 16.674257922863863 loss_input: 82.2903802874701
step: 8000 epoch: 745 loss: 16.654633869962115 loss_input: 82.23210316636445
step: 9000 epoch: 745 loss: 16.68105885919207 loss_input: 82.23067506659416
step: 10000 epoch: 745 loss: 16.691012610746448 loss_input: 82.17208321282129
step: 11000 epoch: 745 loss: 16.69020607185173 loss_input: 82.13054977734623
step: 12000 epoch: 745 loss: 16.698791663593653 loss_input: 82.1060735885128
step: 13000 epoch: 745 loss: 16.703848799598482 loss_input: 82.0671213392239
step: 14000 epoch: 745 loss: 16.73016224703732 loss_input: 82.1136238115037
step: 15000 epoch: 745 loss: 16.766188924006325 loss_input: 82.28051337918617
Save loss: 16.764547635138033 Name: 745_train_model.pth
step: 0 epoch: 746 loss: 15.978996276855469 loss_input: 92.27703857421875
step: 1000 epoch: 746 loss: 16.518986775324894 loss_input: 82.19173057167441
step: 2000 epoch: 746 loss: 16.64688330587895 loss_input: 82.62886324171899
step: 3000 epoch: 746 loss: 16.641180318738968 loss_input: 82.90728680446361
step: 4000 epoch: 746 loss: 16.628550272052273 loss_input: 82.58833797715123
step: 5000 epoch: 746 loss: 16.66857001982172 loss_input: 82.42458877609243
step: 6000 epoch: 746 loss: 16.66801215529223 loss_input: 82.31273899418456
step: 7000 epoch: 746 loss: 16.69473348503674 loss_input: 82.26305568233556
step: 8000 epoch: 746 loss: 16.71020677563191 loss_input: 82.35478010011933
step: 9000 epoch: 746 loss: 16.70011551988587 loss_input: 82.22869317010567
step: 10000 epoch: 746 loss: 16.71502356349963 loss_input: 82.25786325786366
step: 11000 epoch: 746 loss: 16.70332847928017 loss_input: 82.21967794964307
step: 12000 epoch: 746 loss: 16.72510917128528 loss_input: 82.29733082342422
step: 13000 epoch: 746 loss: 16.72871820318379 loss_input: 82.26933517790549
step: 14000 epoch: 746 loss: 16.729465136484762 loss_input: 82.29603384740096
step: 15000 epoch: 746 loss: 16.74420271059027 loss_input: 82.28487339532182
Save loss: 16.748378638207914 Name: 746_train_model.pth
step: 0 epoch: 747 loss: 11.590753555297852 loss_input: 40.10333251953125
step: 1000 epoch: 747 loss: 16.6421882458381 loss_input: 82.42979450968953
step: 2000 epoch: 747 loss: 16.614560861101392 loss_input: 82.2032426474751
step: 3000 epoch: 747 loss: 16.5774179849971 loss_input: 82.20783813029121
step: 4000 epoch: 747 loss: 16.61006825490464 loss_input: 82.16192527795785
step: 5000 epoch: 747 loss: 16.59874485669387 loss_input: 81.71301981058866
step: 6000 epoch: 747 loss: 16.58528839089715 loss_input: 81.68869416742717
step: 7000 epoch: 747 loss: 16.61322538499542 loss_input: 81.7058177044044
step: 8000 epoch: 747 loss: 16.671822151114473 loss_input: 81.75793516533089
step: 9000 epoch: 747 loss: 16.685232886075045 loss_input: 81.83417386132443
step: 10000 epoch: 747 loss: 16.702303416346826 loss_input: 81.91720843174471
step: 11000 epoch: 747 loss: 16.72442895973198 loss_input: 82.02500153905575
step: 12000 epoch: 747 loss: 16.72776798168507 loss_input: 81.99566687517252
step: 13000 epoch: 747 loss: 16.734052168370283 loss_input: 81.96080604038572
step: 14000 epoch: 747 loss: 16.756635533819097 loss_input: 82.18538817749067
step: 15000 epoch: 747 loss: 16.763566666464435 loss_input: 82.20451817290957
Save loss: 16.752370595157146 Name: 747_train_model.pth
step: 0 epoch: 748 loss: 14.279224395751953 loss_input: 56.9903564453125
step: 1000 epoch: 748 loss: 16.5806219399154 loss_input: 82.67294003603818
step: 2000 epoch: 748 loss: 16.69791700028587 loss_input: 82.48547030805886
step: 3000 epoch: 748 loss: 16.644497787185767 loss_input: 82.09451272590127
step: 4000 epoch: 748 loss: 16.747282718605057 loss_input: 82.26734725620352
step: 5000 epoch: 748 loss: 16.725133921999475 loss_input: 82.04155038132998
step: 6000 epoch: 748 loss: 16.700411863077523 loss_input: 82.01871480689091
step: 7000 epoch: 748 loss: 16.717866757617237 loss_input: 82.11482886303631
step: 8000 epoch: 748 loss: 16.72131648538649 loss_input: 82.02633502599285
step: 9000 epoch: 748 loss: 16.71300354958534 loss_input: 82.02248418253006
step: 10000 epoch: 748 loss: 16.705499513019813 loss_input: 81.88634976224057
step: 11000 epoch: 748 loss: 16.718055657089522 loss_input: 81.9343543509095
step: 12000 epoch: 748 loss: 16.724216072552007 loss_input: 82.00698489664435
step: 13000 epoch: 748 loss: 16.71979097725474 loss_input: 82.07177733905534
step: 14000 epoch: 748 loss: 16.725904605991627 loss_input: 82.0696957881566
step: 15000 epoch: 748 loss: 16.735179430198276 loss_input: 82.05558019764828
Save loss: 16.75584792688489 Name: 748_train_model.pth
step: 0 epoch: 749 loss: 14.159486770629883 loss_input: 53.66064453125
step: 1000 epoch: 749 loss: 16.712620939527238 loss_input: 82.12165587098448
step: 2000 epoch: 749 loss: 16.694081484228892 loss_input: 82.04401699296837
step: 3000 epoch: 749 loss: 16.667536062544404 loss_input: 81.69535954242465
step: 4000 epoch: 749 loss: 16.683220262260505 loss_input: 81.8622787845519
step: 5000 epoch: 749 loss: 16.6684588779094 loss_input: 82.07256395400607
step: 6000 epoch: 749 loss: 16.64913668753286 loss_input: 81.92370625810412
step: 7000 epoch: 749 loss: 16.691498595703468 loss_input: 81.9861589945856
step: 8000 epoch: 749 loss: 16.694305349805774 loss_input: 82.04097215051488
step: 9000 epoch: 749 loss: 16.713284578764124 loss_input: 81.99160841530845
step: 10000 epoch: 749 loss: 16.71615569616077 loss_input: 81.98895387560853
step: 11000 epoch: 749 loss: 16.706787714036246 loss_input: 82.00172146770393
step: 12000 epoch: 749 loss: 16.722332903826477 loss_input: 82.16479638659669
step: 13000 epoch: 749 loss: 16.739228315128198 loss_input: 82.2286508309457
step: 14000 epoch: 749 loss: 16.72335673437724 loss_input: 82.1546433424747
step: 15000 epoch: 749 loss: 16.735965494807264 loss_input: 82.17335370594434
Save loss: 16.75322727161646 Name: 749_train_model.pth
step: 0 epoch: 750 loss: 13.411048889160156 loss_input: 94.27850341796875
step: 1000 epoch: 750 loss: 16.549941037441943 loss_input: 81.48645830487871
step: 2000 epoch: 750 loss: 16.460724011711452 loss_input: 81.53925335305979
step: 3000 epoch: 750 loss: 16.600382818932616 loss_input: 81.93545171222223
step: 4000 epoch: 750 loss: 16.593726972912705 loss_input: 81.77118653686188
step: 5000 epoch: 750 loss: 16.6115005883997 loss_input: 81.90853375955668
step: 6000 epoch: 750 loss: 16.68176343313477 loss_input: 82.03557275569632
step: 7000 epoch: 750 loss: 16.671772542910446 loss_input: 82.18915710318448
step: 8000 epoch: 750 loss: 16.691192918055624 loss_input: 82.23974566655731
step: 9000 epoch: 750 loss: 16.691785933454625 loss_input: 82.16995342667003
step: 10000 epoch: 750 loss: 16.706269795698425 loss_input: 82.17676771141245
step: 11000 epoch: 750 loss: 16.727911186460993 loss_input: 82.25295167480336
step: 12000 epoch: 750 loss: 16.72266966604648 loss_input: 82.166310466356
step: 13000 epoch: 750 loss: 16.74063680692963 loss_input: 82.28233068046748
step: 14000 epoch: 750 loss: 16.748076495166575 loss_input: 82.27881487556274
step: 15000 epoch: 750 loss: 16.749133064749877 loss_input: 82.25494215506076
Save loss: 16.749467808693648 Name: 750_train_model.pth
step: 0 epoch: 751 loss: 12.54931354522705 loss_input: 68.49200439453125
step: 1000 epoch: 751 loss: 16.91580668791429 loss_input: 82.66149292601929
step: 2000 epoch: 751 loss: 16.713928746676697 loss_input: 82.24908413713065
step: 3000 epoch: 751 loss: 16.651434453635645 loss_input: 81.97695954375885
step: 4000 epoch: 751 loss: 16.667840661659326 loss_input: 82.28040284545563
step: 5000 epoch: 751 loss: 16.66676053603252 loss_input: 82.1942768428736
step: 6000 epoch: 751 loss: 16.686217061282594 loss_input: 82.3352445714614
step: 7000 epoch: 751 loss: 16.6871778772586 loss_input: 82.11111766225082
step: 8000 epoch: 751 loss: 16.659676053377705 loss_input: 82.08695892536139
step: 9000 epoch: 751 loss: 16.679821064705028 loss_input: 82.20045820239808
step: 10000 epoch: 751 loss: 16.678115147064357 loss_input: 82.13721572789011
step: 11000 epoch: 751 loss: 16.69432563040974 loss_input: 82.19380863058969
step: 12000 epoch: 751 loss: 16.70584035392563 loss_input: 82.27092497846046
step: 13000 epoch: 751 loss: 16.72121824848497 loss_input: 82.23040437694696
step: 14000 epoch: 751 loss: 16.7264892502722 loss_input: 82.21152923716944
step: 15000 epoch: 751 loss: 16.74453753327125 loss_input: 82.21681666941605
Save loss: 16.756076425224542 Name: 751_train_model.pth
step: 0 epoch: 752 loss: 10.54335880279541 loss_input: 62.350830078125
step: 1000 epoch: 752 loss: 16.60384244328136 loss_input: 82.64504849398648
step: 2000 epoch: 752 loss: 16.615028870814683 loss_input: 82.47792375784763
step: 3000 epoch: 752 loss: 16.615377099463956 loss_input: 82.2978038082358
step: 4000 epoch: 752 loss: 16.646534610289212 loss_input: 82.45812356725271
step: 5000 epoch: 752 loss: 16.627161936720857 loss_input: 82.4516535779758
step: 6000 epoch: 752 loss: 16.609079257068466 loss_input: 82.22242245311003
step: 7000 epoch: 752 loss: 16.65679549724779 loss_input: 82.23854728168018
step: 8000 epoch: 752 loss: 16.698490382313594 loss_input: 82.27448396953311
step: 9000 epoch: 752 loss: 16.725879858181404 loss_input: 82.27917281787484
step: 10000 epoch: 752 loss: 16.725801573718933 loss_input: 82.21387206982426
step: 11000 epoch: 752 loss: 16.732473429046255 loss_input: 82.16984439513844
step: 12000 epoch: 752 loss: 16.750998538470785 loss_input: 82.14312475608315
step: 13000 epoch: 752 loss: 16.76011586574378 loss_input: 82.2710915782545
step: 14000 epoch: 752 loss: 16.771850413658324 loss_input: 82.2921355357639
step: 15000 epoch: 752 loss: 16.767629359564886 loss_input: 82.29100986366534
Save loss: 16.76978761409223 Name: 752_train_model.pth
step: 0 epoch: 753 loss: 19.93393325805664 loss_input: 67.50811767578125
step: 1000 epoch: 753 loss: 16.70010726363747 loss_input: 82.80257565920408
step: 2000 epoch: 753 loss: 16.772549975103047 loss_input: 83.15119789958537
step: 3000 epoch: 753 loss: 16.731834813540317 loss_input: 82.96831634735037
step: 4000 epoch: 753 loss: 16.719651787914476 loss_input: 82.65683859796769
step: 5000 epoch: 753 loss: 16.731860066814153 loss_input: 82.49678258928293
step: 6000 epoch: 753 loss: 16.759154976417452 loss_input: 82.75826268131743
step: 7000 epoch: 753 loss: 16.701913771501967 loss_input: 82.50790179039713
step: 8000 epoch: 753 loss: 16.70854702795644 loss_input: 82.49959239714177
step: 9000 epoch: 753 loss: 16.704904001085723 loss_input: 82.30587266426777
step: 10000 epoch: 753 loss: 16.714569846363904 loss_input: 82.2502037515975
step: 11000 epoch: 753 loss: 16.73326575289032 loss_input: 82.21026660151811
step: 12000 epoch: 753 loss: 16.72195720017011 loss_input: 82.15915117628543
step: 13000 epoch: 753 loss: 16.73820040481071 loss_input: 82.29491539168711
step: 14000 epoch: 753 loss: 16.733114670995356 loss_input: 82.30069821324146
step: 15000 epoch: 753 loss: 16.751806628361567 loss_input: 82.27590628886459
Save loss: 16.753617525100708 Name: 753_train_model.pth
step: 0 epoch: 754 loss: 20.86653709411621 loss_input: 81.9229736328125
step: 1000 epoch: 754 loss: 16.884809492827653 loss_input: 84.03366829703499
step: 2000 epoch: 754 loss: 16.77822042095369 loss_input: 83.62145941189561
step: 3000 epoch: 754 loss: 16.787058327047873 loss_input: 83.23335086628025
step: 4000 epoch: 754 loss: 16.7412762535837 loss_input: 82.83494252474479
step: 5000 epoch: 754 loss: 16.728586334963843 loss_input: 82.68069464720337
step: 6000 epoch: 754 loss: 16.762619160549022 loss_input: 82.59117656619405
step: 7000 epoch: 754 loss: 16.777189890838763 loss_input: 82.56194297682642
step: 8000 epoch: 754 loss: 16.768036011561293 loss_input: 82.46487250922247
step: 9000 epoch: 754 loss: 16.761517662933674 loss_input: 82.2965536991658
step: 10000 epoch: 754 loss: 16.737403744996136 loss_input: 82.154753695773
step: 11000 epoch: 754 loss: 16.74531202814317 loss_input: 82.1833183677638
step: 12000 epoch: 754 loss: 16.739557453875005 loss_input: 82.11218795731867
step: 13000 epoch: 754 loss: 16.752792861991438 loss_input: 82.22973198557293
step: 14000 epoch: 754 loss: 16.75225376640079 loss_input: 82.26448485316757
step: 15000 epoch: 754 loss: 16.751757822706814 loss_input: 82.21545924444881
Save loss: 16.75262454240024 Name: 754_train_model.pth
step: 0 epoch: 755 loss: 7.965243816375732 loss_input: 48.15252685546875
step: 1000 epoch: 755 loss: 16.608199260570668 loss_input: 81.55439403126171
step: 2000 epoch: 755 loss: 16.622260197587515 loss_input: 82.07107902347654
step: 3000 epoch: 755 loss: 16.6671599735621 loss_input: 82.65796549262225
step: 4000 epoch: 755 loss: 16.653362600304607 loss_input: 82.32224121703949
step: 5000 epoch: 755 loss: 16.66592540633223 loss_input: 82.20006198955521
step: 6000 epoch: 755 loss: 16.703437017055574 loss_input: 82.22704528900131
step: 7000 epoch: 755 loss: 16.704558747816964 loss_input: 82.17294877761603
step: 8000 epoch: 755 loss: 16.73912303198309 loss_input: 82.35551537503125
step: 9000 epoch: 755 loss: 16.713498581807993 loss_input: 82.17070150571378
step: 10000 epoch: 755 loss: 16.728684453365386 loss_input: 82.11907878426024
step: 11000 epoch: 755 loss: 16.727266617552083 loss_input: 82.13618209446336
step: 12000 epoch: 755 loss: 16.717526565084018 loss_input: 82.18728603365898
step: 13000 epoch: 755 loss: 16.724827167758775 loss_input: 82.1718257671961
step: 14000 epoch: 755 loss: 16.724237957832415 loss_input: 82.15007843503645
step: 15000 epoch: 755 loss: 16.731505694862015 loss_input: 82.17274871291133
Save loss: 16.75230978116393 Name: 755_train_model.pth
step: 0 epoch: 756 loss: 8.258190155029297 loss_input: 61.10589599609375
step: 1000 epoch: 756 loss: 16.94763155583735 loss_input: 84.60398427256338
step: 2000 epoch: 756 loss: 16.78169210954406 loss_input: 83.08985384627023
step: 3000 epoch: 756 loss: 16.789155097772024 loss_input: 83.19683892803961
step: 4000 epoch: 756 loss: 16.737175883307692 loss_input: 82.61437452831616
step: 5000 epoch: 756 loss: 16.74263136025978 loss_input: 82.52744203356595
step: 6000 epoch: 756 loss: 16.726919284245906 loss_input: 82.26482328425406
step: 7000 epoch: 756 loss: 16.714244803911004 loss_input: 82.29353932976093
step: 8000 epoch: 756 loss: 16.708335945776383 loss_input: 82.18571882768805
step: 9000 epoch: 756 loss: 16.71824959275087 loss_input: 82.2357403403745
step: 10000 epoch: 756 loss: 16.701925982309454 loss_input: 82.085601040726
step: 11000 epoch: 756 loss: 16.729042477612495 loss_input: 82.26268225216387
step: 12000 epoch: 756 loss: 16.715341883950924 loss_input: 82.21692191035676
step: 13000 epoch: 756 loss: 16.724680832061168 loss_input: 82.18763873162851
step: 14000 epoch: 756 loss: 16.726258850056787 loss_input: 82.26203765335121
step: 15000 epoch: 756 loss: 16.74823780687926 loss_input: 82.24740853458077
Save loss: 16.753089288011193 Name: 756_train_model.pth
step: 0 epoch: 757 loss: 20.565692901611328 loss_input: 85.0084228515625
step: 1000 epoch: 757 loss: 16.789956873113457 loss_input: 83.35952895980972
step: 2000 epoch: 757 loss: 16.88404467914892 loss_input: 83.39369088038988
step: 3000 epoch: 757 loss: 16.86248830396785 loss_input: 83.21226377909838
step: 4000 epoch: 757 loss: 16.803043519577603 loss_input: 82.97538010211802
step: 5000 epoch: 757 loss: 16.787225480700368 loss_input: 82.80764193840919
step: 6000 epoch: 757 loss: 16.73315899730861 loss_input: 82.63460122368929
step: 7000 epoch: 757 loss: 16.756336611009566 loss_input: 82.75777388770075
step: 8000 epoch: 757 loss: 16.747130896386288 loss_input: 82.62226862145161
step: 9000 epoch: 757 loss: 16.743251988390078 loss_input: 82.41700641389238
step: 10000 epoch: 757 loss: 16.7675842476921 loss_input: 82.4503042310515
step: 11000 epoch: 757 loss: 16.751511063188676 loss_input: 82.39601871909016
step: 12000 epoch: 757 loss: 16.767155805415644 loss_input: 82.42993778431875
step: 13000 epoch: 757 loss: 16.76571776470618 loss_input: 82.38023923039574
step: 14000 epoch: 757 loss: 16.749050113288227 loss_input: 82.33615421164113
step: 15000 epoch: 757 loss: 16.75342152576575 loss_input: 82.28356736666774
Save loss: 16.75277720142901 Name: 757_train_model.pth
step: 0 epoch: 758 loss: 18.702255249023438 loss_input: 78.5838623046875
step: 1000 epoch: 758 loss: 16.681720996117377 loss_input: 81.81190757484703
step: 2000 epoch: 758 loss: 16.632779546525107 loss_input: 82.11780386052091
step: 3000 epoch: 758 loss: 16.622793042552505 loss_input: 82.1096245709438
step: 4000 epoch: 758 loss: 16.698651904316133 loss_input: 82.03988965527174
step: 5000 epoch: 758 loss: 16.705701045574273 loss_input: 82.05929760092903
step: 6000 epoch: 758 loss: 16.710445973619265 loss_input: 81.92317583910169
step: 7000 epoch: 758 loss: 16.738012152354287 loss_input: 82.18744316694992
step: 8000 epoch: 758 loss: 16.764597416937224 loss_input: 82.31670920590493
step: 9000 epoch: 758 loss: 16.77172706853203 loss_input: 82.43854427009195
step: 10000 epoch: 758 loss: 16.73478833857375 loss_input: 82.22477855363877
step: 11000 epoch: 758 loss: 16.748545034464225 loss_input: 82.20014202255584
step: 12000 epoch: 758 loss: 16.753147538190444 loss_input: 82.27574056125205
step: 13000 epoch: 758 loss: 16.756320156122058 loss_input: 82.30399570563382
step: 14000 epoch: 758 loss: 16.751121745570355 loss_input: 82.26954147701782
step: 15000 epoch: 758 loss: 16.747353034626286 loss_input: 82.21164806867694
Save loss: 16.749683805167674 Name: 758_train_model.pth
step: 0 epoch: 759 loss: 15.505271911621094 loss_input: 101.98974609375
step: 1000 epoch: 759 loss: 16.540398519117755 loss_input: 81.47816282838255
step: 2000 epoch: 759 loss: 16.604386735713106 loss_input: 81.77232093181925
step: 3000 epoch: 759 loss: 16.647296269787663 loss_input: 81.5813354980306
step: 4000 epoch: 759 loss: 16.672416338888414 loss_input: 81.75624468516659
step: 5000 epoch: 759 loss: 16.693458722367616 loss_input: 81.88542617404254
step: 6000 epoch: 759 loss: 16.696625288636103 loss_input: 81.92056054215574
step: 7000 epoch: 759 loss: 16.70302177221328 loss_input: 82.16421677483574
step: 8000 epoch: 759 loss: 16.70495513975136 loss_input: 82.048007236095
step: 9000 epoch: 759 loss: 16.69899528829962 loss_input: 81.95278533640577
step: 10000 epoch: 759 loss: 16.702977352888034 loss_input: 81.99153444030597
step: 11000 epoch: 759 loss: 16.708745619804468 loss_input: 82.07031589546547
step: 12000 epoch: 759 loss: 16.72297840676261 loss_input: 82.17894676786375
step: 13000 epoch: 759 loss: 16.7277693525845 loss_input: 82.20036584099901
step: 14000 epoch: 759 loss: 16.720076606134185 loss_input: 82.19036771072574
step: 15000 epoch: 759 loss: 16.74374728518783 loss_input: 82.24192854916443
Save loss: 16.749275965481996 Name: 759_train_model.pth
step: 0 epoch: 760 loss: 17.578758239746094 loss_input: 76.136474609375
step: 1000 epoch: 760 loss: 16.723302153321534 loss_input: 82.81985629116977
step: 2000 epoch: 760 loss: 16.690366181774415 loss_input: 81.9638306457123
step: 3000 epoch: 760 loss: 16.688488384518216 loss_input: 82.07739627969062
step: 4000 epoch: 760 loss: 16.69291566670224 loss_input: 82.20871957913842
step: 5000 epoch: 760 loss: 16.67745320436073 loss_input: 82.16071238877224
step: 6000 epoch: 760 loss: 16.666576228366655 loss_input: 81.96943643954849
step: 7000 epoch: 760 loss: 16.69878845534279 loss_input: 82.07419771331494
step: 8000 epoch: 760 loss: 16.70065295986795 loss_input: 82.06529609824163
step: 9000 epoch: 760 loss: 16.731864680556267 loss_input: 82.23777015769949
step: 10000 epoch: 760 loss: 16.721584688793502 loss_input: 82.19235883418494
step: 11000 epoch: 760 loss: 16.73502404535264 loss_input: 82.22961279865005
step: 12000 epoch: 760 loss: 16.71030202409385 loss_input: 82.12795713068833
step: 13000 epoch: 760 loss: 16.716665321804452 loss_input: 82.19751869954565
step: 14000 epoch: 760 loss: 16.74601232223056 loss_input: 82.3147675077129
step: 15000 epoch: 760 loss: 16.74821251054246 loss_input: 82.22596938768535
Save loss: 16.752268531531094 Name: 760_train_model.pth
step: 0 epoch: 761 loss: 21.133691787719727 loss_input: 95.34796142578125
step: 1000 epoch: 761 loss: 16.515010994273823 loss_input: 82.47844971190918
step: 2000 epoch: 761 loss: 16.64614578606426 loss_input: 82.74593059305309
step: 3000 epoch: 761 loss: 16.73273267892153 loss_input: 82.65861066330754
step: 4000 epoch: 761 loss: 16.72171837781197 loss_input: 82.8008722306728
step: 5000 epoch: 761 loss: 16.686228459273735 loss_input: 82.71888789966616
step: 6000 epoch: 761 loss: 16.679909405957023 loss_input: 82.45293001500076
step: 7000 epoch: 761 loss: 16.713782900146988 loss_input: 82.59289290111178
step: 8000 epoch: 761 loss: 16.738835988529264 loss_input: 82.42585245386807
step: 9000 epoch: 761 loss: 16.752172244971174 loss_input: 82.38574476425363
step: 10000 epoch: 761 loss: 16.748081981891417 loss_input: 82.22711472138919
step: 11000 epoch: 761 loss: 16.74427804428061 loss_input: 82.28669334170971
step: 12000 epoch: 761 loss: 16.751227613052084 loss_input: 82.2755289903413
step: 13000 epoch: 761 loss: 16.760470136570277 loss_input: 82.20233376823327
step: 14000 epoch: 761 loss: 16.757790082165638 loss_input: 82.23608470802587
step: 15000 epoch: 761 loss: 16.751771545197183 loss_input: 82.23652055195018
Save loss: 16.75333205021918 Name: 761_train_model.pth
step: 0 epoch: 762 loss: 11.537388801574707 loss_input: 59.60858154296875
step: 1000 epoch: 762 loss: 16.617465301708027 loss_input: 81.7100330089832
step: 2000 epoch: 762 loss: 16.720705793000413 loss_input: 82.46722308401463
step: 3000 epoch: 762 loss: 16.708351002340116 loss_input: 82.27095564092568
step: 4000 epoch: 762 loss: 16.73935817480385 loss_input: 82.28949566502418
step: 5000 epoch: 762 loss: 16.734238856602992 loss_input: 82.37168306084877
step: 6000 epoch: 762 loss: 16.727940825616653 loss_input: 82.28026542288525
step: 7000 epoch: 762 loss: 16.744745306586594 loss_input: 82.43490919231125
step: 8000 epoch: 762 loss: 16.746487253085625 loss_input: 82.334640283612
step: 9000 epoch: 762 loss: 16.728860927441296 loss_input: 82.23566263769087
step: 10000 epoch: 762 loss: 16.725890126422385 loss_input: 82.12188374058103
step: 11000 epoch: 762 loss: 16.72883350711965 loss_input: 82.34232691714638
step: 12000 epoch: 762 loss: 16.732162881181615 loss_input: 82.38903868098069
step: 13000 epoch: 762 loss: 16.751752135387047 loss_input: 82.40735327112758
step: 14000 epoch: 762 loss: 16.744212661825856 loss_input: 82.29003329508969
step: 15000 epoch: 762 loss: 16.757796680058696 loss_input: 82.25186248162883
Save loss: 16.755532731309533 Name: 762_train_model.pth
step: 0 epoch: 763 loss: 18.566160202026367 loss_input: 136.26422119140625
step: 1000 epoch: 763 loss: 16.63450264906907 loss_input: 82.23743657465582
step: 2000 epoch: 763 loss: 16.681398266854732 loss_input: 82.55439641629381
step: 3000 epoch: 763 loss: 16.725910860790645 loss_input: 81.94546816913893
step: 4000 epoch: 763 loss: 16.792042139678323 loss_input: 82.4442201674923
step: 5000 epoch: 763 loss: 16.748370879508332 loss_input: 82.1391253121446
step: 6000 epoch: 763 loss: 16.760807368541197 loss_input: 82.47941149212761
step: 7000 epoch: 763 loss: 16.762304501709913 loss_input: 82.42419341182287
step: 8000 epoch: 763 loss: 16.748834625182635 loss_input: 82.33727990727233
step: 9000 epoch: 763 loss: 16.759404927276925 loss_input: 82.33587826179036
step: 10000 epoch: 763 loss: 16.78717143220218 loss_input: 82.421152763874
step: 11000 epoch: 763 loss: 16.78773389995212 loss_input: 82.52149378465595
step: 12000 epoch: 763 loss: 16.78220107368445 loss_input: 82.48685991560437
step: 13000 epoch: 763 loss: 16.78009450053281 loss_input: 82.41845801243204
step: 14000 epoch: 763 loss: 16.768934194653912 loss_input: 82.328787127475
step: 15000 epoch: 763 loss: 16.76518428585512 loss_input: 82.33370539783024
Save loss: 16.754727435335518 Name: 763_train_model.pth
step: 0 epoch: 764 loss: 13.732287406921387 loss_input: 64.07464599609375
step: 1000 epoch: 764 loss: 16.739175553088423 loss_input: 80.85136348026973
step: 2000 epoch: 764 loss: 16.690275061196058 loss_input: 81.36902090336667
step: 3000 epoch: 764 loss: 16.692948805972044 loss_input: 81.6657295265185
step: 4000 epoch: 764 loss: 16.77649793968115 loss_input: 82.40976675997017
step: 5000 epoch: 764 loss: 16.785812480047785 loss_input: 82.33435560202413
step: 6000 epoch: 764 loss: 16.83509654757222 loss_input: 82.39834562688445
step: 7000 epoch: 764 loss: 16.821031723817985 loss_input: 82.49443074468714
step: 8000 epoch: 764 loss: 16.79251015137142 loss_input: 82.27322028777165
step: 9000 epoch: 764 loss: 16.78533279391186 loss_input: 82.20792460370072
step: 10000 epoch: 764 loss: 16.769919425055406 loss_input: 82.07340947423812
step: 11000 epoch: 764 loss: 16.77886193917563 loss_input: 82.12530655726965
step: 12000 epoch: 764 loss: 16.794775409824044 loss_input: 82.17437827039804
step: 13000 epoch: 764 loss: 16.775534511831996 loss_input: 82.2089215089299
step: 14000 epoch: 764 loss: 16.76688994963538 loss_input: 82.17198788517757
step: 15000 epoch: 764 loss: 16.75328687619912 loss_input: 82.12165490236849
Save loss: 16.76356021642685 Name: 764_train_model.pth
step: 0 epoch: 765 loss: 15.724185943603516 loss_input: 55.11761474609375
step: 1000 epoch: 765 loss: 16.376120584947127 loss_input: 80.46636243395277
step: 2000 epoch: 765 loss: 16.555483036432072 loss_input: 81.51952963337668
step: 3000 epoch: 765 loss: 16.610364892807695 loss_input: 81.49106496971713
step: 4000 epoch: 765 loss: 16.611517066092706 loss_input: 81.52131313790204
step: 5000 epoch: 765 loss: 16.66054717828407 loss_input: 81.90091306689834
step: 6000 epoch: 765 loss: 16.65716119997304 loss_input: 81.72299470473996
step: 7000 epoch: 765 loss: 16.651977231955122 loss_input: 81.77253702003229
step: 8000 epoch: 765 loss: 16.709284668996204 loss_input: 81.86217846731562
step: 9000 epoch: 765 loss: 16.701697003456 loss_input: 81.84530821038331
step: 10000 epoch: 765 loss: 16.72844514307076 loss_input: 82.08566455366181
step: 11000 epoch: 765 loss: 16.75555054112658 loss_input: 82.17601539286557
step: 12000 epoch: 765 loss: 16.759944761646718 loss_input: 82.25748380729829
step: 13000 epoch: 765 loss: 16.769737940037786 loss_input: 82.25160344392829
step: 14000 epoch: 765 loss: 16.763654965654833 loss_input: 82.37922766433257
step: 15000 epoch: 765 loss: 16.74821668106368 loss_input: 82.26095982028933
Save loss: 16.75133799107373 Name: 765_train_model.pth
step: 0 epoch: 766 loss: 32.99656295776367 loss_input: 121.390380859375
step: 1000 epoch: 766 loss: 16.768373405064022 loss_input: 83.00169935045423
step: 2000 epoch: 766 loss: 16.67387854534647 loss_input: 81.98965046895498
step: 3000 epoch: 766 loss: 16.557303693524126 loss_input: 81.35636245437719
step: 4000 epoch: 766 loss: 16.60582816395215 loss_input: 81.78422105899307
step: 5000 epoch: 766 loss: 16.626819147441037 loss_input: 82.09495117578047
step: 6000 epoch: 766 loss: 16.662841779830913 loss_input: 82.2375160170245
step: 7000 epoch: 766 loss: 16.678073472183886 loss_input: 82.0667190802402
step: 8000 epoch: 766 loss: 16.705667590397685 loss_input: 82.136978528929
step: 9000 epoch: 766 loss: 16.71439933249744 loss_input: 82.23352819060686
step: 10000 epoch: 766 loss: 16.700229584294647 loss_input: 82.13340948619982
step: 11000 epoch: 766 loss: 16.705101085244475 loss_input: 82.09551193588224
step: 12000 epoch: 766 loss: 16.734202406186082 loss_input: 82.24434986151057
step: 13000 epoch: 766 loss: 16.742314135365355 loss_input: 82.19640266366228
step: 14000 epoch: 766 loss: 16.737370555345166 loss_input: 82.16419420010038
step: 15000 epoch: 766 loss: 16.746062648605484 loss_input: 82.18209022067799
Save loss: 16.747066720113157 Name: 766_train_model.pth
step: 0 epoch: 767 loss: 16.823684692382812 loss_input: 147.15240478515625
step: 1000 epoch: 767 loss: 16.59718752717162 loss_input: 81.72325757738355
step: 2000 epoch: 767 loss: 16.579927148966714 loss_input: 81.27838345231682
step: 3000 epoch: 767 loss: 16.622307952822386 loss_input: 81.58803919608178
step: 4000 epoch: 767 loss: 16.66503049468613 loss_input: 81.75761949405226
step: 5000 epoch: 767 loss: 16.646210260568584 loss_input: 82.09608257791801
step: 6000 epoch: 767 loss: 16.655297930926764 loss_input: 81.63716581904318
step: 7000 epoch: 767 loss: 16.680477889226754 loss_input: 81.63103890606308
step: 8000 epoch: 767 loss: 16.689383608924494 loss_input: 81.54025156798028
step: 9000 epoch: 767 loss: 16.69486209226044 loss_input: 81.65483492149961
step: 10000 epoch: 767 loss: 16.691814676402462 loss_input: 81.64762345176615
step: 11000 epoch: 767 loss: 16.72029093770198 loss_input: 81.8713125466932
step: 12000 epoch: 767 loss: 16.709364175299847 loss_input: 81.97129141927789
step: 13000 epoch: 767 loss: 16.71524859299963 loss_input: 82.07782399306875
step: 14000 epoch: 767 loss: 16.7304395258184 loss_input: 82.18302193938575
step: 15000 epoch: 767 loss: 16.74145169563591 loss_input: 82.2002265075232
Save loss: 16.745160123705865 Name: 767_train_model.pth
step: 0 epoch: 768 loss: 11.375274658203125 loss_input: 47.679931640625
step: 1000 epoch: 768 loss: 16.51948476194978 loss_input: 82.43025870613761
step: 2000 epoch: 768 loss: 16.634843492198144 loss_input: 82.3720950803895
step: 3000 epoch: 768 loss: 16.694577033263133 loss_input: 82.40291189964674
step: 4000 epoch: 768 loss: 16.71635871897695 loss_input: 82.72012142228115
step: 5000 epoch: 768 loss: 16.70028409250401 loss_input: 82.730535155176
step: 6000 epoch: 768 loss: 16.714364495759725 loss_input: 82.56910642252984
step: 7000 epoch: 768 loss: 16.697586629513655 loss_input: 82.3880737374432
step: 8000 epoch: 768 loss: 16.680746669993372 loss_input: 82.17588141905846
step: 9000 epoch: 768 loss: 16.697242883559984 loss_input: 82.2020712772908
step: 10000 epoch: 768 loss: 16.732002524253954 loss_input: 82.28489725490341
step: 11000 epoch: 768 loss: 16.72663033046502 loss_input: 82.2562279771885
step: 12000 epoch: 768 loss: 16.741241686126212 loss_input: 82.31140348798245
step: 13000 epoch: 768 loss: 16.731872892500796 loss_input: 82.14140079575459
step: 14000 epoch: 768 loss: 16.735599524565625 loss_input: 82.15288282195105
step: 15000 epoch: 768 loss: 16.740077426597107 loss_input: 82.20914908163023
Save loss: 16.74711672785878 Name: 768_train_model.pth
step: 0 epoch: 769 loss: 20.945556640625 loss_input: 71.302001953125
step: 1000 epoch: 769 loss: 16.748882110302265 loss_input: 82.48670835999937
step: 2000 epoch: 769 loss: 16.775672081290097 loss_input: 82.72198800502093
step: 3000 epoch: 769 loss: 16.80421543272286 loss_input: 82.83492536586112
step: 4000 epoch: 769 loss: 16.75136722138273 loss_input: 82.73615362333733
step: 5000 epoch: 769 loss: 16.73658864652126 loss_input: 83.00589925960121
step: 6000 epoch: 769 loss: 16.742269098867954 loss_input: 82.90480226358758
step: 7000 epoch: 769 loss: 16.720083033284226 loss_input: 82.64919243191262
step: 8000 epoch: 769 loss: 16.697683276809492 loss_input: 82.3489160107428
step: 9000 epoch: 769 loss: 16.72295696644528 loss_input: 82.4318329186297
step: 10000 epoch: 769 loss: 16.717607883772438 loss_input: 82.30508736235751
step: 11000 epoch: 769 loss: 16.726201703251302 loss_input: 82.33996732386012
step: 12000 epoch: 769 loss: 16.722255704264057 loss_input: 82.2148188141304
step: 13000 epoch: 769 loss: 16.72624408994287 loss_input: 82.11142800839238
step: 14000 epoch: 769 loss: 16.73229046116879 loss_input: 82.15558562746355
step: 15000 epoch: 769 loss: 16.736334397517698 loss_input: 82.2482421704113
Save loss: 16.743728658363224 Name: 769_train_model.pth
step: 0 epoch: 770 loss: 18.15875244140625 loss_input: 60.287841796875
step: 1000 epoch: 770 loss: 16.772055470621908 loss_input: 81.7191604172195
step: 2000 epoch: 770 loss: 16.73910253277902 loss_input: 82.51482574752663
step: 3000 epoch: 770 loss: 16.725942829297963 loss_input: 82.17063707035805
step: 4000 epoch: 770 loss: 16.78355110725502 loss_input: 82.33404800350205
step: 5000 epoch: 770 loss: 16.806894258460243 loss_input: 82.4833625804136
step: 6000 epoch: 770 loss: 16.81536136704909 loss_input: 82.58289694027232
step: 7000 epoch: 770 loss: 16.80565815669369 loss_input: 82.51643441108581
step: 8000 epoch: 770 loss: 16.785776329731856 loss_input: 82.63719007322571
step: 9000 epoch: 770 loss: 16.79497952959747 loss_input: 82.62029107090315
step: 10000 epoch: 770 loss: 16.7525396496281 loss_input: 82.39389701776892
step: 11000 epoch: 770 loss: 16.74472941486264 loss_input: 82.28921470784262
step: 12000 epoch: 770 loss: 16.72265672336051 loss_input: 82.17034675617852
step: 13000 epoch: 770 loss: 16.747056931332896 loss_input: 82.3087410963129
step: 14000 epoch: 770 loss: 16.758772064690827 loss_input: 82.3226060286291
step: 15000 epoch: 770 loss: 16.745728704946995 loss_input: 82.25349759804425
Save loss: 16.74470041951537 Name: 770_train_model.pth
step: 0 epoch: 771 loss: 10.940926551818848 loss_input: 53.2354736328125
step: 1000 epoch: 771 loss: 16.795336213621585 loss_input: 82.9676867322131
step: 2000 epoch: 771 loss: 16.830750098411944 loss_input: 82.85980557084739
step: 3000 epoch: 771 loss: 16.781752914796073 loss_input: 82.3680163252874
step: 4000 epoch: 771 loss: 16.765040998785413 loss_input: 82.35665419887704
step: 5000 epoch: 771 loss: 16.751111369828084 loss_input: 82.0520092636746
step: 6000 epoch: 771 loss: 16.746739895179697 loss_input: 81.92905936982508
step: 7000 epoch: 771 loss: 16.727700147232383 loss_input: 81.88356798986864
step: 8000 epoch: 771 loss: 16.76862691125487 loss_input: 82.03750333057734
step: 9000 epoch: 771 loss: 16.760752918799444 loss_input: 81.95735312043236
step: 10000 epoch: 771 loss: 16.76301760755054 loss_input: 82.10068974791974
step: 11000 epoch: 771 loss: 16.773452130548197 loss_input: 82.20732880262838
step: 12000 epoch: 771 loss: 16.762506954352684 loss_input: 82.17543122728868
step: 13000 epoch: 771 loss: 16.76188353380509 loss_input: 82.17885503980914
step: 14000 epoch: 771 loss: 16.76281089105314 loss_input: 82.18067809522394
step: 15000 epoch: 771 loss: 16.76551236994115 loss_input: 82.23637948062577
Save loss: 16.746224680259825 Name: 771_train_model.pth
step: 0 epoch: 772 loss: 17.382816314697266 loss_input: 73.10516357421875
step: 1000 epoch: 772 loss: 16.538536292570573 loss_input: 80.88573773638471
step: 2000 epoch: 772 loss: 16.74889177408652 loss_input: 81.81898570978183
step: 3000 epoch: 772 loss: 16.69211013378282 loss_input: 81.70728584449715
step: 4000 epoch: 772 loss: 16.704915439745868 loss_input: 81.99869853703566
step: 5000 epoch: 772 loss: 16.714817971044766 loss_input: 82.03189983341223
step: 6000 epoch: 772 loss: 16.70685520634574 loss_input: 81.99828839155063
step: 7000 epoch: 772 loss: 16.68153082890232 loss_input: 81.87614773294105
step: 8000 epoch: 772 loss: 16.714780373269953 loss_input: 81.81461986746285
step: 9000 epoch: 772 loss: 16.72768515198115 loss_input: 81.99630952599233
step: 10000 epoch: 772 loss: 16.7470620464008 loss_input: 82.18479554632427
step: 11000 epoch: 772 loss: 16.754067844742483 loss_input: 82.20741306232459
step: 12000 epoch: 772 loss: 16.751197939365667 loss_input: 82.2661726228218
step: 13000 epoch: 772 loss: 16.755625189987388 loss_input: 82.17081589614801
step: 14000 epoch: 772 loss: 16.74796134866516 loss_input: 82.15483429476019
step: 15000 epoch: 772 loss: 16.752842153615056 loss_input: 82.18974972387909
Save loss: 16.750646393820645 Name: 772_train_model.pth
step: 0 epoch: 773 loss: 13.969196319580078 loss_input: 69.59320068359375
step: 1000 epoch: 773 loss: 16.518480331866773 loss_input: 81.8112497243967
step: 2000 epoch: 773 loss: 16.58128254119305 loss_input: 81.48389568643412
step: 3000 epoch: 773 loss: 16.60867720538479 loss_input: 81.57475006703494
step: 4000 epoch: 773 loss: 16.6100880186786 loss_input: 81.75661399286409
step: 5000 epoch: 773 loss: 16.646558639693797 loss_input: 81.76769129816614
step: 6000 epoch: 773 loss: 16.656142667181747 loss_input: 81.69098945952042
step: 7000 epoch: 773 loss: 16.662927827363763 loss_input: 81.7903914988986
step: 8000 epoch: 773 loss: 16.690685667435595 loss_input: 81.92734138823468
step: 9000 epoch: 773 loss: 16.704454705339316 loss_input: 82.00155012751404
step: 10000 epoch: 773 loss: 16.73067300034313 loss_input: 82.08605817885008
step: 11000 epoch: 773 loss: 16.736076269352633 loss_input: 82.12650136719638
step: 12000 epoch: 773 loss: 16.738914258837074 loss_input: 82.21589735787805
step: 13000 epoch: 773 loss: 16.73256321808309 loss_input: 82.1334615495698
step: 14000 epoch: 773 loss: 16.73807752485761 loss_input: 82.17855199150814
step: 15000 epoch: 773 loss: 16.741768918143265 loss_input: 82.2013283505709
Save loss: 16.749779814943672 Name: 773_train_model.pth
step: 0 epoch: 774 loss: 13.77599811553955 loss_input: 73.8092041015625
step: 1000 epoch: 774 loss: 16.727387658842318 loss_input: 83.18221475789835
step: 2000 epoch: 774 loss: 16.664547226299113 loss_input: 82.08738608600973
step: 3000 epoch: 774 loss: 16.712191721869484 loss_input: 82.62897596959867
step: 4000 epoch: 774 loss: 16.712375705702787 loss_input: 82.66507282176038
step: 5000 epoch: 774 loss: 16.75070794542607 loss_input: 82.68674028866101
step: 6000 epoch: 774 loss: 16.761913296104055 loss_input: 82.61617335627982
step: 7000 epoch: 774 loss: 16.733080967922344 loss_input: 82.48636549933435
step: 8000 epoch: 774 loss: 16.748318838962927 loss_input: 82.47389747363123
step: 9000 epoch: 774 loss: 16.76633532539154 loss_input: 82.44991358517673
step: 10000 epoch: 774 loss: 16.76361774725981 loss_input: 82.34569751643
step: 11000 epoch: 774 loss: 16.76328976275043 loss_input: 82.33407204680611
step: 12000 epoch: 774 loss: 16.771327546890273 loss_input: 82.35893264387798
step: 13000 epoch: 774 loss: 16.775761090464872 loss_input: 82.26334839008027
step: 14000 epoch: 774 loss: 16.744932861909145 loss_input: 82.20275258135858
step: 15000 epoch: 774 loss: 16.74492676432439 loss_input: 82.19121840038139
Save loss: 16.741271362259983 Name: 774_train_model.pth
step: 0 epoch: 775 loss: 6.8669753074646 loss_input: 56.3831787109375
step: 1000 epoch: 775 loss: 16.683999846627067 loss_input: 81.37881222781125
step: 2000 epoch: 775 loss: 16.75183249937779 loss_input: 82.10574699222654
step: 3000 epoch: 775 loss: 16.799373660473695 loss_input: 82.0631348998576
step: 4000 epoch: 775 loss: 16.701865806009913 loss_input: 81.81960039775213
step: 5000 epoch: 775 loss: 16.711527387658684 loss_input: 81.85053094361596
step: 6000 epoch: 775 loss: 16.674917815864294 loss_input: 81.81328638830377
step: 7000 epoch: 775 loss: 16.694710952693132 loss_input: 81.95761419197369
step: 8000 epoch: 775 loss: 16.714560658406384 loss_input: 82.07682676140092
step: 9000 epoch: 775 loss: 16.723706384404952 loss_input: 82.19858461771392
step: 10000 epoch: 775 loss: 16.731632576145156 loss_input: 82.41245134519751
step: 11000 epoch: 775 loss: 16.733759473274105 loss_input: 82.21298971489098
step: 12000 epoch: 775 loss: 16.72892772512688 loss_input: 82.26722306222203
step: 13000 epoch: 775 loss: 16.72251442685218 loss_input: 82.12816689882028
step: 14000 epoch: 775 loss: 16.731995878419863 loss_input: 82.17759136240548
step: 15000 epoch: 775 loss: 16.739136034199003 loss_input: 82.19581014748756
Save loss: 16.735351641759276 Name: 775_train_model.pth
step: 0 epoch: 776 loss: 15.1478910446167 loss_input: 57.8118896484375
step: 1000 epoch: 776 loss: 16.76747452033745 loss_input: 82.21954559112763
step: 2000 epoch: 776 loss: 16.731565605337057 loss_input: 82.25536776899636
step: 3000 epoch: 776 loss: 16.73996555801234 loss_input: 82.29634114068097
step: 4000 epoch: 776 loss: 16.68429344804607 loss_input: 82.24040666469125
step: 5000 epoch: 776 loss: 16.68059687012793 loss_input: 82.19995386908947
step: 6000 epoch: 776 loss: 16.688272541273715 loss_input: 82.23637630422598
step: 7000 epoch: 776 loss: 16.681319430187386 loss_input: 82.04095263699773
step: 8000 epoch: 776 loss: 16.739543134250457 loss_input: 82.06951286393156
step: 9000 epoch: 776 loss: 16.75115340736863 loss_input: 82.19645792123357
step: 10000 epoch: 776 loss: 16.74274689621263 loss_input: 82.30980715405022
step: 11000 epoch: 776 loss: 16.72283204158169 loss_input: 82.33069486247877
step: 12000 epoch: 776 loss: 16.740704955940256 loss_input: 82.41079111022161
step: 13000 epoch: 776 loss: 16.712110288857808 loss_input: 82.22314136236065
step: 14000 epoch: 776 loss: 16.714110608969694 loss_input: 82.22175503873747
step: 15000 epoch: 776 loss: 16.729709737372872 loss_input: 82.19296251831909
Save loss: 16.738408618226646 Name: 776_train_model.pth
step: 0 epoch: 777 loss: 28.4167537689209 loss_input: 137.63238525390625
step: 1000 epoch: 777 loss: 16.653010915685723 loss_input: 81.76130772303868
step: 2000 epoch: 777 loss: 16.638477958601037 loss_input: 81.13071397040737
step: 3000 epoch: 777 loss: 16.59883835044792 loss_input: 81.42481105337934
step: 4000 epoch: 777 loss: 16.567741028281098 loss_input: 81.58847467698504
step: 5000 epoch: 777 loss: 16.611346118761478 loss_input: 81.92787639981769
step: 6000 epoch: 777 loss: 16.61559213373864 loss_input: 81.92410623620022
step: 7000 epoch: 777 loss: 16.618812074730727 loss_input: 81.8479109481988
step: 8000 epoch: 777 loss: 16.663959109772744 loss_input: 82.02379415250095
step: 9000 epoch: 777 loss: 16.687019299777425 loss_input: 82.07284082421832
step: 10000 epoch: 777 loss: 16.68198557179423 loss_input: 82.05457210626594
step: 11000 epoch: 777 loss: 16.687894085474138 loss_input: 82.00548415942123
step: 12000 epoch: 777 loss: 16.68759754901826 loss_input: 82.02390512257116
step: 13000 epoch: 777 loss: 16.713010640779228 loss_input: 82.18106008916531
step: 14000 epoch: 777 loss: 16.731230462264115 loss_input: 82.2846504529113
step: 15000 epoch: 777 loss: 16.741328057110543 loss_input: 82.23254700093561
Save loss: 16.73822257563472 Name: 777_train_model.pth
step: 0 epoch: 778 loss: 12.229361534118652 loss_input: 60.867919921875
step: 1000 epoch: 778 loss: 16.73217797303176 loss_input: 81.71353189047281
step: 2000 epoch: 778 loss: 16.74306309550837 loss_input: 82.44104577349997
step: 3000 epoch: 778 loss: 16.72880088277993 loss_input: 82.64306514527709
step: 4000 epoch: 778 loss: 16.713644827761907 loss_input: 82.34237894383826
step: 5000 epoch: 778 loss: 16.6956076869915 loss_input: 82.27253898585518
step: 6000 epoch: 778 loss: 16.719141238730185 loss_input: 82.32069079388461
step: 7000 epoch: 778 loss: 16.75468338477477 loss_input: 82.33475303786122
step: 8000 epoch: 778 loss: 16.703649740728075 loss_input: 82.18682235033121
step: 9000 epoch: 778 loss: 16.716497143405423 loss_input: 82.18668103310786
step: 10000 epoch: 778 loss: 16.72799997517567 loss_input: 82.16206328585892
step: 11000 epoch: 778 loss: 16.72718411182861 loss_input: 82.15732134702954
step: 12000 epoch: 778 loss: 16.74837102263026 loss_input: 82.26898707537718
step: 13000 epoch: 778 loss: 16.745706808987915 loss_input: 82.38584439004993
step: 14000 epoch: 778 loss: 16.75226802998258 loss_input: 82.24499458098836
step: 15000 epoch: 778 loss: 16.749559959914873 loss_input: 82.22010919893752
Save loss: 16.747577421054245 Name: 778_train_model.pth
step: 0 epoch: 779 loss: 18.179309844970703 loss_input: 72.3621826171875
step: 1000 epoch: 779 loss: 16.69563532304335 loss_input: 82.42125672179382
step: 2000 epoch: 779 loss: 16.74982228224305 loss_input: 82.77788751498274
step: 3000 epoch: 779 loss: 16.689420110422226 loss_input: 82.20549062584607
step: 4000 epoch: 779 loss: 16.71859790569125 loss_input: 82.1963288017107
step: 5000 epoch: 779 loss: 16.762103767114695 loss_input: 82.48234051920085
step: 6000 epoch: 779 loss: 16.750374767188728 loss_input: 82.33316271425466
step: 7000 epoch: 779 loss: 16.76819945127517 loss_input: 82.34836320641415
step: 8000 epoch: 779 loss: 16.75099232342523 loss_input: 82.25718394697734
step: 9000 epoch: 779 loss: 16.72287181644569 loss_input: 82.07534148902076
step: 10000 epoch: 779 loss: 16.75094198632295 loss_input: 82.20840189540617
step: 11000 epoch: 779 loss: 16.75644845174081 loss_input: 82.20039636622168
step: 12000 epoch: 779 loss: 16.74976093557892 loss_input: 82.22620870868182
step: 13000 epoch: 779 loss: 16.747615456516932 loss_input: 82.26521973895638
step: 14000 epoch: 779 loss: 16.746197042631408 loss_input: 82.25962874164122
step: 15000 epoch: 779 loss: 16.7371615056284 loss_input: 82.19800438597062
Save loss: 16.749691137894988 Name: 779_train_model.pth
step: 0 epoch: 780 loss: 15.590296745300293 loss_input: 102.44732666015625
step: 1000 epoch: 780 loss: 16.906684264317377 loss_input: 82.70456191757461
step: 2000 epoch: 780 loss: 16.76110799261357 loss_input: 81.94280554424935
step: 3000 epoch: 780 loss: 16.699227583642724 loss_input: 82.0164977152798
step: 4000 epoch: 780 loss: 16.695631417415346 loss_input: 82.30448460882826
step: 5000 epoch: 780 loss: 16.68764231324649 loss_input: 82.11800359654632
step: 6000 epoch: 780 loss: 16.69101713157022 loss_input: 82.09295394940688
step: 7000 epoch: 780 loss: 16.73082739086666 loss_input: 82.40304143039555
step: 8000 epoch: 780 loss: 16.73850942614555 loss_input: 82.30996895956854
step: 9000 epoch: 780 loss: 16.75309582972921 loss_input: 82.39676777043219
step: 10000 epoch: 780 loss: 16.758592987546873 loss_input: 82.42929648075231
step: 11000 epoch: 780 loss: 16.759819055685032 loss_input: 82.40808228082781
step: 12000 epoch: 780 loss: 16.74885254527676 loss_input: 82.41656405138438
step: 13000 epoch: 780 loss: 16.743797148753014 loss_input: 82.38694308858754
step: 14000 epoch: 780 loss: 16.747589686999756 loss_input: 82.35401437739101
step: 15000 epoch: 780 loss: 16.747051326777648 loss_input: 82.25841500296464
Save loss: 16.74749787735939 Name: 780_train_model.pth
step: 0 epoch: 781 loss: 8.83431625366211 loss_input: 48.42181396484375
step: 1000 epoch: 781 loss: 16.62138151860499 loss_input: 81.16619403545673
step: 2000 epoch: 781 loss: 16.71703602563495 loss_input: 81.44372369479323
step: 3000 epoch: 781 loss: 16.658063563931908 loss_input: 81.61725597896404
step: 4000 epoch: 781 loss: 16.667839295921908 loss_input: 81.92640818372067
step: 5000 epoch: 781 loss: 16.67610766968234 loss_input: 82.0301044039239
step: 6000 epoch: 781 loss: 16.67908719793357 loss_input: 82.23564224484721
step: 7000 epoch: 781 loss: 16.711049868675357 loss_input: 82.43889662491017
step: 8000 epoch: 781 loss: 16.72638767603829 loss_input: 82.38258132009622
step: 9000 epoch: 781 loss: 16.71370322496914 loss_input: 82.33347536444518
step: 10000 epoch: 781 loss: 16.738926374772802 loss_input: 82.35638187890196
step: 11000 epoch: 781 loss: 16.748965554774408 loss_input: 82.3788663366363
step: 12000 epoch: 781 loss: 16.74838585126858 loss_input: 82.39573450869653
step: 13000 epoch: 781 loss: 16.746723597677438 loss_input: 82.40202129790346
step: 14000 epoch: 781 loss: 16.741293153101424 loss_input: 82.26391781603351
step: 15000 epoch: 781 loss: 16.74820593008796 loss_input: 82.25435024302416
Save loss: 16.740376853585243 Name: 781_train_model.pth
step: 0 epoch: 782 loss: 19.597476959228516 loss_input: 72.0565185546875
step: 1000 epoch: 782 loss: 16.7158014695723 loss_input: 81.93741238010037
step: 2000 epoch: 782 loss: 16.840285982268266 loss_input: 82.01151340833489
step: 3000 epoch: 782 loss: 16.79748902802307 loss_input: 82.05598024128676
step: 4000 epoch: 782 loss: 16.849101990111258 loss_input: 82.41229624844766
step: 5000 epoch: 782 loss: 16.852402211856518 loss_input: 82.53455906719047
step: 6000 epoch: 782 loss: 16.816763287483703 loss_input: 82.37159737903383
step: 7000 epoch: 782 loss: 16.834524356540722 loss_input: 82.47184179987401
step: 8000 epoch: 782 loss: 16.780805069809333 loss_input: 82.3363183804295
step: 9000 epoch: 782 loss: 16.779156991606644 loss_input: 82.2160556989436
step: 10000 epoch: 782 loss: 16.784775667387944 loss_input: 82.27014293199586
step: 11000 epoch: 782 loss: 16.755631039331377 loss_input: 82.21083538636935
step: 12000 epoch: 782 loss: 16.773577029382615 loss_input: 82.29639325405337
step: 13000 epoch: 782 loss: 16.755988790185512 loss_input: 82.27936887216609
step: 14000 epoch: 782 loss: 16.747278916462005 loss_input: 82.28801690295002
step: 15000 epoch: 782 loss: 16.745945810198855 loss_input: 82.27546163670469
Save loss: 16.74420192569494 Name: 782_train_model.pth
step: 0 epoch: 783 loss: 18.451740264892578 loss_input: 116.2100830078125
step: 1000 epoch: 783 loss: 16.600004280959215 loss_input: 82.17103526356456
step: 2000 epoch: 783 loss: 16.685587140931183 loss_input: 82.14662923055074
step: 3000 epoch: 783 loss: 16.68595244319627 loss_input: 82.37757974718063
step: 4000 epoch: 783 loss: 16.73704718798585 loss_input: 82.42148776770651
step: 5000 epoch: 783 loss: 16.74859170130886 loss_input: 82.33191753983188
step: 6000 epoch: 783 loss: 16.724838859656316 loss_input: 82.50100381015599
step: 7000 epoch: 783 loss: 16.722826549690495 loss_input: 82.37328450323224
step: 8000 epoch: 783 loss: 16.74298896757964 loss_input: 82.46029214152186
step: 9000 epoch: 783 loss: 16.73778449128885 loss_input: 82.37424903906076
step: 10000 epoch: 783 loss: 16.71736148116279 loss_input: 82.21692822783152
step: 11000 epoch: 783 loss: 16.718229239620456 loss_input: 82.18990647512157
step: 12000 epoch: 783 loss: 16.732953515830374 loss_input: 82.25435397168238
step: 13000 epoch: 783 loss: 16.748048926397395 loss_input: 82.32186334975435
step: 14000 epoch: 783 loss: 16.74033510406548 loss_input: 82.19580003144307
step: 15000 epoch: 783 loss: 16.744166114141 loss_input: 82.23380140542714
Save loss: 16.748386615738273 Name: 783_train_model.pth
step: 0 epoch: 784 loss: 12.569075584411621 loss_input: 47.75579833984375
step: 1000 epoch: 784 loss: 16.228193485296213 loss_input: 81.92839210350196
step: 2000 epoch: 784 loss: 16.575419618391145 loss_input: 82.23989616519864
step: 3000 epoch: 784 loss: 16.692704794288517 loss_input: 82.5429774547807
step: 4000 epoch: 784 loss: 16.64425306724209 loss_input: 82.48165395479744
step: 5000 epoch: 784 loss: 16.67691344052547 loss_input: 82.55719280948497
step: 6000 epoch: 784 loss: 16.694470983607594 loss_input: 82.549490774975
step: 7000 epoch: 784 loss: 16.68084677970847 loss_input: 82.27556859552988
step: 8000 epoch: 784 loss: 16.676861155645472 loss_input: 82.23347326487753
step: 9000 epoch: 784 loss: 16.686755438246152 loss_input: 82.15817343130495
step: 10000 epoch: 784 loss: 16.684634610565052 loss_input: 82.07507845406866
step: 11000 epoch: 784 loss: 16.695631041265425 loss_input: 82.08650523625161
step: 12000 epoch: 784 loss: 16.706177781019775 loss_input: 82.02463249420784
step: 13000 epoch: 784 loss: 16.71773690407519 loss_input: 82.10932062185871
step: 14000 epoch: 784 loss: 16.722716535127944 loss_input: 82.19101472000047
step: 15000 epoch: 784 loss: 16.737303572967445 loss_input: 82.17545434308286
Save loss: 16.74766654293239 Name: 784_train_model.pth
step: 0 epoch: 785 loss: 19.378314971923828 loss_input: 60.688232421875
step: 1000 epoch: 785 loss: 16.459843544097808 loss_input: 81.24375514622096
step: 2000 epoch: 785 loss: 16.591647092608557 loss_input: 81.05467298089236
step: 3000 epoch: 785 loss: 16.648942132903432 loss_input: 81.24903749124641
step: 4000 epoch: 785 loss: 16.609427182145847 loss_input: 81.23000951422301
step: 5000 epoch: 785 loss: 16.67619189761253 loss_input: 81.62482632867957
step: 6000 epoch: 785 loss: 16.688796268783197 loss_input: 81.62356704180488
step: 7000 epoch: 785 loss: 16.67485396658722 loss_input: 81.87832134646223
step: 8000 epoch: 785 loss: 16.657832390545995 loss_input: 81.84082645339498
step: 9000 epoch: 785 loss: 16.673546234881425 loss_input: 81.91667620291538
step: 10000 epoch: 785 loss: 16.69565128338431 loss_input: 81.94175250443706
step: 11000 epoch: 785 loss: 16.720567770084894 loss_input: 82.08720404190883
step: 12000 epoch: 785 loss: 16.731561071961515 loss_input: 82.09115278906609
step: 13000 epoch: 785 loss: 16.72657936373616 loss_input: 82.1068641316954
step: 14000 epoch: 785 loss: 16.738050104200767 loss_input: 82.06222087565034
step: 15000 epoch: 785 loss: 16.734252600469286 loss_input: 82.13235334278497
Save loss: 16.74462796419859 Name: 785_train_model.pth
step: 0 epoch: 786 loss: 15.893560409545898 loss_input: 93.1475830078125
step: 1000 epoch: 786 loss: 16.615207291506863 loss_input: 82.74561229785839
step: 2000 epoch: 786 loss: 16.62248925600333 loss_input: 82.19347208109812
step: 3000 epoch: 786 loss: 16.7132824632733 loss_input: 81.98618221823195
step: 4000 epoch: 786 loss: 16.757204550381037 loss_input: 82.38109761194956
step: 5000 epoch: 786 loss: 16.79770527689773 loss_input: 82.25798238789741
step: 6000 epoch: 786 loss: 16.786757290115002 loss_input: 82.35742466180767
step: 7000 epoch: 786 loss: 16.794019600609815 loss_input: 82.5043340191775
step: 8000 epoch: 786 loss: 16.76530672558962 loss_input: 82.35647353773162
step: 9000 epoch: 786 loss: 16.7894788314443 loss_input: 82.3649108364587
step: 10000 epoch: 786 loss: 16.777398214043647 loss_input: 82.33891756698354
step: 11000 epoch: 786 loss: 16.763958527645016 loss_input: 82.2877813553704
step: 12000 epoch: 786 loss: 16.775617224724527 loss_input: 82.26068694002876
step: 13000 epoch: 786 loss: 16.774691471401777 loss_input: 82.23861015254685
step: 14000 epoch: 786 loss: 16.762970019302983 loss_input: 82.25976125257934
step: 15000 epoch: 786 loss: 16.747236588137778 loss_input: 82.2057702419448
Save loss: 16.73857066372037 Name: 786_train_model.pth
step: 0 epoch: 787 loss: 11.504831314086914 loss_input: 62.38568115234375
step: 1000 epoch: 787 loss: 16.780253978161426 loss_input: 82.07560140055257
step: 2000 epoch: 787 loss: 16.65799604351076 loss_input: 81.76561093842727
step: 3000 epoch: 787 loss: 16.760177552084333 loss_input: 81.96597488337221
step: 4000 epoch: 787 loss: 16.76409795355183 loss_input: 82.4933796458112
step: 5000 epoch: 787 loss: 16.79740387433721 loss_input: 82.69853352327581
step: 6000 epoch: 787 loss: 16.74227000959435 loss_input: 82.38946575920615
step: 7000 epoch: 787 loss: 16.753247194606192 loss_input: 82.55776218631713
step: 8000 epoch: 787 loss: 16.737305521771336 loss_input: 82.4672246703415
step: 9000 epoch: 787 loss: 16.73139424104715 loss_input: 82.43249397799116
step: 10000 epoch: 787 loss: 16.721489826973265 loss_input: 82.36619470987472
step: 11000 epoch: 787 loss: 16.721142363021638 loss_input: 82.32531431468766
step: 12000 epoch: 787 loss: 16.736808588480752 loss_input: 82.30196769999799
step: 13000 epoch: 787 loss: 16.750366098999418 loss_input: 82.297314574247
step: 14000 epoch: 787 loss: 16.746719267851763 loss_input: 82.22531672193615
step: 15000 epoch: 787 loss: 16.73594315084932 loss_input: 82.26740154578887
Save loss: 16.737737390726803 Name: 787_train_model.pth
step: 0 epoch: 788 loss: 16.40253257751465 loss_input: 77.427001953125
step: 1000 epoch: 788 loss: 16.507476035412495 loss_input: 81.58340524221872
step: 2000 epoch: 788 loss: 16.64239183608917 loss_input: 81.90557043865762
step: 3000 epoch: 788 loss: 16.75952296986337 loss_input: 82.50468329404521
step: 4000 epoch: 788 loss: 16.770386071599624 loss_input: 82.54516732755287
step: 5000 epoch: 788 loss: 16.6956773094596 loss_input: 82.43779282473584
step: 6000 epoch: 788 loss: 16.692736228492493 loss_input: 82.36813097678707
step: 7000 epoch: 788 loss: 16.69673212326147 loss_input: 82.32297637377138
step: 8000 epoch: 788 loss: 16.708032444229335 loss_input: 82.31604775141632
step: 9000 epoch: 788 loss: 16.70912989501542 loss_input: 82.11665577332771
step: 10000 epoch: 788 loss: 16.70520703812359 loss_input: 82.09377285538048
step: 11000 epoch: 788 loss: 16.71538070411793 loss_input: 82.0700256109086
step: 12000 epoch: 788 loss: 16.71525222320833 loss_input: 82.09799016077274
step: 13000 epoch: 788 loss: 16.717783575030108 loss_input: 82.10609688959472
step: 14000 epoch: 788 loss: 16.717077723418583 loss_input: 82.1515361163865
step: 15000 epoch: 788 loss: 16.72064416834962 loss_input: 82.13929520990918
Save loss: 16.73898033837974 Name: 788_train_model.pth
step: 0 epoch: 789 loss: 19.637027740478516 loss_input: 75.08013916015625
step: 1000 epoch: 789 loss: 16.822097535376304 loss_input: 83.41145743904534
step: 2000 epoch: 789 loss: 16.6987303218384 loss_input: 82.45220623940959
step: 3000 epoch: 789 loss: 16.664162567082105 loss_input: 82.634714006464
step: 4000 epoch: 789 loss: 16.68364822140517 loss_input: 82.53859996914834
step: 5000 epoch: 789 loss: 16.606081960630807 loss_input: 82.07425780468907
step: 6000 epoch: 789 loss: 16.576816833490057 loss_input: 81.8024973742983
step: 7000 epoch: 789 loss: 16.61955077875309 loss_input: 81.86092687616892
step: 8000 epoch: 789 loss: 16.637220496074093 loss_input: 82.09651376890876
step: 9000 epoch: 789 loss: 16.660880789228603 loss_input: 82.02461993155275
step: 10000 epoch: 789 loss: 16.667669937713374 loss_input: 82.0154654029226
step: 11000 epoch: 789 loss: 16.698353079359876 loss_input: 82.10947861163012
step: 12000 epoch: 789 loss: 16.715576059330942 loss_input: 82.20869129843021
step: 13000 epoch: 789 loss: 16.72579375075135 loss_input: 82.1569913698796
step: 14000 epoch: 789 loss: 16.727198476681036 loss_input: 82.14704590192497
step: 15000 epoch: 789 loss: 16.74078782297247 loss_input: 82.28010435697516
Save loss: 16.73440846799314 Name: 789_train_model.pth
step: 0 epoch: 790 loss: 12.577539443969727 loss_input: 86.24908447265625
step: 1000 epoch: 790 loss: 16.71004788239638 loss_input: 82.98959621921048
step: 2000 epoch: 790 loss: 16.76046479052153 loss_input: 82.57793875279157
step: 3000 epoch: 790 loss: 16.724950362665975 loss_input: 82.03140585218338
step: 4000 epoch: 790 loss: 16.716433256216508 loss_input: 81.90122290594344
step: 5000 epoch: 790 loss: 16.73384531091104 loss_input: 81.9204789657303
step: 6000 epoch: 790 loss: 16.722483288465074 loss_input: 81.96978075268706
step: 7000 epoch: 790 loss: 16.75770454845366 loss_input: 82.44634138831171
step: 8000 epoch: 790 loss: 16.75408290061574 loss_input: 82.28017739542811
step: 9000 epoch: 790 loss: 16.72609335850933 loss_input: 82.23660082690465
step: 10000 epoch: 790 loss: 16.7198317488865 loss_input: 82.22591077073933
step: 11000 epoch: 790 loss: 16.72004372639479 loss_input: 82.25612566382459
step: 12000 epoch: 790 loss: 16.729822324142983 loss_input: 82.35299699146165
step: 13000 epoch: 790 loss: 16.729527666737287 loss_input: 82.22528959705173
step: 14000 epoch: 790 loss: 16.72711433270056 loss_input: 82.2228616709437
step: 15000 epoch: 790 loss: 16.724395342440886 loss_input: 82.24718467662052
Save loss: 16.738490880966186 Name: 790_train_model.pth
step: 0 epoch: 791 loss: 27.104820251464844 loss_input: 120.87017822265625
step: 1000 epoch: 791 loss: 16.52374989169461 loss_input: 81.3186763187984
step: 2000 epoch: 791 loss: 16.55576891091274 loss_input: 81.79304360271037
step: 3000 epoch: 791 loss: 16.56832846336784 loss_input: 81.89321300451932
step: 4000 epoch: 791 loss: 16.641600004108927 loss_input: 81.96540491952207
step: 5000 epoch: 791 loss: 16.644831685728132 loss_input: 81.99714401602101
step: 6000 epoch: 791 loss: 16.665690156424606 loss_input: 82.10515255920888
step: 7000 epoch: 791 loss: 16.701905813613564 loss_input: 82.18006650033533
step: 8000 epoch: 791 loss: 16.714725536758372 loss_input: 82.08941231589215
step: 9000 epoch: 791 loss: 16.705429087266857 loss_input: 82.27280056718958
step: 10000 epoch: 791 loss: 16.69940497066817 loss_input: 82.22545029432604
step: 11000 epoch: 791 loss: 16.714060736140297 loss_input: 82.26179680686876
step: 12000 epoch: 791 loss: 16.704670009131473 loss_input: 82.17077305630617
step: 13000 epoch: 791 loss: 16.71681871211361 loss_input: 82.16222887733846
step: 14000 epoch: 791 loss: 16.724290484743847 loss_input: 82.21219899349404
step: 15000 epoch: 791 loss: 16.724819701573473 loss_input: 82.20758190502144
Save loss: 16.72886761173606 Name: 791_train_model.pth
step: 0 epoch: 792 loss: 10.051434516906738 loss_input: 41.51654052734375
step: 1000 epoch: 792 loss: 16.68262654846603 loss_input: 82.2833496459595
step: 2000 epoch: 792 loss: 16.760140189523998 loss_input: 82.63524887336605
step: 3000 epoch: 792 loss: 16.729352545078815 loss_input: 82.39038354402699
step: 4000 epoch: 792 loss: 16.695481559807764 loss_input: 82.27408827724709
step: 5000 epoch: 792 loss: 16.669869243848375 loss_input: 82.06345001751603
step: 6000 epoch: 792 loss: 16.701324763565815 loss_input: 82.30091728486333
step: 7000 epoch: 792 loss: 16.739275697571365 loss_input: 82.40470140182262
step: 8000 epoch: 792 loss: 16.732220837658993 loss_input: 82.38763174470135
step: 9000 epoch: 792 loss: 16.715112292307005 loss_input: 82.32961988327251
step: 10000 epoch: 792 loss: 16.728089146513472 loss_input: 82.28916048214515
step: 11000 epoch: 792 loss: 16.74069681127292 loss_input: 82.25868795308381
step: 12000 epoch: 792 loss: 16.748751947436965 loss_input: 82.29502978125032
step: 13000 epoch: 792 loss: 16.734062915764884 loss_input: 82.21748592299541
step: 14000 epoch: 792 loss: 16.725276400775417 loss_input: 82.17365568684268
step: 15000 epoch: 792 loss: 16.726986350381257 loss_input: 82.1973322546726
Save loss: 16.739649782925845 Name: 792_train_model.pth
step: 0 epoch: 793 loss: 16.313180923461914 loss_input: 58.8140869140625
step: 1000 epoch: 793 loss: 16.604508272536865 loss_input: 81.58341012015329
step: 2000 epoch: 793 loss: 16.56732772446346 loss_input: 82.24615481565857
step: 3000 epoch: 793 loss: 16.631128422620495 loss_input: 82.93847229146274
step: 4000 epoch: 793 loss: 16.732986522000957 loss_input: 82.90013193112854
step: 5000 epoch: 793 loss: 16.744784581997326 loss_input: 82.67092008844325
step: 6000 epoch: 793 loss: 16.757855163298334 loss_input: 82.46174421979475
step: 7000 epoch: 793 loss: 16.70639690639734 loss_input: 82.39536718143223
step: 8000 epoch: 793 loss: 16.714669146547912 loss_input: 82.21572285916653
step: 9000 epoch: 793 loss: 16.713615510559336 loss_input: 82.15856486730424
step: 10000 epoch: 793 loss: 16.70899680776246 loss_input: 82.16761701515395
step: 11000 epoch: 793 loss: 16.713971125040366 loss_input: 82.09298513256607
step: 12000 epoch: 793 loss: 16.72949561698468 loss_input: 82.15955420869875
step: 13000 epoch: 793 loss: 16.740532110346418 loss_input: 82.30279944286137
step: 14000 epoch: 793 loss: 16.75235584076622 loss_input: 82.22862569840156
step: 15000 epoch: 793 loss: 16.738603266229088 loss_input: 82.19807492570156
Save loss: 16.73725325909257 Name: 793_train_model.pth
step: 0 epoch: 794 loss: 13.206014633178711 loss_input: 98.2152099609375
step: 1000 epoch: 794 loss: 16.56876791107071 loss_input: 83.56972873318088
step: 2000 epoch: 794 loss: 16.61250403831745 loss_input: 83.0763204213323
step: 3000 epoch: 794 loss: 16.67408588813647 loss_input: 82.75931684067551
step: 4000 epoch: 794 loss: 16.617310115797046 loss_input: 82.43255876094453
step: 5000 epoch: 794 loss: 16.646485821148605 loss_input: 82.47211857677293
step: 6000 epoch: 794 loss: 16.648315449552403 loss_input: 82.20894870589602
step: 7000 epoch: 794 loss: 16.677742848275745 loss_input: 82.3237456643331
step: 8000 epoch: 794 loss: 16.699712893170517 loss_input: 82.42596398167514
step: 9000 epoch: 794 loss: 16.705396999850326 loss_input: 82.19326250262557
step: 10000 epoch: 794 loss: 16.692460787032868 loss_input: 82.11898230953273
step: 11000 epoch: 794 loss: 16.722077579804306 loss_input: 82.17744905048582
step: 12000 epoch: 794 loss: 16.74230033869982 loss_input: 82.21874273233578
step: 13000 epoch: 794 loss: 16.72930553656121 loss_input: 82.15894960265977
step: 14000 epoch: 794 loss: 16.736982701565655 loss_input: 82.20826034485957
step: 15000 epoch: 794 loss: 16.733809248414584 loss_input: 82.15351305681183
Save loss: 16.735073306247592 Name: 794_train_model.pth
step: 0 epoch: 795 loss: 16.40558624267578 loss_input: 82.0948486328125
step: 1000 epoch: 795 loss: 16.51113088933619 loss_input: 81.41940115548513
step: 2000 epoch: 795 loss: 16.575244815870263 loss_input: 81.41185794431885
step: 3000 epoch: 795 loss: 16.530299925796193 loss_input: 81.37560085358916
step: 4000 epoch: 795 loss: 16.57119538640416 loss_input: 81.62243171812027
step: 5000 epoch: 795 loss: 16.627793458861557 loss_input: 81.9225275062175
step: 6000 epoch: 795 loss: 16.659867078537662 loss_input: 81.86922232728465
step: 7000 epoch: 795 loss: 16.720074508653642 loss_input: 82.19375890637275
step: 8000 epoch: 795 loss: 16.724532211621007 loss_input: 82.11822904918867
step: 9000 epoch: 795 loss: 16.71158974645615 loss_input: 82.01870365098323
step: 10000 epoch: 795 loss: 16.71158353923118 loss_input: 82.07668041789572
step: 11000 epoch: 795 loss: 16.726872980870958 loss_input: 82.22941792001508
step: 12000 epoch: 795 loss: 16.72239555439227 loss_input: 82.22903485866738
step: 13000 epoch: 795 loss: 16.72695217166678 loss_input: 82.17276250966722
step: 14000 epoch: 795 loss: 16.722966264668607 loss_input: 82.1777069416568
step: 15000 epoch: 795 loss: 16.73399119931502 loss_input: 82.26807518658913
Save loss: 16.7340971352309 Name: 795_train_model.pth
step: 0 epoch: 796 loss: 14.945759773254395 loss_input: 64.1834716796875
step: 1000 epoch: 796 loss: 16.73206155283468 loss_input: 82.10484565483345
step: 2000 epoch: 796 loss: 16.767316734117607 loss_input: 82.55007319948425
step: 3000 epoch: 796 loss: 16.627586313105947 loss_input: 82.41349390291127
step: 4000 epoch: 796 loss: 16.7018108966201 loss_input: 82.2082744542136
step: 5000 epoch: 796 loss: 16.678257927706756 loss_input: 82.34073437997947
step: 6000 epoch: 796 loss: 16.658938956010385 loss_input: 82.21682006449446
step: 7000 epoch: 796 loss: 16.672335790269905 loss_input: 82.31303075565098
step: 8000 epoch: 796 loss: 16.694460612209927 loss_input: 82.26649223931237
step: 9000 epoch: 796 loss: 16.6870176292316 loss_input: 82.07247902766663
step: 10000 epoch: 796 loss: 16.67928996146673 loss_input: 82.08044749724246
step: 11000 epoch: 796 loss: 16.689012653946214 loss_input: 82.08693760328428
step: 12000 epoch: 796 loss: 16.71147439714452 loss_input: 82.23433432345807
step: 13000 epoch: 796 loss: 16.709474701357294 loss_input: 82.17323022834631
step: 14000 epoch: 796 loss: 16.720271329455407 loss_input: 82.18132262403272
step: 15000 epoch: 796 loss: 16.7226266904828 loss_input: 82.17597319277021
Save loss: 16.728296243995427 Name: 796_train_model.pth
step: 0 epoch: 797 loss: 17.529895782470703 loss_input: 111.685791015625
step: 1000 epoch: 797 loss: 16.4886575793172 loss_input: 81.78003880884741
step: 2000 epoch: 797 loss: 16.647945194110935 loss_input: 81.93577204317764
step: 3000 epoch: 797 loss: 16.614847323053162 loss_input: 81.75981510801738
step: 4000 epoch: 797 loss: 16.639969204342744 loss_input: 81.70274429683595
step: 5000 epoch: 797 loss: 16.68515362717633 loss_input: 82.17075206784816
step: 6000 epoch: 797 loss: 16.705890667039224 loss_input: 82.23290602689265
step: 7000 epoch: 797 loss: 16.718196509344512 loss_input: 82.10702117540686
step: 8000 epoch: 797 loss: 16.731886612863903 loss_input: 82.22732442105625
step: 9000 epoch: 797 loss: 16.731418282810072 loss_input: 82.31194328559847
step: 10000 epoch: 797 loss: 16.726027233197016 loss_input: 82.35944321217292
step: 11000 epoch: 797 loss: 16.729920510237527 loss_input: 82.30154297718319
step: 12000 epoch: 797 loss: 16.717943577217703 loss_input: 82.20141996732205
step: 13000 epoch: 797 loss: 16.741662497264073 loss_input: 82.1837692595236
step: 14000 epoch: 797 loss: 16.73735970005411 loss_input: 82.19081735512195
step: 15000 epoch: 797 loss: 16.727559666277273 loss_input: 82.1572277261594
Save loss: 16.73080458918214 Name: 797_train_model.pth
step: 0 epoch: 798 loss: 19.190217971801758 loss_input: 189.473388671875
step: 1000 epoch: 798 loss: 16.595647887631014 loss_input: 82.35506455214707
step: 2000 epoch: 798 loss: 16.573287693397337 loss_input: 82.0523910103054
step: 3000 epoch: 798 loss: 16.720531609804382 loss_input: 82.46479257890599
step: 4000 epoch: 798 loss: 16.762278161743705 loss_input: 82.62554975880053
step: 5000 epoch: 798 loss: 16.711704128481248 loss_input: 82.43445817672404
step: 6000 epoch: 798 loss: 16.73262877671684 loss_input: 82.53390338670768
step: 7000 epoch: 798 loss: 16.71173477673459 loss_input: 82.32582089451786
step: 8000 epoch: 798 loss: 16.73243530633047 loss_input: 82.46252309434222
step: 9000 epoch: 798 loss: 16.72434198142502 loss_input: 82.46271946075002
step: 10000 epoch: 798 loss: 16.702707927377926 loss_input: 82.2724544257
step: 11000 epoch: 798 loss: 16.708750346326553 loss_input: 82.31440976068937
step: 12000 epoch: 798 loss: 16.708605930930563 loss_input: 82.12232929360378
step: 13000 epoch: 798 loss: 16.718375040672253 loss_input: 82.23192958921278
step: 14000 epoch: 798 loss: 16.731345829184793 loss_input: 82.25851421902482
step: 15000 epoch: 798 loss: 16.738679444564994 loss_input: 82.27439220930194
Save loss: 16.732268258422614 Name: 798_train_model.pth
step: 0 epoch: 799 loss: 18.95145606994629 loss_input: 88.10394287109375
step: 1000 epoch: 799 loss: 16.69523957726005 loss_input: 83.79110598873783
step: 2000 epoch: 799 loss: 16.775747299909234 loss_input: 83.81316934306284
step: 3000 epoch: 799 loss: 16.710563238046042 loss_input: 83.2552266309993
step: 4000 epoch: 799 loss: 16.731907526512977 loss_input: 82.73744173819826
step: 5000 epoch: 799 loss: 16.72280926509896 loss_input: 82.52391640812462
step: 6000 epoch: 799 loss: 16.709793621014605 loss_input: 82.32276052746091
step: 7000 epoch: 799 loss: 16.697228520687197 loss_input: 82.28581408884948
step: 8000 epoch: 799 loss: 16.67603223557145 loss_input: 82.2553966566557
step: 9000 epoch: 799 loss: 16.675373925935983 loss_input: 82.22404132288253
step: 10000 epoch: 799 loss: 16.658026937389003 loss_input: 82.14230818129124
step: 11000 epoch: 799 loss: 16.66531416934443 loss_input: 82.15804188477539
step: 12000 epoch: 799 loss: 16.687377187691375 loss_input: 82.23993303902735
step: 13000 epoch: 799 loss: 16.686421340359658 loss_input: 82.19540515363
step: 14000 epoch: 799 loss: 16.69690120490089 loss_input: 82.24038860854247
step: 15000 epoch: 799 loss: 16.715144003624044 loss_input: 82.21828028033808
Save loss: 16.72696252655983 Name: 799_train_model.pth
step: 0 epoch: 800 loss: 17.249231338500977 loss_input: 79.65087890625
step: 1000 epoch: 800 loss: 16.597630585109318 loss_input: 81.42355093636831
step: 2000 epoch: 800 loss: 16.653815151750297 loss_input: 81.8332291068821
step: 3000 epoch: 800 loss: 16.710749717920233 loss_input: 81.89464618157879
step: 4000 epoch: 800 loss: 16.72200603295612 loss_input: 81.9703979187088
step: 5000 epoch: 800 loss: 16.688162207198225 loss_input: 81.9120000462798
step: 6000 epoch: 800 loss: 16.686584206744644 loss_input: 81.9702414709853
step: 7000 epoch: 800 loss: 16.67710506781529 loss_input: 81.84543081631475
step: 8000 epoch: 800 loss: 16.677600721615043 loss_input: 81.9010438331439
step: 9000 epoch: 800 loss: 16.670837025154487 loss_input: 81.93713878660834
step: 10000 epoch: 800 loss: 16.691025565450353 loss_input: 82.00306461053114
step: 11000 epoch: 800 loss: 16.689039666396294 loss_input: 82.01304108961507
step: 12000 epoch: 800 loss: 16.70488909643577 loss_input: 82.0657917992511
step: 13000 epoch: 800 loss: 16.720154874756304 loss_input: 82.21893716164712
step: 14000 epoch: 800 loss: 16.728253194037766 loss_input: 82.2313096541165
step: 15000 epoch: 800 loss: 16.735310001314804 loss_input: 82.29616027307164
Save loss: 16.735146602630614 Name: 800_train_model.pth
step: 0 epoch: 801 loss: 19.454975128173828 loss_input: 111.84527587890625
step: 1000 epoch: 801 loss: 16.762045937937337 loss_input: 81.55089325957246
step: 2000 epoch: 801 loss: 16.736519170367437 loss_input: 82.46741518766984
step: 3000 epoch: 801 loss: 16.711959527278495 loss_input: 82.13043741685713
step: 4000 epoch: 801 loss: 16.680189954850174 loss_input: 82.19226307619873
step: 5000 epoch: 801 loss: 16.737025262117147 loss_input: 82.46485833448544
step: 6000 epoch: 801 loss: 16.731280129108008 loss_input: 82.4232567073782
step: 7000 epoch: 801 loss: 16.759682155714565 loss_input: 82.39868558990736
step: 8000 epoch: 801 loss: 16.75555099035677 loss_input: 82.40403418200654
step: 9000 epoch: 801 loss: 16.761727881397146 loss_input: 82.39860485336699
step: 10000 epoch: 801 loss: 16.76042832091932 loss_input: 82.23448836146659
step: 11000 epoch: 801 loss: 16.75050901126974 loss_input: 82.24876020015581
step: 12000 epoch: 801 loss: 16.74856838734187 loss_input: 82.15121981513748
step: 13000 epoch: 801 loss: 16.73613098327696 loss_input: 82.13234774364562
step: 14000 epoch: 801 loss: 16.736947952155667 loss_input: 82.15988044746943
step: 15000 epoch: 801 loss: 16.71902766870456 loss_input: 82.12736909254878
Save loss: 16.720847697004675 Name: 801_train_model.pth
step: 0 epoch: 802 loss: 19.444133758544922 loss_input: 132.913330078125
step: 1000 epoch: 802 loss: 16.941526160492643 loss_input: 83.11035375757055
step: 2000 epoch: 802 loss: 16.876820652679108 loss_input: 82.9106619480787
step: 3000 epoch: 802 loss: 16.855257989723896 loss_input: 82.5761679293751
step: 4000 epoch: 802 loss: 16.80719701685449 loss_input: 82.23832448260005
step: 5000 epoch: 802 loss: 16.817731889098482 loss_input: 82.35788077980106
step: 6000 epoch: 802 loss: 16.777668577614396 loss_input: 82.31859915233855
step: 7000 epoch: 802 loss: 16.745555731283392 loss_input: 82.32908749154697
step: 8000 epoch: 802 loss: 16.740571413795852 loss_input: 82.34998767701644
step: 9000 epoch: 802 loss: 16.733554291335786 loss_input: 82.36634672563508
step: 10000 epoch: 802 loss: 16.740477653041314 loss_input: 82.383110535382
step: 11000 epoch: 802 loss: 16.722592952695503 loss_input: 82.24588936184246
step: 12000 epoch: 802 loss: 16.727870039220313 loss_input: 82.24627012181764
step: 13000 epoch: 802 loss: 16.73695812734418 loss_input: 82.25028030352082
step: 14000 epoch: 802 loss: 16.74842854855989 loss_input: 82.32196835270695
step: 15000 epoch: 802 loss: 16.73345176385773 loss_input: 82.23422906246354
Save loss: 16.73300862801075 Name: 802_train_model.pth
step: 0 epoch: 803 loss: 16.457210540771484 loss_input: 54.6446533203125
step: 1000 epoch: 803 loss: 16.89369691811599 loss_input: 82.7498775638424
step: 2000 epoch: 803 loss: 16.72290099709705 loss_input: 82.67721345626015
step: 3000 epoch: 803 loss: 16.67301073220522 loss_input: 81.9436967259286
step: 4000 epoch: 803 loss: 16.679157074377912 loss_input: 82.05209059978658
step: 5000 epoch: 803 loss: 16.701136881579068 loss_input: 82.27534411330625
step: 6000 epoch: 803 loss: 16.722455937233473 loss_input: 82.20637797753109
step: 7000 epoch: 803 loss: 16.699216409268303 loss_input: 82.14483642578125
step: 8000 epoch: 803 loss: 16.686215192284408 loss_input: 82.07483351080138
step: 9000 epoch: 803 loss: 16.67408875361666 loss_input: 82.07693689280835
step: 10000 epoch: 803 loss: 16.714809241169466 loss_input: 82.09223220134041
step: 11000 epoch: 803 loss: 16.72114015975743 loss_input: 82.18035546351256
step: 12000 epoch: 803 loss: 16.7085981103204 loss_input: 82.06039743778676
step: 13000 epoch: 803 loss: 16.71615275316317 loss_input: 82.0574977180241
step: 14000 epoch: 803 loss: 16.718819693832447 loss_input: 82.0294371542867
step: 15000 epoch: 803 loss: 16.73420784586422 loss_input: 82.13327385840293
Save loss: 16.733060918062925 Name: 803_train_model.pth
step: 0 epoch: 804 loss: 17.947479248046875 loss_input: 143.7060546875
step: 1000 epoch: 804 loss: 16.6116206907964 loss_input: 82.934947742687
step: 2000 epoch: 804 loss: 16.72216815188311 loss_input: 82.81549712814491
step: 3000 epoch: 804 loss: 16.68487805344589 loss_input: 82.15771911478726
step: 4000 epoch: 804 loss: 16.656881138015706 loss_input: 81.90052230594695
step: 5000 epoch: 804 loss: 16.674569172945958 loss_input: 82.04681972541039
step: 6000 epoch: 804 loss: 16.69380934868946 loss_input: 82.08251501540109
step: 7000 epoch: 804 loss: 16.699690210497426 loss_input: 82.15672544826185
step: 8000 epoch: 804 loss: 16.6923179708113 loss_input: 82.08787944933397
step: 9000 epoch: 804 loss: 16.698918082954858 loss_input: 82.10880706315835
step: 10000 epoch: 804 loss: 16.70937595266352 loss_input: 82.10354187969875
step: 11000 epoch: 804 loss: 16.691271518211064 loss_input: 82.0622773473452
step: 12000 epoch: 804 loss: 16.712808135231082 loss_input: 82.06683228566402
step: 13000 epoch: 804 loss: 16.718221460284752 loss_input: 82.05519569595909
step: 14000 epoch: 804 loss: 16.717562556513702 loss_input: 82.12301060090992
step: 15000 epoch: 804 loss: 16.730307801517913 loss_input: 82.18437866674773
Save loss: 16.727045246332885 Name: 804_train_model.pth
step: 0 epoch: 805 loss: 23.98365020751953 loss_input: 110.47113037109375
step: 1000 epoch: 805 loss: 16.81703105720726 loss_input: 82.0239516343032
step: 2000 epoch: 805 loss: 16.74566440818192 loss_input: 82.17482500204976
step: 3000 epoch: 805 loss: 16.723048911099433 loss_input: 82.02089934458697
step: 4000 epoch: 805 loss: 16.733516253402488 loss_input: 82.29698518668673
step: 5000 epoch: 805 loss: 16.773284222025605 loss_input: 82.33085356805591
step: 6000 epoch: 805 loss: 16.719983334144818 loss_input: 81.97763379361903
step: 7000 epoch: 805 loss: 16.716915845802863 loss_input: 82.03713093476881
step: 8000 epoch: 805 loss: 16.714896307127457 loss_input: 82.08996160941308
step: 9000 epoch: 805 loss: 16.710096513969397 loss_input: 82.1046895803895
step: 10000 epoch: 805 loss: 16.69626066699265 loss_input: 82.02809908218163
step: 11000 epoch: 805 loss: 16.71924983840695 loss_input: 82.12975552138194
step: 12000 epoch: 805 loss: 16.722972851973278 loss_input: 82.16798512131639
step: 13000 epoch: 805 loss: 16.73559129644326 loss_input: 82.26652691285616
step: 14000 epoch: 805 loss: 16.726048246144174 loss_input: 82.19467321112108
step: 15000 epoch: 805 loss: 16.73375921770379 loss_input: 82.20861272415823
Save loss: 16.728205840215086 Name: 805_train_model.pth
step: 0 epoch: 806 loss: 15.351325988769531 loss_input: 98.02294921875
step: 1000 epoch: 806 loss: 16.607773876333095 loss_input: 81.67342020772196
step: 2000 epoch: 806 loss: 16.588584199659472 loss_input: 81.53506172328875
step: 3000 epoch: 806 loss: 16.663692892650413 loss_input: 82.36059385234219
step: 4000 epoch: 806 loss: 16.69444922958723 loss_input: 82.3169348959147
step: 5000 epoch: 806 loss: 16.695434328079987 loss_input: 82.34052541298381
step: 6000 epoch: 806 loss: 16.703326014037373 loss_input: 82.33709903940164
step: 7000 epoch: 806 loss: 16.72308567271473 loss_input: 82.28730824272593
step: 8000 epoch: 806 loss: 16.750228693508802 loss_input: 82.35748427564718
step: 9000 epoch: 806 loss: 16.7524030013371 loss_input: 82.36762491748964
step: 10000 epoch: 806 loss: 16.759951141307074 loss_input: 82.30802339077616
step: 11000 epoch: 806 loss: 16.749565725185235 loss_input: 82.23922517275595
step: 12000 epoch: 806 loss: 16.76334247401571 loss_input: 82.36073595022681
step: 13000 epoch: 806 loss: 16.741901748850292 loss_input: 82.2407797385102
step: 14000 epoch: 806 loss: 16.732295111412405 loss_input: 82.25000712316587
step: 15000 epoch: 806 loss: 16.733962156559482 loss_input: 82.26320730229313
Save loss: 16.731848379209637 Name: 806_train_model.pth
step: 0 epoch: 807 loss: 20.167869567871094 loss_input: 134.6025390625
step: 1000 epoch: 807 loss: 16.439469663770524 loss_input: 81.85999718055382
step: 2000 epoch: 807 loss: 16.652613217564955 loss_input: 82.73115279518444
step: 3000 epoch: 807 loss: 16.759106739963226 loss_input: 82.79562824354772
step: 4000 epoch: 807 loss: 16.713072899430134 loss_input: 82.48979484865112
step: 5000 epoch: 807 loss: 16.717635849622983 loss_input: 82.5230526160393
step: 6000 epoch: 807 loss: 16.72469170986265 loss_input: 82.49692504252221
step: 7000 epoch: 807 loss: 16.69430716673556 loss_input: 82.31156990477365
step: 8000 epoch: 807 loss: 16.715391940317247 loss_input: 82.35189864725162
step: 9000 epoch: 807 loss: 16.693089926246802 loss_input: 82.30046018649837
step: 10000 epoch: 807 loss: 16.691155852299786 loss_input: 82.30809244515001
step: 11000 epoch: 807 loss: 16.70279286341411 loss_input: 82.4546993031262
step: 12000 epoch: 807 loss: 16.706458442599146 loss_input: 82.41043581858088
step: 13000 epoch: 807 loss: 16.71358815417566 loss_input: 82.33522576275683
step: 14000 epoch: 807 loss: 16.713458708921216 loss_input: 82.33753815471049
step: 15000 epoch: 807 loss: 16.70850091488804 loss_input: 82.32500757680607
Save loss: 16.724706970989704 Name: 807_train_model.pth
step: 0 epoch: 808 loss: 13.152250289916992 loss_input: 105.87042236328125
step: 1000 epoch: 808 loss: 16.60630001531138 loss_input: 82.87964830579577
step: 2000 epoch: 808 loss: 16.68792641824153 loss_input: 82.97613705354354
step: 3000 epoch: 808 loss: 16.714684789556536 loss_input: 82.68221752010398
step: 4000 epoch: 808 loss: 16.672528205290938 loss_input: 82.14368623115217
step: 5000 epoch: 808 loss: 16.65056793633949 loss_input: 82.18592049412383
step: 6000 epoch: 808 loss: 16.689568942158527 loss_input: 82.18416961227491
step: 7000 epoch: 808 loss: 16.689011277377375 loss_input: 81.98757549667849
step: 8000 epoch: 808 loss: 16.699740133856462 loss_input: 81.99902503184417
step: 9000 epoch: 808 loss: 16.719248353792633 loss_input: 82.0333782425325
step: 10000 epoch: 808 loss: 16.726194922083224 loss_input: 82.12207362027469
step: 11000 epoch: 808 loss: 16.742646867281177 loss_input: 82.13947580693126
step: 12000 epoch: 808 loss: 16.724122423975082 loss_input: 82.17754592431424
step: 13000 epoch: 808 loss: 16.718296334941737 loss_input: 82.17471373333362
step: 14000 epoch: 808 loss: 16.723161958335222 loss_input: 82.18894840030957
step: 15000 epoch: 808 loss: 16.724724828048625 loss_input: 82.18037914903981
Save loss: 16.73118381099403 Name: 808_train_model.pth
step: 0 epoch: 809 loss: 11.819515228271484 loss_input: 57.30126953125
step: 1000 epoch: 809 loss: 16.65544181079655 loss_input: 83.31015542075113
step: 2000 epoch: 809 loss: 16.605061594335393 loss_input: 83.07449912738943
step: 3000 epoch: 809 loss: 16.70560531042608 loss_input: 82.99286895416371
step: 4000 epoch: 809 loss: 16.675029239842846 loss_input: 82.70511031299792
step: 5000 epoch: 809 loss: 16.699233813944684 loss_input: 82.5522705883628
step: 6000 epoch: 809 loss: 16.7159053205828 loss_input: 82.44587673701518
step: 7000 epoch: 809 loss: 16.726351655256643 loss_input: 82.29745279831675
step: 8000 epoch: 809 loss: 16.73645227015905 loss_input: 82.44226904955973
step: 9000 epoch: 809 loss: 16.728909352185685 loss_input: 82.43681423092747
step: 10000 epoch: 809 loss: 16.735589864098802 loss_input: 82.4486165336103
step: 11000 epoch: 809 loss: 16.744186697824578 loss_input: 82.41514694065454
step: 12000 epoch: 809 loss: 16.740071458207023 loss_input: 82.39616950560924
step: 13000 epoch: 809 loss: 16.734221544149115 loss_input: 82.4055828337724
step: 14000 epoch: 809 loss: 16.719554470024928 loss_input: 82.26594653203482
step: 15000 epoch: 809 loss: 16.720592037581355 loss_input: 82.26117964612907
Save loss: 16.73207031786442 Name: 809_train_model.pth
step: 0 epoch: 810 loss: 17.876506805419922 loss_input: 105.76275634765625
step: 1000 epoch: 810 loss: 16.57202559298688 loss_input: 81.82779738571975
step: 2000 epoch: 810 loss: 16.575408294044813 loss_input: 81.60280620187953
step: 3000 epoch: 810 loss: 16.617647216463837 loss_input: 81.8827091002218
step: 4000 epoch: 810 loss: 16.66161026146614 loss_input: 81.84568013575219
step: 5000 epoch: 810 loss: 16.72005794720992 loss_input: 81.89238264407665
step: 6000 epoch: 810 loss: 16.747329984818116 loss_input: 81.97952931103279
step: 7000 epoch: 810 loss: 16.75792158806159 loss_input: 82.02050233755669
step: 8000 epoch: 810 loss: 16.758396052819553 loss_input: 82.16481351238566
step: 9000 epoch: 810 loss: 16.75190840261722 loss_input: 82.08821619782047
step: 10000 epoch: 810 loss: 16.747247828446486 loss_input: 82.1848518639347
step: 11000 epoch: 810 loss: 16.742554260225646 loss_input: 82.27039511855456
step: 12000 epoch: 810 loss: 16.75424995190719 loss_input: 82.26846383406851
step: 13000 epoch: 810 loss: 16.752031100345533 loss_input: 82.24352487458897
step: 14000 epoch: 810 loss: 16.759921494402825 loss_input: 82.24109345757529
step: 15000 epoch: 810 loss: 16.742642283145607 loss_input: 82.11811218607562
Save loss: 16.731883230105044 Name: 810_train_model.pth
step: 0 epoch: 811 loss: 27.94410514831543 loss_input: 80.21795654296875
step: 1000 epoch: 811 loss: 16.62706792557037 loss_input: 83.15094055114807
step: 2000 epoch: 811 loss: 16.593326052208653 loss_input: 82.9306483232993
step: 3000 epoch: 811 loss: 16.682629703005645 loss_input: 82.63536881749053
step: 4000 epoch: 811 loss: 16.6521303398077 loss_input: 82.10166772881975
step: 5000 epoch: 811 loss: 16.647211899973826 loss_input: 82.16309985073298
step: 6000 epoch: 811 loss: 16.697928763890022 loss_input: 82.37474135577013
step: 7000 epoch: 811 loss: 16.732043843527485 loss_input: 82.41699531728447
step: 8000 epoch: 811 loss: 16.728459969503763 loss_input: 82.30205506995952
step: 9000 epoch: 811 loss: 16.738455042390342 loss_input: 82.35350630800137
step: 10000 epoch: 811 loss: 16.72425115071539 loss_input: 82.35122707211701
step: 11000 epoch: 811 loss: 16.720478596204888 loss_input: 82.3522157442807
step: 12000 epoch: 811 loss: 16.712696737174678 loss_input: 82.29259095860664
step: 13000 epoch: 811 loss: 16.717202984069953 loss_input: 82.27469448808175
step: 14000 epoch: 811 loss: 16.714291457405825 loss_input: 82.23520935202453
step: 15000 epoch: 811 loss: 16.724862585750536 loss_input: 82.25794763503397
Save loss: 16.73083858336508 Name: 811_train_model.pth
step: 0 epoch: 812 loss: 18.79685401916504 loss_input: 190.69366455078125
step: 1000 epoch: 812 loss: 16.64772076468606 loss_input: 81.32874004657452
step: 2000 epoch: 812 loss: 16.642883178057044 loss_input: 81.53230913229909
step: 3000 epoch: 812 loss: 16.68601894712337 loss_input: 82.09813013175295
step: 4000 epoch: 812 loss: 16.686482582530864 loss_input: 82.22860523272502
step: 5000 epoch: 812 loss: 16.689402343511247 loss_input: 82.3053135100519
step: 6000 epoch: 812 loss: 16.678228738128773 loss_input: 82.28158776312347
step: 7000 epoch: 812 loss: 16.688569365356873 loss_input: 82.47426823721054
step: 8000 epoch: 812 loss: 16.70071439247789 loss_input: 82.45713067147125
step: 9000 epoch: 812 loss: 16.694491306764128 loss_input: 82.46504567933206
step: 10000 epoch: 812 loss: 16.72608516376241 loss_input: 82.5249905490408
step: 11000 epoch: 812 loss: 16.706133310258004 loss_input: 82.39320319579434
step: 12000 epoch: 812 loss: 16.717703606841784 loss_input: 82.37257859635558
step: 13000 epoch: 812 loss: 16.73369392192636 loss_input: 82.40012452279812
step: 14000 epoch: 812 loss: 16.730355075866562 loss_input: 82.34734436083109
step: 15000 epoch: 812 loss: 16.731242166837927 loss_input: 82.3157057717962
Save loss: 16.72916905376315 Name: 812_train_model.pth
step: 0 epoch: 813 loss: 13.912538528442383 loss_input: 95.4049072265625
step: 1000 epoch: 813 loss: 16.691429223928537 loss_input: 82.8830188976063
step: 2000 epoch: 813 loss: 16.676976876876047 loss_input: 83.02198809042744
step: 3000 epoch: 813 loss: 16.647676550201954 loss_input: 82.54709237545818
step: 4000 epoch: 813 loss: 16.59742233545236 loss_input: 82.35520541861605
step: 5000 epoch: 813 loss: 16.62135893725033 loss_input: 82.06194970722652
step: 6000 epoch: 813 loss: 16.66745473487916 loss_input: 82.06275386393935
step: 7000 epoch: 813 loss: 16.653000073541218 loss_input: 82.10765354007742
step: 8000 epoch: 813 loss: 16.6797923532788 loss_input: 82.24858994371309
step: 9000 epoch: 813 loss: 16.712945847097547 loss_input: 82.28433855114189
step: 10000 epoch: 813 loss: 16.711125073129207 loss_input: 82.30764626174101
step: 11000 epoch: 813 loss: 16.708571496676036 loss_input: 82.26813883175038
step: 12000 epoch: 813 loss: 16.703935353679068 loss_input: 82.22646806105188
step: 13000 epoch: 813 loss: 16.72641736528945 loss_input: 82.34734100888724
step: 14000 epoch: 813 loss: 16.72402018724565 loss_input: 82.27159865516721
step: 15000 epoch: 813 loss: 16.71651140359805 loss_input: 82.23026561412833
Save loss: 16.72281109626591 Name: 813_train_model.pth
step: 0 epoch: 814 loss: 11.094522476196289 loss_input: 59.21392822265625
step: 1000 epoch: 814 loss: 16.70660606988303 loss_input: 82.18349838637924
step: 2000 epoch: 814 loss: 16.758472366371137 loss_input: 82.27203655862499
step: 3000 epoch: 814 loss: 16.75172859618998 loss_input: 82.19210942543891
step: 4000 epoch: 814 loss: 16.719132141541373 loss_input: 82.20238728464857
step: 5000 epoch: 814 loss: 16.6790316697479 loss_input: 81.91229467729501
step: 6000 epoch: 814 loss: 16.716683918149286 loss_input: 82.05493452914098
step: 7000 epoch: 814 loss: 16.691878936951746 loss_input: 82.01849761906223
step: 8000 epoch: 814 loss: 16.663610249843437 loss_input: 81.92053577378755
step: 9000 epoch: 814 loss: 16.680328020876267 loss_input: 82.0119987549669
step: 10000 epoch: 814 loss: 16.670946314410916 loss_input: 81.91426640417014
step: 11000 epoch: 814 loss: 16.687775367802352 loss_input: 81.99934480277183
step: 12000 epoch: 814 loss: 16.709530183726795 loss_input: 82.07788841693205
step: 13000 epoch: 814 loss: 16.725994547205094 loss_input: 82.19484229785058
step: 14000 epoch: 814 loss: 16.72667132055033 loss_input: 82.25094797393099
step: 15000 epoch: 814 loss: 16.72811177935364 loss_input: 82.30219659346929
Save loss: 16.726494326815008 Name: 814_train_model.pth
step: 0 epoch: 815 loss: 12.013411521911621 loss_input: 49.82464599609375
step: 1000 epoch: 815 loss: 16.658420270734972 loss_input: 81.97805959909232
step: 2000 epoch: 815 loss: 16.744728065502162 loss_input: 82.17316472989091
step: 3000 epoch: 815 loss: 16.672833378971994 loss_input: 81.6328231369166
step: 4000 epoch: 815 loss: 16.692702282431483 loss_input: 82.00603050531312
step: 5000 epoch: 815 loss: 16.747941657796524 loss_input: 82.10689233003009
step: 6000 epoch: 815 loss: 16.74835036663468 loss_input: 82.25257581378038
step: 7000 epoch: 815 loss: 16.737408803439347 loss_input: 82.27402861951641
step: 8000 epoch: 815 loss: 16.749251374273417 loss_input: 82.37195892963331
step: 9000 epoch: 815 loss: 16.731227947518423 loss_input: 82.38862092986847
step: 10000 epoch: 815 loss: 16.742292264594685 loss_input: 82.40983474894698
step: 11000 epoch: 815 loss: 16.73029707030983 loss_input: 82.36686567978018
step: 12000 epoch: 815 loss: 16.74503152907446 loss_input: 82.35891184279565
step: 13000 epoch: 815 loss: 16.729818023871335 loss_input: 82.19646187259934
step: 14000 epoch: 815 loss: 16.71583299782947 loss_input: 82.20898078726101
step: 15000 epoch: 815 loss: 16.704192772061656 loss_input: 82.16809145910455
Save loss: 16.71959180545807 Name: 815_train_model.pth
step: 0 epoch: 816 loss: 21.13002586364746 loss_input: 74.68402099609375
step: 1000 epoch: 816 loss: 16.55267450311682 loss_input: 82.03302422674982
step: 2000 epoch: 816 loss: 16.60804965721256 loss_input: 82.00238049310306
step: 3000 epoch: 816 loss: 16.601897022001985 loss_input: 81.62068619135418
step: 4000 epoch: 816 loss: 16.617414212173237 loss_input: 81.82104975770217
step: 5000 epoch: 816 loss: 16.621676979244196 loss_input: 81.747898930956
step: 6000 epoch: 816 loss: 16.650208143765997 loss_input: 81.95884307164626
step: 7000 epoch: 816 loss: 16.6435966642222 loss_input: 82.15499853083209
step: 8000 epoch: 816 loss: 16.658037201819784 loss_input: 82.2591759770263
step: 9000 epoch: 816 loss: 16.667780152878063 loss_input: 82.23509308698773
step: 10000 epoch: 816 loss: 16.636815854804823 loss_input: 82.05792628744938
step: 11000 epoch: 816 loss: 16.667481162788413 loss_input: 82.08292550575125
step: 12000 epoch: 816 loss: 16.68971780029677 loss_input: 82.15376805961156
step: 13000 epoch: 816 loss: 16.703895224927802 loss_input: 82.16992844751125
step: 14000 epoch: 816 loss: 16.7079361396452 loss_input: 82.17435224876516
step: 15000 epoch: 816 loss: 16.715473647674205 loss_input: 82.24640862565542
Save loss: 16.73391080866754 Name: 816_train_model.pth
step: 0 epoch: 817 loss: 16.349258422851562 loss_input: 79.09234619140625
step: 1000 epoch: 817 loss: 16.520739706127078 loss_input: 81.50544548225213
step: 2000 epoch: 817 loss: 16.62107737132277 loss_input: 81.99158391995408
step: 3000 epoch: 817 loss: 16.6756528380393 loss_input: 81.79053305880461
step: 4000 epoch: 817 loss: 16.643381361840994 loss_input: 82.01881242030801
step: 5000 epoch: 817 loss: 16.694127653818374 loss_input: 82.39762114715728
step: 6000 epoch: 817 loss: 16.688467584953887 loss_input: 82.31177453497334
step: 7000 epoch: 817 loss: 16.693271325939197 loss_input: 82.3110976734769
step: 8000 epoch: 817 loss: 16.6522340416059 loss_input: 82.16960307482317
step: 9000 epoch: 817 loss: 16.659842852313922 loss_input: 82.27573323829904
step: 10000 epoch: 817 loss: 16.66077076786817 loss_input: 82.2014588030943
step: 11000 epoch: 817 loss: 16.683393162149223 loss_input: 82.29566713528529
step: 12000 epoch: 817 loss: 16.694100203568535 loss_input: 82.28308770187934
step: 13000 epoch: 817 loss: 16.70334228102349 loss_input: 82.26950192251587
step: 14000 epoch: 817 loss: 16.704297385908145 loss_input: 82.25487047556034
step: 15000 epoch: 817 loss: 16.714945707676545 loss_input: 82.20442525511321
Save loss: 16.727649796023965 Name: 817_train_model.pth
step: 0 epoch: 818 loss: 20.084291458129883 loss_input: 70.19818115234375
step: 1000 epoch: 818 loss: 16.812148690104603 loss_input: 81.73225450087023
step: 2000 epoch: 818 loss: 16.658716344285285 loss_input: 81.67905890804597
step: 3000 epoch: 818 loss: 16.670933747124728 loss_input: 82.12956952174795
step: 4000 epoch: 818 loss: 16.62761423075923 loss_input: 81.95239132721703
step: 5000 epoch: 818 loss: 16.656819147101594 loss_input: 81.9616771225833
step: 6000 epoch: 818 loss: 16.66281859923593 loss_input: 82.04282537179675
step: 7000 epoch: 818 loss: 16.67643843673158 loss_input: 82.2078318035342
step: 8000 epoch: 818 loss: 16.70544662157933 loss_input: 82.39960193574436
step: 9000 epoch: 818 loss: 16.70962022728502 loss_input: 82.39874804627827
step: 10000 epoch: 818 loss: 16.689864701049924 loss_input: 82.39188716016093
step: 11000 epoch: 818 loss: 16.695082899614114 loss_input: 82.40162512636458
step: 12000 epoch: 818 loss: 16.694525941571893 loss_input: 82.32237233099228
step: 13000 epoch: 818 loss: 16.699040097499314 loss_input: 82.23263040669798
step: 14000 epoch: 818 loss: 16.70393790584676 loss_input: 82.22377217632474
step: 15000 epoch: 818 loss: 16.716688265300466 loss_input: 82.22621530228064
Save loss: 16.71825084336102 Name: 818_train_model.pth
step: 0 epoch: 819 loss: 20.38300132751465 loss_input: 69.5526123046875
step: 1000 epoch: 819 loss: 16.577488280914643 loss_input: 82.59210138006524
step: 2000 epoch: 819 loss: 16.70510513171263 loss_input: 82.65940128690538
step: 3000 epoch: 819 loss: 16.635357783183142 loss_input: 81.98535479628535
step: 4000 epoch: 819 loss: 16.64980601561245 loss_input: 81.850223128422
step: 5000 epoch: 819 loss: 16.626177834882853 loss_input: 81.65711840542048
step: 6000 epoch: 819 loss: 16.634003438267822 loss_input: 81.88737572465136
step: 7000 epoch: 819 loss: 16.6393515879453 loss_input: 81.82498411743356
step: 8000 epoch: 819 loss: 16.651506965986684 loss_input: 81.97763751515924
step: 9000 epoch: 819 loss: 16.66083273041607 loss_input: 81.9787174254944
step: 10000 epoch: 819 loss: 16.69064590900186 loss_input: 82.1430195054225
step: 11000 epoch: 819 loss: 16.705449308766937 loss_input: 82.15410045398039
step: 12000 epoch: 819 loss: 16.696220073925634 loss_input: 82.22326239557349
step: 13000 epoch: 819 loss: 16.70455788106297 loss_input: 82.19752422514975
step: 14000 epoch: 819 loss: 16.700813081228564 loss_input: 82.20293921353553
step: 15000 epoch: 819 loss: 16.70589067168128 loss_input: 82.20118212008522
Save loss: 16.723470424756407 Name: 819_train_model.pth
step: 0 epoch: 820 loss: 13.683969497680664 loss_input: 65.853271484375
step: 1000 epoch: 820 loss: 16.558321662239738 loss_input: 82.30566601367383
step: 2000 epoch: 820 loss: 16.64405805286558 loss_input: 82.2242946519904
step: 3000 epoch: 820 loss: 16.71808216223038 loss_input: 82.82405671498172
step: 4000 epoch: 820 loss: 16.729527886585426 loss_input: 82.65166859530234
step: 5000 epoch: 820 loss: 16.699971121755798 loss_input: 82.33988468419598
step: 6000 epoch: 820 loss: 16.668053425226464 loss_input: 82.44675047241475
step: 7000 epoch: 820 loss: 16.686534960292743 loss_input: 82.51767384060926
step: 8000 epoch: 820 loss: 16.70753681309565 loss_input: 82.5672165277436
step: 9000 epoch: 820 loss: 16.70240113505018 loss_input: 82.46450220130706
step: 10000 epoch: 820 loss: 16.723070336775642 loss_input: 82.62900926704205
step: 11000 epoch: 820 loss: 16.73635707802257 loss_input: 82.51614604305325
step: 12000 epoch: 820 loss: 16.7305900608338 loss_input: 82.51915836135564
step: 13000 epoch: 820 loss: 16.727351207163196 loss_input: 82.46975792746261
step: 14000 epoch: 820 loss: 16.729249381225507 loss_input: 82.40596431919901
step: 15000 epoch: 820 loss: 16.733337072442115 loss_input: 82.32806137712564
Save loss: 16.722547151729465 Name: 820_train_model.pth
step: 0 epoch: 821 loss: 17.6590633392334 loss_input: 83.32904052734375
step: 1000 epoch: 821 loss: 16.552540985139814 loss_input: 81.89678473382087
step: 2000 epoch: 821 loss: 16.706020749967614 loss_input: 82.05567443329117
step: 3000 epoch: 821 loss: 16.67881050971062 loss_input: 82.0797970093953
step: 4000 epoch: 821 loss: 16.578779435461684 loss_input: 81.63885023617142
step: 5000 epoch: 821 loss: 16.633730674500324 loss_input: 81.68598179778107
step: 6000 epoch: 821 loss: 16.66536718717199 loss_input: 81.88301772702596
step: 7000 epoch: 821 loss: 16.653271644528534 loss_input: 81.87353764089784
step: 8000 epoch: 821 loss: 16.652838529906948 loss_input: 81.89003320080596
step: 9000 epoch: 821 loss: 16.671865549660726 loss_input: 81.79491679608299
step: 10000 epoch: 821 loss: 16.703895756941105 loss_input: 82.03405895982668
step: 11000 epoch: 821 loss: 16.717168365562085 loss_input: 82.12571937263516
step: 12000 epoch: 821 loss: 16.7204638542647 loss_input: 82.24160447973736
step: 13000 epoch: 821 loss: 16.73393453131198 loss_input: 82.33920102412495
step: 14000 epoch: 821 loss: 16.73095131707135 loss_input: 82.26467301983313
step: 15000 epoch: 821 loss: 16.727327706837492 loss_input: 82.22663827619826
Save loss: 16.7254926918298 Name: 821_train_model.pth
step: 0 epoch: 822 loss: 18.852066040039062 loss_input: 84.862060546875
step: 1000 epoch: 822 loss: 16.611258867856385 loss_input: 81.28247430060175
step: 2000 epoch: 822 loss: 16.653770965078603 loss_input: 82.28992229959239
step: 3000 epoch: 822 loss: 16.654573031402915 loss_input: 81.81424701694488
step: 4000 epoch: 822 loss: 16.66857268470015 loss_input: 81.85514494789389
step: 5000 epoch: 822 loss: 16.659241537503352 loss_input: 81.64335609440612
step: 6000 epoch: 822 loss: 16.665512127551292 loss_input: 81.63999466275477
step: 7000 epoch: 822 loss: 16.65880102985945 loss_input: 81.69398945553816
step: 8000 epoch: 822 loss: 16.66435859617837 loss_input: 81.93360827455162
step: 9000 epoch: 822 loss: 16.668205215750554 loss_input: 81.9718119563321
step: 10000 epoch: 822 loss: 16.675240665063324 loss_input: 82.05058273469528
step: 11000 epoch: 822 loss: 16.661875720826423 loss_input: 82.06586012932596
step: 12000 epoch: 822 loss: 16.673812184251553 loss_input: 82.03826565071401
step: 13000 epoch: 822 loss: 16.69474046318891 loss_input: 82.15242517871972
step: 14000 epoch: 822 loss: 16.69392087743773 loss_input: 82.10099267288665
step: 15000 epoch: 822 loss: 16.70227205876501 loss_input: 82.10453463258699
Save loss: 16.71560014998913 Name: 822_train_model.pth
step: 0 epoch: 823 loss: 21.337339401245117 loss_input: 112.05413818359375
step: 1000 epoch: 823 loss: 16.46306864507906 loss_input: 81.01400930612357
step: 2000 epoch: 823 loss: 16.498628488008766 loss_input: 81.40539044442622
step: 3000 epoch: 823 loss: 16.627751903905743 loss_input: 81.86677170435375
step: 4000 epoch: 823 loss: 16.698446982981295 loss_input: 82.14369320267589
step: 5000 epoch: 823 loss: 16.704407084538826 loss_input: 82.16935051710361
step: 6000 epoch: 823 loss: 16.677392199921062 loss_input: 82.28995301727056
step: 7000 epoch: 823 loss: 16.688447329507966 loss_input: 82.27369022791666
step: 8000 epoch: 823 loss: 16.70927511461227 loss_input: 82.25593758177689
step: 9000 epoch: 823 loss: 16.703972336292637 loss_input: 82.19286680820187
step: 10000 epoch: 823 loss: 16.70441098733373 loss_input: 82.16510940458689
step: 11000 epoch: 823 loss: 16.71503157642102 loss_input: 82.22031047159776
step: 12000 epoch: 823 loss: 16.714175875688948 loss_input: 82.21938622831236
step: 13000 epoch: 823 loss: 16.71945300651288 loss_input: 82.21743793896864
step: 14000 epoch: 823 loss: 16.709753074541098 loss_input: 82.17747894803692
step: 15000 epoch: 823 loss: 16.71439020618281 loss_input: 82.19364600139939
Save loss: 16.71838363939524 Name: 823_train_model.pth
step: 0 epoch: 824 loss: 18.712352752685547 loss_input: 121.62060546875
step: 1000 epoch: 824 loss: 16.694711088776945 loss_input: 81.31605814839457
step: 2000 epoch: 824 loss: 16.64582629349159 loss_input: 81.77822212550952
step: 3000 epoch: 824 loss: 16.58748533065539 loss_input: 81.69658170990529
step: 4000 epoch: 824 loss: 16.686652813455932 loss_input: 81.85819205293892
step: 5000 epoch: 824 loss: 16.700840079433988 loss_input: 81.97652628898048
step: 6000 epoch: 824 loss: 16.688492997569494 loss_input: 81.86117170779154
step: 7000 epoch: 824 loss: 16.696716786418502 loss_input: 82.01125757320662
step: 8000 epoch: 824 loss: 16.68995049306414 loss_input: 81.99359983719076
step: 9000 epoch: 824 loss: 16.66243854478629 loss_input: 82.01735746673764
step: 10000 epoch: 824 loss: 16.666630257130766 loss_input: 82.08290840227501
step: 11000 epoch: 824 loss: 16.691211729003737 loss_input: 82.12307634675257
step: 12000 epoch: 824 loss: 16.69311582806726 loss_input: 82.19044919169333
step: 13000 epoch: 824 loss: 16.711100729417183 loss_input: 82.21202272488955
step: 14000 epoch: 824 loss: 16.72092771870725 loss_input: 82.2598241404562
step: 15000 epoch: 824 loss: 16.72343992269895 loss_input: 82.25744122348223
Save loss: 16.715167251780628 Name: 824_train_model.pth
step: 0 epoch: 825 loss: 17.876585006713867 loss_input: 45.23431396484375
step: 1000 epoch: 825 loss: 16.664419307575358 loss_input: 82.08880639624047
step: 2000 epoch: 825 loss: 16.572209916074296 loss_input: 81.57385397267187
step: 3000 epoch: 825 loss: 16.56036193431993 loss_input: 81.8866221228666
step: 4000 epoch: 825 loss: 16.548229918185545 loss_input: 81.90895141204933
step: 5000 epoch: 825 loss: 16.539058580419535 loss_input: 81.46864449131658
step: 6000 epoch: 825 loss: 16.570468704971823 loss_input: 81.5711517562828
step: 7000 epoch: 825 loss: 16.609804778690936 loss_input: 81.59342889631839
step: 8000 epoch: 825 loss: 16.63304430054897 loss_input: 81.699078455342
step: 9000 epoch: 825 loss: 16.683018791451744 loss_input: 81.9685573062954
step: 10000 epoch: 825 loss: 16.693323065144885 loss_input: 82.01528162613427
step: 11000 epoch: 825 loss: 16.69677839182949 loss_input: 82.06703093797226
step: 12000 epoch: 825 loss: 16.6924895627947 loss_input: 82.03891660253085
step: 13000 epoch: 825 loss: 16.704629966143727 loss_input: 82.16454258908419
step: 14000 epoch: 825 loss: 16.702919544862702 loss_input: 82.19169779847617
step: 15000 epoch: 825 loss: 16.706885399877862 loss_input: 82.19489408713898
Save loss: 16.720100515186786 Name: 825_train_model.pth
step: 0 epoch: 826 loss: 17.03045082092285 loss_input: 58.41461181640625
step: 1000 epoch: 826 loss: 16.818393565319873 loss_input: 82.90973400378918
step: 2000 epoch: 826 loss: 16.686249397683895 loss_input: 82.41359148306707
step: 3000 epoch: 826 loss: 16.815532696243128 loss_input: 82.56844066707582
step: 4000 epoch: 826 loss: 16.719544269120327 loss_input: 82.34361944792123
step: 5000 epoch: 826 loss: 16.74631731902521 loss_input: 82.6204187751317
step: 6000 epoch: 826 loss: 16.7284858806434 loss_input: 82.68344384212332
step: 7000 epoch: 826 loss: 16.74296190493142 loss_input: 82.74773062727925
step: 8000 epoch: 826 loss: 16.71553859506871 loss_input: 82.49450968134599
step: 9000 epoch: 826 loss: 16.715069436905132 loss_input: 82.4554375075404
step: 10000 epoch: 826 loss: 16.686926446429684 loss_input: 82.35318448135655
step: 11000 epoch: 826 loss: 16.691144948395348 loss_input: 82.31831437398624
step: 12000 epoch: 826 loss: 16.700248470843587 loss_input: 82.29580335773613
step: 13000 epoch: 826 loss: 16.708954345445505 loss_input: 82.22407830670177
step: 14000 epoch: 826 loss: 16.72157672888619 loss_input: 82.29328841187683
step: 15000 epoch: 826 loss: 16.722441552949025 loss_input: 82.20259950199284
Save loss: 16.720480959758163 Name: 826_train_model.pth
step: 0 epoch: 827 loss: 18.170310974121094 loss_input: 56.280517578125
step: 1000 epoch: 827 loss: 16.704277333441553 loss_input: 80.99782456313218
step: 2000 epoch: 827 loss: 16.611501998033958 loss_input: 80.95648800355681
step: 3000 epoch: 827 loss: 16.65314908045127 loss_input: 81.45908545621512
step: 4000 epoch: 827 loss: 16.67338515472126 loss_input: 81.38922779681056
step: 5000 epoch: 827 loss: 16.679154535456433 loss_input: 81.90510015672646
step: 6000 epoch: 827 loss: 16.67143966614892 loss_input: 82.02137804913374
step: 7000 epoch: 827 loss: 16.65247716942509 loss_input: 81.90454140793781
step: 8000 epoch: 827 loss: 16.665224582638505 loss_input: 81.96230520318261
step: 9000 epoch: 827 loss: 16.648665487468275 loss_input: 81.96762917320167
step: 10000 epoch: 827 loss: 16.66167335152185 loss_input: 81.95517692493446
step: 11000 epoch: 827 loss: 16.693817522035772 loss_input: 82.11561061143767
step: 12000 epoch: 827 loss: 16.723897173487934 loss_input: 82.2313490324781
step: 13000 epoch: 827 loss: 16.711920352947086 loss_input: 82.26751105721735
step: 14000 epoch: 827 loss: 16.714045221060907 loss_input: 82.30822379002784
step: 15000 epoch: 827 loss: 16.711841296072077 loss_input: 82.31146586890952
Save loss: 16.721273693338038 Name: 827_train_model.pth
step: 0 epoch: 828 loss: 16.013206481933594 loss_input: 118.1495361328125
step: 1000 epoch: 828 loss: 16.768516198976652 loss_input: 83.22242796266234
step: 2000 epoch: 828 loss: 16.762669339530294 loss_input: 82.83099917326493
step: 3000 epoch: 828 loss: 16.779313077929814 loss_input: 82.2886627309126
step: 4000 epoch: 828 loss: 16.765930197710276 loss_input: 82.21076292206246
step: 5000 epoch: 828 loss: 16.776124272578194 loss_input: 82.4647821534326
step: 6000 epoch: 828 loss: 16.75560604312225 loss_input: 82.419826960806
step: 7000 epoch: 828 loss: 16.729291443960307 loss_input: 82.3356412166426
step: 8000 epoch: 828 loss: 16.742664376492353 loss_input: 82.33895028410755
step: 9000 epoch: 828 loss: 16.751553811837322 loss_input: 82.38625298143957
step: 10000 epoch: 828 loss: 16.72601428125849 loss_input: 82.34443308598827
step: 11000 epoch: 828 loss: 16.718788489874097 loss_input: 82.40659806849945
step: 12000 epoch: 828 loss: 16.723590893066383 loss_input: 82.31930342372999
step: 13000 epoch: 828 loss: 16.715746329533268 loss_input: 82.23819859128174
step: 14000 epoch: 828 loss: 16.713964439121266 loss_input: 82.29046825721991
step: 15000 epoch: 828 loss: 16.713129976035578 loss_input: 82.29359446919034
Save loss: 16.71748311261833 Name: 828_train_model.pth
step: 0 epoch: 829 loss: 16.777400970458984 loss_input: 67.6204833984375
step: 1000 epoch: 829 loss: 16.484679969040663 loss_input: 82.59167120721075
step: 2000 epoch: 829 loss: 16.627387176448856 loss_input: 82.89899599760666
step: 3000 epoch: 829 loss: 16.580221958614835 loss_input: 82.1662874460141
step: 4000 epoch: 829 loss: 16.574805627730868 loss_input: 82.22753962693409
step: 5000 epoch: 829 loss: 16.58614255895235 loss_input: 81.92390877293292
step: 6000 epoch: 829 loss: 16.636298305689145 loss_input: 81.99966943782482
step: 7000 epoch: 829 loss: 16.649842069346878 loss_input: 82.01687350502662
step: 8000 epoch: 829 loss: 16.677099014577113 loss_input: 82.03096381140834
step: 9000 epoch: 829 loss: 16.673493554811717 loss_input: 82.02810000684603
step: 10000 epoch: 829 loss: 16.69639478300991 loss_input: 82.24733090655778
step: 11000 epoch: 829 loss: 16.70092176194733 loss_input: 82.17934745411993
step: 12000 epoch: 829 loss: 16.707814328323433 loss_input: 82.23529404674447
step: 13000 epoch: 829 loss: 16.70209773737929 loss_input: 82.2116861747564
step: 14000 epoch: 829 loss: 16.721588824069652 loss_input: 82.27060224893953
step: 15000 epoch: 829 loss: 16.715947118363978 loss_input: 82.16263824599193
Save loss: 16.71698665843904 Name: 829_train_model.pth
step: 0 epoch: 830 loss: 19.690889358520508 loss_input: 60.688232421875
step: 1000 epoch: 830 loss: 16.31771210452298 loss_input: 81.3437251835079
step: 2000 epoch: 830 loss: 16.49482411399357 loss_input: 81.62224814106618
step: 3000 epoch: 830 loss: 16.535652014939874 loss_input: 81.47312874875041
step: 4000 epoch: 830 loss: 16.559795831448135 loss_input: 81.78230726131943
step: 5000 epoch: 830 loss: 16.618116982911403 loss_input: 81.8713401728834
step: 6000 epoch: 830 loss: 16.645289708884274 loss_input: 82.0516686549963
step: 7000 epoch: 830 loss: 16.63818061728083 loss_input: 82.03967384542163
step: 8000 epoch: 830 loss: 16.64055399906038 loss_input: 82.1060621206529
step: 9000 epoch: 830 loss: 16.643889976625854 loss_input: 82.09240916590865
step: 10000 epoch: 830 loss: 16.639050747105962 loss_input: 82.01331319504769
step: 11000 epoch: 830 loss: 16.634193237429695 loss_input: 81.9923611347832
step: 12000 epoch: 830 loss: 16.66009655033347 loss_input: 82.13165321855503
step: 13000 epoch: 830 loss: 16.69570654722812 loss_input: 82.16041202396991
step: 14000 epoch: 830 loss: 16.715947148306235 loss_input: 82.24278270720619
step: 15000 epoch: 830 loss: 16.723166080627685 loss_input: 82.23204642802459
Save loss: 16.713102180868386 Name: 830_train_model.pth
step: 0 epoch: 831 loss: 10.781352996826172 loss_input: 62.06915283203125
step: 1000 epoch: 831 loss: 16.434773339377298 loss_input: 81.21534818940825
step: 2000 epoch: 831 loss: 16.51920511697543 loss_input: 81.33654919366488
step: 3000 epoch: 831 loss: 16.542414332977415 loss_input: 81.66505970584674
step: 4000 epoch: 831 loss: 16.561987021064137 loss_input: 81.9889456194301
step: 5000 epoch: 831 loss: 16.556981721941746 loss_input: 82.01339166981057
step: 6000 epoch: 831 loss: 16.617831814509593 loss_input: 82.1231230341818
step: 7000 epoch: 831 loss: 16.63837208493814 loss_input: 82.10082924990905
step: 8000 epoch: 831 loss: 16.63664575279273 loss_input: 82.10903458082144
step: 9000 epoch: 831 loss: 16.646185040460693 loss_input: 82.08866580742331
step: 10000 epoch: 831 loss: 16.625928747714752 loss_input: 81.93004895677424
step: 11000 epoch: 831 loss: 16.634646374619493 loss_input: 82.03550414095878
step: 12000 epoch: 831 loss: 16.651766892423552 loss_input: 82.10706991966978
step: 13000 epoch: 831 loss: 16.680995999625186 loss_input: 82.2113476481752
step: 14000 epoch: 831 loss: 16.68533100154466 loss_input: 82.25472319516938
step: 15000 epoch: 831 loss: 16.700818452713655 loss_input: 82.22191179279454
Save loss: 16.715730725735426 Name: 831_train_model.pth
step: 0 epoch: 832 loss: 17.933425903320312 loss_input: 113.765625
step: 1000 epoch: 832 loss: 16.44423468796523 loss_input: 81.42789290787337
step: 2000 epoch: 832 loss: 16.493915496141774 loss_input: 81.78199343321504
step: 3000 epoch: 832 loss: 16.537025597682597 loss_input: 81.77065912924859
step: 4000 epoch: 832 loss: 16.599589095536366 loss_input: 81.82522649366955
step: 5000 epoch: 832 loss: 16.642774572946433 loss_input: 81.95989347052465
step: 6000 epoch: 832 loss: 16.639536678264626 loss_input: 81.8913136466188
step: 7000 epoch: 832 loss: 16.653083027950135 loss_input: 82.02710824025698
step: 8000 epoch: 832 loss: 16.64544195771858 loss_input: 82.03778082274627
step: 9000 epoch: 832 loss: 16.671692598476607 loss_input: 82.01427233318476
step: 10000 epoch: 832 loss: 16.68664240348388 loss_input: 82.1099512607333
step: 11000 epoch: 832 loss: 16.688635260893445 loss_input: 82.18027482120766
step: 12000 epoch: 832 loss: 16.698075358325884 loss_input: 82.17126587921058
step: 13000 epoch: 832 loss: 16.71282825133643 loss_input: 82.25042720853654
step: 14000 epoch: 832 loss: 16.714305636321004 loss_input: 82.15825669696166
step: 15000 epoch: 832 loss: 16.71881621647307 loss_input: 82.24057465993153
Save loss: 16.719984042942524 Name: 832_train_model.pth
step: 0 epoch: 833 loss: 16.936565399169922 loss_input: 77.844482421875
step: 1000 epoch: 833 loss: 16.573331069279384 loss_input: 81.49786651336944
step: 2000 epoch: 833 loss: 16.70976754059379 loss_input: 81.70828721381497
step: 3000 epoch: 833 loss: 16.69884991208858 loss_input: 82.31744826106141
step: 4000 epoch: 833 loss: 16.745413099935845 loss_input: 82.62530134678245
step: 5000 epoch: 833 loss: 16.726072081516467 loss_input: 82.67712229038567
step: 6000 epoch: 833 loss: 16.701221095226423 loss_input: 82.40614770178277
step: 7000 epoch: 833 loss: 16.68475621421922 loss_input: 82.48039295516377
step: 8000 epoch: 833 loss: 16.694878191698226 loss_input: 82.51944623102533
step: 9000 epoch: 833 loss: 16.691161439599178 loss_input: 82.55206904365227
step: 10000 epoch: 833 loss: 16.69800241512485 loss_input: 82.5879233426755
step: 11000 epoch: 833 loss: 16.693552845619493 loss_input: 82.52971700282218
step: 12000 epoch: 833 loss: 16.701333434430175 loss_input: 82.46293248145662
step: 13000 epoch: 833 loss: 16.704634230133752 loss_input: 82.33296616625194
step: 14000 epoch: 833 loss: 16.701439212402374 loss_input: 82.27951492059998
step: 15000 epoch: 833 loss: 16.70912852562886 loss_input: 82.3078392755587
Save loss: 16.712234007775784 Name: 833_train_model.pth
step: 0 epoch: 834 loss: 19.524688720703125 loss_input: 93.33892822265625
step: 1000 epoch: 834 loss: 16.568563359838862 loss_input: 82.41437621216674
step: 2000 epoch: 834 loss: 16.608043793140205 loss_input: 81.53405526850833
step: 3000 epoch: 834 loss: 16.68961120390646 loss_input: 82.21747560383518
step: 4000 epoch: 834 loss: 16.73629056832338 loss_input: 82.47122492143077
step: 5000 epoch: 834 loss: 16.73170779948472 loss_input: 82.48743271365258
step: 6000 epoch: 834 loss: 16.703950776594557 loss_input: 82.44125973918084
step: 7000 epoch: 834 loss: 16.718060936660123 loss_input: 82.56500678300142
step: 8000 epoch: 834 loss: 16.75131072510542 loss_input: 82.70508660782636
step: 9000 epoch: 834 loss: 16.7521140325627 loss_input: 82.569176497479
step: 10000 epoch: 834 loss: 16.74578794294948 loss_input: 82.55881451649756
step: 11000 epoch: 834 loss: 16.740502607452903 loss_input: 82.49572603551488
step: 12000 epoch: 834 loss: 16.723748165114245 loss_input: 82.3065650901162
step: 13000 epoch: 834 loss: 16.729470295957782 loss_input: 82.2618947571566
step: 14000 epoch: 834 loss: 16.7281639604907 loss_input: 82.2935578454078
step: 15000 epoch: 834 loss: 16.730960444620184 loss_input: 82.2881153056593
Save loss: 16.71647125943005 Name: 834_train_model.pth
step: 0 epoch: 835 loss: 20.41228485107422 loss_input: 77.406005859375
step: 1000 epoch: 835 loss: 16.74374259697212 loss_input: 82.40848811832699
step: 2000 epoch: 835 loss: 16.75542578585204 loss_input: 82.59451234465774
step: 3000 epoch: 835 loss: 16.741913448449413 loss_input: 82.99384460144145
step: 4000 epoch: 835 loss: 16.7210901520664 loss_input: 82.56619380921371
step: 5000 epoch: 835 loss: 16.62832200643993 loss_input: 82.09146674083152
step: 6000 epoch: 835 loss: 16.63889498949806 loss_input: 82.00412521781851
step: 7000 epoch: 835 loss: 16.6658882448425 loss_input: 82.13667083552797
step: 8000 epoch: 835 loss: 16.656940474448806 loss_input: 81.99808261424597
step: 9000 epoch: 835 loss: 16.658011037764872 loss_input: 81.87692937140014
step: 10000 epoch: 835 loss: 16.679936627318483 loss_input: 81.94687682110695
step: 11000 epoch: 835 loss: 16.703598917489614 loss_input: 82.04195608571273
step: 12000 epoch: 835 loss: 16.701605437547105 loss_input: 82.11249421638367
step: 13000 epoch: 835 loss: 16.711946021262595 loss_input: 82.1106477574141
step: 14000 epoch: 835 loss: 16.70644809157412 loss_input: 82.08204824707694
step: 15000 epoch: 835 loss: 16.72040957255758 loss_input: 82.25177365698407
Save loss: 16.7198783223629 Name: 835_train_model.pth
step: 0 epoch: 836 loss: 17.02621078491211 loss_input: 115.15777587890625
step: 1000 epoch: 836 loss: 16.552605615629183 loss_input: 82.1035544045798
step: 2000 epoch: 836 loss: 16.675206276847387 loss_input: 81.99630924893998
step: 3000 epoch: 836 loss: 16.594670342747587 loss_input: 82.2761897767476
step: 4000 epoch: 836 loss: 16.614772345357704 loss_input: 82.08583141690164
step: 5000 epoch: 836 loss: 16.60095880027295 loss_input: 82.00332125957621
step: 6000 epoch: 836 loss: 16.673820371290898 loss_input: 82.18714235290211
step: 7000 epoch: 836 loss: 16.653709280032835 loss_input: 81.98255591521244
step: 8000 epoch: 836 loss: 16.675928348810164 loss_input: 81.96090165395586
step: 9000 epoch: 836 loss: 16.6769867637875 loss_input: 81.95140252835405
step: 10000 epoch: 836 loss: 16.672100081084764 loss_input: 81.95512603890978
step: 11000 epoch: 836 loss: 16.67003006924717 loss_input: 82.033758694167
step: 12000 epoch: 836 loss: 16.670711664962386 loss_input: 82.06233886027076
step: 13000 epoch: 836 loss: 16.685832738509575 loss_input: 82.1998065109024
step: 14000 epoch: 836 loss: 16.697269298955206 loss_input: 82.21549042360535
step: 15000 epoch: 836 loss: 16.708734641209592 loss_input: 82.19731093854811
Save loss: 16.71174563165009 Name: 836_train_model.pth
step: 0 epoch: 837 loss: 35.03192901611328 loss_input: 91.89031982421875
step: 1000 epoch: 837 loss: 16.688892184913932 loss_input: 82.17435744187453
step: 2000 epoch: 837 loss: 16.668958808111583 loss_input: 81.97480861619971
step: 3000 epoch: 837 loss: 16.617597111857997 loss_input: 81.86365160994512
step: 4000 epoch: 837 loss: 16.562836506521066 loss_input: 81.57094092870825
step: 5000 epoch: 837 loss: 16.56212254758597 loss_input: 81.50375828154682
step: 6000 epoch: 837 loss: 16.558251549692635 loss_input: 81.79458029781495
step: 7000 epoch: 837 loss: 16.65100004481956 loss_input: 82.16189425680538
step: 8000 epoch: 837 loss: 16.641700766739465 loss_input: 82.0995550528718
step: 9000 epoch: 837 loss: 16.658681393093485 loss_input: 82.1232728834166
step: 10000 epoch: 837 loss: 16.67607531124634 loss_input: 82.19880287361889
step: 11000 epoch: 837 loss: 16.675976529965325 loss_input: 82.13921222560201
step: 12000 epoch: 837 loss: 16.69837776699341 loss_input: 82.24453376037023
step: 13000 epoch: 837 loss: 16.69236242656533 loss_input: 82.17472141378248
step: 14000 epoch: 837 loss: 16.695928795424013 loss_input: 82.1997849632183
step: 15000 epoch: 837 loss: 16.702973071880862 loss_input: 82.25691415901049
Save loss: 16.716558513730764 Name: 837_train_model.pth
step: 0 epoch: 838 loss: 20.6151123046875 loss_input: 82.7613525390625
step: 1000 epoch: 838 loss: 16.600801256391314 loss_input: 82.6950245189381
step: 2000 epoch: 838 loss: 16.600900813021223 loss_input: 81.93359320095811
step: 3000 epoch: 838 loss: 16.54261779086028 loss_input: 81.52655472671219
step: 4000 epoch: 838 loss: 16.68330833882697 loss_input: 81.94573831212607
step: 5000 epoch: 838 loss: 16.65383640233814 loss_input: 82.02551260988037
step: 6000 epoch: 838 loss: 16.642510274910286 loss_input: 82.01651306610032
step: 7000 epoch: 838 loss: 16.68177213874515 loss_input: 82.37998970466552
step: 8000 epoch: 838 loss: 16.69103531228976 loss_input: 82.2753819514626
step: 9000 epoch: 838 loss: 16.700724599361685 loss_input: 82.31061538274176
step: 10000 epoch: 838 loss: 16.704625686256065 loss_input: 82.3162186344842
step: 11000 epoch: 838 loss: 16.7119877980564 loss_input: 82.32200091184892
step: 12000 epoch: 838 loss: 16.698639792925555 loss_input: 82.30105322542295
step: 13000 epoch: 838 loss: 16.70739785426122 loss_input: 82.28541654961337
step: 14000 epoch: 838 loss: 16.716442059895012 loss_input: 82.26730529990046
step: 15000 epoch: 838 loss: 16.702934530017995 loss_input: 82.2075649790665
Save loss: 16.71075615161657 Name: 838_train_model.pth
step: 0 epoch: 839 loss: 13.884811401367188 loss_input: 61.29510498046875
step: 1000 epoch: 839 loss: 16.71766556131018 loss_input: 82.60700718958776
step: 2000 epoch: 839 loss: 16.763302712009168 loss_input: 82.94662666225481
step: 3000 epoch: 839 loss: 16.73660655690606 loss_input: 82.54587124522945
step: 4000 epoch: 839 loss: 16.675579792200043 loss_input: 82.59128359084545
step: 5000 epoch: 839 loss: 16.6833189828137 loss_input: 82.53023875205427
step: 6000 epoch: 839 loss: 16.6654896905394 loss_input: 82.41472971667808
step: 7000 epoch: 839 loss: 16.696555032813198 loss_input: 82.35925847437531
step: 8000 epoch: 839 loss: 16.76095027393765 loss_input: 82.28808075778858
step: 9000 epoch: 839 loss: 16.779443620668626 loss_input: 82.22063841457937
step: 10000 epoch: 839 loss: 16.77780729400529 loss_input: 82.29511945045729
step: 11000 epoch: 839 loss: 16.76759023432753 loss_input: 82.21644887862645
step: 12000 epoch: 839 loss: 16.768762884532578 loss_input: 82.27112891552657
step: 13000 epoch: 839 loss: 16.754941395599744 loss_input: 82.26893995872013
step: 14000 epoch: 839 loss: 16.73996123968895 loss_input: 82.16456674516273
step: 15000 epoch: 839 loss: 16.74404259755066 loss_input: 82.17945848694723
Save loss: 16.75006129796803 Name: 839_train_model.pth
step: 0 epoch: 840 loss: 18.444190979003906 loss_input: 52.03472900390625
step: 1000 epoch: 840 loss: 16.59508691324697 loss_input: 81.04521790465394
step: 2000 epoch: 840 loss: 16.61190470476737 loss_input: 81.58733777032383
step: 3000 epoch: 840 loss: 16.634553741352114 loss_input: 82.45682935196731
step: 4000 epoch: 840 loss: 16.575832174528067 loss_input: 82.25270298331179
step: 5000 epoch: 840 loss: 16.581397446602065 loss_input: 82.28737205439771
step: 6000 epoch: 840 loss: 16.637987768981798 loss_input: 82.29116792302194
step: 7000 epoch: 840 loss: 16.672536576889087 loss_input: 82.4242798339774
step: 8000 epoch: 840 loss: 16.662535370059228 loss_input: 82.40945319762275
step: 9000 epoch: 840 loss: 16.67966523314566 loss_input: 82.52007666737116
step: 10000 epoch: 840 loss: 16.701898669257258 loss_input: 82.4297571707673
step: 11000 epoch: 840 loss: 16.705173627299274 loss_input: 82.32110348810619
step: 12000 epoch: 840 loss: 16.723210113384894 loss_input: 82.24008457669218
step: 13000 epoch: 840 loss: 16.71413308559459 loss_input: 82.1705511460056
step: 14000 epoch: 840 loss: 16.70708054428858 loss_input: 82.17115524201536
step: 15000 epoch: 840 loss: 16.702836594607987 loss_input: 82.16132509297557
Save loss: 16.71706018960476 Name: 840_train_model.pth
step: 0 epoch: 841 loss: 16.5676326751709 loss_input: 81.43267822265625
step: 1000 epoch: 841 loss: 16.566412591314936 loss_input: 82.20867377251655
step: 2000 epoch: 841 loss: 16.618376680399884 loss_input: 82.39665085670056
step: 3000 epoch: 841 loss: 16.60482735460657 loss_input: 82.24816074898862
step: 4000 epoch: 841 loss: 16.57838629865849 loss_input: 81.87658377153758
step: 5000 epoch: 841 loss: 16.598511600036712 loss_input: 82.1403719012057
step: 6000 epoch: 841 loss: 16.619373427055574 loss_input: 82.1162138972118
step: 7000 epoch: 841 loss: 16.66436582741576 loss_input: 82.1921255250366
step: 8000 epoch: 841 loss: 16.66875135917959 loss_input: 82.14511396372085
step: 9000 epoch: 841 loss: 16.678377723815693 loss_input: 82.24476428752925
step: 10000 epoch: 841 loss: 16.67988543033171 loss_input: 82.19401339285017
step: 11000 epoch: 841 loss: 16.69070659135864 loss_input: 82.15999070996209
step: 12000 epoch: 841 loss: 16.6976745486031 loss_input: 82.15485174514265
step: 13000 epoch: 841 loss: 16.693929919810692 loss_input: 82.1774068526805
step: 14000 epoch: 841 loss: 16.71796415802513 loss_input: 82.24201667044011
step: 15000 epoch: 841 loss: 16.70751707109322 loss_input: 82.23736227236543
Save loss: 16.711295193433763 Name: 841_train_model.pth
step: 0 epoch: 842 loss: 19.51006507873535 loss_input: 97.56439208984375
step: 1000 epoch: 842 loss: 16.67562943571931 loss_input: 82.5996369353303
step: 2000 epoch: 842 loss: 16.53165214315526 loss_input: 81.93673597771426
step: 3000 epoch: 842 loss: 16.611282116967175 loss_input: 82.06976757666064
step: 4000 epoch: 842 loss: 16.59286537822322 loss_input: 82.12466875346593
step: 5000 epoch: 842 loss: 16.615892941559395 loss_input: 82.39632006460036
step: 6000 epoch: 842 loss: 16.662044812551915 loss_input: 82.54769390445017
step: 7000 epoch: 842 loss: 16.680815864607123 loss_input: 82.38045231993922
step: 8000 epoch: 842 loss: 16.655388429870815 loss_input: 82.28755386137036
step: 9000 epoch: 842 loss: 16.661676755442564 loss_input: 82.23836724576175
step: 10000 epoch: 842 loss: 16.67141265113906 loss_input: 82.27944752521819
step: 11000 epoch: 842 loss: 16.692152656931064 loss_input: 82.28035001856188
step: 12000 epoch: 842 loss: 16.695584888448717 loss_input: 82.21925888907045
step: 13000 epoch: 842 loss: 16.716828604421565 loss_input: 82.21317272187747
step: 14000 epoch: 842 loss: 16.72015905795749 loss_input: 82.16041941479627
step: 15000 epoch: 842 loss: 16.719092219013046 loss_input: 82.16763312377482
Save loss: 16.712863682597877 Name: 842_train_model.pth
step: 0 epoch: 843 loss: 17.07651138305664 loss_input: 92.98406982421875
step: 1000 epoch: 843 loss: 16.844816810958513 loss_input: 82.65203930590893
step: 2000 epoch: 843 loss: 16.646830845927667 loss_input: 82.30606132480635
step: 3000 epoch: 843 loss: 16.715081961224055 loss_input: 82.84540394900918
step: 4000 epoch: 843 loss: 16.64190670073971 loss_input: 82.52567212505956
step: 5000 epoch: 843 loss: 16.64963929678435 loss_input: 82.31796326281619
step: 6000 epoch: 843 loss: 16.65371268972757 loss_input: 82.4063558478372
step: 7000 epoch: 843 loss: 16.68936313402481 loss_input: 82.38763035421557
step: 8000 epoch: 843 loss: 16.686595885459518 loss_input: 82.41874973453025
step: 9000 epoch: 843 loss: 16.65967798704519 loss_input: 82.17819180979674
step: 10000 epoch: 843 loss: 16.656201111484844 loss_input: 82.18800170764173
step: 11000 epoch: 843 loss: 16.675154781549608 loss_input: 82.14605686259523
step: 12000 epoch: 843 loss: 16.686165093362415 loss_input: 82.17903421378297
step: 13000 epoch: 843 loss: 16.69409473730357 loss_input: 82.16563063545892
step: 14000 epoch: 843 loss: 16.709893604149418 loss_input: 82.2153991346112
step: 15000 epoch: 843 loss: 16.712201802176352 loss_input: 82.21002193196885
Save loss: 16.708823999941348 Name: 843_train_model.pth
step: 0 epoch: 844 loss: 23.029178619384766 loss_input: 67.41778564453125
step: 1000 epoch: 844 loss: 16.558705942018644 loss_input: 80.90187528535917
step: 2000 epoch: 844 loss: 16.6606254041463 loss_input: 82.11574406888353
step: 3000 epoch: 844 loss: 16.644826045317558 loss_input: 82.56073130245647
step: 4000 epoch: 844 loss: 16.626923937226678 loss_input: 82.49823144494638
step: 5000 epoch: 844 loss: 16.626564507388135 loss_input: 82.32193200031868
step: 6000 epoch: 844 loss: 16.653340030920305 loss_input: 82.28871570517869
step: 7000 epoch: 844 loss: 16.657342665877586 loss_input: 82.13369858644636
step: 8000 epoch: 844 loss: 16.68801969096834 loss_input: 82.30673220443391
step: 9000 epoch: 844 loss: 16.68740380140109 loss_input: 82.31179490527528
step: 10000 epoch: 844 loss: 16.696223674351163 loss_input: 82.18016732872611
step: 11000 epoch: 844 loss: 16.700504836424102 loss_input: 82.16858431681992
step: 12000 epoch: 844 loss: 16.703067151718006 loss_input: 82.21701236200694
step: 13000 epoch: 844 loss: 16.690349476895474 loss_input: 82.04872320424721
step: 14000 epoch: 844 loss: 16.713811391524338 loss_input: 82.29604065233482
step: 15000 epoch: 844 loss: 16.709434210860756 loss_input: 82.24979485010245
Save loss: 16.709893393874168 Name: 844_train_model.pth
step: 0 epoch: 845 loss: 17.566810607910156 loss_input: 105.19573974609375
step: 1000 epoch: 845 loss: 16.609775407926424 loss_input: 82.7703544624321
step: 2000 epoch: 845 loss: 16.630237387038065 loss_input: 82.2436394412657
step: 3000 epoch: 845 loss: 16.629607424979127 loss_input: 81.82358086041631
step: 4000 epoch: 845 loss: 16.65967630428304 loss_input: 82.04570863509709
step: 5000 epoch: 845 loss: 16.645228566705786 loss_input: 81.95152782187704
step: 6000 epoch: 845 loss: 16.67257271081562 loss_input: 81.9752257476944
step: 7000 epoch: 845 loss: 16.65366288706705 loss_input: 82.0442497298098
step: 8000 epoch: 845 loss: 16.687814162025123 loss_input: 82.23011701021503
step: 9000 epoch: 845 loss: 16.676169474487 loss_input: 82.10253391577368
step: 10000 epoch: 845 loss: 16.700173460689857 loss_input: 82.17882171743763
step: 11000 epoch: 845 loss: 16.690176688631105 loss_input: 82.06970633173972
step: 12000 epoch: 845 loss: 16.684478194065345 loss_input: 82.07522181480807
step: 13000 epoch: 845 loss: 16.699749771333384 loss_input: 82.13636635072542
step: 14000 epoch: 845 loss: 16.704681024356585 loss_input: 82.18520746094168
step: 15000 epoch: 845 loss: 16.71374970581935 loss_input: 82.19491389782999
Save loss: 16.715420333608986 Name: 845_train_model.pth
step: 0 epoch: 846 loss: 17.86627197265625 loss_input: 104.201416015625
step: 1000 epoch: 846 loss: 16.57583880067229 loss_input: 82.13472391865946
step: 2000 epoch: 846 loss: 16.624451975414956 loss_input: 82.15111404868854
step: 3000 epoch: 846 loss: 16.654785079663693 loss_input: 82.27128611751812
step: 4000 epoch: 846 loss: 16.656463691575084 loss_input: 81.89533424139083
step: 5000 epoch: 846 loss: 16.595239504841036 loss_input: 81.8167156607741
step: 6000 epoch: 846 loss: 16.635651492055427 loss_input: 82.22727143742644
step: 7000 epoch: 846 loss: 16.63510843502285 loss_input: 82.2974423017694
step: 8000 epoch: 846 loss: 16.646688623765666 loss_input: 82.35536430901459
step: 9000 epoch: 846 loss: 16.686894024203795 loss_input: 82.44037541971248
step: 10000 epoch: 846 loss: 16.69823826414241 loss_input: 82.43741973458904
step: 11000 epoch: 846 loss: 16.709657655890535 loss_input: 82.43872771043364
step: 12000 epoch: 846 loss: 16.69947658373052 loss_input: 82.35887980709452
step: 13000 epoch: 846 loss: 16.706648181194655 loss_input: 82.31036976460044
step: 14000 epoch: 846 loss: 16.712516544359207 loss_input: 82.25641333410616
step: 15000 epoch: 846 loss: 16.722386578481743 loss_input: 82.29195123970902
Save loss: 16.7136477060318 Name: 846_train_model.pth
step: 0 epoch: 847 loss: 10.402438163757324 loss_input: 63.3555908203125
step: 1000 epoch: 847 loss: 16.43720757949364 loss_input: 80.38320200307506
step: 2000 epoch: 847 loss: 16.671060339204672 loss_input: 82.15650246776026
step: 3000 epoch: 847 loss: 16.70382767723068 loss_input: 82.38381533954828
step: 4000 epoch: 847 loss: 16.708009924003107 loss_input: 82.35779374553334
step: 5000 epoch: 847 loss: 16.66548923844458 loss_input: 82.08125435459783
step: 6000 epoch: 847 loss: 16.69931160690506 loss_input: 82.33553654550414
step: 7000 epoch: 847 loss: 16.722521282335535 loss_input: 82.3368531319605
step: 8000 epoch: 847 loss: 16.70042483488659 loss_input: 82.32302196203746
step: 9000 epoch: 847 loss: 16.696190752621266 loss_input: 82.39979929394781
step: 10000 epoch: 847 loss: 16.70040105655305 loss_input: 82.46410188693045
step: 11000 epoch: 847 loss: 16.715085626374524 loss_input: 82.39902100408308
step: 12000 epoch: 847 loss: 16.714658787941676 loss_input: 82.30958035194658
step: 13000 epoch: 847 loss: 16.704961709192776 loss_input: 82.21662369402324
step: 14000 epoch: 847 loss: 16.71007120082246 loss_input: 82.17186057057587
step: 15000 epoch: 847 loss: 16.709053476225034 loss_input: 82.15640065727389
Save loss: 16.708592670872807 Name: 847_train_model.pth
step: 0 epoch: 848 loss: 10.871826171875 loss_input: 60.8370361328125
step: 1000 epoch: 848 loss: 16.48863520846143 loss_input: 81.66717059795673
step: 2000 epoch: 848 loss: 16.60927498823163 loss_input: 82.35360231261322
step: 3000 epoch: 848 loss: 16.70365500021124 loss_input: 82.56381633368305
step: 4000 epoch: 848 loss: 16.67175680975472 loss_input: 82.56862906770866
step: 5000 epoch: 848 loss: 16.66815344030155 loss_input: 82.39195875424524
step: 6000 epoch: 848 loss: 16.676203346355738 loss_input: 82.33523396328476
step: 7000 epoch: 848 loss: 16.66120550990394 loss_input: 82.31338365410689
step: 8000 epoch: 848 loss: 16.702696478228287 loss_input: 82.26705814757774
step: 9000 epoch: 848 loss: 16.696367525071675 loss_input: 82.27767366261605
step: 10000 epoch: 848 loss: 16.699595235369348 loss_input: 82.30224416523191
step: 11000 epoch: 848 loss: 16.713527591323192 loss_input: 82.41594619549856
step: 12000 epoch: 848 loss: 16.715760361104298 loss_input: 82.34989956281312
step: 13000 epoch: 848 loss: 16.71270917281858 loss_input: 82.29718191280207
step: 14000 epoch: 848 loss: 16.71592843761598 loss_input: 82.30933606322344
step: 15000 epoch: 848 loss: 16.720483679492016 loss_input: 82.33755158510266
Save loss: 16.70783560717106 Name: 848_train_model.pth
step: 0 epoch: 849 loss: 19.964324951171875 loss_input: 103.37738037109375
step: 1000 epoch: 849 loss: 16.622618913888694 loss_input: 81.63274579424481
step: 2000 epoch: 849 loss: 16.530530845445732 loss_input: 81.32777309465385
step: 3000 epoch: 849 loss: 16.662164322180338 loss_input: 81.65470225745064
step: 4000 epoch: 849 loss: 16.666853854192016 loss_input: 81.99122855699649
step: 5000 epoch: 849 loss: 16.678796669121528 loss_input: 82.0510301992336
step: 6000 epoch: 849 loss: 16.666429282227988 loss_input: 82.14143014331596
step: 7000 epoch: 849 loss: 16.675643697157533 loss_input: 82.13533098264143
step: 8000 epoch: 849 loss: 16.67866720144279 loss_input: 82.1505987593836
step: 9000 epoch: 849 loss: 16.666273501459433 loss_input: 82.11607441702441
step: 10000 epoch: 849 loss: 16.666268716870682 loss_input: 82.19643728376185
step: 11000 epoch: 849 loss: 16.68953479886998 loss_input: 82.25548745502007
step: 12000 epoch: 849 loss: 16.698759317616602 loss_input: 82.32775460341684
step: 13000 epoch: 849 loss: 16.69864573303383 loss_input: 82.28097206727787
step: 14000 epoch: 849 loss: 16.695482784590972 loss_input: 82.2251250677965
step: 15000 epoch: 849 loss: 16.70379762394922 loss_input: 82.29426277587353
Save loss: 16.71375710360706 Name: 849_train_model.pth
step: 0 epoch: 850 loss: 22.83238983154297 loss_input: 105.85015869140625
step: 1000 epoch: 850 loss: 16.73125921334182 loss_input: 82.85797234991571
step: 2000 epoch: 850 loss: 16.60944816423976 loss_input: 82.24498721708481
step: 3000 epoch: 850 loss: 16.605924896223073 loss_input: 82.06667461350774
step: 4000 epoch: 850 loss: 16.629387233234052 loss_input: 82.3439673833983
step: 5000 epoch: 850 loss: 16.69243327032874 loss_input: 82.48216660378861
step: 6000 epoch: 850 loss: 16.697171931822208 loss_input: 82.31218083932626
step: 7000 epoch: 850 loss: 16.689261015236948 loss_input: 82.33200401477244
step: 8000 epoch: 850 loss: 16.697251503057235 loss_input: 82.27185314146523
step: 9000 epoch: 850 loss: 16.68209627553789 loss_input: 82.25065763499151
step: 10000 epoch: 850 loss: 16.698032748352325 loss_input: 82.12906434576269
step: 11000 epoch: 850 loss: 16.711836513264938 loss_input: 82.32059903401004
step: 12000 epoch: 850 loss: 16.706370883321338 loss_input: 82.35603361890253
step: 13000 epoch: 850 loss: 16.714405571293 loss_input: 82.20960310651434
step: 14000 epoch: 850 loss: 16.703712665901023 loss_input: 82.19305738153888
step: 15000 epoch: 850 loss: 16.706392862965096 loss_input: 82.16527087484532
Save loss: 16.71105325962603 Name: 850_train_model.pth
step: 0 epoch: 851 loss: 21.41759490966797 loss_input: 71.27191162109375
step: 1000 epoch: 851 loss: 16.42856827553931 loss_input: 81.02537013767483
step: 2000 epoch: 851 loss: 16.52794659203258 loss_input: 81.51995599490294
step: 3000 epoch: 851 loss: 16.68631136143299 loss_input: 81.65043774877775
step: 4000 epoch: 851 loss: 16.645547557073783 loss_input: 81.74016812264219
step: 5000 epoch: 851 loss: 16.716500363429052 loss_input: 81.75389136039979
step: 6000 epoch: 851 loss: 16.738060724693543 loss_input: 82.05424738780516
step: 7000 epoch: 851 loss: 16.729656001735187 loss_input: 82.03816986628863
step: 8000 epoch: 851 loss: 16.71406401093193 loss_input: 81.9972000341388
step: 9000 epoch: 851 loss: 16.72776824010106 loss_input: 82.08676052866956
step: 10000 epoch: 851 loss: 16.72484532328513 loss_input: 82.05429593759374
step: 11000 epoch: 851 loss: 16.731348920351763 loss_input: 82.1451052556568
step: 12000 epoch: 851 loss: 16.73392408129156 loss_input: 82.21582988913522
step: 13000 epoch: 851 loss: 16.734123626696807 loss_input: 82.20855143429472
step: 14000 epoch: 851 loss: 16.745844274918937 loss_input: 82.2492044112502
step: 15000 epoch: 851 loss: 16.723269382879483 loss_input: 82.21779885084484
Save loss: 16.728049019917847 Name: 851_train_model.pth
step: 0 epoch: 852 loss: 17.369312286376953 loss_input: 76.54571533203125
step: 1000 epoch: 852 loss: 16.783968390522897 loss_input: 83.1421316720389
step: 2000 epoch: 852 loss: 16.700638335684072 loss_input: 82.64470599783117
step: 3000 epoch: 852 loss: 16.67694263480497 loss_input: 82.31578410223936
step: 4000 epoch: 852 loss: 16.65917796821661 loss_input: 82.1292366117455
step: 5000 epoch: 852 loss: 16.663290456971318 loss_input: 82.39465602973155
step: 6000 epoch: 852 loss: 16.678409774985596 loss_input: 82.46222282858456
step: 7000 epoch: 852 loss: 16.663943813521765 loss_input: 82.31156004464485
step: 8000 epoch: 852 loss: 16.6809030972545 loss_input: 82.2186700768239
step: 9000 epoch: 852 loss: 16.717904041560672 loss_input: 82.38769016577368
step: 10000 epoch: 852 loss: 16.734590853801812 loss_input: 82.4653464402095
step: 11000 epoch: 852 loss: 16.72720927418866 loss_input: 82.35264289605597
step: 12000 epoch: 852 loss: 16.726942195245478 loss_input: 82.23870535304512
step: 13000 epoch: 852 loss: 16.70791357072094 loss_input: 82.2153212382476
step: 14000 epoch: 852 loss: 16.701675315795903 loss_input: 82.25775410660539
step: 15000 epoch: 852 loss: 16.726367972213566 loss_input: 82.26854636285967
Save loss: 16.714361526221037 Name: 852_train_model.pth
step: 0 epoch: 853 loss: 15.379884719848633 loss_input: 75.7275390625
step: 1000 epoch: 853 loss: 16.82594215000545 loss_input: 81.74658764087475
step: 2000 epoch: 853 loss: 16.752981253590125 loss_input: 81.98409157237788
step: 3000 epoch: 853 loss: 16.6152511221375 loss_input: 81.87848347832623
step: 4000 epoch: 853 loss: 16.58940869681509 loss_input: 81.71329227324516
step: 5000 epoch: 853 loss: 16.59290788727173 loss_input: 81.67237114100617
step: 6000 epoch: 853 loss: 16.600679061866128 loss_input: 81.63947475022067
step: 7000 epoch: 853 loss: 16.615728319925744 loss_input: 81.68346101749694
step: 8000 epoch: 853 loss: 16.638804985752493 loss_input: 81.77188371625665
step: 9000 epoch: 853 loss: 16.658701325268126 loss_input: 81.83493916393188
step: 10000 epoch: 853 loss: 16.67611101407693 loss_input: 81.87188894635928
step: 11000 epoch: 853 loss: 16.682774553969928 loss_input: 81.99682229372081
step: 12000 epoch: 853 loss: 16.702332689209626 loss_input: 82.1370204928398
step: 13000 epoch: 853 loss: 16.70212119908051 loss_input: 82.23613084168555
step: 14000 epoch: 853 loss: 16.699683178612933 loss_input: 82.24939955722331
step: 15000 epoch: 853 loss: 16.71114051617763 loss_input: 82.21853602093844
Save loss: 16.70392367385328 Name: 853_train_model.pth
step: 0 epoch: 854 loss: 16.597488403320312 loss_input: 59.06231689453125
step: 1000 epoch: 854 loss: 16.51242749626701 loss_input: 81.8173032411924
step: 2000 epoch: 854 loss: 16.51273053815995 loss_input: 82.41551685094953
step: 3000 epoch: 854 loss: 16.65559448142403 loss_input: 82.31925669958972
step: 4000 epoch: 854 loss: 16.712479101661085 loss_input: 82.61064126973628
step: 5000 epoch: 854 loss: 16.66857792753812 loss_input: 82.43087254131206
step: 6000 epoch: 854 loss: 16.668849962311096 loss_input: 82.52018049481987
step: 7000 epoch: 854 loss: 16.662843727312875 loss_input: 82.516783586929
step: 8000 epoch: 854 loss: 16.701142609141765 loss_input: 82.66310055359291
step: 9000 epoch: 854 loss: 16.716028819758552 loss_input: 82.58394287570478
step: 10000 epoch: 854 loss: 16.696015424292607 loss_input: 82.32122349898799
step: 11000 epoch: 854 loss: 16.693875018666738 loss_input: 82.28761426739969
step: 12000 epoch: 854 loss: 16.68977063723916 loss_input: 82.19371642125526
step: 13000 epoch: 854 loss: 16.707537833603244 loss_input: 82.22515832052883
step: 14000 epoch: 854 loss: 16.71180959282905 loss_input: 82.14985755586376
step: 15000 epoch: 854 loss: 16.703942496032287 loss_input: 82.15270334109887
Save loss: 16.708670629233122 Name: 854_train_model.pth
step: 0 epoch: 855 loss: 17.91288185119629 loss_input: 80.9793701171875
step: 1000 epoch: 855 loss: 16.700550402318324 loss_input: 82.19536298662275
step: 2000 epoch: 855 loss: 16.845685527302038 loss_input: 82.50017602892889
step: 3000 epoch: 855 loss: 16.810544815432106 loss_input: 83.16849978229914
step: 4000 epoch: 855 loss: 16.781687384455005 loss_input: 83.03714761922312
step: 5000 epoch: 855 loss: 16.73235305312442 loss_input: 82.6470027039514
step: 6000 epoch: 855 loss: 16.713014512712846 loss_input: 82.52656758338128
step: 7000 epoch: 855 loss: 16.728514299854485 loss_input: 82.77723813067162
step: 8000 epoch: 855 loss: 16.72400833335374 loss_input: 82.64583547692524
step: 9000 epoch: 855 loss: 16.704488451673434 loss_input: 82.37100144119186
step: 10000 epoch: 855 loss: 16.71248309765562 loss_input: 82.34467372964852
step: 11000 epoch: 855 loss: 16.722069008026804 loss_input: 82.39855555345466
step: 12000 epoch: 855 loss: 16.71592710352115 loss_input: 82.3582332545286
step: 13000 epoch: 855 loss: 16.713485159073304 loss_input: 82.28768430542152
step: 14000 epoch: 855 loss: 16.71281202370231 loss_input: 82.26546444541071
step: 15000 epoch: 855 loss: 16.71827662298659 loss_input: 82.2633410987119
Save loss: 16.7170043220222 Name: 855_train_model.pth
step: 0 epoch: 856 loss: 18.793960571289062 loss_input: 83.92059326171875
step: 1000 epoch: 856 loss: 16.51851792435546 loss_input: 80.94293529956371
step: 2000 epoch: 856 loss: 16.697982028744807 loss_input: 82.23185007837878
step: 3000 epoch: 856 loss: 16.601997820626334 loss_input: 82.00411087336123
step: 4000 epoch: 856 loss: 16.669043442333557 loss_input: 82.31617350484156
step: 5000 epoch: 856 loss: 16.691428361809557 loss_input: 82.17024851865564
step: 6000 epoch: 856 loss: 16.705668305063462 loss_input: 82.04060258755702
step: 7000 epoch: 856 loss: 16.710049037847533 loss_input: 82.06241690814538
step: 8000 epoch: 856 loss: 16.679638062845303 loss_input: 81.8863769653305
step: 9000 epoch: 856 loss: 16.675372718665457 loss_input: 82.0238981625197
step: 10000 epoch: 856 loss: 16.697683516960957 loss_input: 82.22689888603914
step: 11000 epoch: 856 loss: 16.687004668876675 loss_input: 82.14151544417481
step: 12000 epoch: 856 loss: 16.690686693947253 loss_input: 82.2007724637042
step: 13000 epoch: 856 loss: 16.681389588482993 loss_input: 82.17221034830253
step: 14000 epoch: 856 loss: 16.67841183394247 loss_input: 82.18985683936219
step: 15000 epoch: 856 loss: 16.702599199730017 loss_input: 82.29703840870243
Save loss: 16.70136873346567 Name: 856_train_model.pth
step: 0 epoch: 857 loss: 9.992863655090332 loss_input: 71.51806640625
step: 1000 epoch: 857 loss: 16.841570909444865 loss_input: 83.49311997816636
step: 2000 epoch: 857 loss: 16.803492486506684 loss_input: 83.09951463477246
step: 3000 epoch: 857 loss: 16.74898371176893 loss_input: 83.01824983713111
step: 4000 epoch: 857 loss: 16.774967757084166 loss_input: 83.02794723682361
step: 5000 epoch: 857 loss: 16.767498281521217 loss_input: 82.75994129787324
step: 6000 epoch: 857 loss: 16.787424209097626 loss_input: 82.69220764027935
step: 7000 epoch: 857 loss: 16.818058926111426 loss_input: 82.41818841902328
step: 8000 epoch: 857 loss: 16.810872383556312 loss_input: 82.40034828715258
step: 9000 epoch: 857 loss: 16.803339789011996 loss_input: 82.39015752264505
step: 10000 epoch: 857 loss: 16.76977255966363 loss_input: 82.38714000914362
step: 11000 epoch: 857 loss: 16.75949353318292 loss_input: 82.39301145186198
step: 12000 epoch: 857 loss: 16.747971155237988 loss_input: 82.34543073762408
step: 13000 epoch: 857 loss: 16.73481395488317 loss_input: 82.41438308362548
step: 14000 epoch: 857 loss: 16.739137298796365 loss_input: 82.43264619892524
step: 15000 epoch: 857 loss: 16.727241771855788 loss_input: 82.32392109324873
Save loss: 16.713865801647305 Name: 857_train_model.pth
step: 0 epoch: 858 loss: 18.853092193603516 loss_input: 83.73602294921875
step: 1000 epoch: 858 loss: 16.594737530231 loss_input: 82.77070732002373
step: 2000 epoch: 858 loss: 16.59223986934984 loss_input: 82.89774430411747
step: 3000 epoch: 858 loss: 16.67030400794492 loss_input: 82.88911073424744
step: 4000 epoch: 858 loss: 16.691156631766244 loss_input: 82.74867208490846
step: 5000 epoch: 858 loss: 16.633929880398128 loss_input: 82.54504473861087
step: 6000 epoch: 858 loss: 16.627869442966933 loss_input: 82.40621036427197
step: 7000 epoch: 858 loss: 16.631549151654482 loss_input: 82.19241056209324
step: 8000 epoch: 858 loss: 16.655706696831544 loss_input: 82.20529575679261
step: 9000 epoch: 858 loss: 16.6694614475349 loss_input: 82.14658058284323
step: 10000 epoch: 858 loss: 16.68215003441291 loss_input: 82.14597577107524
step: 11000 epoch: 858 loss: 16.699832903863125 loss_input: 82.13361066740737
step: 12000 epoch: 858 loss: 16.688572106942686 loss_input: 82.08405605990353
step: 13000 epoch: 858 loss: 16.682730165300676 loss_input: 82.12550132299017
step: 14000 epoch: 858 loss: 16.691359018191346 loss_input: 82.19755112270859
step: 15000 epoch: 858 loss: 16.699981532202713 loss_input: 82.13781127156626
Save loss: 16.713145814433695 Name: 858_train_model.pth
step: 0 epoch: 859 loss: 22.330108642578125 loss_input: 69.594482421875
step: 1000 epoch: 859 loss: 16.73252671105521 loss_input: 82.29653580062515
step: 2000 epoch: 859 loss: 16.71758729156883 loss_input: 81.88511606623446
step: 3000 epoch: 859 loss: 16.751720534607156 loss_input: 81.97941181064645
step: 4000 epoch: 859 loss: 16.769382185651136 loss_input: 82.06725241297097
step: 5000 epoch: 859 loss: 16.746526673087548 loss_input: 82.43409972790599
step: 6000 epoch: 859 loss: 16.731450583413928 loss_input: 82.54293494215808
step: 7000 epoch: 859 loss: 16.740587247948767 loss_input: 82.56296740149008
step: 8000 epoch: 859 loss: 16.737553718551638 loss_input: 82.47431923487592
step: 9000 epoch: 859 loss: 16.723588105665684 loss_input: 82.40373415947491
step: 10000 epoch: 859 loss: 16.727244358732158 loss_input: 82.38919958712137
step: 11000 epoch: 859 loss: 16.72264957202585 loss_input: 82.3318314779001
step: 12000 epoch: 859 loss: 16.721064538362075 loss_input: 82.28290409799834
step: 13000 epoch: 859 loss: 16.716905443203117 loss_input: 82.25265626633738
step: 14000 epoch: 859 loss: 16.722626895140294 loss_input: 82.27206518765679
step: 15000 epoch: 859 loss: 16.714904086795315 loss_input: 82.24979444729726
Save loss: 16.706673483520746 Name: 859_train_model.pth
step: 0 epoch: 860 loss: 15.652604103088379 loss_input: 76.42083740234375
step: 1000 epoch: 860 loss: 16.68390517325311 loss_input: 83.26655912446928
step: 2000 epoch: 860 loss: 16.76110516995683 loss_input: 83.3382517420489
step: 3000 epoch: 860 loss: 16.75784189984704 loss_input: 83.00987022473629
step: 4000 epoch: 860 loss: 16.740478406039454 loss_input: 82.82590726219813
step: 5000 epoch: 860 loss: 16.70410283125298 loss_input: 82.43223739285347
step: 6000 epoch: 860 loss: 16.698325790776508 loss_input: 82.38797615965373
step: 7000 epoch: 860 loss: 16.69063565840365 loss_input: 82.37562954873634
step: 8000 epoch: 860 loss: 16.7256035454913 loss_input: 82.58230884133869
step: 9000 epoch: 860 loss: 16.731808380688076 loss_input: 82.47472423718328
step: 10000 epoch: 860 loss: 16.715387800552524 loss_input: 82.42935132145965
step: 11000 epoch: 860 loss: 16.708461857310166 loss_input: 82.29485877038522
step: 12000 epoch: 860 loss: 16.710542500391572 loss_input: 82.27946808469086
step: 13000 epoch: 860 loss: 16.710800096480884 loss_input: 82.21907943773707
step: 14000 epoch: 860 loss: 16.709053307905783 loss_input: 82.2054637306529
step: 15000 epoch: 860 loss: 16.7067497876094 loss_input: 82.1556972821278
Save loss: 16.711238146081566 Name: 860_train_model.pth
step: 0 epoch: 861 loss: 15.191648483276367 loss_input: 73.14605712890625
step: 1000 epoch: 861 loss: 16.917618492385603 loss_input: 84.47015368664539
step: 2000 epoch: 861 loss: 16.77702852548926 loss_input: 83.10075815363803
step: 3000 epoch: 861 loss: 16.679785727421788 loss_input: 82.58714559339595
step: 4000 epoch: 861 loss: 16.67270356784669 loss_input: 82.26549896339392
step: 5000 epoch: 861 loss: 16.704571136353707 loss_input: 82.33896930330731
step: 6000 epoch: 861 loss: 16.70135383485973 loss_input: 82.36952007835933
step: 7000 epoch: 861 loss: 16.681040597905298 loss_input: 82.49876931469332
step: 8000 epoch: 861 loss: 16.693013451215908 loss_input: 82.39819757027442
step: 9000 epoch: 861 loss: 16.693084884121635 loss_input: 82.40137490419902
step: 10000 epoch: 861 loss: 16.70947869824548 loss_input: 82.41833363660704
step: 11000 epoch: 861 loss: 16.696067836477305 loss_input: 82.30701880883699
step: 12000 epoch: 861 loss: 16.710223260437207 loss_input: 82.35301928778071
step: 13000 epoch: 861 loss: 16.710146648995135 loss_input: 82.30675079906715
step: 14000 epoch: 861 loss: 16.70945780589388 loss_input: 82.28931858522382
step: 15000 epoch: 861 loss: 16.718885515445184 loss_input: 82.24213327998186
Save loss: 16.70657956750691 Name: 861_train_model.pth
step: 0 epoch: 862 loss: 14.837993621826172 loss_input: 59.06591796875
step: 1000 epoch: 862 loss: 16.798390811497157 loss_input: 83.6981938959478
step: 2000 epoch: 862 loss: 16.79729957797419 loss_input: 82.97425353485367
step: 3000 epoch: 862 loss: 16.81267484948064 loss_input: 82.79453487834785
step: 4000 epoch: 862 loss: 16.77762972471327 loss_input: 82.69666840797125
step: 5000 epoch: 862 loss: 16.74855138525632 loss_input: 82.82486715547516
step: 6000 epoch: 862 loss: 16.709185705286647 loss_input: 82.50728985151238
step: 7000 epoch: 862 loss: 16.69633475560288 loss_input: 82.45517358778545
step: 8000 epoch: 862 loss: 16.678427428904808 loss_input: 82.26795654144306
step: 9000 epoch: 862 loss: 16.712979211315634 loss_input: 82.42530048880202
step: 10000 epoch: 862 loss: 16.719213984749484 loss_input: 82.3102487358686
step: 11000 epoch: 862 loss: 16.697854734856307 loss_input: 82.21099316455074
step: 12000 epoch: 862 loss: 16.71139757155021 loss_input: 82.22659000930423
step: 13000 epoch: 862 loss: 16.718650822804143 loss_input: 82.33762748981749
step: 14000 epoch: 862 loss: 16.705519177966284 loss_input: 82.35985598806977
step: 15000 epoch: 862 loss: 16.701808302905206 loss_input: 82.30025447847747
Save loss: 16.70527007739246 Name: 862_train_model.pth
step: 0 epoch: 863 loss: 18.61526870727539 loss_input: 122.7069091796875
step: 1000 epoch: 863 loss: 16.66416063413515 loss_input: 83.6545325402137
step: 2000 epoch: 863 loss: 16.570199618870948 loss_input: 83.06057756034092
step: 3000 epoch: 863 loss: 16.572610238519204 loss_input: 82.85603492683428
step: 4000 epoch: 863 loss: 16.54773252762964 loss_input: 82.52359323804303
step: 5000 epoch: 863 loss: 16.559437188499572 loss_input: 82.57306112019783
step: 6000 epoch: 863 loss: 16.587358018752436 loss_input: 82.38230564681277
step: 7000 epoch: 863 loss: 16.59016142986686 loss_input: 82.22728796486105
step: 8000 epoch: 863 loss: 16.620829857970577 loss_input: 82.35337705436967
step: 9000 epoch: 863 loss: 16.634512154397243 loss_input: 82.39987295519681
step: 10000 epoch: 863 loss: 16.62956058539196 loss_input: 82.31579432999082
step: 11000 epoch: 863 loss: 16.66217657284632 loss_input: 82.18997224839468
step: 12000 epoch: 863 loss: 16.679866412512194 loss_input: 82.17707130871418
step: 13000 epoch: 863 loss: 16.684272899984187 loss_input: 82.18738448342015
step: 14000 epoch: 863 loss: 16.69761356208403 loss_input: 82.1848823237305
step: 15000 epoch: 863 loss: 16.704545623262312 loss_input: 82.18070740677835
Save loss: 16.71329380658269 Name: 863_train_model.pth
step: 0 epoch: 864 loss: 19.78036880493164 loss_input: 68.28973388671875
step: 1000 epoch: 864 loss: 16.672158726683627 loss_input: 82.88530975860077
step: 2000 epoch: 864 loss: 16.672862889348476 loss_input: 82.19946316514594
step: 3000 epoch: 864 loss: 16.611763855490196 loss_input: 82.00355785538935
step: 4000 epoch: 864 loss: 16.620140887534312 loss_input: 81.78338734408194
step: 5000 epoch: 864 loss: 16.57927076350782 loss_input: 81.66357114319324
step: 6000 epoch: 864 loss: 16.616490904002802 loss_input: 81.72153733655068
step: 7000 epoch: 864 loss: 16.63436300811419 loss_input: 81.78437771829192
step: 8000 epoch: 864 loss: 16.663131089705765 loss_input: 82.1436054547747
step: 9000 epoch: 864 loss: 16.671489218369732 loss_input: 82.17614122926229
step: 10000 epoch: 864 loss: 16.689342946651493 loss_input: 82.23788958994726
step: 11000 epoch: 864 loss: 16.682123992282925 loss_input: 82.27343939746599
step: 12000 epoch: 864 loss: 16.67314773336588 loss_input: 82.24608718418159
step: 13000 epoch: 864 loss: 16.67608489400469 loss_input: 82.19600302211579
step: 14000 epoch: 864 loss: 16.693452740131075 loss_input: 82.23821582934505
step: 15000 epoch: 864 loss: 16.700097440854762 loss_input: 82.30553237368811
Save loss: 16.704292127877473 Name: 864_train_model.pth
step: 0 epoch: 865 loss: 19.848342895507812 loss_input: 97.3865966796875
step: 1000 epoch: 865 loss: 16.466798668974764 loss_input: 81.85110769357595
step: 2000 epoch: 865 loss: 16.454993992671557 loss_input: 81.98586391008597
step: 3000 epoch: 865 loss: 16.52331083784577 loss_input: 81.99782555391693
step: 4000 epoch: 865 loss: 16.576416889091515 loss_input: 81.52435427825172
step: 5000 epoch: 865 loss: 16.583784307057655 loss_input: 81.60557370947686
step: 6000 epoch: 865 loss: 16.587801070436598 loss_input: 81.62473737897525
step: 7000 epoch: 865 loss: 16.611137509226815 loss_input: 81.72634094787519
step: 8000 epoch: 865 loss: 16.6491198133579 loss_input: 81.94283604106371
step: 9000 epoch: 865 loss: 16.642561884962603 loss_input: 81.952946803925
step: 10000 epoch: 865 loss: 16.64927121391655 loss_input: 82.07290136464869
step: 11000 epoch: 865 loss: 16.67095888769526 loss_input: 82.18582773325649
step: 12000 epoch: 865 loss: 16.685759576198787 loss_input: 82.24343049435822
step: 13000 epoch: 865 loss: 16.676022438899928 loss_input: 82.24186378504238
step: 14000 epoch: 865 loss: 16.7017838345469 loss_input: 82.23772523764346
step: 15000 epoch: 865 loss: 16.708956754364163 loss_input: 82.2363567851987
Save loss: 16.703269192248584 Name: 865_train_model.pth
step: 0 epoch: 866 loss: 20.925100326538086 loss_input: 64.582275390625
step: 1000 epoch: 866 loss: 16.802075188357634 loss_input: 82.92596844073894
step: 2000 epoch: 866 loss: 16.75162886334085 loss_input: 82.68373701942974
step: 3000 epoch: 866 loss: 16.72978966127273 loss_input: 82.54738795157473
step: 4000 epoch: 866 loss: 16.709385049608283 loss_input: 82.40755314101162
step: 5000 epoch: 866 loss: 16.62645397398906 loss_input: 82.03068318221122
step: 6000 epoch: 866 loss: 16.63140375676542 loss_input: 82.01458897882254
step: 7000 epoch: 866 loss: 16.678071469958212 loss_input: 82.34064883692676
step: 8000 epoch: 866 loss: 16.670805999210902 loss_input: 82.2222997195541
step: 9000 epoch: 866 loss: 16.650113092370994 loss_input: 82.14894079423033
step: 10000 epoch: 866 loss: 16.683687673808457 loss_input: 82.2629563764815
step: 11000 epoch: 866 loss: 16.692788479209174 loss_input: 82.28173137935613
step: 12000 epoch: 866 loss: 16.680696718872174 loss_input: 82.20255716096737
step: 13000 epoch: 866 loss: 16.707808948096822 loss_input: 82.25911583837001
step: 14000 epoch: 866 loss: 16.710564312224097 loss_input: 82.22363050640774
step: 15000 epoch: 866 loss: 16.724575547200587 loss_input: 82.22050653572965
Save loss: 16.707447132244706 Name: 866_train_model.pth
step: 0 epoch: 867 loss: 9.468843460083008 loss_input: 63.2891845703125
step: 1000 epoch: 867 loss: 16.860079232748454 loss_input: 82.8052927297312
step: 2000 epoch: 867 loss: 16.733332423077172 loss_input: 82.40320791142514
step: 3000 epoch: 867 loss: 16.682437044109992 loss_input: 82.18659360454068
step: 4000 epoch: 867 loss: 16.683783095468733 loss_input: 82.09991783060094
step: 5000 epoch: 867 loss: 16.69987804015811 loss_input: 81.92643608729426
step: 6000 epoch: 867 loss: 16.672259880808706 loss_input: 81.7847003221631
step: 7000 epoch: 867 loss: 16.695031928157928 loss_input: 82.12605007320383
step: 8000 epoch: 867 loss: 16.676106566385155 loss_input: 81.853492167544
step: 9000 epoch: 867 loss: 16.669028645448588 loss_input: 81.8566629668101
step: 10000 epoch: 867 loss: 16.689875541144808 loss_input: 82.01437757225254
step: 11000 epoch: 867 loss: 16.718909207544222 loss_input: 82.13664555885545
step: 12000 epoch: 867 loss: 16.722189775060368 loss_input: 82.16057158505834
step: 13000 epoch: 867 loss: 16.714997469136442 loss_input: 82.25120671306534
step: 14000 epoch: 867 loss: 16.711858981848188 loss_input: 82.23484951008662
step: 15000 epoch: 867 loss: 16.703049059669443 loss_input: 82.20482052905457
Save loss: 16.697499852880835 Name: 867_train_model.pth
step: 0 epoch: 868 loss: 19.216529846191406 loss_input: 68.20343017578125
step: 1000 epoch: 868 loss: 16.851309771542546 loss_input: 83.42464767898117
step: 2000 epoch: 868 loss: 16.73104752498171 loss_input: 82.19815470682627
step: 3000 epoch: 868 loss: 16.732680716462152 loss_input: 82.51136287090502
step: 4000 epoch: 868 loss: 16.74987075955115 loss_input: 82.68806926991397
step: 5000 epoch: 868 loss: 16.69815491318965 loss_input: 82.65702727374448
step: 6000 epoch: 868 loss: 16.72557523329642 loss_input: 82.55710971389209
step: 7000 epoch: 868 loss: 16.71801101298115 loss_input: 82.36882878861893
step: 8000 epoch: 868 loss: 16.725121699635825 loss_input: 82.24160249011842
step: 9000 epoch: 868 loss: 16.717857181913335 loss_input: 82.25305167644133
step: 10000 epoch: 868 loss: 16.697275048434143 loss_input: 82.2584755806646
step: 11000 epoch: 868 loss: 16.690193849437378 loss_input: 82.35238207215538
step: 12000 epoch: 868 loss: 16.689017944738833 loss_input: 82.2929263900453
step: 13000 epoch: 868 loss: 16.6989805707564 loss_input: 82.21964496477284
step: 14000 epoch: 868 loss: 16.701600546309304 loss_input: 82.1114304823651
step: 15000 epoch: 868 loss: 16.70802516174685 loss_input: 82.18814149369209
Save loss: 16.710039293840527 Name: 868_train_model.pth
step: 0 epoch: 869 loss: 16.50966453552246 loss_input: 101.3436279296875
step: 1000 epoch: 869 loss: 16.47001413913159 loss_input: 80.9176025390625
step: 2000 epoch: 869 loss: 16.484610367988957 loss_input: 81.14324066604394
step: 3000 epoch: 869 loss: 16.48680111035948 loss_input: 81.53651950407807
step: 4000 epoch: 869 loss: 16.543313426811736 loss_input: 81.45276003997047
step: 5000 epoch: 869 loss: 16.621634572440446 loss_input: 81.67073128343081
step: 6000 epoch: 869 loss: 16.660351123000915 loss_input: 82.06790864445769
step: 7000 epoch: 869 loss: 16.706471606061008 loss_input: 82.12225945086807
step: 8000 epoch: 869 loss: 16.71720415373889 loss_input: 82.08501730405037
step: 9000 epoch: 869 loss: 16.74796188898768 loss_input: 82.26363466803384
step: 10000 epoch: 869 loss: 16.74337864191505 loss_input: 82.34305191917332
step: 11000 epoch: 869 loss: 16.749177353196377 loss_input: 82.30484348613017
step: 12000 epoch: 869 loss: 16.734907053538276 loss_input: 82.24197120943046
step: 13000 epoch: 869 loss: 16.731100968567834 loss_input: 82.22248506424987
step: 14000 epoch: 869 loss: 16.717796481240605 loss_input: 82.21696659728686
step: 15000 epoch: 869 loss: 16.707898519284072 loss_input: 82.14849281857772
Save loss: 16.70461969484389 Name: 869_train_model.pth
step: 0 epoch: 870 loss: 26.868846893310547 loss_input: 77.106201171875
step: 1000 epoch: 870 loss: 16.4685919668291 loss_input: 82.77987058107908
step: 2000 epoch: 870 loss: 16.512189969487455 loss_input: 83.06509159970796
step: 3000 epoch: 870 loss: 16.554271643815618 loss_input: 82.64169650155678
step: 4000 epoch: 870 loss: 16.591453682509282 loss_input: 82.62520055716051
step: 5000 epoch: 870 loss: 16.59484177173507 loss_input: 82.2721009142898
step: 6000 epoch: 870 loss: 16.614830341523458 loss_input: 82.20666610699975
step: 7000 epoch: 870 loss: 16.63524981809163 loss_input: 82.12316082007816
step: 8000 epoch: 870 loss: 16.648383374244567 loss_input: 82.04141440938047
step: 9000 epoch: 870 loss: 16.668480611883155 loss_input: 82.0974084747962
step: 10000 epoch: 870 loss: 16.666993704215013 loss_input: 82.08684600900261
step: 11000 epoch: 870 loss: 16.67319003977176 loss_input: 82.06326555540406
step: 12000 epoch: 870 loss: 16.68318553910972 loss_input: 82.1032157486632
step: 13000 epoch: 870 loss: 16.678495633786444 loss_input: 82.05467794638544
step: 14000 epoch: 870 loss: 16.704446156921833 loss_input: 82.16727258041837
step: 15000 epoch: 870 loss: 16.706998804920268 loss_input: 82.16153767645959
Save loss: 16.70536976787448 Name: 870_train_model.pth
step: 0 epoch: 871 loss: 13.602859497070312 loss_input: 86.98590087890625
step: 1000 epoch: 871 loss: 16.37486456157444 loss_input: 79.887428733376
step: 2000 epoch: 871 loss: 16.578031655373067 loss_input: 81.09158018646927
step: 3000 epoch: 871 loss: 16.589610786447523 loss_input: 81.58918153584221
step: 4000 epoch: 871 loss: 16.64411047314799 loss_input: 81.95042329810613
step: 5000 epoch: 871 loss: 16.61906001997385 loss_input: 81.61470664851404
step: 6000 epoch: 871 loss: 16.653906997651422 loss_input: 81.6774838165807
step: 7000 epoch: 871 loss: 16.677598318871798 loss_input: 81.7877416350538
step: 8000 epoch: 871 loss: 16.701794732855106 loss_input: 82.00929861947188
step: 9000 epoch: 871 loss: 16.7027383124533 loss_input: 82.14780188947105
step: 10000 epoch: 871 loss: 16.720141647076822 loss_input: 82.19058403607882
step: 11000 epoch: 871 loss: 16.703852371631584 loss_input: 82.13196717889815
step: 12000 epoch: 871 loss: 16.70397386420976 loss_input: 82.17554529757611
step: 13000 epoch: 871 loss: 16.707187710482472 loss_input: 82.2786218217589
step: 14000 epoch: 871 loss: 16.70558555934542 loss_input: 82.21836108560605
step: 15000 epoch: 871 loss: 16.705532063087553 loss_input: 82.21377337458102
Save loss: 16.701558768317103 Name: 871_train_model.pth
step: 0 epoch: 872 loss: 28.837875366210938 loss_input: 140.54949951171875
step: 1000 epoch: 872 loss: 16.551306475888005 loss_input: 81.92199963122815
step: 2000 epoch: 872 loss: 16.59772771742867 loss_input: 81.58722658934205
step: 3000 epoch: 872 loss: 16.674040560800208 loss_input: 82.06055165449408
step: 4000 epoch: 872 loss: 16.666906324394702 loss_input: 82.17478063040392
step: 5000 epoch: 872 loss: 16.630343714753906 loss_input: 82.19528718767965
step: 6000 epoch: 872 loss: 16.605875410093624 loss_input: 82.16847736274097
step: 7000 epoch: 872 loss: 16.632716106833808 loss_input: 82.06120269125896
step: 8000 epoch: 872 loss: 16.621630690810292 loss_input: 82.24018813139081
step: 9000 epoch: 872 loss: 16.627757195617978 loss_input: 82.30475943211171
step: 10000 epoch: 872 loss: 16.65597168854053 loss_input: 82.3066569807863
step: 11000 epoch: 872 loss: 16.672580789386245 loss_input: 82.30898547131369
step: 12000 epoch: 872 loss: 16.67917744519959 loss_input: 82.26474224580406
step: 13000 epoch: 872 loss: 16.677706555159 loss_input: 82.23139999972959
step: 14000 epoch: 872 loss: 16.686125603788845 loss_input: 82.30435136416735
step: 15000 epoch: 872 loss: 16.69270836249263 loss_input: 82.25082929448584
Save loss: 16.696561702489852 Name: 872_train_model.pth
step: 0 epoch: 873 loss: 17.459794998168945 loss_input: 107.39678955078125
step: 1000 epoch: 873 loss: 16.90174638022195 loss_input: 83.71104030247096
step: 2000 epoch: 873 loss: 16.841399708251725 loss_input: 83.28601919133207
step: 3000 epoch: 873 loss: 16.870893988915977 loss_input: 83.38498851213006
step: 4000 epoch: 873 loss: 16.861206641408153 loss_input: 83.01356345789041
step: 5000 epoch: 873 loss: 16.792957793233683 loss_input: 82.81215260854079
step: 6000 epoch: 873 loss: 16.706010045418836 loss_input: 82.30060256884269
step: 7000 epoch: 873 loss: 16.690235512952366 loss_input: 82.2137873997911
step: 8000 epoch: 873 loss: 16.720448572923804 loss_input: 82.48711644809048
step: 9000 epoch: 873 loss: 16.73305007873966 loss_input: 82.52023646000053
step: 10000 epoch: 873 loss: 16.73555286032428 loss_input: 82.63093340324171
step: 11000 epoch: 873 loss: 16.734991580028446 loss_input: 82.53064004790403
step: 12000 epoch: 873 loss: 16.756883837105722 loss_input: 82.54182133381788
step: 13000 epoch: 873 loss: 16.73473033466006 loss_input: 82.40143389848917
step: 14000 epoch: 873 loss: 16.73417012831644 loss_input: 82.33904717167943
step: 15000 epoch: 873 loss: 16.71687117950669 loss_input: 82.28129430450079
Save loss: 16.70646410705149 Name: 873_train_model.pth
step: 0 epoch: 874 loss: 19.44133186340332 loss_input: 114.708740234375
step: 1000 epoch: 874 loss: 16.55844909232575 loss_input: 82.60306447702688
step: 2000 epoch: 874 loss: 16.663700372204072 loss_input: 82.93731860266156
step: 3000 epoch: 874 loss: 16.63582140149692 loss_input: 82.71005636467531
step: 4000 epoch: 874 loss: 16.724373796587436 loss_input: 82.78605484432114
step: 5000 epoch: 874 loss: 16.740571002201232 loss_input: 82.5850460156992
step: 6000 epoch: 874 loss: 16.679061665890156 loss_input: 82.35807756812666
step: 7000 epoch: 874 loss: 16.674466678337136 loss_input: 82.19309941980944
step: 8000 epoch: 874 loss: 16.71422548309563 loss_input: 82.39493581673872
step: 9000 epoch: 874 loss: 16.707702965540907 loss_input: 82.32208089888884
step: 10000 epoch: 874 loss: 16.696840737917558 loss_input: 82.18871209802847
step: 11000 epoch: 874 loss: 16.694611380722467 loss_input: 82.14491069326617
step: 12000 epoch: 874 loss: 16.68297080560164 loss_input: 82.16044628523795
step: 13000 epoch: 874 loss: 16.687866047633335 loss_input: 82.22348282309865
step: 14000 epoch: 874 loss: 16.682816312599332 loss_input: 82.15111279806047
step: 15000 epoch: 874 loss: 16.692242319000123 loss_input: 82.16058144138047
Save loss: 16.69791652828455 Name: 874_train_model.pth
step: 0 epoch: 875 loss: 22.468185424804688 loss_input: 87.0250244140625
step: 1000 epoch: 875 loss: 16.645275132639426 loss_input: 82.91959858988668
step: 2000 epoch: 875 loss: 16.656442525326042 loss_input: 82.69812411299233
step: 3000 epoch: 875 loss: 16.6548699194175 loss_input: 82.35701124527502
step: 4000 epoch: 875 loss: 16.666591647683724 loss_input: 82.61876012015063
step: 5000 epoch: 875 loss: 16.66715196947221 loss_input: 82.66914818940509
step: 6000 epoch: 875 loss: 16.674522316747854 loss_input: 82.48925314408862
step: 7000 epoch: 875 loss: 16.637721094229004 loss_input: 82.22550893085307
step: 8000 epoch: 875 loss: 16.651163757450803 loss_input: 82.10291128464898
step: 9000 epoch: 875 loss: 16.639862864855303 loss_input: 82.0547782695369
step: 10000 epoch: 875 loss: 16.690647045358062 loss_input: 82.2078761423174
step: 11000 epoch: 875 loss: 16.673165565793617 loss_input: 82.27117104344384
step: 12000 epoch: 875 loss: 16.674047194603194 loss_input: 82.21884042113516
step: 13000 epoch: 875 loss: 16.68176032389029 loss_input: 82.12352206756331
step: 14000 epoch: 875 loss: 16.696077489229655 loss_input: 82.15750819347167
step: 15000 epoch: 875 loss: 16.70315156826281 loss_input: 82.22341651425712
Save loss: 16.700553475469352 Name: 875_train_model.pth
step: 0 epoch: 876 loss: 16.445751190185547 loss_input: 150.67388916015625
step: 1000 epoch: 876 loss: 16.602010322498394 loss_input: 82.24260413658607
step: 2000 epoch: 876 loss: 16.62215912765053 loss_input: 81.73960959583685
step: 3000 epoch: 876 loss: 16.661594861191695 loss_input: 81.87155180857683
step: 4000 epoch: 876 loss: 16.708925971803712 loss_input: 82.17995919111043
step: 5000 epoch: 876 loss: 16.676492224691202 loss_input: 81.97927138205172
step: 6000 epoch: 876 loss: 16.69155382812073 loss_input: 82.02154424051069
step: 7000 epoch: 876 loss: 16.72102363503604 loss_input: 82.13735184698781
step: 8000 epoch: 876 loss: 16.722279669686923 loss_input: 82.04251129527998
step: 9000 epoch: 876 loss: 16.695626566375896 loss_input: 81.92748579286547
step: 10000 epoch: 876 loss: 16.67787276920158 loss_input: 81.99832687019384
step: 11000 epoch: 876 loss: 16.671829695702034 loss_input: 82.0503674748052
step: 12000 epoch: 876 loss: 16.681573964389063 loss_input: 82.08207056353702
step: 13000 epoch: 876 loss: 16.683919951453795 loss_input: 82.1628386702375
step: 14000 epoch: 876 loss: 16.682736589758576 loss_input: 82.21222392893392
step: 15000 epoch: 876 loss: 16.697598865704332 loss_input: 82.22826824158989
Save loss: 16.708271458178757 Name: 876_train_model.pth
step: 0 epoch: 877 loss: 15.421615600585938 loss_input: 76.8865966796875
step: 1000 epoch: 877 loss: 16.389194312271897 loss_input: 81.77085475559596
step: 2000 epoch: 877 loss: 16.581442889662043 loss_input: 81.798033478378
step: 3000 epoch: 877 loss: 16.62176195489133 loss_input: 82.4632001531716
step: 4000 epoch: 877 loss: 16.625395821202133 loss_input: 82.22292733091142
step: 5000 epoch: 877 loss: 16.657329021084667 loss_input: 82.21248410230064
step: 6000 epoch: 877 loss: 16.670957739721477 loss_input: 82.15112271123758
step: 7000 epoch: 877 loss: 16.674602982930125 loss_input: 82.10935010121322
step: 8000 epoch: 877 loss: 16.669624931796495 loss_input: 82.24117802077718
step: 9000 epoch: 877 loss: 16.686094346834203 loss_input: 82.23620386444692
step: 10000 epoch: 877 loss: 16.67573807232619 loss_input: 82.17202286729335
step: 11000 epoch: 877 loss: 16.688212008121262 loss_input: 82.22783985524576
step: 12000 epoch: 877 loss: 16.684172530222334 loss_input: 82.17513124414926
step: 13000 epoch: 877 loss: 16.681344946387473 loss_input: 82.08578849178068
step: 14000 epoch: 877 loss: 16.704973025182667 loss_input: 82.20583052138296
step: 15000 epoch: 877 loss: 16.70089155459005 loss_input: 82.1801097537508
Save loss: 16.692634702339767 Name: 877_train_model.pth
step: 0 epoch: 878 loss: 10.78633975982666 loss_input: 58.8388671875
step: 1000 epoch: 878 loss: 16.604787623608388 loss_input: 81.71531251951174
step: 2000 epoch: 878 loss: 16.544253249933337 loss_input: 82.55852661437835
step: 3000 epoch: 878 loss: 16.613219926135613 loss_input: 82.57466444290705
step: 4000 epoch: 878 loss: 16.607256037269465 loss_input: 82.50391869805986
step: 5000 epoch: 878 loss: 16.618899220253223 loss_input: 82.38038205347212
step: 6000 epoch: 878 loss: 16.60912278194742 loss_input: 82.20363814893156
step: 7000 epoch: 878 loss: 16.635054204076074 loss_input: 82.35555031619367
step: 8000 epoch: 878 loss: 16.69708333270518 loss_input: 82.2918089981184
step: 9000 epoch: 878 loss: 16.71081265159851 loss_input: 82.37441962693164
step: 10000 epoch: 878 loss: 16.707495625383103 loss_input: 82.30488284179394
step: 11000 epoch: 878 loss: 16.69200170693555 loss_input: 82.28309641195615
step: 12000 epoch: 878 loss: 16.694932258960534 loss_input: 82.25932011057184
step: 13000 epoch: 878 loss: 16.698756503138245 loss_input: 82.25215612170459
step: 14000 epoch: 878 loss: 16.698312838463586 loss_input: 82.27241190797336
step: 15000 epoch: 878 loss: 16.69112416356653 loss_input: 82.26739820942275
Save loss: 16.694676766142248 Name: 878_train_model.pth
step: 0 epoch: 879 loss: 9.459397315979004 loss_input: 49.43768310546875
step: 1000 epoch: 879 loss: 16.442184044288233 loss_input: 82.92710207273196
step: 2000 epoch: 879 loss: 16.374279515496617 loss_input: 82.73970329898528
step: 3000 epoch: 879 loss: 16.412403474367288 loss_input: 82.29305711979471
step: 4000 epoch: 879 loss: 16.450405390552806 loss_input: 82.08257118459643
step: 5000 epoch: 879 loss: 16.475502131152595 loss_input: 81.8500741868228
step: 6000 epoch: 879 loss: 16.49580679534177 loss_input: 81.70876186773233
step: 7000 epoch: 879 loss: 16.511647016827403 loss_input: 81.94288239235912
step: 8000 epoch: 879 loss: 16.578813381425114 loss_input: 81.99251273095257
step: 9000 epoch: 879 loss: 16.607218211789274 loss_input: 82.02927516246025
step: 10000 epoch: 879 loss: 16.616690229814107 loss_input: 81.97449951049816
step: 11000 epoch: 879 loss: 16.645439426591857 loss_input: 82.00379717344588
step: 12000 epoch: 879 loss: 16.64647221303008 loss_input: 81.97797371357562
step: 13000 epoch: 879 loss: 16.652265575957035 loss_input: 82.0699636217136
step: 14000 epoch: 879 loss: 16.661169782961004 loss_input: 82.06556873309273
step: 15000 epoch: 879 loss: 16.689117773764373 loss_input: 82.14363720559905
Save loss: 16.702178511738776 Name: 879_train_model.pth
step: 0 epoch: 880 loss: 20.618267059326172 loss_input: 67.9229736328125
step: 1000 epoch: 880 loss: 16.67288205959461 loss_input: 81.53187638777239
step: 2000 epoch: 880 loss: 16.73114129390078 loss_input: 81.5996346004244
step: 3000 epoch: 880 loss: 16.698598947496425 loss_input: 81.92348235402652
step: 4000 epoch: 880 loss: 16.693645351918093 loss_input: 82.04900308431193
step: 5000 epoch: 880 loss: 16.67325077617533 loss_input: 82.05135238821376
step: 6000 epoch: 880 loss: 16.679064907206833 loss_input: 82.28268006967915
step: 7000 epoch: 880 loss: 16.715437330530126 loss_input: 82.23193523274577
step: 8000 epoch: 880 loss: 16.686647267658074 loss_input: 82.15653068085727
step: 9000 epoch: 880 loss: 16.682822253436278 loss_input: 82.25301240129134
step: 10000 epoch: 880 loss: 16.67439976197197 loss_input: 82.28653354147008
step: 11000 epoch: 880 loss: 16.66623824362906 loss_input: 82.14998543278736
step: 12000 epoch: 880 loss: 16.675247532021512 loss_input: 82.14836569303633
step: 13000 epoch: 880 loss: 16.661184142749224 loss_input: 82.06098051990291
step: 14000 epoch: 880 loss: 16.66560858003191 loss_input: 82.16271417275998
step: 15000 epoch: 880 loss: 16.67895412855121 loss_input: 82.23062252799683
Save loss: 16.68859787401557 Name: 880_train_model.pth
step: 0 epoch: 881 loss: 15.857914924621582 loss_input: 58.07354736328125
step: 1000 epoch: 881 loss: 16.62656635766501 loss_input: 82.96293855070711
step: 2000 epoch: 881 loss: 16.644499472532793 loss_input: 82.50208327842914
step: 3000 epoch: 881 loss: 16.75289031133299 loss_input: 82.55124449483635
step: 4000 epoch: 881 loss: 16.73941716311187 loss_input: 82.84291387480279
step: 5000 epoch: 881 loss: 16.753576333225787 loss_input: 82.96456142121185
step: 6000 epoch: 881 loss: 16.71682843620073 loss_input: 82.76612762212157
step: 7000 epoch: 881 loss: 16.68876374331462 loss_input: 82.46942828270626
step: 8000 epoch: 881 loss: 16.694832395663962 loss_input: 82.36855086259969
step: 9000 epoch: 881 loss: 16.655828379880134 loss_input: 82.28642410771633
step: 10000 epoch: 881 loss: 16.67477128105442 loss_input: 82.2352745682463
step: 11000 epoch: 881 loss: 16.696468949133717 loss_input: 82.24657675496297
step: 12000 epoch: 881 loss: 16.702055459141324 loss_input: 82.21320114091242
step: 13000 epoch: 881 loss: 16.70227230469013 loss_input: 82.24162537658611
step: 14000 epoch: 881 loss: 16.713013243023035 loss_input: 82.31035737001626
step: 15000 epoch: 881 loss: 16.715453783755827 loss_input: 82.2703760707945
Save loss: 16.6993405661881 Name: 881_train_model.pth
step: 0 epoch: 882 loss: 16.474994659423828 loss_input: 63.95147705078125
step: 1000 epoch: 882 loss: 16.915786184392847 loss_input: 81.84601073974854
step: 2000 epoch: 882 loss: 16.744459968873823 loss_input: 81.82144907770724
step: 3000 epoch: 882 loss: 16.713966315764264 loss_input: 82.09695594225752
step: 4000 epoch: 882 loss: 16.64930265493853 loss_input: 82.0767108942979
step: 5000 epoch: 882 loss: 16.66609875580426 loss_input: 82.15718773969816
step: 6000 epoch: 882 loss: 16.690033765499482 loss_input: 82.24839218487999
step: 7000 epoch: 882 loss: 16.66241204583667 loss_input: 82.16612845416614
step: 8000 epoch: 882 loss: 16.692625510932476 loss_input: 82.19528376366284
step: 9000 epoch: 882 loss: 16.69524937358886 loss_input: 82.3268391931604
step: 10000 epoch: 882 loss: 16.670020048075493 loss_input: 82.1183000962599
step: 11000 epoch: 882 loss: 16.68401241308992 loss_input: 82.22301016624554
step: 12000 epoch: 882 loss: 16.658827624910227 loss_input: 82.11984550705573
step: 13000 epoch: 882 loss: 16.661279690136368 loss_input: 82.13365060785588
step: 14000 epoch: 882 loss: 16.67199543131887 loss_input: 82.13558600969071
step: 15000 epoch: 882 loss: 16.681696651236358 loss_input: 82.19064805673112
Save loss: 16.696067966505886 Name: 882_train_model.pth
step: 0 epoch: 883 loss: 20.14999771118164 loss_input: 132.2059326171875
step: 1000 epoch: 883 loss: 16.7390793968033 loss_input: 83.24398416524882
step: 2000 epoch: 883 loss: 16.670068557592465 loss_input: 82.83508682060337
step: 3000 epoch: 883 loss: 16.638693430152824 loss_input: 82.66092896199314
step: 4000 epoch: 883 loss: 16.648315501850686 loss_input: 82.97254995333198
step: 5000 epoch: 883 loss: 16.660954513113108 loss_input: 82.80309627771712
step: 6000 epoch: 883 loss: 16.668926128763456 loss_input: 82.64373275840745
step: 7000 epoch: 883 loss: 16.71327275978533 loss_input: 82.66862155852735
step: 8000 epoch: 883 loss: 16.70982206855591 loss_input: 82.65785017229291
step: 9000 epoch: 883 loss: 16.709777798947727 loss_input: 82.58452833447109
step: 10000 epoch: 883 loss: 16.704943632915036 loss_input: 82.46628990665387
step: 11000 epoch: 883 loss: 16.71737989654607 loss_input: 82.47688995713635
step: 12000 epoch: 883 loss: 16.706142594343266 loss_input: 82.4706766582928
step: 13000 epoch: 883 loss: 16.70146777264733 loss_input: 82.3944865475342
step: 14000 epoch: 883 loss: 16.69095300975914 loss_input: 82.30693250042687
step: 15000 epoch: 883 loss: 16.6939502642255 loss_input: 82.24564244952029
Save loss: 16.692860638558866 Name: 883_train_model.pth
step: 0 epoch: 884 loss: 13.545272827148438 loss_input: 51.830322265625
step: 1000 epoch: 884 loss: 16.879162729322374 loss_input: 82.67332258805648
step: 2000 epoch: 884 loss: 16.80019124491938 loss_input: 82.50714815931877
step: 3000 epoch: 884 loss: 16.71430931254968 loss_input: 82.36826559115711
step: 4000 epoch: 884 loss: 16.692754307498756 loss_input: 82.23844971069245
step: 5000 epoch: 884 loss: 16.633212112422182 loss_input: 82.049689305303
step: 6000 epoch: 884 loss: 16.623606257827216 loss_input: 82.18283689778917
step: 7000 epoch: 884 loss: 16.623819062206135 loss_input: 82.04053171149936
step: 8000 epoch: 884 loss: 16.65108929251361 loss_input: 82.29064566849799
step: 9000 epoch: 884 loss: 16.65415956968467 loss_input: 82.22915014379913
step: 10000 epoch: 884 loss: 16.673633417288382 loss_input: 82.3759956645937
step: 11000 epoch: 884 loss: 16.684952379259105 loss_input: 82.34068754536165
step: 12000 epoch: 884 loss: 16.683575703773723 loss_input: 82.33325230913374
step: 13000 epoch: 884 loss: 16.688406502794333 loss_input: 82.21916819950148
step: 14000 epoch: 884 loss: 16.692297131459583 loss_input: 82.2573302302835
step: 15000 epoch: 884 loss: 16.701083931586926 loss_input: 82.24147306195212
Save loss: 16.699528936356305 Name: 884_train_model.pth
step: 0 epoch: 885 loss: 21.59768295288086 loss_input: 94.62689208984375
step: 1000 epoch: 885 loss: 16.319364541536803 loss_input: 81.03371677150975
step: 2000 epoch: 885 loss: 16.46293745655706 loss_input: 81.62604979858703
step: 3000 epoch: 885 loss: 16.510479411455997 loss_input: 81.73405261804008
step: 4000 epoch: 885 loss: 16.588219138152358 loss_input: 82.06308521136317
step: 5000 epoch: 885 loss: 16.614624966814194 loss_input: 82.25650264234262
step: 6000 epoch: 885 loss: 16.625177943930986 loss_input: 82.28222252468014
step: 7000 epoch: 885 loss: 16.63762323945918 loss_input: 82.3288750933879
step: 8000 epoch: 885 loss: 16.63056362725663 loss_input: 82.2813796300975
step: 9000 epoch: 885 loss: 16.621308576556103 loss_input: 82.15928871130204
step: 10000 epoch: 885 loss: 16.636866167061044 loss_input: 82.22238446931674
step: 11000 epoch: 885 loss: 16.656980056414202 loss_input: 82.18000966975303
step: 12000 epoch: 885 loss: 16.681434963198665 loss_input: 82.34908374843275
step: 13000 epoch: 885 loss: 16.697201069222352 loss_input: 82.24459510006892
step: 14000 epoch: 885 loss: 16.690299376088646 loss_input: 82.26477038575091
step: 15000 epoch: 885 loss: 16.695071782106464 loss_input: 82.24353698685712
Save loss: 16.693547320276497 Name: 885_train_model.pth
step: 0 epoch: 886 loss: 14.775287628173828 loss_input: 97.86181640625
step: 1000 epoch: 886 loss: 16.534299770911613 loss_input: 82.39945503715035
step: 2000 epoch: 886 loss: 16.62077996589016 loss_input: 82.37136454965876
step: 3000 epoch: 886 loss: 16.67987645033875 loss_input: 82.67093986656141
step: 4000 epoch: 886 loss: 16.69186897749783 loss_input: 82.66548575868728
step: 5000 epoch: 886 loss: 16.705604332014456 loss_input: 82.73104621019155
step: 6000 epoch: 886 loss: 16.706815298547983 loss_input: 82.70239813139867
step: 7000 epoch: 886 loss: 16.684472431473825 loss_input: 82.67845577724387
step: 8000 epoch: 886 loss: 16.695804998794507 loss_input: 82.61416847582773
step: 9000 epoch: 886 loss: 16.69395881469429 loss_input: 82.46598072219406
step: 10000 epoch: 886 loss: 16.7128274793828 loss_input: 82.4655833794062
step: 11000 epoch: 886 loss: 16.71731220758045 loss_input: 82.44936926565802
step: 12000 epoch: 886 loss: 16.711574909577976 loss_input: 82.4310851649
step: 13000 epoch: 886 loss: 16.713783630379602 loss_input: 82.40956853134945
step: 14000 epoch: 886 loss: 16.70139638758261 loss_input: 82.40503367801435
step: 15000 epoch: 886 loss: 16.698309554250134 loss_input: 82.31534615628489
Save loss: 16.69498778216541 Name: 886_train_model.pth
step: 0 epoch: 887 loss: 14.733016967773438 loss_input: 72.390625
step: 1000 epoch: 887 loss: 16.569304274274156 loss_input: 83.21437845601665
step: 2000 epoch: 887 loss: 16.669115421117873 loss_input: 82.57135842479151
step: 3000 epoch: 887 loss: 16.705872668222124 loss_input: 82.80014098083842
step: 4000 epoch: 887 loss: 16.705724491413996 loss_input: 82.94651300553768
step: 5000 epoch: 887 loss: 16.646727973950956 loss_input: 82.73033819749722
step: 6000 epoch: 887 loss: 16.674682084569692 loss_input: 82.60678791693897
step: 7000 epoch: 887 loss: 16.652926445143542 loss_input: 82.34505675912091
step: 8000 epoch: 887 loss: 16.667099294446732 loss_input: 82.43737998936656
step: 9000 epoch: 887 loss: 16.662155448244803 loss_input: 82.39011205650544
step: 10000 epoch: 887 loss: 16.6653011304571 loss_input: 82.19963377197436
step: 11000 epoch: 887 loss: 16.674332194540266 loss_input: 82.19469590677303
step: 12000 epoch: 887 loss: 16.677210817892504 loss_input: 82.17541552221722
step: 13000 epoch: 887 loss: 16.67375649720758 loss_input: 82.28264204379862
step: 14000 epoch: 887 loss: 16.68195428380659 loss_input: 82.2351129931149
step: 15000 epoch: 887 loss: 16.673842910305435 loss_input: 82.17214418778332
Save loss: 16.70028169025481 Name: 887_train_model.pth
step: 0 epoch: 888 loss: 10.008651733398438 loss_input: 51.14404296875
step: 1000 epoch: 888 loss: 16.538829258986407 loss_input: 83.40607732111638
step: 2000 epoch: 888 loss: 16.660829626042386 loss_input: 83.1827199193372
step: 3000 epoch: 888 loss: 16.65767197432577 loss_input: 83.07166318502557
step: 4000 epoch: 888 loss: 16.65586120442908 loss_input: 82.68462671014375
step: 5000 epoch: 888 loss: 16.662011049862173 loss_input: 82.53924776568124
step: 6000 epoch: 888 loss: 16.675930623034002 loss_input: 82.53673910590096
step: 7000 epoch: 888 loss: 16.687031551320764 loss_input: 82.26350667643183
step: 8000 epoch: 888 loss: 16.65192503953573 loss_input: 82.09783024711022
step: 9000 epoch: 888 loss: 16.66902809272328 loss_input: 82.01219106893939
step: 10000 epoch: 888 loss: 16.671442477944588 loss_input: 82.00856420095295
step: 11000 epoch: 888 loss: 16.684140706818685 loss_input: 82.09419258116982
step: 12000 epoch: 888 loss: 16.686104831829855 loss_input: 82.21715935293153
step: 13000 epoch: 888 loss: 16.6963738497473 loss_input: 82.33846403435463
step: 14000 epoch: 888 loss: 16.7089412065244 loss_input: 82.43893372676226
step: 15000 epoch: 888 loss: 16.6966822398169 loss_input: 82.35194367169142
Save loss: 16.691511302009225 Name: 888_train_model.pth
step: 0 epoch: 889 loss: 21.218080520629883 loss_input: 108.6876220703125
step: 1000 epoch: 889 loss: 16.70822744293289 loss_input: 81.9462805261145
step: 2000 epoch: 889 loss: 16.613614763634494 loss_input: 81.93964303248767
step: 3000 epoch: 889 loss: 16.68751912059803 loss_input: 82.28535521932142
step: 4000 epoch: 889 loss: 16.67404282739597 loss_input: 82.59785778347893
step: 5000 epoch: 889 loss: 16.693446192687045 loss_input: 82.90378173144668
step: 6000 epoch: 889 loss: 16.668517456513964 loss_input: 82.81926928804509
step: 7000 epoch: 889 loss: 16.663963872319442 loss_input: 82.62916860265777
step: 8000 epoch: 889 loss: 16.672319491436117 loss_input: 82.50663820068145
step: 9000 epoch: 889 loss: 16.694082831478216 loss_input: 82.58809578883596
step: 10000 epoch: 889 loss: 16.68005568904169 loss_input: 82.50006447109196
step: 11000 epoch: 889 loss: 16.683559281318235 loss_input: 82.49163818359375
step: 12000 epoch: 889 loss: 16.696776635586623 loss_input: 82.47886244871846
step: 13000 epoch: 889 loss: 16.69317582988453 loss_input: 82.4051623714492
step: 14000 epoch: 889 loss: 16.688385285732448 loss_input: 82.28590175426226
step: 15000 epoch: 889 loss: 16.696538533014373 loss_input: 82.23106016565042
Save loss: 16.69223171943426 Name: 889_train_model.pth
step: 0 epoch: 890 loss: 15.63627815246582 loss_input: 62.5760498046875
step: 1000 epoch: 890 loss: 16.703340989607316 loss_input: 82.46610987889063
step: 2000 epoch: 890 loss: 16.610563730967158 loss_input: 82.55401104989497
step: 3000 epoch: 890 loss: 16.605128101570056 loss_input: 82.09355707114555
step: 4000 epoch: 890 loss: 16.64412769690897 loss_input: 81.91891547376828
step: 5000 epoch: 890 loss: 16.583524817253345 loss_input: 82.16900208825422
step: 6000 epoch: 890 loss: 16.628722084698886 loss_input: 82.37155230191127
step: 7000 epoch: 890 loss: 16.6510365013735 loss_input: 82.38690812375576
step: 8000 epoch: 890 loss: 16.66053877602725 loss_input: 82.3736627953244
step: 9000 epoch: 890 loss: 16.655548594366405 loss_input: 82.29660223028604
step: 10000 epoch: 890 loss: 16.66656142591822 loss_input: 82.24120410224603
step: 11000 epoch: 890 loss: 16.69161382348697 loss_input: 82.2988429108096
step: 12000 epoch: 890 loss: 16.703249772979106 loss_input: 82.28947614655972
step: 13000 epoch: 890 loss: 16.687896420374145 loss_input: 82.25859561471533
step: 14000 epoch: 890 loss: 16.691367400849636 loss_input: 82.26483268511652
step: 15000 epoch: 890 loss: 16.6928113105734 loss_input: 82.20485122769159
Save loss: 16.69859054325521 Name: 890_train_model.pth
step: 0 epoch: 891 loss: 11.938924789428711 loss_input: 51.8099365234375
step: 1000 epoch: 891 loss: 16.364221214176297 loss_input: 81.91915640511831
step: 2000 epoch: 891 loss: 16.550127247462925 loss_input: 82.73560888013024
step: 3000 epoch: 891 loss: 16.64060206541972 loss_input: 82.17055028694863
step: 4000 epoch: 891 loss: 16.58940134880335 loss_input: 82.02933032916087
step: 5000 epoch: 891 loss: 16.684715206159208 loss_input: 82.12736872791457
step: 6000 epoch: 891 loss: 16.697291860419934 loss_input: 82.23994492332332
step: 7000 epoch: 891 loss: 16.694357359380387 loss_input: 82.37561166798996
step: 8000 epoch: 891 loss: 16.673391970764502 loss_input: 82.45496420186247
step: 9000 epoch: 891 loss: 16.695928700512454 loss_input: 82.41313129514472
step: 10000 epoch: 891 loss: 16.69998200639893 loss_input: 82.34880496935658
step: 11000 epoch: 891 loss: 16.69619806542547 loss_input: 82.201598333257
step: 12000 epoch: 891 loss: 16.70088356010676 loss_input: 82.27337642924287
step: 13000 epoch: 891 loss: 16.701836971858494 loss_input: 82.216398275666
step: 14000 epoch: 891 loss: 16.691078850443247 loss_input: 82.16253488171604
step: 15000 epoch: 891 loss: 16.700973205984088 loss_input: 82.27748168839526
Save loss: 16.698559995293618 Name: 891_train_model.pth
step: 0 epoch: 892 loss: 20.730899810791016 loss_input: 87.81298828125
step: 1000 epoch: 892 loss: 16.540118587600602 loss_input: 82.06492037015718
step: 2000 epoch: 892 loss: 16.512763987535955 loss_input: 81.68271314782062
step: 3000 epoch: 892 loss: 16.54218706247291 loss_input: 81.62497644828026
step: 4000 epoch: 892 loss: 16.629978728395674 loss_input: 82.12326059720421
step: 5000 epoch: 892 loss: 16.634300724217187 loss_input: 82.33056935976086
step: 6000 epoch: 892 loss: 16.65391772056774 loss_input: 82.14975316817315
step: 7000 epoch: 892 loss: 16.639316740555007 loss_input: 82.19573712195691
step: 8000 epoch: 892 loss: 16.593232652780756 loss_input: 81.99724440115152
step: 9000 epoch: 892 loss: 16.605780547995685 loss_input: 82.15952730512582
step: 10000 epoch: 892 loss: 16.621663255722996 loss_input: 82.13868197530833
step: 11000 epoch: 892 loss: 16.621267844065677 loss_input: 82.11761986703269
step: 12000 epoch: 892 loss: 16.629534842043995 loss_input: 82.07694563966186
step: 13000 epoch: 892 loss: 16.655791983880608 loss_input: 82.10491966363165
step: 14000 epoch: 892 loss: 16.673139055799854 loss_input: 82.1903748949224
step: 15000 epoch: 892 loss: 16.68162433488092 loss_input: 82.14278537436697
Save loss: 16.69166219961643 Name: 892_train_model.pth
step: 0 epoch: 893 loss: 16.175357818603516 loss_input: 192.9608154296875
step: 1000 epoch: 893 loss: 16.50198004986499 loss_input: 81.16770448789492
step: 2000 epoch: 893 loss: 16.501722506437822 loss_input: 80.7700720257547
step: 3000 epoch: 893 loss: 16.52535589358601 loss_input: 81.34308123691842
step: 4000 epoch: 893 loss: 16.586374506536824 loss_input: 81.86052754389527
step: 5000 epoch: 893 loss: 16.544718448983694 loss_input: 81.83173398279328
step: 6000 epoch: 893 loss: 16.5482833721979 loss_input: 81.9303364905491
step: 7000 epoch: 893 loss: 16.56094242868858 loss_input: 81.94501796025511
step: 8000 epoch: 893 loss: 16.604866281686284 loss_input: 81.996723035353
step: 9000 epoch: 893 loss: 16.645775331840053 loss_input: 82.15547510915671
step: 10000 epoch: 893 loss: 16.651625654480718 loss_input: 82.14015835965232
step: 11000 epoch: 893 loss: 16.664168134774634 loss_input: 82.18915454596457
step: 12000 epoch: 893 loss: 16.671479118495135 loss_input: 82.18225840697357
step: 13000 epoch: 893 loss: 16.688039930937208 loss_input: 82.19834984992892
step: 14000 epoch: 893 loss: 16.664944651978125 loss_input: 82.17334028840295
step: 15000 epoch: 893 loss: 16.667756002336 loss_input: 82.19274229374292
Save loss: 16.688183129787443 Name: 893_train_model.pth
step: 0 epoch: 894 loss: 17.59470558166504 loss_input: 66.7252197265625
step: 1000 epoch: 894 loss: 16.658088246782818 loss_input: 81.60856825059706
step: 2000 epoch: 894 loss: 16.688968959896044 loss_input: 82.3268934380466
step: 3000 epoch: 894 loss: 16.638403859308504 loss_input: 82.25473033646988
step: 4000 epoch: 894 loss: 16.66904820158791 loss_input: 82.12555933892503
step: 5000 epoch: 894 loss: 16.63719879582128 loss_input: 82.11986401547816
step: 6000 epoch: 894 loss: 16.63874983513401 loss_input: 82.1525722885704
step: 7000 epoch: 894 loss: 16.636154242233452 loss_input: 82.18726487385173
step: 8000 epoch: 894 loss: 16.642614637638776 loss_input: 82.28980212312328
step: 9000 epoch: 894 loss: 16.66191246350359 loss_input: 82.28926238827196
step: 10000 epoch: 894 loss: 16.690838930309088 loss_input: 82.31265306086578
step: 11000 epoch: 894 loss: 16.69126819933602 loss_input: 82.33290215360394
step: 12000 epoch: 894 loss: 16.699292732550596 loss_input: 82.34139860293217
step: 13000 epoch: 894 loss: 16.700836153876168 loss_input: 82.30211617867586
step: 14000 epoch: 894 loss: 16.693219067190267 loss_input: 82.26062695263676
step: 15000 epoch: 894 loss: 16.702279810809777 loss_input: 82.26306209306098
Save loss: 16.689004742607473 Name: 894_train_model.pth
step: 0 epoch: 895 loss: 15.589590072631836 loss_input: 74.52984619140625
step: 1000 epoch: 895 loss: 16.44226837586928 loss_input: 81.93042364130011
step: 2000 epoch: 895 loss: 16.498667864010727 loss_input: 82.12704460147856
step: 3000 epoch: 895 loss: 16.66011937750613 loss_input: 82.15794478826068
step: 4000 epoch: 895 loss: 16.65477557636386 loss_input: 82.01574392778758
step: 5000 epoch: 895 loss: 16.657890589374038 loss_input: 81.92509795160109
step: 6000 epoch: 895 loss: 16.68400457314185 loss_input: 81.93773232211949
step: 7000 epoch: 895 loss: 16.729128926400712 loss_input: 81.98382922312716
step: 8000 epoch: 895 loss: 16.742720971955553 loss_input: 82.10071677289118
step: 9000 epoch: 895 loss: 16.71361363375138 loss_input: 82.00838260673945
step: 10000 epoch: 895 loss: 16.712723088209636 loss_input: 82.26690583505126
step: 11000 epoch: 895 loss: 16.709451175149965 loss_input: 82.2979498146209
step: 12000 epoch: 895 loss: 16.698928984351262 loss_input: 82.23169995029711
step: 13000 epoch: 895 loss: 16.698503374329036 loss_input: 82.27133763899171
step: 14000 epoch: 895 loss: 16.710639820244644 loss_input: 82.30394124157148
step: 15000 epoch: 895 loss: 16.703770531724988 loss_input: 82.23559977593291
Save loss: 16.695607737243176 Name: 895_train_model.pth
step: 0 epoch: 896 loss: 14.776925086975098 loss_input: 91.60528564453125
step: 1000 epoch: 896 loss: 16.599501082470844 loss_input: 81.31275968904143
step: 2000 epoch: 896 loss: 16.630491103249035 loss_input: 82.16760998163028
step: 3000 epoch: 896 loss: 16.64269762998897 loss_input: 82.41473116139021
step: 4000 epoch: 896 loss: 16.64668464350778 loss_input: 82.18094020806322
step: 5000 epoch: 896 loss: 16.610343723815813 loss_input: 82.19565083565318
step: 6000 epoch: 896 loss: 16.6454234945637 loss_input: 82.19937555370004
step: 7000 epoch: 896 loss: 16.69361718159678 loss_input: 82.34371803086445
step: 8000 epoch: 896 loss: 16.689198530609794 loss_input: 82.41396916602466
step: 9000 epoch: 896 loss: 16.684426191025025 loss_input: 82.35664883535075
step: 10000 epoch: 896 loss: 16.67606510466164 loss_input: 82.22924350631344
step: 11000 epoch: 896 loss: 16.68785936283465 loss_input: 82.30728324458074
step: 12000 epoch: 896 loss: 16.69815828140433 loss_input: 82.34809749740907
step: 13000 epoch: 896 loss: 16.688496240881094 loss_input: 82.39698785283494
step: 14000 epoch: 896 loss: 16.692398499270933 loss_input: 82.38660868095778
step: 15000 epoch: 896 loss: 16.673100841290935 loss_input: 82.28563354565425
Save loss: 16.69151661619544 Name: 896_train_model.pth
step: 0 epoch: 897 loss: 13.897112846374512 loss_input: 66.8350830078125
step: 1000 epoch: 897 loss: 16.452388012206757 loss_input: 81.54553033636286
step: 2000 epoch: 897 loss: 16.67298383917706 loss_input: 81.96491500343578
step: 3000 epoch: 897 loss: 16.58167760867113 loss_input: 81.5572015138834
step: 4000 epoch: 897 loss: 16.588160426698305 loss_input: 81.62711412600952
step: 5000 epoch: 897 loss: 16.591840029716302 loss_input: 81.66985272379118
step: 6000 epoch: 897 loss: 16.625321541879796 loss_input: 81.74933879428298
step: 7000 epoch: 897 loss: 16.61963545218687 loss_input: 81.79012181435019
step: 8000 epoch: 897 loss: 16.60142889813682 loss_input: 81.67586357613501
step: 9000 epoch: 897 loss: 16.610281343208978 loss_input: 81.77488177040658
step: 10000 epoch: 897 loss: 16.619543739872306 loss_input: 81.84860429362337
step: 11000 epoch: 897 loss: 16.63555042956723 loss_input: 81.93468143632006
step: 12000 epoch: 897 loss: 16.66607127641799 loss_input: 82.05287559858293
step: 13000 epoch: 897 loss: 16.683259906094676 loss_input: 82.13309835729282
step: 14000 epoch: 897 loss: 16.688983659779684 loss_input: 82.17610017514792
step: 15000 epoch: 897 loss: 16.70453663880281 loss_input: 82.2992034051571
Save loss: 16.692815799847246 Name: 897_train_model.pth
step: 0 epoch: 898 loss: 11.624402046203613 loss_input: 56.14630126953125
step: 1000 epoch: 898 loss: 16.79025405127328 loss_input: 82.57807951326018
step: 2000 epoch: 898 loss: 16.807195924390022 loss_input: 82.38844664581772
step: 3000 epoch: 898 loss: 16.67561257040449 loss_input: 82.02474641021352
step: 4000 epoch: 898 loss: 16.667206248710286 loss_input: 81.92374169030984
step: 5000 epoch: 898 loss: 16.690352915334977 loss_input: 82.24982494956087
step: 6000 epoch: 898 loss: 16.664892929630504 loss_input: 82.12424665672583
step: 7000 epoch: 898 loss: 16.636610331390266 loss_input: 82.22928469761017
step: 8000 epoch: 898 loss: 16.662135403479 loss_input: 82.22815759937967
step: 9000 epoch: 898 loss: 16.674470764281367 loss_input: 82.26585330037113
step: 10000 epoch: 898 loss: 16.666918940334817 loss_input: 82.2647906474001
step: 11000 epoch: 898 loss: 16.666936738763482 loss_input: 82.23060826923228
step: 12000 epoch: 898 loss: 16.664644037581574 loss_input: 82.33774076057627
step: 13000 epoch: 898 loss: 16.682502288780217 loss_input: 82.42240202150916
step: 14000 epoch: 898 loss: 16.672125258145012 loss_input: 82.31025788109862
step: 15000 epoch: 898 loss: 16.682064524365064 loss_input: 82.31785802089105
Save loss: 16.685650047153235 Name: 898_train_model.pth
step: 0 epoch: 899 loss: 15.470291137695312 loss_input: 96.45233154296875
step: 1000 epoch: 899 loss: 16.546065426253893 loss_input: 81.8199000706325
step: 2000 epoch: 899 loss: 16.65653990340912 loss_input: 82.27248174008699
step: 3000 epoch: 899 loss: 16.689477143546654 loss_input: 82.41661544133726
step: 4000 epoch: 899 loss: 16.734416296052444 loss_input: 82.59630083018796
step: 5000 epoch: 899 loss: 16.71206306729453 loss_input: 82.32802531290616
step: 6000 epoch: 899 loss: 16.687614836865237 loss_input: 82.36542255571337
step: 7000 epoch: 899 loss: 16.72072020320786 loss_input: 82.34367190359525
step: 8000 epoch: 899 loss: 16.715201244579525 loss_input: 82.28609569250621
step: 9000 epoch: 899 loss: 16.71950588302074 loss_input: 82.43823872814045
step: 10000 epoch: 899 loss: 16.716383063808678 loss_input: 82.34978573065546
step: 11000 epoch: 899 loss: 16.717071052963394 loss_input: 82.33399663085494
step: 12000 epoch: 899 loss: 16.69706859841723 loss_input: 82.31145183905574
step: 13000 epoch: 899 loss: 16.68933848619149 loss_input: 82.2969510768177
step: 14000 epoch: 899 loss: 16.687268027986136 loss_input: 82.307334623629
step: 15000 epoch: 899 loss: 16.688533436718817 loss_input: 82.2667636163123
Save loss: 16.68691453872621 Name: 899_train_model.pth
step: 0 epoch: 900 loss: 17.99009132385254 loss_input: 136.3323974609375
step: 1000 epoch: 900 loss: 16.521503697146663 loss_input: 81.69353912688874
step: 2000 epoch: 900 loss: 16.579369409986285 loss_input: 81.47540255751031
step: 3000 epoch: 900 loss: 16.57082136604477 loss_input: 81.63762081340725
step: 4000 epoch: 900 loss: 16.523674305901768 loss_input: 81.77380497710045
step: 5000 epoch: 900 loss: 16.600377654676507 loss_input: 82.05693275407418
step: 6000 epoch: 900 loss: 16.65221347599859 loss_input: 82.32105958821316
step: 7000 epoch: 900 loss: 16.621397916631448 loss_input: 82.34257142255348
step: 8000 epoch: 900 loss: 16.637678143829422 loss_input: 82.46467065304581
step: 9000 epoch: 900 loss: 16.616273830022006 loss_input: 82.30846110143578
step: 10000 epoch: 900 loss: 16.599700811803014 loss_input: 82.18883711604425
step: 11000 epoch: 900 loss: 16.61809515289887 loss_input: 82.1614184033252
step: 12000 epoch: 900 loss: 16.644551235460735 loss_input: 82.24874328404762
step: 13000 epoch: 900 loss: 16.66384141817981 loss_input: 82.11948546588664
step: 14000 epoch: 900 loss: 16.674166794938962 loss_input: 82.18967237813789
step: 15000 epoch: 900 loss: 16.68232786060214 loss_input: 82.23079945305079
Save loss: 16.695053372174502 Name: 900_train_model.pth
step: 0 epoch: 901 loss: 15.359893798828125 loss_input: 84.3514404296875
step: 1000 epoch: 901 loss: 16.807969301492424 loss_input: 83.0550261506072
step: 2000 epoch: 901 loss: 16.760123486402094 loss_input: 82.54141871325079
step: 3000 epoch: 901 loss: 16.749357910563017 loss_input: 82.45248281904078
step: 4000 epoch: 901 loss: 16.66653727436328 loss_input: 82.12049332942644
step: 5000 epoch: 901 loss: 16.706958162191032 loss_input: 82.2719104250439
step: 6000 epoch: 901 loss: 16.660154427712886 loss_input: 82.20118387658047
step: 7000 epoch: 901 loss: 16.664357262293862 loss_input: 82.1963512745947
step: 8000 epoch: 901 loss: 16.65684297751999 loss_input: 82.12341882252511
step: 9000 epoch: 901 loss: 16.648724997338952 loss_input: 82.21085331474671
step: 10000 epoch: 901 loss: 16.66274159396366 loss_input: 82.22510456473884
step: 11000 epoch: 901 loss: 16.681451420709877 loss_input: 82.21153960146911
step: 12000 epoch: 901 loss: 16.673948130004852 loss_input: 82.14410935929493
step: 13000 epoch: 901 loss: 16.667859083120202 loss_input: 82.15958781697532
step: 14000 epoch: 901 loss: 16.666485639327203 loss_input: 82.1983298320837
step: 15000 epoch: 901 loss: 16.673366574523975 loss_input: 82.21146333164377
Save loss: 16.69060177759826 Name: 901_train_model.pth
step: 0 epoch: 902 loss: 12.803237915039062 loss_input: 43.5135498046875
step: 1000 epoch: 902 loss: 16.357553490153798 loss_input: 82.29734992790412
step: 2000 epoch: 902 loss: 16.475806401170296 loss_input: 81.99177092972069
step: 3000 epoch: 902 loss: 16.554989935993472 loss_input: 82.23350460328328
step: 4000 epoch: 902 loss: 16.608363193859013 loss_input: 82.40573801251836
step: 5000 epoch: 902 loss: 16.634026114641728 loss_input: 82.41446949867839
step: 6000 epoch: 902 loss: 16.630150730064404 loss_input: 82.2422506913724
step: 7000 epoch: 902 loss: 16.643169157676333 loss_input: 82.26966967539794
step: 8000 epoch: 902 loss: 16.63735096693903 loss_input: 82.26837705925188
step: 9000 epoch: 902 loss: 16.643277489997296 loss_input: 82.2689843408241
step: 10000 epoch: 902 loss: 16.652412086328905 loss_input: 82.3036539546729
step: 11000 epoch: 902 loss: 16.65166979180218 loss_input: 82.24610030236065
step: 12000 epoch: 902 loss: 16.654688747135186 loss_input: 82.16367880903317
step: 13000 epoch: 902 loss: 16.66369609034673 loss_input: 82.24875486121711
step: 14000 epoch: 902 loss: 16.670211647963594 loss_input: 82.2017475959446
step: 15000 epoch: 902 loss: 16.679295034124394 loss_input: 82.18045357441666
Save loss: 16.68499171629548 Name: 902_train_model.pth
step: 0 epoch: 903 loss: 18.090890884399414 loss_input: 94.9091796875
step: 1000 epoch: 903 loss: 16.624865570030252 loss_input: 81.69848608422828
step: 2000 epoch: 903 loss: 16.727966764460557 loss_input: 83.12109908790721
step: 3000 epoch: 903 loss: 16.65661404364032 loss_input: 82.42093746664523
step: 4000 epoch: 903 loss: 16.676749902795297 loss_input: 82.3876499442034
step: 5000 epoch: 903 loss: 16.680089939310417 loss_input: 82.0867022396302
step: 6000 epoch: 903 loss: 16.67332314705972 loss_input: 82.14320910300758
step: 7000 epoch: 903 loss: 16.693899961696864 loss_input: 82.28448973667824
step: 8000 epoch: 903 loss: 16.658973958220813 loss_input: 82.1721589076878
step: 9000 epoch: 903 loss: 16.654759880119318 loss_input: 82.2412578411969
step: 10000 epoch: 903 loss: 16.67642721921465 loss_input: 82.31295553305998
step: 11000 epoch: 903 loss: 16.69848368339306 loss_input: 82.24117080221045
step: 12000 epoch: 903 loss: 16.692324195620557 loss_input: 82.25445324217773
step: 13000 epoch: 903 loss: 16.685458484901556 loss_input: 82.2454558267528
step: 14000 epoch: 903 loss: 16.683161505562314 loss_input: 82.20951820266187
step: 15000 epoch: 903 loss: 16.694653363730396 loss_input: 82.23435418026168
Save loss: 16.691124390766024 Name: 903_train_model.pth
step: 0 epoch: 904 loss: 15.318401336669922 loss_input: 71.274169921875
step: 1000 epoch: 904 loss: 16.86260159508689 loss_input: 82.57275195507617
step: 2000 epoch: 904 loss: 16.722128713685 loss_input: 82.06921566682479
step: 3000 epoch: 904 loss: 16.77004785253302 loss_input: 82.56235350342203
step: 4000 epoch: 904 loss: 16.723843560281978 loss_input: 82.32544183290115
step: 5000 epoch: 904 loss: 16.73944811529218 loss_input: 82.27370454864105
step: 6000 epoch: 904 loss: 16.737260299133393 loss_input: 82.14002428021436
step: 7000 epoch: 904 loss: 16.69361652113543 loss_input: 82.02827326983422
step: 8000 epoch: 904 loss: 16.716781248615437 loss_input: 81.95423942368532
step: 9000 epoch: 904 loss: 16.709158714844538 loss_input: 82.02696369240223
step: 10000 epoch: 904 loss: 16.680071889251582 loss_input: 82.00925064964207
step: 11000 epoch: 904 loss: 16.689786466942323 loss_input: 82.1397066320054
step: 12000 epoch: 904 loss: 16.700521687568898 loss_input: 82.22829540307676
step: 13000 epoch: 904 loss: 16.71586961403653 loss_input: 82.32188664461208
step: 14000 epoch: 904 loss: 16.70563433090454 loss_input: 82.26564779936199
step: 15000 epoch: 904 loss: 16.69152601848689 loss_input: 82.26817078975334
Save loss: 16.691713830679657 Name: 904_train_model.pth
step: 0 epoch: 905 loss: 20.624361038208008 loss_input: 101.7919921875
step: 1000 epoch: 905 loss: 17.110015806737362 loss_input: 81.80459085592142
step: 2000 epoch: 905 loss: 16.738983096151813 loss_input: 81.99805480560501
step: 3000 epoch: 905 loss: 16.645650857609535 loss_input: 81.83998565012834
step: 4000 epoch: 905 loss: 16.60975648653087 loss_input: 81.92881583071029
step: 5000 epoch: 905 loss: 16.651135138382173 loss_input: 82.36610978585533
step: 6000 epoch: 905 loss: 16.64532362590371 loss_input: 82.40964532848041
step: 7000 epoch: 905 loss: 16.635958052553324 loss_input: 82.19905778841024
step: 8000 epoch: 905 loss: 16.65331663809453 loss_input: 82.22732240714784
step: 9000 epoch: 905 loss: 16.635087611131993 loss_input: 82.2589836616461
step: 10000 epoch: 905 loss: 16.625903237618132 loss_input: 82.23441727482525
step: 11000 epoch: 905 loss: 16.648381669264946 loss_input: 82.26110249520995
step: 12000 epoch: 905 loss: 16.659339672743744 loss_input: 82.26396086255507
step: 13000 epoch: 905 loss: 16.67663300993736 loss_input: 82.34246100378847
step: 14000 epoch: 905 loss: 16.68300383217496 loss_input: 82.29650660939119
step: 15000 epoch: 905 loss: 16.68583972367769 loss_input: 82.24196567640934
Save loss: 16.689416138917206 Name: 905_train_model.pth
step: 0 epoch: 906 loss: 15.73310375213623 loss_input: 55.11761474609375
step: 1000 epoch: 906 loss: 16.738959143807243 loss_input: 82.77876341188109
step: 2000 epoch: 906 loss: 16.658303418319147 loss_input: 82.27336325733617
step: 3000 epoch: 906 loss: 16.657087492887197 loss_input: 82.36389001517723
step: 4000 epoch: 906 loss: 16.665892708870626 loss_input: 82.2365930276464
step: 5000 epoch: 906 loss: 16.66375666078485 loss_input: 82.08219183799959
step: 6000 epoch: 906 loss: 16.632592931387325 loss_input: 82.13446409037621
step: 7000 epoch: 906 loss: 16.653448577608966 loss_input: 82.09691694957361
step: 8000 epoch: 906 loss: 16.666962856382835 loss_input: 82.13595209931034
step: 9000 epoch: 906 loss: 16.673870698039682 loss_input: 82.13205058107692
step: 10000 epoch: 906 loss: 16.68168396220757 loss_input: 82.11946829609032
step: 11000 epoch: 906 loss: 16.687955912324842 loss_input: 82.19400739847514
step: 12000 epoch: 906 loss: 16.688880624875218 loss_input: 82.22533860805382
step: 13000 epoch: 906 loss: 16.703644980926548 loss_input: 82.29465818115403
step: 14000 epoch: 906 loss: 16.698643111678567 loss_input: 82.22841308890186
step: 15000 epoch: 906 loss: 16.68690963697691 loss_input: 82.20287277072154
Save loss: 16.689611111432313 Name: 906_train_model.pth
step: 0 epoch: 907 loss: 21.676361083984375 loss_input: 83.10272216796875
step: 1000 epoch: 907 loss: 16.43028787561468 loss_input: 83.17624904392483
step: 2000 epoch: 907 loss: 16.610037434047488 loss_input: 83.01969230228636
step: 3000 epoch: 907 loss: 16.625460011686574 loss_input: 82.4136482297242
step: 4000 epoch: 907 loss: 16.59949311355566 loss_input: 82.3807127136673
step: 5000 epoch: 907 loss: 16.60988120173626 loss_input: 82.21758183763637
step: 6000 epoch: 907 loss: 16.641264949276376 loss_input: 82.19152819826253
step: 7000 epoch: 907 loss: 16.664594122314945 loss_input: 82.12608872909338
step: 8000 epoch: 907 loss: 16.670559350467745 loss_input: 82.06210187166546
step: 9000 epoch: 907 loss: 16.669745405863793 loss_input: 82.09399582229578
step: 10000 epoch: 907 loss: 16.688656253941524 loss_input: 82.07543750019529
step: 11000 epoch: 907 loss: 16.69629907085726 loss_input: 82.12761010318309
step: 12000 epoch: 907 loss: 16.702888898636516 loss_input: 81.97811846673096
step: 13000 epoch: 907 loss: 16.709396020750496 loss_input: 82.03805964276036
step: 14000 epoch: 907 loss: 16.703871179228127 loss_input: 82.12082286916727
step: 15000 epoch: 907 loss: 16.70076785106022 loss_input: 82.18692696691315
Save loss: 16.687309307754038 Name: 907_train_model.pth
step: 0 epoch: 908 loss: 13.605048179626465 loss_input: 76.85321044921875
step: 1000 epoch: 908 loss: 16.727972476513354 loss_input: 83.4840781776817
step: 2000 epoch: 908 loss: 16.66491909232037 loss_input: 82.78333147879185
step: 3000 epoch: 908 loss: 16.630383781813176 loss_input: 82.44157841347806
step: 4000 epoch: 908 loss: 16.622178272675406 loss_input: 82.60649431148073
step: 5000 epoch: 908 loss: 16.569211945536612 loss_input: 82.13082625174184
step: 6000 epoch: 908 loss: 16.603193765957144 loss_input: 82.30783146656547
step: 7000 epoch: 908 loss: 16.600553360483914 loss_input: 82.22168021058376
step: 8000 epoch: 908 loss: 16.648957386059756 loss_input: 82.40420029893724
step: 9000 epoch: 908 loss: 16.641716044951167 loss_input: 82.25814734913352
step: 10000 epoch: 908 loss: 16.644173851991557 loss_input: 82.24972311118498
step: 11000 epoch: 908 loss: 16.663712998843497 loss_input: 82.13350608485166
step: 12000 epoch: 908 loss: 16.659673885070983 loss_input: 82.10777081951342
step: 13000 epoch: 908 loss: 16.68060501954453 loss_input: 82.18776570315654
step: 14000 epoch: 908 loss: 16.672064352968693 loss_input: 82.15985174119817
step: 15000 epoch: 908 loss: 16.679129136958828 loss_input: 82.19196228796335
Save loss: 16.689442947790027 Name: 908_train_model.pth
step: 0 epoch: 909 loss: 32.601043701171875 loss_input: 78.96844482421875
step: 1000 epoch: 909 loss: 16.7938356571026 loss_input: 82.69579669597981
step: 2000 epoch: 909 loss: 16.787561128283667 loss_input: 82.92346810603487
step: 3000 epoch: 909 loss: 16.675922993380638 loss_input: 82.30391153388324
step: 4000 epoch: 909 loss: 16.69089380123412 loss_input: 82.28238021061141
step: 5000 epoch: 909 loss: 16.726317761016354 loss_input: 82.18421396287148
step: 6000 epoch: 909 loss: 16.737603098526534 loss_input: 82.01994057508969
step: 7000 epoch: 909 loss: 16.714751144729842 loss_input: 81.99353281911178
step: 8000 epoch: 909 loss: 16.736570951417452 loss_input: 82.20507088560952
step: 9000 epoch: 909 loss: 16.7343741932017 loss_input: 82.21797746892055
step: 10000 epoch: 909 loss: 16.750064360739028 loss_input: 82.36629432149559
step: 11000 epoch: 909 loss: 16.729944958122044 loss_input: 82.35690927622524
step: 12000 epoch: 909 loss: 16.7150575045039 loss_input: 82.35732557972057
step: 13000 epoch: 909 loss: 16.716516033061403 loss_input: 82.33013073795254
step: 14000 epoch: 909 loss: 16.70276867526828 loss_input: 82.2931197358322
step: 15000 epoch: 909 loss: 16.704799386407764 loss_input: 82.27685434984673
Save loss: 16.69929481418431 Name: 909_train_model.pth
step: 0 epoch: 910 loss: 17.636627197265625 loss_input: 87.12188720703125
step: 1000 epoch: 910 loss: 16.67373169957103 loss_input: 83.82741971163603
step: 2000 epoch: 910 loss: 16.659616675274425 loss_input: 82.34689765712847
step: 3000 epoch: 910 loss: 16.613422020003622 loss_input: 82.0979218881792
step: 4000 epoch: 910 loss: 16.642821867863436 loss_input: 82.43800477187833
step: 5000 epoch: 910 loss: 16.6795409424642 loss_input: 82.18809781744822
step: 6000 epoch: 910 loss: 16.68451066157 loss_input: 82.25621559818156
step: 7000 epoch: 910 loss: 16.67854841970202 loss_input: 82.16992932894352
step: 8000 epoch: 910 loss: 16.682701376673133 loss_input: 82.22216825100232
step: 9000 epoch: 910 loss: 16.656161990275372 loss_input: 82.36937266844483
step: 10000 epoch: 910 loss: 16.636360462946055 loss_input: 82.36900450579942
step: 11000 epoch: 910 loss: 16.642190416253012 loss_input: 82.36399340449697
step: 12000 epoch: 910 loss: 16.656227226168323 loss_input: 82.3698042202131
step: 13000 epoch: 910 loss: 16.660007526100475 loss_input: 82.3067518741422
step: 14000 epoch: 910 loss: 16.669824682604492 loss_input: 82.28062737252864
step: 15000 epoch: 910 loss: 16.67698067365094 loss_input: 82.20734819664366
Save loss: 16.69809335228801 Name: 910_train_model.pth
step: 0 epoch: 911 loss: 11.910835266113281 loss_input: 62.38568115234375
step: 1000 epoch: 911 loss: 16.82493695298156 loss_input: 82.59778892982018
step: 2000 epoch: 911 loss: 16.784575001470213 loss_input: 82.91584835023895
step: 3000 epoch: 911 loss: 16.67763616608922 loss_input: 82.66206365456306
step: 4000 epoch: 911 loss: 16.59717602790579 loss_input: 81.81979704963896
step: 5000 epoch: 911 loss: 16.60727603977572 loss_input: 81.73782523768683
step: 6000 epoch: 911 loss: 16.589999317228944 loss_input: 81.69321803096294
step: 7000 epoch: 911 loss: 16.615311562410508 loss_input: 81.96735297405895
step: 8000 epoch: 911 loss: 16.660860759469067 loss_input: 82.19226670762835
step: 9000 epoch: 911 loss: 16.68672960851606 loss_input: 82.31047604817998
step: 10000 epoch: 911 loss: 16.646833979359936 loss_input: 82.26367400491397
step: 11000 epoch: 911 loss: 16.673322302397246 loss_input: 82.2758333004883
step: 12000 epoch: 911 loss: 16.673631690758565 loss_input: 82.17380991769566
step: 13000 epoch: 911 loss: 16.677057921414228 loss_input: 82.29220668140462
step: 14000 epoch: 911 loss: 16.666532096603277 loss_input: 82.24449172645592
step: 15000 epoch: 911 loss: 16.68028103595622 loss_input: 82.23261258494226
Save loss: 16.685290873333813 Name: 911_train_model.pth
step: 0 epoch: 912 loss: 18.480419158935547 loss_input: 91.55596923828125
step: 1000 epoch: 912 loss: 16.573336320680813 loss_input: 81.82751342895386
step: 2000 epoch: 912 loss: 16.56151957978969 loss_input: 82.18642619608164
step: 3000 epoch: 912 loss: 16.54709914039668 loss_input: 82.21495024119564
step: 4000 epoch: 912 loss: 16.558531265799864 loss_input: 82.3216120713474
step: 5000 epoch: 912 loss: 16.550178639532636 loss_input: 81.92654702702038
step: 6000 epoch: 912 loss: 16.596556566214883 loss_input: 81.86178796860679
step: 7000 epoch: 912 loss: 16.628144800075546 loss_input: 81.78808134308102
step: 8000 epoch: 912 loss: 16.62456485629454 loss_input: 81.84956071317501
step: 9000 epoch: 912 loss: 16.642443169833367 loss_input: 81.88214930464541
step: 10000 epoch: 912 loss: 16.643203739761866 loss_input: 81.91296016882687
step: 11000 epoch: 912 loss: 16.649768667560892 loss_input: 81.88207303752748
step: 12000 epoch: 912 loss: 16.662030270810664 loss_input: 82.03057949313143
step: 13000 epoch: 912 loss: 16.6660214775792 loss_input: 82.04398672871017
step: 14000 epoch: 912 loss: 16.67437480638797 loss_input: 82.07398165843205
step: 15000 epoch: 912 loss: 16.682971465938067 loss_input: 82.1405855250926
Save loss: 16.692066134721042 Name: 912_train_model.pth
step: 0 epoch: 913 loss: 12.5899076461792 loss_input: 87.6885986328125
step: 1000 epoch: 913 loss: 16.800446058248546 loss_input: 83.03984504265266
step: 2000 epoch: 913 loss: 16.84086350284178 loss_input: 82.9517795118554
step: 3000 epoch: 913 loss: 16.774468271464595 loss_input: 82.7975661351815
step: 4000 epoch: 913 loss: 16.721031896652683 loss_input: 82.73733268037971
step: 5000 epoch: 913 loss: 16.671521209902917 loss_input: 82.63679557789614
step: 6000 epoch: 913 loss: 16.648439810566458 loss_input: 82.52082417619525
step: 7000 epoch: 913 loss: 16.68292744022457 loss_input: 82.58071294816422
step: 8000 epoch: 913 loss: 16.652983930763224 loss_input: 82.44268203809729
step: 9000 epoch: 913 loss: 16.689290667915195 loss_input: 82.52446880495266
step: 10000 epoch: 913 loss: 16.69763079045737 loss_input: 82.456009623647
step: 11000 epoch: 913 loss: 16.700435931894326 loss_input: 82.37765307912349
step: 12000 epoch: 913 loss: 16.69842241086738 loss_input: 82.3506138434997
step: 13000 epoch: 913 loss: 16.68608512209797 loss_input: 82.33839321085127
step: 14000 epoch: 913 loss: 16.68846985454449 loss_input: 82.36044194911022
step: 15000 epoch: 913 loss: 16.69179429558784 loss_input: 82.29779678913333
Save loss: 16.687182436451316 Name: 913_train_model.pth
step: 0 epoch: 914 loss: 21.48125457763672 loss_input: 118.93280029296875
step: 1000 epoch: 914 loss: 16.533894587468197 loss_input: 81.38914216886629
step: 2000 epoch: 914 loss: 16.572556817609033 loss_input: 81.00114551488905
step: 3000 epoch: 914 loss: 16.668081370642565 loss_input: 81.8192736820474
step: 4000 epoch: 914 loss: 16.72254181176357 loss_input: 82.08016971587182
step: 5000 epoch: 914 loss: 16.665238595681057 loss_input: 82.1112959292907
step: 6000 epoch: 914 loss: 16.64281389872604 loss_input: 82.0025428399466
step: 7000 epoch: 914 loss: 16.675882745923154 loss_input: 82.06257086913087
step: 8000 epoch: 914 loss: 16.65874289140271 loss_input: 82.06387607158236
step: 9000 epoch: 914 loss: 16.690683469999076 loss_input: 82.21097877552451
step: 10000 epoch: 914 loss: 16.69313299890733 loss_input: 82.15031945291798
step: 11000 epoch: 914 loss: 16.7063210257161 loss_input: 82.31238992477957
step: 12000 epoch: 914 loss: 16.70371438187268 loss_input: 82.37452845624681
step: 13000 epoch: 914 loss: 16.698660030923214 loss_input: 82.29135350841402
step: 14000 epoch: 914 loss: 16.690425513957315 loss_input: 82.23587945709653
step: 15000 epoch: 914 loss: 16.703594134287965 loss_input: 82.27079918711799
Save loss: 16.690767599910497 Name: 914_train_model.pth
step: 0 epoch: 915 loss: 14.505656242370605 loss_input: 89.609375
step: 1000 epoch: 915 loss: 16.748347494390224 loss_input: 83.79504540869287
step: 2000 epoch: 915 loss: 16.61116634661528 loss_input: 82.82101274192006
step: 3000 epoch: 915 loss: 16.718287909837294 loss_input: 82.82962860849769
step: 4000 epoch: 915 loss: 16.680945306204464 loss_input: 82.43208509396088
step: 5000 epoch: 915 loss: 16.62428793910026 loss_input: 82.16930705655744
step: 6000 epoch: 915 loss: 16.61604910726409 loss_input: 82.07776416943204
step: 7000 epoch: 915 loss: 16.630205579526663 loss_input: 81.94615431612448
step: 8000 epoch: 915 loss: 16.634091591897597 loss_input: 81.9921045254475
step: 9000 epoch: 915 loss: 16.630985332931257 loss_input: 81.87415841192342
step: 10000 epoch: 915 loss: 16.621452496679005 loss_input: 81.93594670074008
step: 11000 epoch: 915 loss: 16.618722676537317 loss_input: 81.93179924095146
step: 12000 epoch: 915 loss: 16.64436716968224 loss_input: 82.09676964073795
step: 13000 epoch: 915 loss: 16.674664898199648 loss_input: 82.19227607022266
step: 14000 epoch: 915 loss: 16.657545245517092 loss_input: 82.17426541065984
step: 15000 epoch: 915 loss: 16.67594695866851 loss_input: 82.18572135474172
Save loss: 16.682023579761385 Name: 915_train_model.pth
step: 0 epoch: 916 loss: 13.792787551879883 loss_input: 66.264404296875
step: 1000 epoch: 916 loss: 16.854909268292513 loss_input: 82.7578951200167
step: 2000 epoch: 916 loss: 16.94358667345538 loss_input: 82.74128938948495
step: 3000 epoch: 916 loss: 16.793314100781586 loss_input: 82.42304255921854
step: 4000 epoch: 916 loss: 16.743147341914845 loss_input: 82.22328866025681
step: 5000 epoch: 916 loss: 16.70335479884881 loss_input: 82.05602910179682
step: 6000 epoch: 916 loss: 16.726973466367806 loss_input: 82.2689581541911
step: 7000 epoch: 916 loss: 16.708911023230538 loss_input: 82.10638791277586
step: 8000 epoch: 916 loss: 16.67276897389894 loss_input: 81.99861909198606
step: 9000 epoch: 916 loss: 16.698134561523226 loss_input: 82.2524518217594
step: 10000 epoch: 916 loss: 16.680993900026348 loss_input: 82.18125949856186
step: 11000 epoch: 916 loss: 16.661687062811975 loss_input: 82.17983810442836
step: 12000 epoch: 916 loss: 16.676470736306605 loss_input: 82.22726719896552
step: 13000 epoch: 916 loss: 16.67297380400221 loss_input: 82.24707560806621
step: 14000 epoch: 916 loss: 16.668345450248115 loss_input: 82.25379821677079
step: 15000 epoch: 916 loss: 16.674543228874793 loss_input: 82.24777587662776
Save loss: 16.67701867775619 Name: 916_train_model.pth
step: 0 epoch: 917 loss: 15.613201141357422 loss_input: 64.77142333984375
step: 1000 epoch: 917 loss: 16.623018436260395 loss_input: 83.08012257517873
step: 2000 epoch: 917 loss: 16.700663573976637 loss_input: 83.40927939960683
step: 3000 epoch: 917 loss: 16.667298755658464 loss_input: 82.86359413398658
step: 4000 epoch: 917 loss: 16.684383142652944 loss_input: 82.77400985141898
step: 5000 epoch: 917 loss: 16.720944735079474 loss_input: 82.71304894294579
step: 6000 epoch: 917 loss: 16.709710463665463 loss_input: 82.54816642091227
step: 7000 epoch: 917 loss: 16.681717720836797 loss_input: 82.34417104755124
step: 8000 epoch: 917 loss: 16.688327105875807 loss_input: 82.32264363713568
step: 9000 epoch: 917 loss: 16.677230662179646 loss_input: 82.2693842868913
step: 10000 epoch: 917 loss: 16.637360593912877 loss_input: 82.17053342883484
step: 11000 epoch: 917 loss: 16.667076710147995 loss_input: 82.22758763097175
step: 12000 epoch: 917 loss: 16.673541255120824 loss_input: 82.20683976815408
step: 13000 epoch: 917 loss: 16.675808279214113 loss_input: 82.22233025344204
step: 14000 epoch: 917 loss: 16.67391619356383 loss_input: 82.22623937680511
step: 15000 epoch: 917 loss: 16.693369435593333 loss_input: 82.22792531604664
Save loss: 16.69059275045991 Name: 917_train_model.pth
step: 0 epoch: 918 loss: 9.560691833496094 loss_input: 49.43768310546875
step: 1000 epoch: 918 loss: 16.374921744877284 loss_input: 81.12984250856566
step: 2000 epoch: 918 loss: 16.550533895907193 loss_input: 81.5910590237108
step: 3000 epoch: 918 loss: 16.513244251535955 loss_input: 81.68666419868508
step: 4000 epoch: 918 loss: 16.587037579889923 loss_input: 82.5524771303512
step: 5000 epoch: 918 loss: 16.618749974179664 loss_input: 82.49209373213247
step: 6000 epoch: 918 loss: 16.66590215444287 loss_input: 82.51392123326126
step: 7000 epoch: 918 loss: 16.649256185878432 loss_input: 82.5074963830502
step: 8000 epoch: 918 loss: 16.64945747649397 loss_input: 82.29380788613581
step: 9000 epoch: 918 loss: 16.668718890473123 loss_input: 82.34625139714294
step: 10000 epoch: 918 loss: 16.653777287204484 loss_input: 82.23795528772318
step: 11000 epoch: 918 loss: 16.65613218405368 loss_input: 82.2297036328995
step: 12000 epoch: 918 loss: 16.654425171353143 loss_input: 82.23373279582658
step: 13000 epoch: 918 loss: 16.67223788567593 loss_input: 82.25122248709233
step: 14000 epoch: 918 loss: 16.67237570968477 loss_input: 82.17200397987058
step: 15000 epoch: 918 loss: 16.669346725580716 loss_input: 82.1815832191591
Save loss: 16.677922753989698 Name: 918_train_model.pth
step: 0 epoch: 919 loss: 16.09848976135254 loss_input: 63.88299560546875
step: 1000 epoch: 919 loss: 16.60539648987792 loss_input: 81.96427815348714
step: 2000 epoch: 919 loss: 16.685249080066978 loss_input: 82.23475792621268
step: 3000 epoch: 919 loss: 16.65881955039696 loss_input: 82.28984658108756
step: 4000 epoch: 919 loss: 16.681270635476622 loss_input: 82.20022251212099
step: 5000 epoch: 919 loss: 16.63131457551721 loss_input: 82.20763538942602
step: 6000 epoch: 919 loss: 16.607515500835767 loss_input: 82.22435393458187
step: 7000 epoch: 919 loss: 16.635945827003003 loss_input: 82.19516184316024
step: 8000 epoch: 919 loss: 16.63228737796907 loss_input: 82.09765717304136
step: 9000 epoch: 919 loss: 16.62400315563065 loss_input: 81.991214063509
step: 10000 epoch: 919 loss: 16.62725499610569 loss_input: 82.15670643018706
step: 11000 epoch: 919 loss: 16.648099204690094 loss_input: 82.2695189275673
step: 12000 epoch: 919 loss: 16.65756481232797 loss_input: 82.27863155060157
step: 13000 epoch: 919 loss: 16.672834719357954 loss_input: 82.34136905615884
step: 14000 epoch: 919 loss: 16.670986202459456 loss_input: 82.30431828111608
step: 15000 epoch: 919 loss: 16.68186883814819 loss_input: 82.33005337892838
Save loss: 16.680439487203955 Name: 919_train_model.pth
step: 0 epoch: 920 loss: 14.841035842895508 loss_input: 72.862548828125
step: 1000 epoch: 920 loss: 16.624350970798915 loss_input: 81.9058256196928
step: 2000 epoch: 920 loss: 16.528117458442637 loss_input: 82.01579523348677
step: 3000 epoch: 920 loss: 16.551226717756016 loss_input: 82.10630994095717
step: 4000 epoch: 920 loss: 16.646318995097495 loss_input: 82.15492106282511
step: 5000 epoch: 920 loss: 16.645629897496146 loss_input: 82.15663446900464
step: 6000 epoch: 920 loss: 16.648724524185393 loss_input: 82.13276385291579
step: 7000 epoch: 920 loss: 16.640656534254067 loss_input: 82.05195417814195
step: 8000 epoch: 920 loss: 16.655867229326265 loss_input: 82.0750263272755
step: 9000 epoch: 920 loss: 16.667762267696844 loss_input: 82.1538615256942
step: 10000 epoch: 920 loss: 16.683222863855583 loss_input: 82.13207432918817
step: 11000 epoch: 920 loss: 16.68839774583862 loss_input: 82.17397527848144
step: 12000 epoch: 920 loss: 16.686621263239566 loss_input: 82.23031454735543
step: 13000 epoch: 920 loss: 16.70348103819751 loss_input: 82.29948069086332
step: 14000 epoch: 920 loss: 16.702266859638716 loss_input: 82.21072784613459
step: 15000 epoch: 920 loss: 16.698189618658855 loss_input: 82.2436239114007
Save loss: 16.680906051918864 Name: 920_train_model.pth
step: 0 epoch: 921 loss: 21.066204071044922 loss_input: 107.685546875
step: 1000 epoch: 921 loss: 16.627103598801405 loss_input: 83.73691512345077
step: 2000 epoch: 921 loss: 16.470106018596383 loss_input: 82.55287435017843
step: 3000 epoch: 921 loss: 16.595822935698628 loss_input: 82.4747524954247
step: 4000 epoch: 921 loss: 16.6666329435574 loss_input: 82.39559639814168
step: 5000 epoch: 921 loss: 16.649016155383272 loss_input: 82.18923971553346
step: 6000 epoch: 921 loss: 16.625133601889335 loss_input: 82.1792364745036
step: 7000 epoch: 921 loss: 16.63602356692073 loss_input: 82.0277648062693
step: 8000 epoch: 921 loss: 16.65965527430905 loss_input: 82.09582347128485
step: 9000 epoch: 921 loss: 16.656674506068033 loss_input: 82.01563221491016
step: 10000 epoch: 921 loss: 16.66879443525851 loss_input: 82.10891553666899
step: 11000 epoch: 921 loss: 16.690650408944805 loss_input: 82.20280073304771
step: 12000 epoch: 921 loss: 16.703169223378374 loss_input: 82.18351439056511
step: 13000 epoch: 921 loss: 16.697144343553898 loss_input: 82.17823228028433
step: 14000 epoch: 921 loss: 16.669250834355157 loss_input: 82.14409440027443
step: 15000 epoch: 921 loss: 16.678682816377457 loss_input: 82.1962484971705
Save loss: 16.68055991744995 Name: 921_train_model.pth
step: 0 epoch: 922 loss: 20.435710906982422 loss_input: 77.81414794921875
step: 1000 epoch: 922 loss: 16.735694452718302 loss_input: 82.35820264940138
step: 2000 epoch: 922 loss: 16.72919159802957 loss_input: 82.5623621904868
step: 3000 epoch: 922 loss: 16.678925199216305 loss_input: 82.52988456972676
step: 4000 epoch: 922 loss: 16.658503554994184 loss_input: 82.58189665535276
step: 5000 epoch: 922 loss: 16.67943569064927 loss_input: 82.55124066739387
step: 6000 epoch: 922 loss: 16.664969841215417 loss_input: 82.27897393077716
step: 7000 epoch: 922 loss: 16.665955348144923 loss_input: 82.46032807255213
step: 8000 epoch: 922 loss: 16.671254955102707 loss_input: 82.47539865927403
step: 9000 epoch: 922 loss: 16.661843284423213 loss_input: 82.27317053476263
step: 10000 epoch: 922 loss: 16.6786588949986 loss_input: 82.3152212427683
step: 11000 epoch: 922 loss: 16.682383015918447 loss_input: 82.36805150171007
step: 12000 epoch: 922 loss: 16.681447116288233 loss_input: 82.2433065270277
step: 13000 epoch: 922 loss: 16.672339988409504 loss_input: 82.20816797990449
step: 14000 epoch: 922 loss: 16.67508088874013 loss_input: 82.23631552649978
step: 15000 epoch: 922 loss: 16.67456467381621 loss_input: 82.31441443129052
Save loss: 16.681146515101194 Name: 922_train_model.pth
step: 0 epoch: 923 loss: 14.942413330078125 loss_input: 102.65582275390625
step: 1000 epoch: 923 loss: 16.31809470894096 loss_input: 82.14777763740166
step: 2000 epoch: 923 loss: 16.592378889662452 loss_input: 82.2444986918162
step: 3000 epoch: 923 loss: 16.657355757404748 loss_input: 82.29332757814453
step: 4000 epoch: 923 loss: 16.678692865181016 loss_input: 82.26882860583682
step: 5000 epoch: 923 loss: 16.682139700495036 loss_input: 82.56185956412233
step: 6000 epoch: 923 loss: 16.68655189091594 loss_input: 82.36844774731556
step: 7000 epoch: 923 loss: 16.679179642102053 loss_input: 82.2545253196251
step: 8000 epoch: 923 loss: 16.67058458779398 loss_input: 82.13715525702109
step: 9000 epoch: 923 loss: 16.69419512676141 loss_input: 82.24112290067814
step: 10000 epoch: 923 loss: 16.694662337326523 loss_input: 82.19029337300645
step: 11000 epoch: 923 loss: 16.712555928139693 loss_input: 82.11619351077282
step: 12000 epoch: 923 loss: 16.695809651033034 loss_input: 82.13932113323239
step: 13000 epoch: 923 loss: 16.702319465099084 loss_input: 82.26175884701327
step: 14000 epoch: 923 loss: 16.69254529066626 loss_input: 82.25084458340169
step: 15000 epoch: 923 loss: 16.695543761873523 loss_input: 82.2653256791344
Save loss: 16.678428892955186 Name: 923_train_model.pth
step: 0 epoch: 924 loss: 11.190776824951172 loss_input: 66.43353271484375
step: 1000 epoch: 924 loss: 16.371077404631958 loss_input: 81.69501945808098
step: 2000 epoch: 924 loss: 16.43846873269565 loss_input: 82.31193436484882
step: 3000 epoch: 924 loss: 16.46970817066359 loss_input: 82.07312579156557
step: 4000 epoch: 924 loss: 16.50875945402306 loss_input: 82.24224278403055
step: 5000 epoch: 924 loss: 16.562263838888715 loss_input: 82.0252393979212
step: 6000 epoch: 924 loss: 16.574909168886077 loss_input: 81.9503293945638
step: 7000 epoch: 924 loss: 16.599107027768987 loss_input: 82.10812089797719
step: 8000 epoch: 924 loss: 16.600899839562157 loss_input: 82.0938698733215
step: 9000 epoch: 924 loss: 16.608492386922507 loss_input: 81.99004257637823
step: 10000 epoch: 924 loss: 16.635842011768975 loss_input: 82.09128420654028
step: 11000 epoch: 924 loss: 16.627281051474846 loss_input: 82.07394281294918
step: 12000 epoch: 924 loss: 16.647811599159606 loss_input: 82.22580988225754
step: 13000 epoch: 924 loss: 16.655526496164523 loss_input: 82.22739763144722
step: 14000 epoch: 924 loss: 16.676293910839636 loss_input: 82.3112456383905
step: 15000 epoch: 924 loss: 16.685121307388304 loss_input: 82.2652531375838
Save loss: 16.679346290946008 Name: 924_train_model.pth
step: 0 epoch: 925 loss: 14.928738594055176 loss_input: 64.71563720703125
step: 1000 epoch: 925 loss: 16.559515762043286 loss_input: 81.06825449940685
step: 2000 epoch: 925 loss: 16.615573777728294 loss_input: 81.36684297621697
step: 3000 epoch: 925 loss: 16.61756130219777 loss_input: 82.32468842173051
step: 4000 epoch: 925 loss: 16.58935684789511 loss_input: 82.51425612529972
step: 5000 epoch: 925 loss: 16.5898497660049 loss_input: 82.56528675954262
step: 6000 epoch: 925 loss: 16.602891197722666 loss_input: 82.50371159046735
step: 7000 epoch: 925 loss: 16.611781359093477 loss_input: 82.33923637129685
step: 8000 epoch: 925 loss: 16.607657200425077 loss_input: 82.256328333409
step: 9000 epoch: 925 loss: 16.631518813772026 loss_input: 82.19587642931167
step: 10000 epoch: 925 loss: 16.636799472437513 loss_input: 82.19104296235416
step: 11000 epoch: 925 loss: 16.635714909150682 loss_input: 82.15449493272274
step: 12000 epoch: 925 loss: 16.63174198657391 loss_input: 82.11383368402568
step: 13000 epoch: 925 loss: 16.629964084480736 loss_input: 82.09773431265504
step: 14000 epoch: 925 loss: 16.64461855511692 loss_input: 82.12043393297661
step: 15000 epoch: 925 loss: 16.670395419991944 loss_input: 82.20599942588463
Save loss: 16.683009138554336 Name: 925_train_model.pth
step: 0 epoch: 926 loss: 11.508091926574707 loss_input: 46.8760986328125
step: 1000 epoch: 926 loss: 16.60676458022454 loss_input: 82.35487967842704
step: 2000 epoch: 926 loss: 16.649952311685002 loss_input: 82.13855968744143
step: 3000 epoch: 926 loss: 16.641167148356832 loss_input: 82.36745480591756
step: 4000 epoch: 926 loss: 16.62855721574758 loss_input: 82.1429850057017
step: 5000 epoch: 926 loss: 16.586714466293678 loss_input: 81.97994280538424
step: 6000 epoch: 926 loss: 16.580451778323823 loss_input: 82.03462142702779
step: 7000 epoch: 926 loss: 16.61028046993473 loss_input: 82.04391340439089
step: 8000 epoch: 926 loss: 16.64609002593219 loss_input: 82.01498749881607
step: 9000 epoch: 926 loss: 16.648800029501412 loss_input: 82.10011853473529
step: 10000 epoch: 926 loss: 16.627662990036256 loss_input: 81.98742295570247
step: 11000 epoch: 926 loss: 16.625449393816552 loss_input: 81.98050008038155
step: 12000 epoch: 926 loss: 16.640829360740362 loss_input: 82.08235500943769
step: 13000 epoch: 926 loss: 16.65129533880225 loss_input: 82.07984686649705
step: 14000 epoch: 926 loss: 16.656900215605294 loss_input: 82.08754559610837
step: 15000 epoch: 926 loss: 16.67991716119021 loss_input: 82.2524473791591
Save loss: 16.677235116243363 Name: 926_train_model.pth
step: 0 epoch: 927 loss: 15.706050872802734 loss_input: 82.73272705078125
step: 1000 epoch: 927 loss: 16.736215096968156 loss_input: 82.63748592715878
step: 2000 epoch: 927 loss: 16.655976324067122 loss_input: 81.82813317462363
step: 3000 epoch: 927 loss: 16.618111115938344 loss_input: 82.09595206545735
step: 4000 epoch: 927 loss: 16.6159471338673 loss_input: 82.04543669990706
step: 5000 epoch: 927 loss: 16.595631573253144 loss_input: 81.84338205601067
step: 6000 epoch: 927 loss: 16.607495437719646 loss_input: 81.76206045940586
step: 7000 epoch: 927 loss: 16.593325916349404 loss_input: 81.8360390131209
step: 8000 epoch: 927 loss: 16.648956924568875 loss_input: 81.90637810440931
step: 9000 epoch: 927 loss: 16.65135491239986 loss_input: 81.90338684021003
step: 10000 epoch: 927 loss: 16.66636386929411 loss_input: 81.98148550013163
step: 11000 epoch: 927 loss: 16.66699151342191 loss_input: 82.03557423623775
step: 12000 epoch: 927 loss: 16.66193056760177 loss_input: 82.16597746594847
step: 13000 epoch: 927 loss: 16.679229034899603 loss_input: 82.20446737439218
step: 14000 epoch: 927 loss: 16.671645961380985 loss_input: 82.21725492420882
step: 15000 epoch: 927 loss: 16.678178033545194 loss_input: 82.21746836750597
Save loss: 16.682702697083354 Name: 927_train_model.pth
step: 0 epoch: 928 loss: 15.028636932373047 loss_input: 91.45843505859375
step: 1000 epoch: 928 loss: 16.568691083124943 loss_input: 82.12822516838631
step: 2000 epoch: 928 loss: 16.495563086243287 loss_input: 81.58680580974162
step: 3000 epoch: 928 loss: 16.55918678511226 loss_input: 81.72576139577824
step: 4000 epoch: 928 loss: 16.643539224675642 loss_input: 81.77176984135313
step: 5000 epoch: 928 loss: 16.70889403929212 loss_input: 81.88554764633011
step: 6000 epoch: 928 loss: 16.735170569147314 loss_input: 82.04681278502737
step: 7000 epoch: 928 loss: 16.7311576864512 loss_input: 82.12953489205783
step: 8000 epoch: 928 loss: 16.707448499796616 loss_input: 82.14670479117356
step: 9000 epoch: 928 loss: 16.707118238903206 loss_input: 82.27221941431421
step: 10000 epoch: 928 loss: 16.701801400699086 loss_input: 82.26592503713496
step: 11000 epoch: 928 loss: 16.711601027336222 loss_input: 82.35357755895593
step: 12000 epoch: 928 loss: 16.71076919209827 loss_input: 82.37967489322179
step: 13000 epoch: 928 loss: 16.70459135682131 loss_input: 82.31389711871623
step: 14000 epoch: 928 loss: 16.71729615487147 loss_input: 82.32952464106219
step: 15000 epoch: 928 loss: 16.692961720750855 loss_input: 82.28293855523565
Save loss: 16.678724959403276 Name: 928_train_model.pth
step: 0 epoch: 929 loss: 10.044859886169434 loss_input: 60.289306640625
step: 1000 epoch: 929 loss: 16.941631552460905 loss_input: 81.67243474299138
step: 2000 epoch: 929 loss: 16.800230816446025 loss_input: 82.03519904476472
step: 3000 epoch: 929 loss: 16.772029330117906 loss_input: 82.32341758476342
step: 4000 epoch: 929 loss: 16.722956743695622 loss_input: 82.41631839049515
step: 5000 epoch: 929 loss: 16.732526977499397 loss_input: 82.44593215146033
step: 6000 epoch: 929 loss: 16.722582498245927 loss_input: 82.31309505463283
step: 7000 epoch: 929 loss: 16.713004011134558 loss_input: 82.15524147708251
step: 8000 epoch: 929 loss: 16.70347290881767 loss_input: 82.1821938776818
step: 9000 epoch: 929 loss: 16.692123462194708 loss_input: 82.1057852634972
step: 10000 epoch: 929 loss: 16.70345953828918 loss_input: 82.22824519916172
step: 11000 epoch: 929 loss: 16.69763893362024 loss_input: 82.22570662077595
step: 12000 epoch: 929 loss: 16.66931538845676 loss_input: 82.13694374244041
step: 13000 epoch: 929 loss: 16.6797996911349 loss_input: 82.12952176098383
step: 14000 epoch: 929 loss: 16.687576116216956 loss_input: 82.2658196838828
step: 15000 epoch: 929 loss: 16.683211688844054 loss_input: 82.2435167367423
Save loss: 16.681186233222483 Name: 929_train_model.pth
step: 0 epoch: 930 loss: 18.31704330444336 loss_input: 64.494873046875
step: 1000 epoch: 930 loss: 16.74878212169453 loss_input: 81.34653530063686
step: 2000 epoch: 930 loss: 16.74062604775493 loss_input: 81.62620481141265
step: 3000 epoch: 930 loss: 16.71196343580193 loss_input: 82.06231992493387
step: 4000 epoch: 930 loss: 16.67562625599456 loss_input: 82.15087504674125
step: 5000 epoch: 930 loss: 16.675150215184775 loss_input: 82.54525645163936
step: 6000 epoch: 930 loss: 16.663270589014505 loss_input: 82.44329817711045
step: 7000 epoch: 930 loss: 16.656994954498643 loss_input: 82.37755370361432
step: 8000 epoch: 930 loss: 16.671543677081736 loss_input: 82.47229120621397
step: 9000 epoch: 930 loss: 16.653609208538324 loss_input: 82.17504892983897
step: 10000 epoch: 930 loss: 16.639499627450814 loss_input: 82.02148425294189
step: 11000 epoch: 930 loss: 16.63641856789708 loss_input: 81.924543924629
step: 12000 epoch: 930 loss: 16.62708001362861 loss_input: 82.01621633560973
step: 13000 epoch: 930 loss: 16.64039847836312 loss_input: 82.03637246503875
step: 14000 epoch: 930 loss: 16.659462166329757 loss_input: 82.18428970597385
step: 15000 epoch: 930 loss: 16.682301537162996 loss_input: 82.25111052573709
Save loss: 16.668973108008505 Name: 930_train_model.pth
step: 0 epoch: 931 loss: 11.333154678344727 loss_input: 47.679931640625
step: 1000 epoch: 931 loss: 16.38709550518375 loss_input: 81.46294159941621
step: 2000 epoch: 931 loss: 16.589058367030017 loss_input: 82.07232949687265
step: 3000 epoch: 931 loss: 16.54298234136849 loss_input: 82.22638395030432
step: 4000 epoch: 931 loss: 16.61100561408453 loss_input: 82.27385379302147
step: 5000 epoch: 931 loss: 16.664738613041706 loss_input: 82.50641666100374
step: 6000 epoch: 931 loss: 16.655828157875305 loss_input: 82.45894854316273
step: 7000 epoch: 931 loss: 16.657887186838582 loss_input: 82.45444775676577
step: 8000 epoch: 931 loss: 16.64912922035797 loss_input: 82.39446681255907
step: 9000 epoch: 931 loss: 16.644977650183094 loss_input: 82.39320438462566
step: 10000 epoch: 931 loss: 16.65590323580347 loss_input: 82.41068089235998
step: 11000 epoch: 931 loss: 16.655073762340944 loss_input: 82.33893853111188
step: 12000 epoch: 931 loss: 16.669250010490497 loss_input: 82.35057436721581
step: 13000 epoch: 931 loss: 16.66688477729854 loss_input: 82.30823606924316
step: 14000 epoch: 931 loss: 16.6736575677628 loss_input: 82.26542380325931
step: 15000 epoch: 931 loss: 16.68615352067222 loss_input: 82.3409924730604
Save loss: 16.677670944094658 Name: 931_train_model.pth
step: 0 epoch: 932 loss: 15.463456153869629 loss_input: 95.95892333984375
step: 1000 epoch: 932 loss: 16.812939983981472 loss_input: 82.28040380030126
step: 2000 epoch: 932 loss: 16.76459834112161 loss_input: 81.90091669863311
step: 3000 epoch: 932 loss: 16.7123157317064 loss_input: 82.39982758025971
step: 4000 epoch: 932 loss: 16.680433462989594 loss_input: 82.26410186698931
step: 5000 epoch: 932 loss: 16.67760355270903 loss_input: 82.25663202320473
step: 6000 epoch: 932 loss: 16.658737920995673 loss_input: 82.16264085176884
step: 7000 epoch: 932 loss: 16.68234623260046 loss_input: 82.25618709569113
step: 8000 epoch: 932 loss: 16.69541861009425 loss_input: 82.29110873775055
step: 9000 epoch: 932 loss: 16.688995582582475 loss_input: 82.33282503251591
step: 10000 epoch: 932 loss: 16.6993760441127 loss_input: 82.29376302482056
step: 11000 epoch: 932 loss: 16.678867990196256 loss_input: 82.26783391118386
step: 12000 epoch: 932 loss: 16.66965398865137 loss_input: 82.2375321821733
step: 13000 epoch: 932 loss: 16.66655088472289 loss_input: 82.17783081721328
step: 14000 epoch: 932 loss: 16.675541572541512 loss_input: 82.22777401363072
step: 15000 epoch: 932 loss: 16.667278188664504 loss_input: 82.21980027584749
Save loss: 16.67575299333036 Name: 932_train_model.pth
step: 0 epoch: 933 loss: 23.55516242980957 loss_input: 75.4273681640625
step: 1000 epoch: 933 loss: 16.651753891002645 loss_input: 82.71028788106425
step: 2000 epoch: 933 loss: 16.674939605011335 loss_input: 82.12968198517929
step: 3000 epoch: 933 loss: 16.592361031036223 loss_input: 81.83795491427988
step: 4000 epoch: 933 loss: 16.609131829972803 loss_input: 82.09530765501984
step: 5000 epoch: 933 loss: 16.70296729476279 loss_input: 82.4290337245051
step: 6000 epoch: 933 loss: 16.699613390078525 loss_input: 82.51177916473557
step: 7000 epoch: 933 loss: 16.680561734785133 loss_input: 82.4019863268696
step: 8000 epoch: 933 loss: 16.682216541541546 loss_input: 82.52321460458312
step: 9000 epoch: 933 loss: 16.682399114891233 loss_input: 82.32668193659906
step: 10000 epoch: 933 loss: 16.69146392846296 loss_input: 82.44827407518038
step: 11000 epoch: 933 loss: 16.679173340505713 loss_input: 82.23223055827489
step: 12000 epoch: 933 loss: 16.680077052396513 loss_input: 82.11556565639387
step: 13000 epoch: 933 loss: 16.69619547490954 loss_input: 82.28745595290746
step: 14000 epoch: 933 loss: 16.69449865628154 loss_input: 82.2224029068027
step: 15000 epoch: 933 loss: 16.678633058160933 loss_input: 82.20619819193044
Save loss: 16.68230250285566 Name: 933_train_model.pth
step: 0 epoch: 934 loss: 18.441295623779297 loss_input: 66.93505859375
step: 1000 epoch: 934 loss: 16.601284645415923 loss_input: 81.70757300560767
step: 2000 epoch: 934 loss: 16.64004713031782 loss_input: 82.47285058032507
step: 3000 epoch: 934 loss: 16.633742094913828 loss_input: 82.19804984845547
step: 4000 epoch: 934 loss: 16.66077180601185 loss_input: 82.27215777537549
step: 5000 epoch: 934 loss: 16.652769969096543 loss_input: 82.29799931127056
step: 6000 epoch: 934 loss: 16.64796992568766 loss_input: 82.2013363983607
step: 7000 epoch: 934 loss: 16.643705443848273 loss_input: 82.23769369617119
step: 8000 epoch: 934 loss: 16.617620208832253 loss_input: 82.12426696022128
step: 9000 epoch: 934 loss: 16.64178635557285 loss_input: 82.22183625240642
step: 10000 epoch: 934 loss: 16.662119922489182 loss_input: 82.24470762762591
step: 11000 epoch: 934 loss: 16.668294656559528 loss_input: 82.22772927514401
step: 12000 epoch: 934 loss: 16.675106493952512 loss_input: 82.19567765816879
step: 13000 epoch: 934 loss: 16.681034322793955 loss_input: 82.24586758049861
step: 14000 epoch: 934 loss: 16.683678601130563 loss_input: 82.26676435343207
step: 15000 epoch: 934 loss: 16.675762799602932 loss_input: 82.18803435565233
Save loss: 16.678154183000327 Name: 934_train_model.pth
step: 0 epoch: 935 loss: 7.17503547668457 loss_input: 54.5985107421875
step: 1000 epoch: 935 loss: 16.494139130179818 loss_input: 82.82252872859563
step: 2000 epoch: 935 loss: 16.59256413029409 loss_input: 82.56882557256527
step: 3000 epoch: 935 loss: 16.568003402158286 loss_input: 82.09944764505582
step: 4000 epoch: 935 loss: 16.545133231133708 loss_input: 82.03051530513368
step: 5000 epoch: 935 loss: 16.566985299028985 loss_input: 82.04450911575498
step: 6000 epoch: 935 loss: 16.576870344138467 loss_input: 81.89481491447388
step: 7000 epoch: 935 loss: 16.603807138930115 loss_input: 82.0459111397172
step: 8000 epoch: 935 loss: 16.5930099140747 loss_input: 81.96273919281818
step: 9000 epoch: 935 loss: 16.602847093344398 loss_input: 81.98242557060718
step: 10000 epoch: 935 loss: 16.605874425589402 loss_input: 81.88560065502239
step: 11000 epoch: 935 loss: 16.630614440621663 loss_input: 81.96288079368495
step: 12000 epoch: 935 loss: 16.645978742714238 loss_input: 82.0285557614258
step: 13000 epoch: 935 loss: 16.667038151231438 loss_input: 82.10198925785005
step: 14000 epoch: 935 loss: 16.663657848259934 loss_input: 82.10836606502158
step: 15000 epoch: 935 loss: 16.667988293346234 loss_input: 82.20527025900097
Save loss: 16.673316730871797 Name: 935_train_model.pth
step: 0 epoch: 936 loss: 16.045040130615234 loss_input: 72.4688720703125
step: 1000 epoch: 936 loss: 16.349468167368826 loss_input: 81.19251085828235
step: 2000 epoch: 936 loss: 16.473865381543007 loss_input: 81.90793336242035
step: 3000 epoch: 936 loss: 16.466765161912786 loss_input: 81.70434627259664
step: 4000 epoch: 936 loss: 16.538707426862995 loss_input: 81.79960290994146
step: 5000 epoch: 936 loss: 16.581137092893922 loss_input: 81.82766218973002
step: 6000 epoch: 936 loss: 16.65029286885337 loss_input: 81.93523623748256
step: 7000 epoch: 936 loss: 16.67464650533758 loss_input: 82.13573638998837
step: 8000 epoch: 936 loss: 16.65577453131259 loss_input: 82.13937169291827
step: 9000 epoch: 936 loss: 16.66537548512727 loss_input: 82.05276205475867
step: 10000 epoch: 936 loss: 16.663864870427098 loss_input: 82.13617271085391
step: 11000 epoch: 936 loss: 16.666746820864727 loss_input: 82.06050203488248
step: 12000 epoch: 936 loss: 16.65331260626162 loss_input: 82.0751875413377
step: 13000 epoch: 936 loss: 16.660451206498198 loss_input: 82.09279696709027
step: 14000 epoch: 936 loss: 16.66911628440264 loss_input: 82.15624762415827
step: 15000 epoch: 936 loss: 16.686121139150643 loss_input: 82.24510765851286
Save loss: 16.682018370583652 Name: 936_train_model.pth
step: 0 epoch: 937 loss: 19.23798370361328 loss_input: 97.58837890625
step: 1000 epoch: 937 loss: 16.697350792594246 loss_input: 82.93297669127747
step: 2000 epoch: 937 loss: 16.638053340235096 loss_input: 81.81457333467056
step: 3000 epoch: 937 loss: 16.659483695260608 loss_input: 81.61425234150465
step: 4000 epoch: 937 loss: 16.692495281831587 loss_input: 81.81681168147904
step: 5000 epoch: 937 loss: 16.73915441969208 loss_input: 82.16579359909268
step: 6000 epoch: 937 loss: 16.70425443275037 loss_input: 82.22891371132155
step: 7000 epoch: 937 loss: 16.676895137685246 loss_input: 82.18274657226702
step: 8000 epoch: 937 loss: 16.649914144680004 loss_input: 82.10235622402668
step: 9000 epoch: 937 loss: 16.652511415422236 loss_input: 82.13986105357868
step: 10000 epoch: 937 loss: 16.650685269288356 loss_input: 82.17361564014497
step: 11000 epoch: 937 loss: 16.656268345227122 loss_input: 82.12642765452608
step: 12000 epoch: 937 loss: 16.645262222033363 loss_input: 82.0950818897416
step: 13000 epoch: 937 loss: 16.635738874818625 loss_input: 82.10442931673525
step: 14000 epoch: 937 loss: 16.651270424210253 loss_input: 82.13556663677204
step: 15000 epoch: 937 loss: 16.6730684045553 loss_input: 82.22654809666335
Save loss: 16.680306364417078 Name: 937_train_model.pth
step: 0 epoch: 938 loss: 15.65818977355957 loss_input: 69.6123046875
step: 1000 epoch: 938 loss: 16.770005732983144 loss_input: 82.65550776128168
step: 2000 epoch: 938 loss: 16.708536860109984 loss_input: 82.82768933526401
step: 3000 epoch: 938 loss: 16.666125414014775 loss_input: 82.36492840602611
step: 4000 epoch: 938 loss: 16.615280710914917 loss_input: 82.50916801134188
step: 5000 epoch: 938 loss: 16.603208319517737 loss_input: 82.27389860162734
step: 6000 epoch: 938 loss: 16.61952383346189 loss_input: 82.25649575371878
step: 7000 epoch: 938 loss: 16.631011901455526 loss_input: 82.26047264244059
step: 8000 epoch: 938 loss: 16.61497431959961 loss_input: 82.30563635600238
step: 9000 epoch: 938 loss: 16.63124659861105 loss_input: 82.23718808261755
step: 10000 epoch: 938 loss: 16.608450978723194 loss_input: 82.10755167159066
step: 11000 epoch: 938 loss: 16.633347782045373 loss_input: 82.2215651238293
step: 12000 epoch: 938 loss: 16.64937735964741 loss_input: 82.19532166468224
step: 13000 epoch: 938 loss: 16.647134603532347 loss_input: 82.24052935869416
step: 14000 epoch: 938 loss: 16.668365442346705 loss_input: 82.31227740324631
step: 15000 epoch: 938 loss: 16.67023988886123 loss_input: 82.28056482432422
Save loss: 16.672883456379175 Name: 938_train_model.pth
step: 0 epoch: 939 loss: 17.190082550048828 loss_input: 62.79766845703125
step: 1000 epoch: 939 loss: 16.85245167649352 loss_input: 84.59439535074301
step: 2000 epoch: 939 loss: 16.69793312338696 loss_input: 83.24538771263782
step: 3000 epoch: 939 loss: 16.750613601713805 loss_input: 82.8995719891872
step: 4000 epoch: 939 loss: 16.72006738641744 loss_input: 82.65228690996196
step: 5000 epoch: 939 loss: 16.704218920601104 loss_input: 82.50854322543694
step: 6000 epoch: 939 loss: 16.684598721617203 loss_input: 82.46957065383309
step: 7000 epoch: 939 loss: 16.670724490458174 loss_input: 82.30710895060471
step: 8000 epoch: 939 loss: 16.695870787125052 loss_input: 82.47713071968299
step: 9000 epoch: 939 loss: 16.673034072120963 loss_input: 82.3561319928211
step: 10000 epoch: 939 loss: 16.68374443795607 loss_input: 82.38900840140596
step: 11000 epoch: 939 loss: 16.670991321399356 loss_input: 82.34241664176659
step: 12000 epoch: 939 loss: 16.692536158935596 loss_input: 82.34613459742613
step: 13000 epoch: 939 loss: 16.69423570116523 loss_input: 82.3045659273205
step: 14000 epoch: 939 loss: 16.68478860014567 loss_input: 82.28151300785993
step: 15000 epoch: 939 loss: 16.676340668926667 loss_input: 82.2022187534503
Save loss: 16.676144836992027 Name: 939_train_model.pth
step: 0 epoch: 940 loss: 14.329366683959961 loss_input: 62.4512939453125
step: 1000 epoch: 940 loss: 16.294862068854606 loss_input: 82.32576987221762
step: 2000 epoch: 940 loss: 16.484081185739793 loss_input: 81.9440155410576
step: 3000 epoch: 940 loss: 16.485773411642427 loss_input: 81.95679400404943
step: 4000 epoch: 940 loss: 16.52474861352392 loss_input: 82.23416397792641
step: 5000 epoch: 940 loss: 16.573279991123204 loss_input: 82.45990162319099
step: 6000 epoch: 940 loss: 16.58283828222996 loss_input: 82.26732117929889
step: 7000 epoch: 940 loss: 16.59751083400995 loss_input: 82.08585425122025
step: 8000 epoch: 940 loss: 16.592043639808697 loss_input: 81.92903835781513
step: 9000 epoch: 940 loss: 16.620856203512144 loss_input: 82.0031619479346
step: 10000 epoch: 940 loss: 16.62576737678977 loss_input: 82.10311054465842
step: 11000 epoch: 940 loss: 16.62978028696284 loss_input: 82.10566917678116
step: 12000 epoch: 940 loss: 16.64211979664104 loss_input: 82.15335469785883
step: 13000 epoch: 940 loss: 16.66199324400771 loss_input: 82.210374019832
step: 14000 epoch: 940 loss: 16.676053497588818 loss_input: 82.25215878570755
step: 15000 epoch: 940 loss: 16.675924680493814 loss_input: 82.22453246769226
Save loss: 16.67618092572689 Name: 940_train_model.pth
step: 0 epoch: 941 loss: 17.954435348510742 loss_input: 84.239501953125
step: 1000 epoch: 941 loss: 16.652729802317435 loss_input: 81.94889357468703
step: 2000 epoch: 941 loss: 16.669652970298298 loss_input: 82.24419165539301
step: 3000 epoch: 941 loss: 16.663572189530623 loss_input: 82.21233809514666
step: 4000 epoch: 941 loss: 16.661239514020764 loss_input: 82.20735470124168
step: 5000 epoch: 941 loss: 16.65601340143043 loss_input: 82.12198767521886
step: 6000 epoch: 941 loss: 16.687132389142977 loss_input: 82.3260503640832
step: 7000 epoch: 941 loss: 16.71058940451548 loss_input: 82.5181120453077
step: 8000 epoch: 941 loss: 16.703711877180893 loss_input: 82.52889583260101
step: 9000 epoch: 941 loss: 16.65524662824329 loss_input: 82.36169704830974
step: 10000 epoch: 941 loss: 16.65353068734703 loss_input: 82.42003094002123
step: 11000 epoch: 941 loss: 16.66050775486083 loss_input: 82.39422054826514
step: 12000 epoch: 941 loss: 16.66334314601003 loss_input: 82.24756753974115
step: 13000 epoch: 941 loss: 16.67141299370243 loss_input: 82.2273368028556
step: 14000 epoch: 941 loss: 16.665380592627507 loss_input: 82.15977748415571
step: 15000 epoch: 941 loss: 16.664715210236466 loss_input: 82.12961349858577
Save loss: 16.67538217398524 Name: 941_train_model.pth
step: 0 epoch: 942 loss: 16.039196014404297 loss_input: 93.7554931640625
step: 1000 epoch: 942 loss: 16.557394834665153 loss_input: 82.2239830482018
step: 2000 epoch: 942 loss: 16.596115159726274 loss_input: 81.81848470906148
step: 3000 epoch: 942 loss: 16.60512286653045 loss_input: 81.98895485359047
step: 4000 epoch: 942 loss: 16.63727467014205 loss_input: 81.9232295807884
step: 5000 epoch: 942 loss: 16.681642496211605 loss_input: 82.233278258899
step: 6000 epoch: 942 loss: 16.70957704354 loss_input: 82.24616416266234
step: 7000 epoch: 942 loss: 16.687689759189478 loss_input: 82.3450404214716
step: 8000 epoch: 942 loss: 16.695365375465876 loss_input: 82.26839088198692
step: 9000 epoch: 942 loss: 16.69420675399131 loss_input: 82.24964719496228
step: 10000 epoch: 942 loss: 16.691525543776645 loss_input: 82.17663218419369
step: 11000 epoch: 942 loss: 16.685107026032107 loss_input: 82.09719750708032
step: 12000 epoch: 942 loss: 16.678291987284116 loss_input: 82.1084751777277
step: 13000 epoch: 942 loss: 16.67510574266146 loss_input: 82.17225455782999
step: 14000 epoch: 942 loss: 16.672816357572763 loss_input: 82.20222678360585
step: 15000 epoch: 942 loss: 16.662602636259592 loss_input: 82.19223446220121
Save loss: 16.68199107697606 Name: 942_train_model.pth
step: 0 epoch: 943 loss: 18.02879524230957 loss_input: 96.46942138671875
step: 1000 epoch: 943 loss: 16.673886924595028 loss_input: 82.69022213138425
step: 2000 epoch: 943 loss: 16.697238130965037 loss_input: 82.28174975012493
step: 3000 epoch: 943 loss: 16.60125696059268 loss_input: 81.531117374124
step: 4000 epoch: 943 loss: 16.62824177408302 loss_input: 81.72943278462611
step: 5000 epoch: 943 loss: 16.672430353912205 loss_input: 81.91275332579968
step: 6000 epoch: 943 loss: 16.65588769422454 loss_input: 81.98040860987687
step: 7000 epoch: 943 loss: 16.677367372523715 loss_input: 82.0469589375067
step: 8000 epoch: 943 loss: 16.669821274726036 loss_input: 82.10507918077623
step: 9000 epoch: 943 loss: 16.65555826299337 loss_input: 81.93931224438604
step: 10000 epoch: 943 loss: 16.660722831048652 loss_input: 81.94685406947586
step: 11000 epoch: 943 loss: 16.625680616081613 loss_input: 81.88408432929215
step: 12000 epoch: 943 loss: 16.62178268530758 loss_input: 81.95602233647685
step: 13000 epoch: 943 loss: 16.628603774945045 loss_input: 82.08630330250287
step: 14000 epoch: 943 loss: 16.647440091515172 loss_input: 82.14012532778769
step: 15000 epoch: 943 loss: 16.65868862197682 loss_input: 82.12138809072884
Save loss: 16.675020217046143 Name: 943_train_model.pth
step: 0 epoch: 944 loss: 10.777908325195312 loss_input: 71.158447265625
step: 1000 epoch: 944 loss: 16.687177520412785 loss_input: 81.3085947865611
step: 2000 epoch: 944 loss: 16.7156400263518 loss_input: 81.82752486671703
step: 3000 epoch: 944 loss: 16.701115961275033 loss_input: 81.7188214890284
step: 4000 epoch: 944 loss: 16.68772738839769 loss_input: 81.82323314129964
step: 5000 epoch: 944 loss: 16.717540727331983 loss_input: 81.85905091930833
step: 6000 epoch: 944 loss: 16.667721376163207 loss_input: 81.92593061882265
step: 7000 epoch: 944 loss: 16.651044248393767 loss_input: 81.93825047698265
step: 8000 epoch: 944 loss: 16.649725043048054 loss_input: 82.03763589762461
step: 9000 epoch: 944 loss: 16.638429461737605 loss_input: 82.0155782726073
step: 10000 epoch: 944 loss: 16.675531637810455 loss_input: 82.07735783428492
step: 11000 epoch: 944 loss: 16.663919587751245 loss_input: 82.084971639874
step: 12000 epoch: 944 loss: 16.671870236694392 loss_input: 82.19382310181834
step: 13000 epoch: 944 loss: 16.678534206138483 loss_input: 82.1912373364609
step: 14000 epoch: 944 loss: 16.671102030091127 loss_input: 82.18374148812194
step: 15000 epoch: 944 loss: 16.674165109468852 loss_input: 82.21997229400202
Save loss: 16.67449574147165 Name: 944_train_model.pth
step: 0 epoch: 945 loss: 18.655292510986328 loss_input: 86.709228515625
step: 1000 epoch: 945 loss: 16.408125761624696 loss_input: 82.02767483980863
step: 2000 epoch: 945 loss: 16.522149262101813 loss_input: 82.35888854888962
step: 3000 epoch: 945 loss: 16.5700544282938 loss_input: 82.00938422137283
step: 4000 epoch: 945 loss: 16.600923081214237 loss_input: 81.73522576997352
step: 5000 epoch: 945 loss: 16.639117202146654 loss_input: 82.03649277468725
step: 6000 epoch: 945 loss: 16.65836953508955 loss_input: 81.97300406190439
step: 7000 epoch: 945 loss: 16.683150165678008 loss_input: 82.03207197381401
step: 8000 epoch: 945 loss: 16.687501331669527 loss_input: 82.08577831732215
step: 9000 epoch: 945 loss: 16.676518248923685 loss_input: 82.17470750320277
step: 10000 epoch: 945 loss: 16.675304802593356 loss_input: 82.14891218397692
step: 11000 epoch: 945 loss: 16.668008443323355 loss_input: 82.23101969201308
step: 12000 epoch: 945 loss: 16.67973782594835 loss_input: 82.22227802122447
step: 13000 epoch: 945 loss: 16.66635021012835 loss_input: 82.12957072619264
step: 14000 epoch: 945 loss: 16.675086535893886 loss_input: 82.1781245100099
step: 15000 epoch: 945 loss: 16.66420192132353 loss_input: 82.19925795230188
Save loss: 16.668342882722616 Name: 945_train_model.pth
step: 0 epoch: 946 loss: 12.149731636047363 loss_input: 50.52459716796875
step: 1000 epoch: 946 loss: 16.72702645588588 loss_input: 81.56814364834385
step: 2000 epoch: 946 loss: 16.656729816615968 loss_input: 81.29476219531836
step: 3000 epoch: 946 loss: 16.603110408115608 loss_input: 81.62191702293897
step: 4000 epoch: 946 loss: 16.64950484998284 loss_input: 82.04479001099334
step: 5000 epoch: 946 loss: 16.66374541816414 loss_input: 82.0401740086553
step: 6000 epoch: 946 loss: 16.645492566345016 loss_input: 82.01423404733652
step: 7000 epoch: 946 loss: 16.65065579236601 loss_input: 82.12373704914364
step: 8000 epoch: 946 loss: 16.63051522324911 loss_input: 82.1290502520803
step: 9000 epoch: 946 loss: 16.646751313643406 loss_input: 82.24142916818921
step: 10000 epoch: 946 loss: 16.68097958728297 loss_input: 82.26510145616298
step: 11000 epoch: 946 loss: 16.690123025574106 loss_input: 82.37698320150051
step: 12000 epoch: 946 loss: 16.676603325902697 loss_input: 82.33557139586512
step: 13000 epoch: 946 loss: 16.672487590195775 loss_input: 82.26243834608876
step: 14000 epoch: 946 loss: 16.67124869959515 loss_input: 82.22458675437106
step: 15000 epoch: 946 loss: 16.667596679553615 loss_input: 82.16302765281142
Save loss: 16.674729577556253 Name: 946_train_model.pth
step: 0 epoch: 947 loss: 24.016407012939453 loss_input: 71.92596435546875
step: 1000 epoch: 947 loss: 16.34492649589028 loss_input: 80.94838065986748
step: 2000 epoch: 947 loss: 16.522050817033044 loss_input: 81.5120977414125
step: 3000 epoch: 947 loss: 16.529087417644487 loss_input: 81.56955841786461
step: 4000 epoch: 947 loss: 16.53773666518654 loss_input: 81.93669995281941
step: 5000 epoch: 947 loss: 16.59343072429368 loss_input: 82.15882374062296
step: 6000 epoch: 947 loss: 16.62612323080812 loss_input: 82.36883476777308
step: 7000 epoch: 947 loss: 16.62193716878364 loss_input: 82.34213776868371
step: 8000 epoch: 947 loss: 16.615697481590217 loss_input: 82.31967772979793
step: 9000 epoch: 947 loss: 16.645459966174286 loss_input: 82.28580378140494
step: 10000 epoch: 947 loss: 16.624290419511706 loss_input: 82.26677061908067
step: 11000 epoch: 947 loss: 16.639887688322702 loss_input: 82.34617893401736
step: 12000 epoch: 947 loss: 16.650171163350123 loss_input: 82.32866062024128
step: 13000 epoch: 947 loss: 16.669510666475546 loss_input: 82.30577687496094
step: 14000 epoch: 947 loss: 16.66417641253499 loss_input: 82.2473487567922
step: 15000 epoch: 947 loss: 16.67662039615895 loss_input: 82.2841766440831
Save loss: 16.66811836604774 Name: 947_train_model.pth
step: 0 epoch: 948 loss: 8.924901962280273 loss_input: 46.986572265625
step: 1000 epoch: 948 loss: 16.438448518187133 loss_input: 82.10177889737216
step: 2000 epoch: 948 loss: 16.576945100886295 loss_input: 82.62823434474168
step: 3000 epoch: 948 loss: 16.605300867728335 loss_input: 82.30592915154584
step: 4000 epoch: 948 loss: 16.619359618751623 loss_input: 82.09750034415224
step: 5000 epoch: 948 loss: 16.619966490367393 loss_input: 82.33847261309457
step: 6000 epoch: 948 loss: 16.664484858095715 loss_input: 82.24006039276657
step: 7000 epoch: 948 loss: 16.673856127996 loss_input: 82.29174793354018
step: 8000 epoch: 948 loss: 16.654775155274486 loss_input: 82.26607546706808
step: 9000 epoch: 948 loss: 16.638264570166278 loss_input: 82.19953538555394
step: 10000 epoch: 948 loss: 16.657428795070054 loss_input: 82.26767716905067
step: 11000 epoch: 948 loss: 16.67902432655315 loss_input: 82.45683917317886
step: 12000 epoch: 948 loss: 16.663325197129893 loss_input: 82.35235866269771
step: 13000 epoch: 948 loss: 16.66999188906119 loss_input: 82.31345694824857
step: 14000 epoch: 948 loss: 16.681877017557582 loss_input: 82.21611211817738
step: 15000 epoch: 948 loss: 16.671236470368502 loss_input: 82.24851629764396
Save loss: 16.672502597510814 Name: 948_train_model.pth
step: 0 epoch: 949 loss: 17.669160842895508 loss_input: 101.70623779296875
step: 1000 epoch: 949 loss: 16.43045845327082 loss_input: 81.74502761642654
step: 2000 epoch: 949 loss: 16.611339641415675 loss_input: 81.86764418083926
step: 3000 epoch: 949 loss: 16.560021039367875 loss_input: 81.51281933528668
step: 4000 epoch: 949 loss: 16.58735853277186 loss_input: 81.67072236457487
step: 5000 epoch: 949 loss: 16.606800400955727 loss_input: 81.68032756973996
step: 6000 epoch: 949 loss: 16.60678469985748 loss_input: 81.6442266844686
step: 7000 epoch: 949 loss: 16.597277427022753 loss_input: 81.58578312054752
step: 8000 epoch: 949 loss: 16.617967269343445 loss_input: 81.79712262682253
step: 9000 epoch: 949 loss: 16.653393281988137 loss_input: 81.88560582976675
step: 10000 epoch: 949 loss: 16.636946192408498 loss_input: 81.97530087689474
step: 11000 epoch: 949 loss: 16.658114641170677 loss_input: 82.09981799084494
step: 12000 epoch: 949 loss: 16.653500849143157 loss_input: 82.16453930759677
step: 13000 epoch: 949 loss: 16.664997923127522 loss_input: 82.15936691956688
step: 14000 epoch: 949 loss: 16.66527768283493 loss_input: 82.24964773895894
step: 15000 epoch: 949 loss: 16.67213612049073 loss_input: 82.18919662354541
Save loss: 16.670495624899864 Name: 949_train_model.pth
step: 0 epoch: 950 loss: 19.69708824157715 loss_input: 87.89617919921875
step: 1000 epoch: 950 loss: 16.54605134550508 loss_input: 83.53747961023352
step: 2000 epoch: 950 loss: 16.581284193680442 loss_input: 82.93128536439788
step: 3000 epoch: 950 loss: 16.602927252913428 loss_input: 82.93275508098863
step: 4000 epoch: 950 loss: 16.63792406151039 loss_input: 82.505096046545
step: 5000 epoch: 950 loss: 16.65592418697161 loss_input: 82.43289126598508
step: 6000 epoch: 950 loss: 16.63883446959769 loss_input: 82.25156860656688
step: 7000 epoch: 950 loss: 16.59864395513754 loss_input: 82.26322907588258
step: 8000 epoch: 950 loss: 16.611213268540588 loss_input: 82.17609871519966
step: 9000 epoch: 950 loss: 16.594735567867193 loss_input: 82.1115589004108
step: 10000 epoch: 950 loss: 16.60781603235684 loss_input: 82.11486063307731
step: 11000 epoch: 950 loss: 16.626384450569876 loss_input: 82.19589141021804
step: 12000 epoch: 950 loss: 16.656394535914508 loss_input: 82.21185447680793
step: 13000 epoch: 950 loss: 16.6660385987326 loss_input: 82.29134534441611
step: 14000 epoch: 950 loss: 16.67094964770264 loss_input: 82.32446970427755
step: 15000 epoch: 950 loss: 16.670823950567577 loss_input: 82.26468681384783
Save loss: 16.667864390805363 Name: 950_train_model.pth
step: 0 epoch: 951 loss: 17.894350051879883 loss_input: 101.23944091796875
step: 1000 epoch: 951 loss: 16.468495071708382 loss_input: 82.6922355598503
step: 2000 epoch: 951 loss: 16.556517662732738 loss_input: 82.86164980254014
step: 3000 epoch: 951 loss: 16.59792798330529 loss_input: 82.65115593386189
step: 4000 epoch: 951 loss: 16.60640729400284 loss_input: 82.55425090260638
step: 5000 epoch: 951 loss: 16.642660589509905 loss_input: 82.67082047457696
step: 6000 epoch: 951 loss: 16.670435482493005 loss_input: 82.68908735140823
step: 7000 epoch: 951 loss: 16.69060032893174 loss_input: 82.55008944557865
step: 8000 epoch: 951 loss: 16.691445443439804 loss_input: 82.47096995260846
step: 9000 epoch: 951 loss: 16.6760783709363 loss_input: 82.40384958447542
step: 10000 epoch: 951 loss: 16.68917929354983 loss_input: 82.44094700637358
step: 11000 epoch: 951 loss: 16.674795405191354 loss_input: 82.21075034438454
step: 12000 epoch: 951 loss: 16.699488242619555 loss_input: 82.25318233673399
step: 13000 epoch: 951 loss: 16.685412850455865 loss_input: 82.2492439358631
step: 14000 epoch: 951 loss: 16.675389283419115 loss_input: 82.21216366974066
step: 15000 epoch: 951 loss: 16.662278374372566 loss_input: 82.25743708964325
Save loss: 16.6699343983233 Name: 951_train_model.pth
step: 0 epoch: 952 loss: 17.02935218811035 loss_input: 74.32037353515625
step: 1000 epoch: 952 loss: 16.57460309742214 loss_input: 83.06229750474135
step: 2000 epoch: 952 loss: 16.685790424880715 loss_input: 83.08052461806206
step: 3000 epoch: 952 loss: 16.714408025071048 loss_input: 82.77261776592047
step: 4000 epoch: 952 loss: 16.67962504529679 loss_input: 82.60162881337772
step: 5000 epoch: 952 loss: 16.691515303258775 loss_input: 82.49543035728792
step: 6000 epoch: 952 loss: 16.654883461303662 loss_input: 82.25396067411616
step: 7000 epoch: 952 loss: 16.634282227499693 loss_input: 82.20798470963412
step: 8000 epoch: 952 loss: 16.663161830654175 loss_input: 82.18270158094252
step: 9000 epoch: 952 loss: 16.66450866594962 loss_input: 82.24610080894874
step: 10000 epoch: 952 loss: 16.657063553612442 loss_input: 82.3061994825908
step: 11000 epoch: 952 loss: 16.66610385877264 loss_input: 82.24689656675388
step: 12000 epoch: 952 loss: 16.656387997611287 loss_input: 82.27232607121607
step: 13000 epoch: 952 loss: 16.657742534414748 loss_input: 82.30941064272851
step: 14000 epoch: 952 loss: 16.649870722134295 loss_input: 82.2598595165217
step: 15000 epoch: 952 loss: 16.659709624851253 loss_input: 82.2470016362516
Save loss: 16.66648007452488 Name: 952_train_model.pth
step: 0 epoch: 953 loss: 14.910970687866211 loss_input: 71.14556884765625
step: 1000 epoch: 953 loss: 16.51232118682785 loss_input: 81.8615896432669
step: 2000 epoch: 953 loss: 16.675665099760224 loss_input: 81.7218841140953
step: 3000 epoch: 953 loss: 16.70474160603387 loss_input: 82.35615909198887
step: 4000 epoch: 953 loss: 16.732329718740665 loss_input: 82.59330728536128
step: 5000 epoch: 953 loss: 16.687285386760387 loss_input: 82.56540933024333
step: 6000 epoch: 953 loss: 16.700268179113678 loss_input: 82.59819836545798
step: 7000 epoch: 953 loss: 16.685267167812654 loss_input: 82.3005754514143
step: 8000 epoch: 953 loss: 16.688343352041397 loss_input: 82.15601564666477
step: 9000 epoch: 953 loss: 16.69609710892337 loss_input: 82.14643121928509
step: 10000 epoch: 953 loss: 16.701424162812906 loss_input: 82.26969757247907
step: 11000 epoch: 953 loss: 16.676117811015754 loss_input: 82.18322519219666
step: 12000 epoch: 953 loss: 16.67460118131969 loss_input: 82.15841757958158
step: 13000 epoch: 953 loss: 16.655718423927595 loss_input: 82.11236630479296
step: 14000 epoch: 953 loss: 16.671562805421676 loss_input: 82.33277566878594
step: 15000 epoch: 953 loss: 16.67345510094287 loss_input: 82.36750554113449
Save loss: 16.662925622656942 Name: 953_train_model.pth
step: 0 epoch: 954 loss: 17.505672454833984 loss_input: 152.054931640625
step: 1000 epoch: 954 loss: 16.59824110554172 loss_input: 81.7746361304711
step: 2000 epoch: 954 loss: 16.646055598666464 loss_input: 81.607466560909
step: 3000 epoch: 954 loss: 16.63786462107248 loss_input: 81.5158853502283
step: 4000 epoch: 954 loss: 16.659475813147964 loss_input: 81.97405265993072
step: 5000 epoch: 954 loss: 16.645713594240608 loss_input: 82.17611293415145
step: 6000 epoch: 954 loss: 16.661351995932183 loss_input: 82.05429739778072
step: 7000 epoch: 954 loss: 16.638845452818252 loss_input: 81.95752620056788
step: 8000 epoch: 954 loss: 16.635208948420487 loss_input: 81.91880053994716
step: 9000 epoch: 954 loss: 16.64961949431516 loss_input: 81.89390586511438
step: 10000 epoch: 954 loss: 16.64626136127442 loss_input: 81.84620262339
step: 11000 epoch: 954 loss: 16.649608630547142 loss_input: 81.96353858744553
step: 12000 epoch: 954 loss: 16.641496641776353 loss_input: 81.94901923252814
step: 13000 epoch: 954 loss: 16.651514844485096 loss_input: 82.032988448004
step: 14000 epoch: 954 loss: 16.658389699364566 loss_input: 82.01737276508709
step: 15000 epoch: 954 loss: 16.658492768671454 loss_input: 82.0634161864795
Save loss: 16.683273107334973 Name: 954_train_model.pth
step: 0 epoch: 955 loss: 21.759187698364258 loss_input: 118.64593505859375
step: 1000 epoch: 955 loss: 16.448854508338037 loss_input: 81.92633093225135
step: 2000 epoch: 955 loss: 16.480427650020815 loss_input: 81.89409131493824
step: 3000 epoch: 955 loss: 16.529243490212124 loss_input: 82.03348409823678
step: 4000 epoch: 955 loss: 16.581033562219492 loss_input: 82.01913259173
step: 5000 epoch: 955 loss: 16.619608249980864 loss_input: 82.07921385693564
step: 6000 epoch: 955 loss: 16.675949082694796 loss_input: 82.28716718600285
step: 7000 epoch: 955 loss: 16.70312173903593 loss_input: 82.18450095159398
step: 8000 epoch: 955 loss: 16.687593465089172 loss_input: 82.04249574851727
step: 9000 epoch: 955 loss: 16.694239765998326 loss_input: 82.08258016633134
step: 10000 epoch: 955 loss: 16.68978432843285 loss_input: 82.16880316362895
step: 11000 epoch: 955 loss: 16.694597592452215 loss_input: 82.18682233832791
step: 12000 epoch: 955 loss: 16.69180022048092 loss_input: 82.08417779455065
step: 13000 epoch: 955 loss: 16.68472002473357 loss_input: 82.2247772932731
step: 14000 epoch: 955 loss: 16.667795421991865 loss_input: 82.20641408478495
step: 15000 epoch: 955 loss: 16.67822696653877 loss_input: 82.3113023788323
Save loss: 16.66715078625083 Name: 955_train_model.pth
step: 0 epoch: 956 loss: 13.0310697555542 loss_input: 103.39556884765625
step: 1000 epoch: 956 loss: 16.413663754096397 loss_input: 81.69788987867601
step: 2000 epoch: 956 loss: 16.67475310329912 loss_input: 82.62786602914363
step: 3000 epoch: 956 loss: 16.580186831001757 loss_input: 82.13489404085357
step: 4000 epoch: 956 loss: 16.55835568264287 loss_input: 82.18505070692775
step: 5000 epoch: 956 loss: 16.571930174397554 loss_input: 82.24978367363637
step: 6000 epoch: 956 loss: 16.54898234832845 loss_input: 82.11878966013802
step: 7000 epoch: 956 loss: 16.572254308988803 loss_input: 82.30165434653308
step: 8000 epoch: 956 loss: 16.572947110493264 loss_input: 82.34707127063218
step: 9000 epoch: 956 loss: 16.60385576124629 loss_input: 82.45279546711606
step: 10000 epoch: 956 loss: 16.624329509025646 loss_input: 82.43867025650951
step: 11000 epoch: 956 loss: 16.632057881552505 loss_input: 82.4209922566521
step: 12000 epoch: 956 loss: 16.64525567975683 loss_input: 82.40943050074603
step: 13000 epoch: 956 loss: 16.656436569901047 loss_input: 82.34150826664175
step: 14000 epoch: 956 loss: 16.65843914842684 loss_input: 82.35036815781991
step: 15000 epoch: 956 loss: 16.66141813413738 loss_input: 82.27009080742273
Save loss: 16.665967072933913 Name: 956_train_model.pth
step: 0 epoch: 957 loss: 16.951324462890625 loss_input: 78.07623291015625
step: 1000 epoch: 957 loss: 16.670341024389277 loss_input: 81.89501331188343
step: 2000 epoch: 957 loss: 16.449112409594534 loss_input: 80.96849991142125
step: 3000 epoch: 957 loss: 16.610940042633647 loss_input: 81.94349429910082
step: 4000 epoch: 957 loss: 16.577858643125396 loss_input: 81.65442291160608
step: 5000 epoch: 957 loss: 16.618168288482426 loss_input: 81.73473309488493
step: 6000 epoch: 957 loss: 16.618258531283107 loss_input: 81.6787431892048
step: 7000 epoch: 957 loss: 16.631871277631102 loss_input: 81.75721896558025
step: 8000 epoch: 957 loss: 16.624945162445467 loss_input: 81.87121593473911
step: 9000 epoch: 957 loss: 16.64188782470304 loss_input: 81.91588665170016
step: 10000 epoch: 957 loss: 16.651572029538876 loss_input: 82.06914805345637
step: 11000 epoch: 957 loss: 16.639245495819175 loss_input: 82.00786126265687
step: 12000 epoch: 957 loss: 16.637210154849264 loss_input: 81.95929041028808
step: 13000 epoch: 957 loss: 16.672061782315662 loss_input: 82.01149269786488
step: 14000 epoch: 957 loss: 16.676293469950025 loss_input: 82.10803646384355
step: 15000 epoch: 957 loss: 16.678546223765366 loss_input: 82.209174714687
Save loss: 16.68309279420972 Name: 957_train_model.pth
step: 0 epoch: 958 loss: 13.391302108764648 loss_input: 52.90118408203125
step: 1000 epoch: 958 loss: 16.69131086399029 loss_input: 82.88896040482955
step: 2000 epoch: 958 loss: 16.756463481449355 loss_input: 82.68263463483102
step: 3000 epoch: 958 loss: 16.68933057197131 loss_input: 82.22458732831244
step: 4000 epoch: 958 loss: 16.655068966842418 loss_input: 82.31745837039276
step: 5000 epoch: 958 loss: 16.688808424094752 loss_input: 82.24016346633017
step: 6000 epoch: 958 loss: 16.6982877912809 loss_input: 82.23374660538347
step: 7000 epoch: 958 loss: 16.68790867914048 loss_input: 82.21547220897851
step: 8000 epoch: 958 loss: 16.686068583899566 loss_input: 82.14232025318124
step: 9000 epoch: 958 loss: 16.664369684764377 loss_input: 82.04748203568744
step: 10000 epoch: 958 loss: 16.65335855847799 loss_input: 82.13251802871471
step: 11000 epoch: 958 loss: 16.638280434062747 loss_input: 82.16341250138481
step: 12000 epoch: 958 loss: 16.641455715273374 loss_input: 82.20177965312786
step: 13000 epoch: 958 loss: 16.646206044901355 loss_input: 82.1929134066385
step: 14000 epoch: 958 loss: 16.668539318014286 loss_input: 82.24849636681923
step: 15000 epoch: 958 loss: 16.672366056686386 loss_input: 82.26014789118226
Save loss: 16.665092988744377 Name: 958_train_model.pth
step: 0 epoch: 959 loss: 14.404350280761719 loss_input: 71.8656005859375
step: 1000 epoch: 959 loss: 16.70153818120966 loss_input: 83.59143200549451
step: 2000 epoch: 959 loss: 16.581120397852754 loss_input: 82.05192249456327
step: 3000 epoch: 959 loss: 16.64914326443747 loss_input: 82.38457271648383
step: 4000 epoch: 959 loss: 16.652954696506537 loss_input: 82.46278927094399
step: 5000 epoch: 959 loss: 16.62914311687986 loss_input: 82.32571113951037
step: 6000 epoch: 959 loss: 16.621888682357948 loss_input: 82.24087854457525
step: 7000 epoch: 959 loss: 16.638333726552464 loss_input: 82.24055740537345
step: 8000 epoch: 959 loss: 16.658350657618147 loss_input: 82.33210858713461
step: 9000 epoch: 959 loss: 16.665363706068096 loss_input: 82.26480585002274
step: 10000 epoch: 959 loss: 16.66075525187502 loss_input: 82.28661947648008
step: 11000 epoch: 959 loss: 16.66222083131873 loss_input: 82.2382186835554
step: 12000 epoch: 959 loss: 16.64786299637244 loss_input: 82.15724715506292
step: 13000 epoch: 959 loss: 16.66505610023166 loss_input: 82.2115573065894
step: 14000 epoch: 959 loss: 16.666204854800235 loss_input: 82.2853405978814
step: 15000 epoch: 959 loss: 16.678582060981547 loss_input: 82.30352757570068
Save loss: 16.668806494385002 Name: 959_train_model.pth
step: 0 epoch: 960 loss: 18.617712020874023 loss_input: 91.720703125
step: 1000 epoch: 960 loss: 16.43197060345889 loss_input: 82.67005431092345
step: 2000 epoch: 960 loss: 16.568620059086285 loss_input: 82.22225029453047
step: 3000 epoch: 960 loss: 16.575930028309706 loss_input: 82.24309304125187
step: 4000 epoch: 960 loss: 16.559292324362204 loss_input: 81.99803883562801
step: 5000 epoch: 960 loss: 16.55186011228197 loss_input: 81.82740518546682
step: 6000 epoch: 960 loss: 16.580042290898128 loss_input: 82.09984665031413
step: 7000 epoch: 960 loss: 16.60900419869605 loss_input: 82.39806030430363
step: 8000 epoch: 960 loss: 16.60435138009754 loss_input: 82.26202955557069
step: 9000 epoch: 960 loss: 16.642227215470772 loss_input: 82.39260132256355
step: 10000 epoch: 960 loss: 16.65341674896517 loss_input: 82.41158697240627
step: 11000 epoch: 960 loss: 16.64458333727512 loss_input: 82.3526856001372
step: 12000 epoch: 960 loss: 16.66025546604112 loss_input: 82.37121233651264
step: 13000 epoch: 960 loss: 16.660199145483663 loss_input: 82.28458831930297
step: 14000 epoch: 960 loss: 16.67204828722444 loss_input: 82.30880945899233
step: 15000 epoch: 960 loss: 16.66241667029238 loss_input: 82.23746179779421
Save loss: 16.664602737665177 Name: 960_train_model.pth
step: 0 epoch: 961 loss: 9.294626235961914 loss_input: 59.11114501953125
step: 1000 epoch: 961 loss: 16.818380723585495 loss_input: 82.75466336641874
step: 2000 epoch: 961 loss: 16.666562668744593 loss_input: 82.60473092921313
step: 3000 epoch: 961 loss: 16.662348369406445 loss_input: 82.47110399235412
step: 4000 epoch: 961 loss: 16.621596990540993 loss_input: 82.0660532651261
step: 5000 epoch: 961 loss: 16.64501911638928 loss_input: 82.17740708450107
step: 6000 epoch: 961 loss: 16.67956846011835 loss_input: 82.41171596726066
step: 7000 epoch: 961 loss: 16.64205446939369 loss_input: 82.29647260910272
step: 8000 epoch: 961 loss: 16.670984660457215 loss_input: 82.28962837012317
step: 9000 epoch: 961 loss: 16.63245754540728 loss_input: 82.16412057188958
step: 10000 epoch: 961 loss: 16.63204922278921 loss_input: 82.09323770992042
step: 11000 epoch: 961 loss: 16.62087765033435 loss_input: 82.076205158329
step: 12000 epoch: 961 loss: 16.62315790206827 loss_input: 82.04255423376176
step: 13000 epoch: 961 loss: 16.62516108641908 loss_input: 82.14063066644363
step: 14000 epoch: 961 loss: 16.627849956144903 loss_input: 82.0836702276916
step: 15000 epoch: 961 loss: 16.654471886252555 loss_input: 82.20963684208162
Save loss: 16.663311119854452 Name: 961_train_model.pth
step: 0 epoch: 962 loss: 18.87594223022461 loss_input: 75.37176513671875
step: 1000 epoch: 962 loss: 16.419610477470375 loss_input: 81.01564518245426
step: 2000 epoch: 962 loss: 16.46730680908935 loss_input: 81.03355814158351
step: 3000 epoch: 962 loss: 16.61428888064152 loss_input: 82.03631473834893
step: 4000 epoch: 962 loss: 16.581037202199617 loss_input: 82.08586394050901
step: 5000 epoch: 962 loss: 16.573220432531688 loss_input: 81.94432865135957
step: 6000 epoch: 962 loss: 16.573721220286323 loss_input: 81.99041787713115
step: 7000 epoch: 962 loss: 16.611325994932113 loss_input: 82.20334080692156
step: 8000 epoch: 962 loss: 16.617698138899364 loss_input: 82.16437725710311
step: 9000 epoch: 962 loss: 16.620488567307795 loss_input: 82.03913230968044
step: 10000 epoch: 962 loss: 16.624065587501764 loss_input: 82.01914594917461
step: 11000 epoch: 962 loss: 16.635500765726704 loss_input: 82.05469111184317
step: 12000 epoch: 962 loss: 16.637979298569284 loss_input: 82.1187015543301
step: 13000 epoch: 962 loss: 16.62513935539358 loss_input: 82.09376091506333
step: 14000 epoch: 962 loss: 16.655970309616947 loss_input: 82.13484871842999
step: 15000 epoch: 962 loss: 16.64980448888831 loss_input: 82.15993366962385
Save loss: 16.6680299782753 Name: 962_train_model.pth
step: 0 epoch: 963 loss: 19.890304565429688 loss_input: 106.827880859375
step: 1000 epoch: 963 loss: 16.440485829239957 loss_input: 81.13030133928571
step: 2000 epoch: 963 loss: 16.650260672576422 loss_input: 82.41971604529766
step: 3000 epoch: 963 loss: 16.643220714710825 loss_input: 82.65283083129191
step: 4000 epoch: 963 loss: 16.691285527667173 loss_input: 82.58999683367732
step: 5000 epoch: 963 loss: 16.706730832149688 loss_input: 82.57943098880224
step: 6000 epoch: 963 loss: 16.712209657000972 loss_input: 82.61627446450981
step: 7000 epoch: 963 loss: 16.68390807133541 loss_input: 82.29555713406734
step: 8000 epoch: 963 loss: 16.673325862158627 loss_input: 82.2597001119489
step: 9000 epoch: 963 loss: 16.667554134237093 loss_input: 82.11261117878179
step: 10000 epoch: 963 loss: 16.660485376680914 loss_input: 82.19355776214371
step: 11000 epoch: 963 loss: 16.65854895820337 loss_input: 82.15721264774935
step: 12000 epoch: 963 loss: 16.65581850881984 loss_input: 82.20261306451188
step: 13000 epoch: 963 loss: 16.642729636715263 loss_input: 82.18065078602727
step: 14000 epoch: 963 loss: 16.66153862806535 loss_input: 82.13199363671987
step: 15000 epoch: 963 loss: 16.667847984354143 loss_input: 82.24094457749041
Save loss: 16.668190726667643 Name: 963_train_model.pth
step: 0 epoch: 964 loss: 17.416305541992188 loss_input: 96.4163818359375
step: 1000 epoch: 964 loss: 16.370890819823945 loss_input: 82.72428980931178
step: 2000 epoch: 964 loss: 16.47615993100366 loss_input: 82.6783361249063
step: 3000 epoch: 964 loss: 16.45655846031695 loss_input: 82.47594378940744
step: 4000 epoch: 964 loss: 16.474860483811696 loss_input: 82.49510789722004
step: 5000 epoch: 964 loss: 16.50589013037694 loss_input: 82.08810909937154
step: 6000 epoch: 964 loss: 16.55670789980209 loss_input: 82.16367006866044
step: 7000 epoch: 964 loss: 16.582103554546247 loss_input: 82.27100618926649
step: 8000 epoch: 964 loss: 16.61850908219464 loss_input: 82.41587472006672
step: 9000 epoch: 964 loss: 16.612649010467656 loss_input: 82.41464291375394
step: 10000 epoch: 964 loss: 16.612713117883654 loss_input: 82.29626944532109
step: 11000 epoch: 964 loss: 16.61696443498357 loss_input: 82.24335090187114
step: 12000 epoch: 964 loss: 16.646967484050865 loss_input: 82.32832282388739
step: 13000 epoch: 964 loss: 16.64530036664176 loss_input: 82.29968959813338
step: 14000 epoch: 964 loss: 16.64519379936059 loss_input: 82.23460299797922
step: 15000 epoch: 964 loss: 16.65258747518448 loss_input: 82.21284795206361
Save loss: 16.662359937652944 Name: 964_train_model.pth
step: 0 epoch: 965 loss: 17.099349975585938 loss_input: 81.844482421875
step: 1000 epoch: 965 loss: 16.914127267681277 loss_input: 83.69740245106456
step: 2000 epoch: 965 loss: 16.79844928955448 loss_input: 83.69670602930957
step: 3000 epoch: 965 loss: 16.70675537793567 loss_input: 83.4582860197317
step: 4000 epoch: 965 loss: 16.686441283081805 loss_input: 83.21635509091477
step: 5000 epoch: 965 loss: 16.657607607497283 loss_input: 83.03750994185928
step: 6000 epoch: 965 loss: 16.69927849040153 loss_input: 83.03915854315622
step: 7000 epoch: 965 loss: 16.70090109555147 loss_input: 82.81732683382006
step: 8000 epoch: 965 loss: 16.70146471735031 loss_input: 82.65332418774801
step: 9000 epoch: 965 loss: 16.700130845583647 loss_input: 82.49163262323067
step: 10000 epoch: 965 loss: 16.686736320593443 loss_input: 82.42121400652904
step: 11000 epoch: 965 loss: 16.67629119387844 loss_input: 82.35569458174257
step: 12000 epoch: 965 loss: 16.695595770813547 loss_input: 82.33455822040871
step: 13000 epoch: 965 loss: 16.697582189765914 loss_input: 82.34412649222295
step: 14000 epoch: 965 loss: 16.669201071675644 loss_input: 82.29220116130796
step: 15000 epoch: 965 loss: 16.675827183093112 loss_input: 82.31822371509868
Save loss: 16.667242712840437 Name: 965_train_model.pth
step: 0 epoch: 966 loss: 15.650985717773438 loss_input: 82.30755615234375
step: 1000 epoch: 966 loss: 16.635403543085484 loss_input: 81.91252381651552
step: 2000 epoch: 966 loss: 16.632395193018002 loss_input: 81.9953133479647
step: 3000 epoch: 966 loss: 16.597608441632815 loss_input: 81.89469641711227
step: 4000 epoch: 966 loss: 16.635020406744953 loss_input: 81.8328614562668
step: 5000 epoch: 966 loss: 16.626916823923956 loss_input: 81.9597469666223
step: 6000 epoch: 966 loss: 16.629109642184233 loss_input: 82.0696929641772
step: 7000 epoch: 966 loss: 16.63623318476705 loss_input: 82.08042023049441
step: 8000 epoch: 966 loss: 16.647491626956437 loss_input: 82.08029815212308
step: 9000 epoch: 966 loss: 16.651498822791883 loss_input: 82.09300355516054
step: 10000 epoch: 966 loss: 16.654575348472537 loss_input: 82.15761479882285
step: 11000 epoch: 966 loss: 16.646567488613915 loss_input: 82.14560974858998
step: 12000 epoch: 966 loss: 16.649232670045198 loss_input: 82.21366902794722
step: 13000 epoch: 966 loss: 16.66740886688819 loss_input: 82.19391350811807
step: 14000 epoch: 966 loss: 16.668683761614115 loss_input: 82.25597576944838
step: 15000 epoch: 966 loss: 16.676111818687033 loss_input: 82.32036558483992
Save loss: 16.667471631854774 Name: 966_train_model.pth
step: 0 epoch: 967 loss: 11.602716445922852 loss_input: 62.97674560546875
step: 1000 epoch: 967 loss: 16.712844738593468 loss_input: 82.72601867127014
step: 2000 epoch: 967 loss: 16.662569790467924 loss_input: 82.82677658363201
step: 3000 epoch: 967 loss: 16.604667661985292 loss_input: 82.51817655833472
step: 4000 epoch: 967 loss: 16.6530041811437 loss_input: 82.40127024213184
step: 5000 epoch: 967 loss: 16.67759043239303 loss_input: 82.5104924815818
step: 6000 epoch: 967 loss: 16.7261681627818 loss_input: 82.6513125904797
step: 7000 epoch: 967 loss: 16.704831295227294 loss_input: 82.57243621133085
step: 8000 epoch: 967 loss: 16.692699225183397 loss_input: 82.47665114248414
step: 9000 epoch: 967 loss: 16.680857706356015 loss_input: 82.52373799693915
step: 10000 epoch: 967 loss: 16.681383676593775 loss_input: 82.44781052290278
step: 11000 epoch: 967 loss: 16.667655110630097 loss_input: 82.31195085003813
step: 12000 epoch: 967 loss: 16.68087123477571 loss_input: 82.36027382544782
step: 13000 epoch: 967 loss: 16.68404762596912 loss_input: 82.3936640916444
step: 14000 epoch: 967 loss: 16.6825541236317 loss_input: 82.33122095718748
step: 15000 epoch: 967 loss: 16.6806238129587 loss_input: 82.35189076180728
Save loss: 16.66537010638416 Name: 967_train_model.pth
step: 0 epoch: 968 loss: 21.312179565429688 loss_input: 72.4449462890625
step: 1000 epoch: 968 loss: 16.66096449851037 loss_input: 82.69164026891077
step: 2000 epoch: 968 loss: 16.676225242109552 loss_input: 83.50550649357938
step: 3000 epoch: 968 loss: 16.605427447178887 loss_input: 83.03362793050103
step: 4000 epoch: 968 loss: 16.663397290056512 loss_input: 82.63555204948763
step: 5000 epoch: 968 loss: 16.660659693308148 loss_input: 82.72267840133145
step: 6000 epoch: 968 loss: 16.649572995598565 loss_input: 82.67051762124932
step: 7000 epoch: 968 loss: 16.645465674119034 loss_input: 82.69822379238792
step: 8000 epoch: 968 loss: 16.628701528181598 loss_input: 82.47475780920452
step: 9000 epoch: 968 loss: 16.627522387857926 loss_input: 82.35658854609687
step: 10000 epoch: 968 loss: 16.634502407050515 loss_input: 82.43971823520774
step: 11000 epoch: 968 loss: 16.64472760316318 loss_input: 82.28849479703727
step: 12000 epoch: 968 loss: 16.64265278553667 loss_input: 82.28285307177364
step: 13000 epoch: 968 loss: 16.663152143722517 loss_input: 82.39693768110085
step: 14000 epoch: 968 loss: 16.673648023135357 loss_input: 82.3613234125506
step: 15000 epoch: 968 loss: 16.669703294496426 loss_input: 82.3243257252902
Save loss: 16.673269755795598 Name: 968_train_model.pth
step: 0 epoch: 969 loss: 16.871171951293945 loss_input: 69.49591064453125
step: 1000 epoch: 969 loss: 16.753100270157926 loss_input: 84.83279336630167
step: 2000 epoch: 969 loss: 16.669812059831404 loss_input: 83.12664206226965
step: 3000 epoch: 969 loss: 16.738392684587595 loss_input: 82.93835554977768
step: 4000 epoch: 969 loss: 16.640351462739613 loss_input: 82.57428758044864
step: 5000 epoch: 969 loss: 16.62276324265672 loss_input: 82.53132953731519
step: 6000 epoch: 969 loss: 16.64315344873736 loss_input: 82.39319676578968
step: 7000 epoch: 969 loss: 16.643194011953316 loss_input: 82.44310638554147
step: 8000 epoch: 969 loss: 16.644772619444822 loss_input: 82.36365072591023
step: 9000 epoch: 969 loss: 16.64130248677504 loss_input: 82.18013047107179
step: 10000 epoch: 969 loss: 16.644276587513254 loss_input: 82.18153229843031
step: 11000 epoch: 969 loss: 16.63189157069071 loss_input: 82.06332665158992
step: 12000 epoch: 969 loss: 16.63203405459795 loss_input: 82.11416471621342
step: 13000 epoch: 969 loss: 16.632291810456245 loss_input: 82.11535274123294
step: 14000 epoch: 969 loss: 16.64491371139187 loss_input: 82.16283857532079
step: 15000 epoch: 969 loss: 16.650265241573592 loss_input: 82.16280557289019
Save loss: 16.657214189454912 Name: 969_train_model.pth
step: 0 epoch: 970 loss: 21.41183090209961 loss_input: 97.8343505859375
step: 1000 epoch: 970 loss: 16.53334312362747 loss_input: 82.36925245164991
step: 2000 epoch: 970 loss: 16.470445660100705 loss_input: 82.28374695098739
step: 3000 epoch: 970 loss: 16.508647163325016 loss_input: 82.0031603641885
step: 4000 epoch: 970 loss: 16.57827869769008 loss_input: 82.0521756920389
step: 5000 epoch: 970 loss: 16.630755874353085 loss_input: 82.22274376911727
step: 6000 epoch: 970 loss: 16.657689693986804 loss_input: 82.24953673899402
step: 7000 epoch: 970 loss: 16.63394131283814 loss_input: 82.00031615182206
step: 8000 epoch: 970 loss: 16.660396290337975 loss_input: 82.10571133442303
step: 9000 epoch: 970 loss: 16.688185301506177 loss_input: 82.19570758413995
step: 10000 epoch: 970 loss: 16.666585162477176 loss_input: 82.1606898392192
step: 11000 epoch: 970 loss: 16.657821484797978 loss_input: 82.19769481294492
step: 12000 epoch: 970 loss: 16.65875470098898 loss_input: 82.20981283277663
step: 13000 epoch: 970 loss: 16.658341616504458 loss_input: 82.20193437080862
step: 14000 epoch: 970 loss: 16.66485843637332 loss_input: 82.25536325565194
step: 15000 epoch: 970 loss: 16.653823935089967 loss_input: 82.1593194893778
Save loss: 16.660504184514284 Name: 970_train_model.pth
step: 0 epoch: 971 loss: 15.713359832763672 loss_input: 72.91180419921875
step: 1000 epoch: 971 loss: 16.617406073864643 loss_input: 81.32148070435424
step: 2000 epoch: 971 loss: 16.66624332272607 loss_input: 81.52797124387025
step: 3000 epoch: 971 loss: 16.633916463028864 loss_input: 81.41876985422176
step: 4000 epoch: 971 loss: 16.589637703074423 loss_input: 81.47539334343661
step: 5000 epoch: 971 loss: 16.582359849441243 loss_input: 81.5833555945061
step: 6000 epoch: 971 loss: 16.576350432677064 loss_input: 81.56755890009542
step: 7000 epoch: 971 loss: 16.57265088776489 loss_input: 81.43638791521691
step: 8000 epoch: 971 loss: 16.597222393385366 loss_input: 81.58670917020382
step: 9000 epoch: 971 loss: 16.640594948187044 loss_input: 81.81488275798132
step: 10000 epoch: 971 loss: 16.642761505242525 loss_input: 81.88729610315336
step: 11000 epoch: 971 loss: 16.64856739328532 loss_input: 81.867527201895
step: 12000 epoch: 971 loss: 16.639849460022656 loss_input: 81.93666768715725
step: 13000 epoch: 971 loss: 16.635628893433896 loss_input: 82.02999117630975
step: 14000 epoch: 971 loss: 16.6410562431682 loss_input: 82.0317500601938
step: 15000 epoch: 971 loss: 16.65256609512038 loss_input: 82.11461650033878
Save loss: 16.6649542376101 Name: 971_train_model.pth
step: 0 epoch: 972 loss: 12.38992691040039 loss_input: 59.3033447265625
step: 1000 epoch: 972 loss: 16.41299716099635 loss_input: 82.68843180745036
step: 2000 epoch: 972 loss: 16.372837519419306 loss_input: 82.10533054395654
step: 3000 epoch: 972 loss: 16.45096448420366 loss_input: 82.05364231616805
step: 4000 epoch: 972 loss: 16.51678296775408 loss_input: 82.05159752847433
step: 5000 epoch: 972 loss: 16.500898729155384 loss_input: 81.98739419596549
step: 6000 epoch: 972 loss: 16.531258477546476 loss_input: 81.86136139378789
step: 7000 epoch: 972 loss: 16.596918940765484 loss_input: 82.01343781245536
step: 8000 epoch: 972 loss: 16.60070576153462 loss_input: 82.05802564474362
step: 9000 epoch: 972 loss: 16.61955311864949 loss_input: 82.11613819845755
step: 10000 epoch: 972 loss: 16.64932276608765 loss_input: 82.144826770985
step: 11000 epoch: 972 loss: 16.628077503768175 loss_input: 82.08715191707536
step: 12000 epoch: 972 loss: 16.626243461738895 loss_input: 82.19161894996707
step: 13000 epoch: 972 loss: 16.63406971742278 loss_input: 82.14072078965681
step: 14000 epoch: 972 loss: 16.64198686550212 loss_input: 82.15750873403016
step: 15000 epoch: 972 loss: 16.656092770043536 loss_input: 82.20729649517506
Save loss: 16.659428214237092 Name: 972_train_model.pth
step: 0 epoch: 973 loss: 17.488597869873047 loss_input: 101.42657470703125
step: 1000 epoch: 973 loss: 16.620204013068 loss_input: 82.59464400345747
step: 2000 epoch: 973 loss: 16.62235536663488 loss_input: 82.21588473341454
step: 3000 epoch: 973 loss: 16.616460916877628 loss_input: 81.92138383071529
step: 4000 epoch: 973 loss: 16.666382118392903 loss_input: 82.4133156164084
step: 5000 epoch: 973 loss: 16.633857986970416 loss_input: 82.27205919900004
step: 6000 epoch: 973 loss: 16.585889247750625 loss_input: 81.9021439745394
step: 7000 epoch: 973 loss: 16.601454363399156 loss_input: 82.12691698863053
step: 8000 epoch: 973 loss: 16.63221237966678 loss_input: 82.1780978473272
step: 9000 epoch: 973 loss: 16.688819714988977 loss_input: 82.35791442145535
step: 10000 epoch: 973 loss: 16.663750248901273 loss_input: 82.23231034440501
step: 11000 epoch: 973 loss: 16.64248163233496 loss_input: 82.02455132918492
step: 12000 epoch: 973 loss: 16.6403612130563 loss_input: 82.03285371258939
step: 13000 epoch: 973 loss: 16.645947476862283 loss_input: 82.14435861829739
step: 14000 epoch: 973 loss: 16.641364475155157 loss_input: 82.04568874004117
step: 15000 epoch: 973 loss: 16.6586716545875 loss_input: 82.19788752364425
Save loss: 16.660269537895918 Name: 973_train_model.pth
step: 0 epoch: 974 loss: 27.042713165283203 loss_input: 107.67626953125
step: 1000 epoch: 974 loss: 16.807737960682047 loss_input: 82.97209644603443
step: 2000 epoch: 974 loss: 16.632633031099694 loss_input: 81.88525463830585
step: 3000 epoch: 974 loss: 16.62688071208968 loss_input: 82.22555510467865
step: 4000 epoch: 974 loss: 16.62982355591894 loss_input: 82.36635724796828
step: 5000 epoch: 974 loss: 16.605164670057473 loss_input: 82.45831497762947
step: 6000 epoch: 974 loss: 16.605729735348547 loss_input: 82.59120328496682
step: 7000 epoch: 974 loss: 16.620784388731384 loss_input: 82.4108822379448
step: 8000 epoch: 974 loss: 16.63327095648927 loss_input: 82.33935106713956
step: 9000 epoch: 974 loss: 16.6509143890586 loss_input: 82.5024069387798
step: 10000 epoch: 974 loss: 16.642861724769983 loss_input: 82.32964295123222
step: 11000 epoch: 974 loss: 16.655963392563706 loss_input: 82.44638721475427
step: 12000 epoch: 974 loss: 16.646664674158306 loss_input: 82.24415013043347
step: 13000 epoch: 974 loss: 16.66151841494975 loss_input: 82.21980580820706
step: 14000 epoch: 974 loss: 16.664529947372568 loss_input: 82.21864473931271
step: 15000 epoch: 974 loss: 16.665371207600952 loss_input: 82.23134366725812
Save loss: 16.664253383859993 Name: 974_train_model.pth
step: 0 epoch: 975 loss: 18.267499923706055 loss_input: 104.60577392578125
step: 1000 epoch: 975 loss: 16.476955290917275 loss_input: 81.81242585539461
step: 2000 epoch: 975 loss: 16.53180107374539 loss_input: 81.53740848081819
step: 3000 epoch: 975 loss: 16.597257530796494 loss_input: 82.28219088916975
step: 4000 epoch: 975 loss: 16.60311436784235 loss_input: 82.24126256033648
step: 5000 epoch: 975 loss: 16.631105923600206 loss_input: 82.10771434580272
step: 6000 epoch: 975 loss: 16.68273577323021 loss_input: 82.06117108940502
step: 7000 epoch: 975 loss: 16.650627225284115 loss_input: 82.0411352455369
step: 8000 epoch: 975 loss: 16.640117957940355 loss_input: 82.03716586360542
step: 9000 epoch: 975 loss: 16.647739966251283 loss_input: 82.09650945223751
step: 10000 epoch: 975 loss: 16.61985545265187 loss_input: 81.92739274046228
step: 11000 epoch: 975 loss: 16.62869566514832 loss_input: 81.91348023870167
step: 12000 epoch: 975 loss: 16.621279521978217 loss_input: 82.00033586882083
step: 13000 epoch: 975 loss: 16.637960704544234 loss_input: 82.05553722711684
step: 14000 epoch: 975 loss: 16.644943777846144 loss_input: 82.13100182523932
step: 15000 epoch: 975 loss: 16.655318043198747 loss_input: 82.16091136323334
Save loss: 16.662415390387178 Name: 975_train_model.pth
step: 0 epoch: 976 loss: 13.618163108825684 loss_input: 42.16259765625
step: 1000 epoch: 976 loss: 16.70550411064308 loss_input: 82.47231223366477
step: 2000 epoch: 976 loss: 16.72404589336077 loss_input: 82.37616061354089
step: 3000 epoch: 976 loss: 16.76764404420176 loss_input: 82.92147154968526
step: 4000 epoch: 976 loss: 16.734903086247787 loss_input: 82.39498197350076
step: 5000 epoch: 976 loss: 16.73828677925151 loss_input: 82.30857589468434
step: 6000 epoch: 976 loss: 16.695949116382494 loss_input: 82.00771115784089
step: 7000 epoch: 976 loss: 16.666181794951054 loss_input: 82.10089677130388
step: 8000 epoch: 976 loss: 16.66137955537693 loss_input: 82.30668498438428
step: 9000 epoch: 976 loss: 16.646764482634424 loss_input: 82.31271691518867
step: 10000 epoch: 976 loss: 16.65445218635981 loss_input: 82.24598266491711
step: 11000 epoch: 976 loss: 16.649451756215118 loss_input: 82.15785422982677
step: 12000 epoch: 976 loss: 16.66146415734766 loss_input: 82.13208923263556
step: 13000 epoch: 976 loss: 16.671684086996724 loss_input: 82.15966653688147
step: 14000 epoch: 976 loss: 16.663207505091744 loss_input: 82.15941677034927
step: 15000 epoch: 976 loss: 16.66216320557941 loss_input: 82.09153025052237
Save loss: 16.664937653794883 Name: 976_train_model.pth
step: 0 epoch: 977 loss: 17.603910446166992 loss_input: 91.3955078125
step: 1000 epoch: 977 loss: 16.473600890133884 loss_input: 82.91063855578015
step: 2000 epoch: 977 loss: 16.471563127861806 loss_input: 82.41179800724638
step: 3000 epoch: 977 loss: 16.512496990825447 loss_input: 82.05821128878264
step: 4000 epoch: 977 loss: 16.588513069944185 loss_input: 82.44212317282008
step: 5000 epoch: 977 loss: 16.591293906765063 loss_input: 82.36881969309263
step: 6000 epoch: 977 loss: 16.601216315190168 loss_input: 82.25685848623212
step: 7000 epoch: 977 loss: 16.59741131248959 loss_input: 82.25060084015621
step: 8000 epoch: 977 loss: 16.594481232136193 loss_input: 82.18955545290726
step: 9000 epoch: 977 loss: 16.612605865657788 loss_input: 82.27534810178426
step: 10000 epoch: 977 loss: 16.61768158201384 loss_input: 82.3117990324991
step: 11000 epoch: 977 loss: 16.611704466115583 loss_input: 82.23726231075939
step: 12000 epoch: 977 loss: 16.602460450365605 loss_input: 82.08726632298057
step: 13000 epoch: 977 loss: 16.617954650665006 loss_input: 82.04239564110303
step: 14000 epoch: 977 loss: 16.648168211178767 loss_input: 82.13218539548375
step: 15000 epoch: 977 loss: 16.64997950281098 loss_input: 82.16362976517712
Save loss: 16.655184809952974 Name: 977_train_model.pth
step: 0 epoch: 978 loss: 24.17780876159668 loss_input: 110.9854736328125
step: 1000 epoch: 978 loss: 16.50102376175689 loss_input: 83.3992705902496
step: 2000 epoch: 978 loss: 16.529680052380275 loss_input: 82.95593249517819
step: 3000 epoch: 978 loss: 16.56069174054383 loss_input: 82.80774631519311
step: 4000 epoch: 978 loss: 16.55352524202247 loss_input: 82.53809617358844
step: 5000 epoch: 978 loss: 16.597307715599978 loss_input: 82.39576320575729
step: 6000 epoch: 978 loss: 16.585357504593414 loss_input: 82.309185244842
step: 7000 epoch: 978 loss: 16.616367259820414 loss_input: 82.46066506926186
step: 8000 epoch: 978 loss: 16.6323058840603 loss_input: 82.40501909764942
step: 9000 epoch: 978 loss: 16.62455884751552 loss_input: 82.26512194310224
step: 10000 epoch: 978 loss: 16.633756061516195 loss_input: 82.20554660232219
step: 11000 epoch: 978 loss: 16.629399521504084 loss_input: 82.14909613153065
step: 12000 epoch: 978 loss: 16.630865715690557 loss_input: 82.11593877390101
step: 13000 epoch: 978 loss: 16.62803302410153 loss_input: 82.08120093526459
step: 14000 epoch: 978 loss: 16.648642266752823 loss_input: 82.2048540547885
step: 15000 epoch: 978 loss: 16.65543970012099 loss_input: 82.20551729656825
Save loss: 16.6584771720618 Name: 978_train_model.pth
step: 0 epoch: 979 loss: 12.052250862121582 loss_input: 49.07757568359375
step: 1000 epoch: 979 loss: 16.461840143689624 loss_input: 80.93657044859438
step: 2000 epoch: 979 loss: 16.494308962457364 loss_input: 81.62539296147824
step: 3000 epoch: 979 loss: 16.568746747354076 loss_input: 81.55159417075818
step: 4000 epoch: 979 loss: 16.518993345805033 loss_input: 81.1577753602371
step: 5000 epoch: 979 loss: 16.5830448583421 loss_input: 81.32784640581649
step: 6000 epoch: 979 loss: 16.586444365622658 loss_input: 81.36447877380137
step: 7000 epoch: 979 loss: 16.61496178430585 loss_input: 81.5167606235521
step: 8000 epoch: 979 loss: 16.61257062809361 loss_input: 81.67833389417632
step: 9000 epoch: 979 loss: 16.638313922520783 loss_input: 81.92265218279783
step: 10000 epoch: 979 loss: 16.63394106644271 loss_input: 81.97172718030932
step: 11000 epoch: 979 loss: 16.63432597949303 loss_input: 81.91606543070836
step: 12000 epoch: 979 loss: 16.640436246508152 loss_input: 81.93781925338416
step: 13000 epoch: 979 loss: 16.657883823417663 loss_input: 82.03702792280701
step: 14000 epoch: 979 loss: 16.678114741574475 loss_input: 82.1300864330352
step: 15000 epoch: 979 loss: 16.668122657020366 loss_input: 82.21696784341981
Save loss: 16.662340654581786 Name: 979_train_model.pth
step: 0 epoch: 980 loss: 17.658611297607422 loss_input: 71.96112060546875
step: 1000 epoch: 980 loss: 16.415950310694708 loss_input: 81.59389914284934
step: 2000 epoch: 980 loss: 16.491794408648566 loss_input: 81.50305755325462
step: 3000 epoch: 980 loss: 16.597028099588854 loss_input: 81.70623049152887
step: 4000 epoch: 980 loss: 16.600725626713096 loss_input: 81.72379256260153
step: 5000 epoch: 980 loss: 16.570518877524847 loss_input: 81.83033985243966
step: 6000 epoch: 980 loss: 16.57953861383096 loss_input: 81.9405975062099
step: 7000 epoch: 980 loss: 16.603384819188776 loss_input: 82.10950277192543
step: 8000 epoch: 980 loss: 16.610019284298293 loss_input: 82.11081280092674
step: 9000 epoch: 980 loss: 16.63055495860351 loss_input: 82.12015702546722
step: 10000 epoch: 980 loss: 16.609153803652877 loss_input: 81.99110429533219
step: 11000 epoch: 980 loss: 16.6150028025949 loss_input: 82.01860431566769
step: 12000 epoch: 980 loss: 16.638832433413846 loss_input: 82.16129325546927
step: 13000 epoch: 980 loss: 16.65123889745359 loss_input: 82.15199341167717
step: 14000 epoch: 980 loss: 16.664763636524683 loss_input: 82.25755960145564
step: 15000 epoch: 980 loss: 16.654414497258767 loss_input: 82.24351838051291
Save loss: 16.662633007377387 Name: 980_train_model.pth
step: 0 epoch: 981 loss: 18.116649627685547 loss_input: 75.35107421875
step: 1000 epoch: 981 loss: 16.60730982874776 loss_input: 81.66445060066886
step: 2000 epoch: 981 loss: 16.609897526784398 loss_input: 81.78397416282094
step: 3000 epoch: 981 loss: 16.534861934538565 loss_input: 82.03075238348285
step: 4000 epoch: 981 loss: 16.614055432608293 loss_input: 82.19342927329691
step: 5000 epoch: 981 loss: 16.60526114715335 loss_input: 82.06531234036396
step: 6000 epoch: 981 loss: 16.64733555276798 loss_input: 82.19016535777565
step: 7000 epoch: 981 loss: 16.654212536258097 loss_input: 82.3004156231727
step: 8000 epoch: 981 loss: 16.67538611368304 loss_input: 82.4283844554831
step: 9000 epoch: 981 loss: 16.6848334290295 loss_input: 82.42772086738626
step: 10000 epoch: 981 loss: 16.67383181678094 loss_input: 82.34867808995467
step: 11000 epoch: 981 loss: 16.6756616185312 loss_input: 82.22757366073965
step: 12000 epoch: 981 loss: 16.655509465675078 loss_input: 82.14863372184804
step: 13000 epoch: 981 loss: 16.647905399300356 loss_input: 82.17836655668916
step: 14000 epoch: 981 loss: 16.656859006722325 loss_input: 82.18945060094062
step: 15000 epoch: 981 loss: 16.663853151179513 loss_input: 82.21032928859431
Save loss: 16.6672714163512 Name: 981_train_model.pth
step: 0 epoch: 982 loss: 20.749099731445312 loss_input: 112.892822265625
step: 1000 epoch: 982 loss: 16.570565460921525 loss_input: 80.78651103535137
step: 2000 epoch: 982 loss: 16.621013488369186 loss_input: 81.93411985413543
step: 3000 epoch: 982 loss: 16.677348909279537 loss_input: 82.23255382971301
step: 4000 epoch: 982 loss: 16.611320409081156 loss_input: 82.07059230675729
step: 5000 epoch: 982 loss: 16.641544936442703 loss_input: 82.11094603589048
step: 6000 epoch: 982 loss: 16.62429227560406 loss_input: 82.08933801560417
step: 7000 epoch: 982 loss: 16.624161407855933 loss_input: 82.10230607227706
step: 8000 epoch: 982 loss: 16.637632700640356 loss_input: 82.14174851678696
step: 9000 epoch: 982 loss: 16.654488318788385 loss_input: 82.2712258248975
step: 10000 epoch: 982 loss: 16.661647281459828 loss_input: 82.32962495986729
step: 11000 epoch: 982 loss: 16.68138546538823 loss_input: 82.39054128957027
step: 12000 epoch: 982 loss: 16.692567104340075 loss_input: 82.35126103688137
step: 13000 epoch: 982 loss: 16.694091216957247 loss_input: 82.24810520134268
step: 14000 epoch: 982 loss: 16.68928025577862 loss_input: 82.22624389744342
step: 15000 epoch: 982 loss: 16.670511835949014 loss_input: 82.19965338899281
Save loss: 16.661248847037555 Name: 982_train_model.pth
step: 0 epoch: 983 loss: 23.21844482421875 loss_input: 86.21356201171875
step: 1000 epoch: 983 loss: 16.523898620586415 loss_input: 82.14655754401848
step: 2000 epoch: 983 loss: 16.4371230764308 loss_input: 81.38022410303637
step: 3000 epoch: 983 loss: 16.52373157100493 loss_input: 81.42779146453135
step: 4000 epoch: 983 loss: 16.616260220545048 loss_input: 81.95324765000156
step: 5000 epoch: 983 loss: 16.683886949216525 loss_input: 82.29419598258083
step: 6000 epoch: 983 loss: 16.658267179184648 loss_input: 82.2428532113951
step: 7000 epoch: 983 loss: 16.64819749994391 loss_input: 82.22180218499757
step: 8000 epoch: 983 loss: 16.677038483374744 loss_input: 82.26076228844197
step: 9000 epoch: 983 loss: 16.65612929503847 loss_input: 82.22257843882147
step: 10000 epoch: 983 loss: 16.65835490091814 loss_input: 82.21276687638853
step: 11000 epoch: 983 loss: 16.646355457516563 loss_input: 82.23263371986602
step: 12000 epoch: 983 loss: 16.670319442142695 loss_input: 82.31269164967861
step: 13000 epoch: 983 loss: 16.67297171022459 loss_input: 82.33834739575322
step: 14000 epoch: 983 loss: 16.657076226363106 loss_input: 82.25604091982409
step: 15000 epoch: 983 loss: 16.64761833694488 loss_input: 82.25244811560088
Save loss: 16.655735019907354 Name: 983_train_model.pth
step: 0 epoch: 984 loss: 16.373788833618164 loss_input: 117.11651611328125
step: 1000 epoch: 984 loss: 16.57122077546515 loss_input: 82.78132127881884
step: 2000 epoch: 984 loss: 16.646195146097416 loss_input: 83.56385039901924
step: 3000 epoch: 984 loss: 16.673140707511738 loss_input: 82.88671641109865
step: 4000 epoch: 984 loss: 16.604097984934175 loss_input: 82.57085237357653
step: 5000 epoch: 984 loss: 16.660567490393294 loss_input: 82.70673109030538
step: 6000 epoch: 984 loss: 16.656005668393814 loss_input: 82.59953631792162
step: 7000 epoch: 984 loss: 16.66940631694137 loss_input: 82.59749797357352
step: 8000 epoch: 984 loss: 16.65246388188393 loss_input: 82.43080825529132
step: 9000 epoch: 984 loss: 16.63518129882647 loss_input: 82.30143746099608
step: 10000 epoch: 984 loss: 16.641065689626068 loss_input: 82.31806813825453
step: 11000 epoch: 984 loss: 16.65322345931382 loss_input: 82.36927763092204
step: 12000 epoch: 984 loss: 16.657856430831366 loss_input: 82.34856862920928
step: 13000 epoch: 984 loss: 16.64862068701997 loss_input: 82.28769207037409
step: 14000 epoch: 984 loss: 16.653399404274822 loss_input: 82.26792219483625
step: 15000 epoch: 984 loss: 16.65472642608217 loss_input: 82.23335237123514
Save loss: 16.656266190186145 Name: 984_train_model.pth
step: 0 epoch: 985 loss: 15.879990577697754 loss_input: 65.66802978515625
step: 1000 epoch: 985 loss: 16.568857332090516 loss_input: 81.90652554232877
step: 2000 epoch: 985 loss: 16.682315505664985 loss_input: 81.94554874052231
step: 3000 epoch: 985 loss: 16.650983125438138 loss_input: 82.35464783630066
step: 4000 epoch: 985 loss: 16.674872572497705 loss_input: 82.32396518180025
step: 5000 epoch: 985 loss: 16.64351774391902 loss_input: 82.199667378536
step: 6000 epoch: 985 loss: 16.613843102670476 loss_input: 82.04053236814046
step: 7000 epoch: 985 loss: 16.61111512369402 loss_input: 81.99591654665963
step: 8000 epoch: 985 loss: 16.618270085641942 loss_input: 81.89300511172675
step: 9000 epoch: 985 loss: 16.63367462001924 loss_input: 82.01132299454791
step: 10000 epoch: 985 loss: 16.638347080928924 loss_input: 82.03956443344923
step: 11000 epoch: 985 loss: 16.63056949118833 loss_input: 82.02358865369483
step: 12000 epoch: 985 loss: 16.636026815477685 loss_input: 82.09667023801076
step: 13000 epoch: 985 loss: 16.64023267618556 loss_input: 82.17031026816296
step: 14000 epoch: 985 loss: 16.64433030499432 loss_input: 82.16382979393073
step: 15000 epoch: 985 loss: 16.65815070748099 loss_input: 82.19813709196731
Save loss: 16.6514096686095 Name: 985_train_model.pth
step: 0 epoch: 986 loss: 17.284318923950195 loss_input: 87.87957763671875
step: 1000 epoch: 986 loss: 16.788410293472396 loss_input: 83.4390450857736
step: 2000 epoch: 986 loss: 16.760717467746993 loss_input: 82.83876158844406
step: 3000 epoch: 986 loss: 16.65404911193797 loss_input: 82.38443142650367
step: 4000 epoch: 986 loss: 16.639912477286867 loss_input: 82.5755684491963
step: 5000 epoch: 986 loss: 16.61313554649948 loss_input: 82.12867414310107
step: 6000 epoch: 986 loss: 16.618321934335928 loss_input: 82.20629506491757
step: 7000 epoch: 986 loss: 16.578511035096696 loss_input: 82.12087606869498
step: 8000 epoch: 986 loss: 16.591748253909696 loss_input: 82.29758558927692
step: 9000 epoch: 986 loss: 16.573529157091837 loss_input: 82.11100045687198
step: 10000 epoch: 986 loss: 16.570777107293505 loss_input: 82.06897574402716
step: 11000 epoch: 986 loss: 16.582591375170292 loss_input: 82.15577868498106
step: 12000 epoch: 986 loss: 16.611630096126422 loss_input: 82.25144703314112
step: 13000 epoch: 986 loss: 16.619886322999292 loss_input: 82.29040468191148
step: 14000 epoch: 986 loss: 16.628625618560886 loss_input: 82.31328184374247
step: 15000 epoch: 986 loss: 16.644276359654864 loss_input: 82.36335170428742
Save loss: 16.657233777478336 Name: 986_train_model.pth
step: 0 epoch: 987 loss: 21.437108993530273 loss_input: 82.15106201171875
step: 1000 epoch: 987 loss: 16.56864676132545 loss_input: 81.20303433139127
step: 2000 epoch: 987 loss: 16.645456331363622 loss_input: 81.64782900907944
step: 3000 epoch: 987 loss: 16.679660549484783 loss_input: 81.85637945717313
step: 4000 epoch: 987 loss: 16.680634268758535 loss_input: 81.90268988538998
step: 5000 epoch: 987 loss: 16.677148661692602 loss_input: 82.01729793201515
step: 6000 epoch: 987 loss: 16.710710251814046 loss_input: 82.2245913726293
step: 7000 epoch: 987 loss: 16.72605793970923 loss_input: 82.44053204627296
step: 8000 epoch: 987 loss: 16.704162100109066 loss_input: 82.29892430438383
step: 9000 epoch: 987 loss: 16.69017900605822 loss_input: 82.3821038859723
step: 10000 epoch: 987 loss: 16.67100569756314 loss_input: 82.35581923985336
step: 11000 epoch: 987 loss: 16.657914383239547 loss_input: 82.34975768249583
step: 12000 epoch: 987 loss: 16.634940175828948 loss_input: 82.2339206972417
step: 13000 epoch: 987 loss: 16.652205371515592 loss_input: 82.26075100874903
step: 14000 epoch: 987 loss: 16.64702918103623 loss_input: 82.27224902985445
step: 15000 epoch: 987 loss: 16.646850551337323 loss_input: 82.1824902550442
Save loss: 16.65446876220405 Name: 987_train_model.pth
step: 0 epoch: 988 loss: 12.613512992858887 loss_input: 46.37188720703125
step: 1000 epoch: 988 loss: 16.48388358286687 loss_input: 81.18303449480207
step: 2000 epoch: 988 loss: 16.59545715935882 loss_input: 81.69326689194465
step: 3000 epoch: 988 loss: 16.523959382698163 loss_input: 81.35275812603481
step: 4000 epoch: 988 loss: 16.529069138657775 loss_input: 81.55521869349558
step: 5000 epoch: 988 loss: 16.48287679171281 loss_input: 81.50736396910071
step: 6000 epoch: 988 loss: 16.508966826653605 loss_input: 81.91036016498877
step: 7000 epoch: 988 loss: 16.5374502385451 loss_input: 81.87394667495882
step: 8000 epoch: 988 loss: 16.562808313960954 loss_input: 82.05201191586892
step: 9000 epoch: 988 loss: 16.57612470925722 loss_input: 82.11330895010147
step: 10000 epoch: 988 loss: 16.584831138167807 loss_input: 82.18672326492937
step: 11000 epoch: 988 loss: 16.608981006265324 loss_input: 82.21259263406807
step: 12000 epoch: 988 loss: 16.636234682745243 loss_input: 82.21611031722291
step: 13000 epoch: 988 loss: 16.633888959737934 loss_input: 82.15212190896673
step: 14000 epoch: 988 loss: 16.652815198482816 loss_input: 82.25262826075341
step: 15000 epoch: 988 loss: 16.655212246158204 loss_input: 82.25147448259308
Save loss: 16.656581579223275 Name: 988_train_model.pth
step: 0 epoch: 989 loss: 12.359090805053711 loss_input: 104.86480712890625
step: 1000 epoch: 989 loss: 16.57089262980443 loss_input: 81.87106216537369
step: 2000 epoch: 989 loss: 16.57533330288248 loss_input: 82.24189937501953
step: 3000 epoch: 989 loss: 16.591215709494335 loss_input: 82.33144436880414
step: 4000 epoch: 989 loss: 16.609783427294 loss_input: 82.48701299711693
step: 5000 epoch: 989 loss: 16.607508789704003 loss_input: 82.50632207542867
step: 6000 epoch: 989 loss: 16.60104890394918 loss_input: 82.21620956036949
step: 7000 epoch: 989 loss: 16.637463152195757 loss_input: 82.11804823432033
step: 8000 epoch: 989 loss: 16.637342584474343 loss_input: 82.07506128079206
step: 9000 epoch: 989 loss: 16.6231628103715 loss_input: 81.96957801265354
step: 10000 epoch: 989 loss: 16.641541308515635 loss_input: 81.99791342887195
step: 11000 epoch: 989 loss: 16.658982700872894 loss_input: 82.21037476817627
step: 12000 epoch: 989 loss: 16.650910053717258 loss_input: 82.12619465345661
step: 13000 epoch: 989 loss: 16.66771636551302 loss_input: 82.17276735924159
step: 14000 epoch: 989 loss: 16.662365571301915 loss_input: 82.17882083652786
step: 15000 epoch: 989 loss: 16.66145886073835 loss_input: 82.1896427079015
Save loss: 16.658412209257484 Name: 989_train_model.pth
step: 0 epoch: 990 loss: 18.34517478942871 loss_input: 135.18548583984375
step: 1000 epoch: 990 loss: 16.75173223554552 loss_input: 82.87078381823255
step: 2000 epoch: 990 loss: 16.65586951778627 loss_input: 82.66116368085488
step: 3000 epoch: 990 loss: 16.64361466141154 loss_input: 82.81244388670574
step: 4000 epoch: 990 loss: 16.65525025327454 loss_input: 82.5998520358924
step: 5000 epoch: 990 loss: 16.629354794391084 loss_input: 82.42331238342176
step: 6000 epoch: 990 loss: 16.638835656088684 loss_input: 82.47403441792905
step: 7000 epoch: 990 loss: 16.674443735256993 loss_input: 82.56329495653802
step: 8000 epoch: 990 loss: 16.679120800045485 loss_input: 82.45180616461029
step: 9000 epoch: 990 loss: 16.66273035503549 loss_input: 82.50542753143532
step: 10000 epoch: 990 loss: 16.64499015400927 loss_input: 82.29085697387292
step: 11000 epoch: 990 loss: 16.64266910835587 loss_input: 82.101864402375
step: 12000 epoch: 990 loss: 16.639228605169144 loss_input: 82.1524459702821
step: 13000 epoch: 990 loss: 16.655406284567007 loss_input: 82.20154855500898
step: 14000 epoch: 990 loss: 16.669363670404977 loss_input: 82.29596578062504
step: 15000 epoch: 990 loss: 16.662497620448757 loss_input: 82.28309513252502
Save loss: 16.65932952272892 Name: 990_train_model.pth
step: 0 epoch: 991 loss: 15.063020706176758 loss_input: 67.32623291015625
step: 1000 epoch: 991 loss: 16.668269112155393 loss_input: 82.52412431318682
step: 2000 epoch: 991 loss: 16.64743123705062 loss_input: 82.28827352681081
step: 3000 epoch: 991 loss: 16.6268915119508 loss_input: 82.16849833828176
step: 4000 epoch: 991 loss: 16.68368516055324 loss_input: 82.02982640570326
step: 5000 epoch: 991 loss: 16.704642281535147 loss_input: 82.08233231283431
step: 6000 epoch: 991 loss: 16.67766544712005 loss_input: 82.11587366257642
step: 7000 epoch: 991 loss: 16.631262532780298 loss_input: 81.93838856237613
step: 8000 epoch: 991 loss: 16.603687528490557 loss_input: 82.01276206898699
step: 9000 epoch: 991 loss: 16.63492710216087 loss_input: 82.26351132968733
step: 10000 epoch: 991 loss: 16.62586829233451 loss_input: 82.21221359919672
step: 11000 epoch: 991 loss: 16.60928339629203 loss_input: 82.10370340200784
step: 12000 epoch: 991 loss: 16.598255844217928 loss_input: 82.0407793470824
step: 13000 epoch: 991 loss: 16.622165984056995 loss_input: 82.148969230244
step: 14000 epoch: 991 loss: 16.62580835736655 loss_input: 82.13764699790556
step: 15000 epoch: 991 loss: 16.64140825355524 loss_input: 82.21429457599962
Save loss: 16.659543303400277 Name: 991_train_model.pth
step: 0 epoch: 992 loss: 15.0573091506958 loss_input: 59.01055908203125
step: 1000 epoch: 992 loss: 16.518129913718788 loss_input: 82.60801692204281
step: 2000 epoch: 992 loss: 16.52573949727102 loss_input: 81.85205099476629
step: 3000 epoch: 992 loss: 16.54512416708672 loss_input: 81.83294940098092
step: 4000 epoch: 992 loss: 16.57223415803802 loss_input: 82.09084525438465
step: 5000 epoch: 992 loss: 16.603367920185036 loss_input: 82.42972633500644
step: 6000 epoch: 992 loss: 16.658793400295814 loss_input: 82.41649110117389
step: 7000 epoch: 992 loss: 16.652578084030555 loss_input: 82.56435663348316
step: 8000 epoch: 992 loss: 16.642995024841884 loss_input: 82.54120042398264
step: 9000 epoch: 992 loss: 16.658083482207676 loss_input: 82.53938554353336
step: 10000 epoch: 992 loss: 16.66436315839165 loss_input: 82.53714034114167
step: 11000 epoch: 992 loss: 16.648648136843878 loss_input: 82.48127596088565
step: 12000 epoch: 992 loss: 16.65814145296159 loss_input: 82.4530425788899
step: 13000 epoch: 992 loss: 16.66485787090838 loss_input: 82.41438104145234
step: 14000 epoch: 992 loss: 16.672500826479595 loss_input: 82.3835532133447
step: 15000 epoch: 992 loss: 16.660016266014345 loss_input: 82.25001557513353
Save loss: 16.649414706513287 Name: 992_train_model.pth
step: 0 epoch: 993 loss: 17.737897872924805 loss_input: 84.8404541015625
step: 1000 epoch: 993 loss: 16.690142292361873 loss_input: 82.30235737163227
step: 2000 epoch: 993 loss: 16.599565970188735 loss_input: 81.9779997391441
step: 3000 epoch: 993 loss: 16.603751638419467 loss_input: 81.9458245160142
step: 4000 epoch: 993 loss: 16.59612471614591 loss_input: 81.82428927375089
step: 5000 epoch: 993 loss: 16.62003831762334 loss_input: 81.90684774422068
step: 6000 epoch: 993 loss: 16.62707854095965 loss_input: 81.69145970788723
step: 7000 epoch: 993 loss: 16.624324443799704 loss_input: 81.61124346389411
step: 8000 epoch: 993 loss: 16.653017175121615 loss_input: 81.90752518545045
step: 9000 epoch: 993 loss: 16.663969689720858 loss_input: 82.0248160496273
step: 10000 epoch: 993 loss: 16.681096769120618 loss_input: 82.02974639669821
step: 11000 epoch: 993 loss: 16.658016018060845 loss_input: 82.02350129813306
step: 12000 epoch: 993 loss: 16.66702811788116 loss_input: 82.20696190966834
step: 13000 epoch: 993 loss: 16.67370758393335 loss_input: 82.25175728763803
step: 14000 epoch: 993 loss: 16.68048864585588 loss_input: 82.34539355500138
step: 15000 epoch: 993 loss: 16.67453382687683 loss_input: 82.30313750160796
Save loss: 16.65418451909721 Name: 993_train_model.pth
step: 0 epoch: 994 loss: 21.19733238220215 loss_input: 88.5035400390625
step: 1000 epoch: 994 loss: 16.548637458732674 loss_input: 83.71506551310019
step: 2000 epoch: 994 loss: 16.687305616772456 loss_input: 83.47562003910154
step: 3000 epoch: 994 loss: 16.719049371587797 loss_input: 83.72023231432621
step: 4000 epoch: 994 loss: 16.7348956861546 loss_input: 83.42052626186715
step: 5000 epoch: 994 loss: 16.738530235656665 loss_input: 83.24433372602823
step: 6000 epoch: 994 loss: 16.701177698833668 loss_input: 83.05302912567957
step: 7000 epoch: 994 loss: 16.68639332695427 loss_input: 82.72107636269595
step: 8000 epoch: 994 loss: 16.6793355526082 loss_input: 82.66656639081003
step: 9000 epoch: 994 loss: 16.670202753726887 loss_input: 82.50645516790452
step: 10000 epoch: 994 loss: 16.69451607998437 loss_input: 82.5157745333865
step: 11000 epoch: 994 loss: 16.688101735529862 loss_input: 82.43436718377164
step: 12000 epoch: 994 loss: 16.68960978627036 loss_input: 82.4983659402081
step: 13000 epoch: 994 loss: 16.6777664540997 loss_input: 82.40601553974514
step: 14000 epoch: 994 loss: 16.674565330436167 loss_input: 82.28439454642051
step: 15000 epoch: 994 loss: 16.659211295373073 loss_input: 82.20197830317171
Save loss: 16.6565501973629 Name: 994_train_model.pth
step: 0 epoch: 995 loss: 15.72965145111084 loss_input: 87.76007080078125
step: 1000 epoch: 995 loss: 16.68691469739367 loss_input: 81.60602239557318
step: 2000 epoch: 995 loss: 16.663302947734966 loss_input: 81.8851248204023
step: 3000 epoch: 995 loss: 16.581201070390197 loss_input: 82.06125078261674
step: 4000 epoch: 995 loss: 16.574401078478033 loss_input: 81.90675709063457
step: 5000 epoch: 995 loss: 16.601728285963215 loss_input: 81.99321548043906
step: 6000 epoch: 995 loss: 16.636317408734453 loss_input: 82.24828839155063
step: 7000 epoch: 995 loss: 16.626807472430336 loss_input: 82.2233543837348
step: 8000 epoch: 995 loss: 16.667412315274966 loss_input: 82.38144149370245
step: 9000 epoch: 995 loss: 16.668963798296847 loss_input: 82.29427668979706
step: 10000 epoch: 995 loss: 16.670089314859066 loss_input: 82.35586244232022
step: 11000 epoch: 995 loss: 16.665336994201137 loss_input: 82.253675657948
step: 12000 epoch: 995 loss: 16.668969000630472 loss_input: 82.18918817820683
step: 13000 epoch: 995 loss: 16.67356548614845 loss_input: 82.29058686722867
step: 14000 epoch: 995 loss: 16.655862184381768 loss_input: 82.09644955780223
step: 15000 epoch: 995 loss: 16.654620491022364 loss_input: 82.21768391710084
Save loss: 16.657923473015426 Name: 995_train_model.pth
step: 0 epoch: 996 loss: 9.349071502685547 loss_input: 43.16290283203125
step: 1000 epoch: 996 loss: 16.660257062235555 loss_input: 82.44374954879105
step: 2000 epoch: 996 loss: 16.633410529813904 loss_input: 81.85184778826407
step: 3000 epoch: 996 loss: 16.682078913027983 loss_input: 82.30762206868543
step: 4000 epoch: 996 loss: 16.73169721457041 loss_input: 82.62685532535323
step: 5000 epoch: 996 loss: 16.727229301702067 loss_input: 82.52089402676106
step: 6000 epoch: 996 loss: 16.6876953882568 loss_input: 82.31916620667388
step: 7000 epoch: 996 loss: 16.66809649553287 loss_input: 82.20009661531333
step: 8000 epoch: 996 loss: 16.632757305935044 loss_input: 82.06197468266906
step: 9000 epoch: 996 loss: 16.63227298540243 loss_input: 82.13342100036844
step: 10000 epoch: 996 loss: 16.646205121094603 loss_input: 82.16771063372178
step: 11000 epoch: 996 loss: 16.64319945376479 loss_input: 82.09792170660266
step: 12000 epoch: 996 loss: 16.639737353941946 loss_input: 82.12387338490022
step: 13000 epoch: 996 loss: 16.636140577041793 loss_input: 82.04688988204333
step: 14000 epoch: 996 loss: 16.636810416281627 loss_input: 82.12365179482157
step: 15000 epoch: 996 loss: 16.642629533360445 loss_input: 82.22417964290662
Save loss: 16.6521985334754 Name: 996_train_model.pth
step: 0 epoch: 997 loss: 15.645406723022461 loss_input: 78.03265380859375
step: 1000 epoch: 997 loss: 16.54116729518155 loss_input: 81.55931794036042
step: 2000 epoch: 997 loss: 16.427066586364333 loss_input: 81.64615488886416
step: 3000 epoch: 997 loss: 16.530431102173043 loss_input: 82.10672734332697
step: 4000 epoch: 997 loss: 16.537847317388373 loss_input: 82.043504626773
step: 5000 epoch: 997 loss: 16.589130481036513 loss_input: 82.20199924858778
step: 6000 epoch: 997 loss: 16.597330784602992 loss_input: 82.07662963358646
step: 7000 epoch: 997 loss: 16.610795090903796 loss_input: 82.29641933302219
step: 8000 epoch: 997 loss: 16.625828782344904 loss_input: 82.42433903983438
step: 9000 epoch: 997 loss: 16.626390860698155 loss_input: 82.38169920681557
step: 10000 epoch: 997 loss: 16.620046457902465 loss_input: 82.34353881506381
step: 11000 epoch: 997 loss: 16.631707044723154 loss_input: 82.31429205676793
step: 12000 epoch: 997 loss: 16.613350782302227 loss_input: 82.2011042435103
step: 13000 epoch: 997 loss: 16.620545765865987 loss_input: 82.21057657993406
step: 14000 epoch: 997 loss: 16.63397886826135 loss_input: 82.205388654054
step: 15000 epoch: 997 loss: 16.6584188286189 loss_input: 82.30474527609617
Save loss: 16.652415780231358 Name: 997_train_model.pth
step: 0 epoch: 998 loss: 20.43225860595703 loss_input: 99.8018798828125
step: 1000 epoch: 998 loss: 16.532218495329897 loss_input: 81.42488279542722
step: 2000 epoch: 998 loss: 16.638425756013138 loss_input: 81.82361285177724
step: 3000 epoch: 998 loss: 16.66430471746654 loss_input: 82.18886317272498
step: 4000 epoch: 998 loss: 16.631044946113725 loss_input: 82.23664971513469
step: 5000 epoch: 998 loss: 16.65030149254077 loss_input: 82.2243709095095
step: 6000 epoch: 998 loss: 16.668092256584796 loss_input: 82.06248142806277
step: 7000 epoch: 998 loss: 16.664936000594853 loss_input: 82.16024201829984
step: 8000 epoch: 998 loss: 16.667554073699666 loss_input: 82.34558912557806
step: 9000 epoch: 998 loss: 16.653845787260245 loss_input: 82.28744372234465
step: 10000 epoch: 998 loss: 16.64896633410237 loss_input: 82.20588904244342
step: 11000 epoch: 998 loss: 16.669861671415504 loss_input: 82.27784529686754
step: 12000 epoch: 998 loss: 16.668006283175437 loss_input: 82.24198474284857
step: 13000 epoch: 998 loss: 16.646283361748743 loss_input: 82.18638915170588
step: 14000 epoch: 998 loss: 16.664482134136588 loss_input: 82.23243786070945
step: 15000 epoch: 998 loss: 16.668930354066536 loss_input: 82.16778903785972
Save loss: 16.662992400169372 Name: 998_train_model.pth
step: 0 epoch: 999 loss: 17.627452850341797 loss_input: 66.200927734375
step: 1000 epoch: 999 loss: 16.657390113358016 loss_input: 83.19098930854302
step: 2000 epoch: 999 loss: 16.620916136856497 loss_input: 81.45296928562087
step: 3000 epoch: 999 loss: 16.615220054313763 loss_input: 81.88930610369461
step: 4000 epoch: 999 loss: 16.582978898243617 loss_input: 81.94255443097234
step: 5000 epoch: 999 loss: 16.615604414746322 loss_input: 81.75581608531809
step: 6000 epoch: 999 loss: 16.576751891264735 loss_input: 81.58082402322594
step: 7000 epoch: 999 loss: 16.614674290151804 loss_input: 81.84747392043882
step: 8000 epoch: 999 loss: 16.617636057633547 loss_input: 81.90588842715461
step: 9000 epoch: 999 loss: 16.603953016902217 loss_input: 81.86073012829304
step: 10000 epoch: 999 loss: 16.61718497115628 loss_input: 82.01779107977875
step: 11000 epoch: 999 loss: 16.626793424104218 loss_input: 82.03734064392583
step: 12000 epoch: 999 loss: 16.643649216860833 loss_input: 82.11111087829269
step: 13000 epoch: 999 loss: 16.669870854946165 loss_input: 82.27198089536232
step: 14000 epoch: 999 loss: 16.66620415812279 loss_input: 82.29449567950782
step: 15000 epoch: 999 loss: 16.657290993654826 loss_input: 82.2590641906425
Save loss: 16.65825604939461 Name: 999_train_model.pth
step: 0 epoch: 1000 loss: 11.978702545166016 loss_input: 42.46270751953125
step: 1000 epoch: 1000 loss: 16.7279511298333 loss_input: 81.55579442530126
step: 2000 epoch: 1000 loss: 16.532294628680916 loss_input: 82.03730391884136
step: 3000 epoch: 1000 loss: 16.579099408073134 loss_input: 82.5585207762777
step: 4000 epoch: 1000 loss: 16.56386463643908 loss_input: 82.25299491777446
step: 5000 epoch: 1000 loss: 16.581326162307363 loss_input: 82.46925537988106
step: 6000 epoch: 1000 loss: 16.61335063811005 loss_input: 82.52243616135593
step: 7000 epoch: 1000 loss: 16.572427724637468 loss_input: 82.2955010856429
step: 8000 epoch: 1000 loss: 16.56024929357013 loss_input: 82.17451528733469
step: 9000 epoch: 1000 loss: 16.589702444306454 loss_input: 82.22789970629348
step: 10000 epoch: 1000 loss: 16.59472817953152 loss_input: 82.22687761131114
step: 11000 epoch: 1000 loss: 16.622598326148776 loss_input: 82.24031793562979
step: 12000 epoch: 1000 loss: 16.63772900940309 loss_input: 82.2786659613896
step: 13000 epoch: 1000 loss: 16.639385996245576 loss_input: 82.23301532581928
step: 14000 epoch: 1000 loss: 16.653863755271022 loss_input: 82.32699257792272
step: 15000 epoch: 1000 loss: 16.655653708029266 loss_input: 82.31184991719952
Save loss: 16.655448478221892 Name: 1000_train_model.pth
step: 0 epoch: 1001 loss: 19.09667205810547 loss_input: 100.62408447265625
step: 1000 epoch: 1001 loss: 16.49742629025485 loss_input: 81.73025223067948
step: 2000 epoch: 1001 loss: 16.678535596779856 loss_input: 82.30643943165136
step: 3000 epoch: 1001 loss: 16.723776738193184 loss_input: 82.2416832938825
step: 4000 epoch: 1001 loss: 16.69256138962467 loss_input: 82.42853437641566
step: 5000 epoch: 1001 loss: 16.68381498871124 loss_input: 82.37825104656802
step: 6000 epoch: 1001 loss: 16.64907880688365 loss_input: 82.28209769807246
step: 7000 epoch: 1001 loss: 16.687716155133234 loss_input: 82.412109514489
step: 8000 epoch: 1001 loss: 16.684748357719904 loss_input: 82.68176815727624
step: 9000 epoch: 1001 loss: 16.686608035409996 loss_input: 82.57488282687557
step: 10000 epoch: 1001 loss: 16.691630671351163 loss_input: 82.51198561174157
step: 11000 epoch: 1001 loss: 16.6719598934202 loss_input: 82.25209682198636
step: 12000 epoch: 1001 loss: 16.679211784309075 loss_input: 82.29850806805325
step: 13000 epoch: 1001 loss: 16.660541216067813 loss_input: 82.34774191329483
step: 14000 epoch: 1001 loss: 16.656336368352086 loss_input: 82.32467940609571
step: 15000 epoch: 1001 loss: 16.655807816929027 loss_input: 82.28986475771407
Save loss: 16.650795100390912 Name: 1001_train_model.pth
step: 0 epoch: 1002 loss: 9.584334373474121 loss_input: 37.4205322265625
step: 1000 epoch: 1002 loss: 16.711079221148115 loss_input: 84.1644559054227
step: 2000 epoch: 1002 loss: 16.71645534187481 loss_input: 83.13464562884573
step: 3000 epoch: 1002 loss: 16.7879983856852 loss_input: 83.06348099624344
step: 4000 epoch: 1002 loss: 16.710316590206887 loss_input: 82.74489036127198
step: 5000 epoch: 1002 loss: 16.73950957145912 loss_input: 82.85312927648845
step: 6000 epoch: 1002 loss: 16.72339842689373 loss_input: 82.71453981506012
step: 7000 epoch: 1002 loss: 16.70016316008081 loss_input: 82.46581305035386
step: 8000 epoch: 1002 loss: 16.686767880938348 loss_input: 82.40943996227766
step: 9000 epoch: 1002 loss: 16.66521555062811 loss_input: 82.35749603803788
step: 10000 epoch: 1002 loss: 16.66160155892217 loss_input: 82.25352801633899
step: 11000 epoch: 1002 loss: 16.650857756803408 loss_input: 82.17763718812317
step: 12000 epoch: 1002 loss: 16.648633327020047 loss_input: 82.23281216230028
step: 13000 epoch: 1002 loss: 16.66308129676424 loss_input: 82.32238294902045
step: 14000 epoch: 1002 loss: 16.652839754813144 loss_input: 82.28852528948825
step: 15000 epoch: 1002 loss: 16.650590360518272 loss_input: 82.34309679647659
Save loss: 16.65655182287097 Name: 1002_train_model.pth
step: 0 epoch: 1003 loss: 17.72494888305664 loss_input: 83.262939453125
step: 1000 epoch: 1003 loss: 16.775580047012923 loss_input: 81.7075926392943
step: 2000 epoch: 1003 loss: 16.732318782615756 loss_input: 82.75526897195934
step: 3000 epoch: 1003 loss: 16.742411713090114 loss_input: 82.961618303339
step: 4000 epoch: 1003 loss: 16.67086067070993 loss_input: 82.56547488799187
step: 5000 epoch: 1003 loss: 16.619815040840862 loss_input: 82.33144928631461
step: 6000 epoch: 1003 loss: 16.612660379970777 loss_input: 82.34890750630103
step: 7000 epoch: 1003 loss: 16.633761004198792 loss_input: 82.13625035465078
step: 8000 epoch: 1003 loss: 16.62981934175538 loss_input: 82.04175664412799
step: 9000 epoch: 1003 loss: 16.648520852496738 loss_input: 82.20124679072117
step: 10000 epoch: 1003 loss: 16.645722964491537 loss_input: 82.21519616837145
step: 11000 epoch: 1003 loss: 16.650632883698147 loss_input: 82.31035870407378
step: 12000 epoch: 1003 loss: 16.679031768388306 loss_input: 82.34446094437605
step: 13000 epoch: 1003 loss: 16.674943174548208 loss_input: 82.23669871135213
step: 14000 epoch: 1003 loss: 16.669563206934296 loss_input: 82.22793146000872
step: 15000 epoch: 1003 loss: 16.667944136559743 loss_input: 82.25899193390363
Save loss: 16.653586852297188 Name: 1003_train_model.pth
step: 0 epoch: 1004 loss: 16.957088470458984 loss_input: 74.26861572265625
step: 1000 epoch: 1004 loss: 16.285734218793674 loss_input: 82.77470051920736
step: 2000 epoch: 1004 loss: 16.427347114716454 loss_input: 82.08305002772052
step: 3000 epoch: 1004 loss: 16.54895283523618 loss_input: 82.41412890446023
step: 4000 epoch: 1004 loss: 16.601882671719938 loss_input: 82.56990476501969
step: 5000 epoch: 1004 loss: 16.608152631091443 loss_input: 82.64440423682842
step: 6000 epoch: 1004 loss: 16.613217804793855 loss_input: 82.50373492235343
step: 7000 epoch: 1004 loss: 16.613635709159393 loss_input: 82.5020602525532
step: 8000 epoch: 1004 loss: 16.637381246277368 loss_input: 82.4895600360746
step: 9000 epoch: 1004 loss: 16.639298519258062 loss_input: 82.47039000874902
step: 10000 epoch: 1004 loss: 16.637522952745655 loss_input: 82.39740410114263
step: 11000 epoch: 1004 loss: 16.632984045711108 loss_input: 82.31767136048711
step: 12000 epoch: 1004 loss: 16.62933517312936 loss_input: 82.10399770152617
step: 13000 epoch: 1004 loss: 16.636903085558977 loss_input: 82.12634764648513
step: 14000 epoch: 1004 loss: 16.655572552109145 loss_input: 82.1919683523582
step: 15000 epoch: 1004 loss: 16.663778184342483 loss_input: 82.24591013593886
Save loss: 16.66241316495836 Name: 1004_train_model.pth
step: 0 epoch: 1005 loss: 10.722980499267578 loss_input: 66.982666015625
step: 1000 epoch: 1005 loss: 16.5903633960358 loss_input: 83.21045202356237
step: 2000 epoch: 1005 loss: 16.525478328364542 loss_input: 82.71394164368012
step: 3000 epoch: 1005 loss: 16.530996407083336 loss_input: 82.28667877309127
step: 4000 epoch: 1005 loss: 16.586193453875282 loss_input: 82.34880058671855
step: 5000 epoch: 1005 loss: 16.570637540897355 loss_input: 82.23438173693387
step: 6000 epoch: 1005 loss: 16.57535078314578 loss_input: 82.1730214560594
step: 7000 epoch: 1005 loss: 16.627085394117596 loss_input: 82.37401446661359
step: 8000 epoch: 1005 loss: 16.64772328971669 loss_input: 82.44043775076211
step: 9000 epoch: 1005 loss: 16.640698556197563 loss_input: 82.33334424159034
step: 10000 epoch: 1005 loss: 16.64063166689961 loss_input: 82.26956175842376
step: 11000 epoch: 1005 loss: 16.638643173594527 loss_input: 82.14946645919262
step: 12000 epoch: 1005 loss: 16.639182783843935 loss_input: 82.22310769960251
step: 13000 epoch: 1005 loss: 16.654932255671948 loss_input: 82.15801489297247
step: 14000 epoch: 1005 loss: 16.64907357715026 loss_input: 82.15482368847957
step: 15000 epoch: 1005 loss: 16.65533653224884 loss_input: 82.21473948855883
Save loss: 16.650131070449948 Name: 1005_train_model.pth
step: 0 epoch: 1006 loss: 23.35077667236328 loss_input: 97.80615234375
step: 1000 epoch: 1006 loss: 16.588249109841726 loss_input: 83.59375628034076
step: 2000 epoch: 1006 loss: 16.592134343690123 loss_input: 82.80225127914558
step: 3000 epoch: 1006 loss: 16.666485147689112 loss_input: 82.52444934940307
step: 4000 epoch: 1006 loss: 16.641068693579093 loss_input: 82.30184766430463
step: 5000 epoch: 1006 loss: 16.592963403855485 loss_input: 81.94864642794097
step: 6000 epoch: 1006 loss: 16.629434540358766 loss_input: 82.00464809006442
step: 7000 epoch: 1006 loss: 16.631536805651866 loss_input: 82.1024558485925
step: 8000 epoch: 1006 loss: 16.61272716265949 loss_input: 81.9697678171088
step: 9000 epoch: 1006 loss: 16.63646458925214 loss_input: 82.0564788319961
step: 10000 epoch: 1006 loss: 16.65272085374146 loss_input: 82.13476620477601
step: 11000 epoch: 1006 loss: 16.645766650402656 loss_input: 82.18154939683218
step: 12000 epoch: 1006 loss: 16.65236197329692 loss_input: 82.27015312098828
step: 13000 epoch: 1006 loss: 16.646079928496647 loss_input: 82.20727363952022
step: 14000 epoch: 1006 loss: 16.647828583768092 loss_input: 82.23578373029159
step: 15000 epoch: 1006 loss: 16.651683409796835 loss_input: 82.26049394151718
Save loss: 16.649668965131045 Name: 1006_train_model.pth
step: 0 epoch: 1007 loss: 14.389934539794922 loss_input: 73.2030029296875
step: 1000 epoch: 1007 loss: 16.544976664589836 loss_input: 81.49968573906563
step: 2000 epoch: 1007 loss: 16.507009264828262 loss_input: 81.25682895246713
step: 3000 epoch: 1007 loss: 16.594361820684913 loss_input: 82.09729520498375
step: 4000 epoch: 1007 loss: 16.56380763437652 loss_input: 81.73228130040243
step: 5000 epoch: 1007 loss: 16.60432945602156 loss_input: 81.83272497111906
step: 6000 epoch: 1007 loss: 16.61779835100592 loss_input: 81.99266335249463
step: 7000 epoch: 1007 loss: 16.615080770603708 loss_input: 81.91132071290318
step: 8000 epoch: 1007 loss: 16.616361846865424 loss_input: 81.90399780959997
step: 9000 epoch: 1007 loss: 16.62506793337575 loss_input: 82.04277535107862
step: 10000 epoch: 1007 loss: 16.638731503484248 loss_input: 82.04094628989249
step: 11000 epoch: 1007 loss: 16.661530169646596 loss_input: 82.14518889950878
step: 12000 epoch: 1007 loss: 16.665960437684703 loss_input: 82.1253349177689
step: 13000 epoch: 1007 loss: 16.648736646859227 loss_input: 82.05692854324457
step: 14000 epoch: 1007 loss: 16.661541343254665 loss_input: 82.18996301551145
step: 15000 epoch: 1007 loss: 16.64863390617709 loss_input: 82.1650238861029
Save loss: 16.65651112176478 Name: 1007_train_model.pth
step: 0 epoch: 1008 loss: 17.004505157470703 loss_input: 65.59979248046875
step: 1000 epoch: 1008 loss: 16.458837344334437 loss_input: 81.78006630820352
step: 2000 epoch: 1008 loss: 16.52942872988707 loss_input: 81.72900317419415
step: 3000 epoch: 1008 loss: 16.528030219772425 loss_input: 82.13519047117758
step: 4000 epoch: 1008 loss: 16.556100682418545 loss_input: 82.34713161614381
step: 5000 epoch: 1008 loss: 16.593556274535345 loss_input: 82.38466586467862
step: 6000 epoch: 1008 loss: 16.620857381240622 loss_input: 82.28738990685797
step: 7000 epoch: 1008 loss: 16.625629607787594 loss_input: 82.40292455604836
step: 8000 epoch: 1008 loss: 16.62506739805079 loss_input: 82.3500669594035
step: 9000 epoch: 1008 loss: 16.629382923223908 loss_input: 82.23325385449793
step: 10000 epoch: 1008 loss: 16.62741032379554 loss_input: 82.24444289579831
step: 11000 epoch: 1008 loss: 16.64212748779187 loss_input: 82.23690935434314
step: 12000 epoch: 1008 loss: 16.640892405677224 loss_input: 82.24273361232756
step: 13000 epoch: 1008 loss: 16.624999002643424 loss_input: 82.1847073446649
step: 14000 epoch: 1008 loss: 16.65309396696707 loss_input: 82.25920223663572
step: 15000 epoch: 1008 loss: 16.655341219786017 loss_input: 82.25183535127606
Save loss: 16.649187913641335 Name: 1008_train_model.pth
step: 0 epoch: 1009 loss: 20.153345108032227 loss_input: 77.51055908203125
step: 1000 epoch: 1009 loss: 16.507911753583027 loss_input: 82.17373172481815
step: 2000 epoch: 1009 loss: 16.471910337041106 loss_input: 82.07206943403298
step: 3000 epoch: 1009 loss: 16.397664767350804 loss_input: 81.79976307539414
step: 4000 epoch: 1009 loss: 16.458778388617606 loss_input: 81.7724390618654
step: 5000 epoch: 1009 loss: 16.527873951348035 loss_input: 81.99439553007367
step: 6000 epoch: 1009 loss: 16.544574039496098 loss_input: 82.02113041991373
step: 7000 epoch: 1009 loss: 16.58159063863952 loss_input: 82.14171736452549
step: 8000 epoch: 1009 loss: 16.601553312555996 loss_input: 82.3106977808194
step: 9000 epoch: 1009 loss: 16.586448150003715 loss_input: 82.35623045871422
step: 10000 epoch: 1009 loss: 16.602114184810308 loss_input: 82.39097056480875
step: 11000 epoch: 1009 loss: 16.61275932916933 loss_input: 82.27862850092463
step: 12000 epoch: 1009 loss: 16.621442164592807 loss_input: 82.3153747044924
step: 13000 epoch: 1009 loss: 16.64320720979667 loss_input: 82.36469016637392
step: 14000 epoch: 1009 loss: 16.64934386512942 loss_input: 82.24281417730067
step: 15000 epoch: 1009 loss: 16.639887068511786 loss_input: 82.22986717470788
Save loss: 16.65032762989402 Name: 1009_train_model.pth
step: 0 epoch: 1010 loss: 14.899381637573242 loss_input: 60.789306640625
step: 1000 epoch: 1010 loss: 16.4256471596755 loss_input: 82.15389840872018
step: 2000 epoch: 1010 loss: 16.450910141204727 loss_input: 82.17916502564148
step: 3000 epoch: 1010 loss: 16.535255087808306 loss_input: 82.32017330486192
step: 4000 epoch: 1010 loss: 16.575139658178756 loss_input: 82.32995780900966
step: 5000 epoch: 1010 loss: 16.556403490572638 loss_input: 82.15009077042014
step: 6000 epoch: 1010 loss: 16.61910913284968 loss_input: 82.4645104518713
step: 7000 epoch: 1010 loss: 16.613145115715728 loss_input: 82.48898986284604
step: 8000 epoch: 1010 loss: 16.62505766716976 loss_input: 82.43799285068614
step: 9000 epoch: 1010 loss: 16.638803180436692 loss_input: 82.48154471416153
step: 10000 epoch: 1010 loss: 16.66922975716478 loss_input: 82.48751278436609
step: 11000 epoch: 1010 loss: 16.671595583156222 loss_input: 82.33804762644093
step: 12000 epoch: 1010 loss: 16.67137090665739 loss_input: 82.43807146523596
step: 13000 epoch: 1010 loss: 16.662621203023427 loss_input: 82.41794219705456
step: 14000 epoch: 1010 loss: 16.664485566727254 loss_input: 82.37939107865063
step: 15000 epoch: 1010 loss: 16.651875913099452 loss_input: 82.23451980235822
Save loss: 16.65218549968302 Name: 1010_train_model.pth
step: 0 epoch: 1011 loss: 19.242691040039062 loss_input: 108.84063720703125
step: 1000 epoch: 1011 loss: 16.62566653593675 loss_input: 82.55931879399897
step: 2000 epoch: 1011 loss: 16.632666612374432 loss_input: 82.56103591880817
step: 3000 epoch: 1011 loss: 16.62207961527358 loss_input: 82.11455605293027
step: 4000 epoch: 1011 loss: 16.61852726230798 loss_input: 82.14602455953394
step: 5000 epoch: 1011 loss: 16.582968019576246 loss_input: 82.07849582427265
step: 6000 epoch: 1011 loss: 16.63024574799292 loss_input: 82.27390462592172
step: 7000 epoch: 1011 loss: 16.633636824728132 loss_input: 82.3805545392232
step: 8000 epoch: 1011 loss: 16.62603449207621 loss_input: 82.2776185340724
step: 9000 epoch: 1011 loss: 16.59418942996918 loss_input: 82.18718130304315
step: 10000 epoch: 1011 loss: 16.570837143957228 loss_input: 82.16638627274968
step: 11000 epoch: 1011 loss: 16.608277311195906 loss_input: 82.33682558621268
step: 12000 epoch: 1011 loss: 16.62186696360324 loss_input: 82.41243403463123
step: 13000 epoch: 1011 loss: 16.63629561363298 loss_input: 82.39558529301833
step: 14000 epoch: 1011 loss: 16.64088734098268 loss_input: 82.30911476555386
step: 15000 epoch: 1011 loss: 16.645447512180294 loss_input: 82.31548282930227
Save loss: 16.653578357174993 Name: 1011_train_model.pth
step: 0 epoch: 1012 loss: 19.200265884399414 loss_input: 74.81866455078125
step: 1000 epoch: 1012 loss: 16.661227328198535 loss_input: 83.80630965713974
step: 2000 epoch: 1012 loss: 16.622959193678156 loss_input: 83.10212011303918
step: 3000 epoch: 1012 loss: 16.656753618532083 loss_input: 82.56700952384361
step: 4000 epoch: 1012 loss: 16.62432021529816 loss_input: 82.57439894176846
step: 5000 epoch: 1012 loss: 16.663513154798544 loss_input: 82.71805372812156
step: 6000 epoch: 1012 loss: 16.638560771068082 loss_input: 82.44841140978457
step: 7000 epoch: 1012 loss: 16.647935710588772 loss_input: 82.55619421660465
step: 8000 epoch: 1012 loss: 16.64257239300137 loss_input: 82.42836792465151
step: 9000 epoch: 1012 loss: 16.637846313546916 loss_input: 82.37159031754719
step: 10000 epoch: 1012 loss: 16.656113095288752 loss_input: 82.30312565056971
step: 11000 epoch: 1012 loss: 16.64206584125592 loss_input: 82.16551210299241
step: 12000 epoch: 1012 loss: 16.62231803065528 loss_input: 82.08467815975459
step: 13000 epoch: 1012 loss: 16.62096731509257 loss_input: 82.03057897007471
step: 14000 epoch: 1012 loss: 16.626569893145405 loss_input: 82.11826285392283
step: 15000 epoch: 1012 loss: 16.636281950633833 loss_input: 82.22599513059806
Save loss: 16.645258744597434 Name: 1012_train_model.pth
step: 0 epoch: 1013 loss: 13.371442794799805 loss_input: 103.00946044921875
step: 1000 epoch: 1013 loss: 16.572837799579112 loss_input: 81.31643557858158
step: 2000 epoch: 1013 loss: 16.707660907629073 loss_input: 82.97518077509098
step: 3000 epoch: 1013 loss: 16.620688455734836 loss_input: 82.2989847907143
step: 4000 epoch: 1013 loss: 16.594194704102026 loss_input: 81.91939170358778
step: 5000 epoch: 1013 loss: 16.619750411814152 loss_input: 81.96143342132355
step: 6000 epoch: 1013 loss: 16.572954073764347 loss_input: 81.79360001279083
step: 7000 epoch: 1013 loss: 16.602211256603432 loss_input: 82.00738058102469
step: 8000 epoch: 1013 loss: 16.629553191200852 loss_input: 82.17890971178652
step: 9000 epoch: 1013 loss: 16.653933567519243 loss_input: 82.32879144342034
step: 10000 epoch: 1013 loss: 16.61910190852615 loss_input: 82.39380520566108
step: 11000 epoch: 1013 loss: 16.633902710574876 loss_input: 82.29529412232056
step: 12000 epoch: 1013 loss: 16.63097764779981 loss_input: 82.22399875388803
step: 13000 epoch: 1013 loss: 16.64372227896306 loss_input: 82.21703677635524
step: 14000 epoch: 1013 loss: 16.649567143047427 loss_input: 82.2243631898433
step: 15000 epoch: 1013 loss: 16.645498248563037 loss_input: 82.15139593314221
Save loss: 16.65310595433414 Name: 1013_train_model.pth
step: 0 epoch: 1014 loss: 13.672211647033691 loss_input: 49.4547119140625
step: 1000 epoch: 1014 loss: 16.630525873376655 loss_input: 83.01307713282812
step: 2000 epoch: 1014 loss: 16.63258685546181 loss_input: 82.65023454459293
step: 3000 epoch: 1014 loss: 16.61751710252339 loss_input: 82.54488351701777
step: 4000 epoch: 1014 loss: 16.600937471959448 loss_input: 82.14342457781551
step: 5000 epoch: 1014 loss: 16.569218121250017 loss_input: 81.78543890831209
step: 6000 epoch: 1014 loss: 16.631165117089935 loss_input: 82.09395406755124
step: 7000 epoch: 1014 loss: 16.614429949454077 loss_input: 82.10616237649644
step: 8000 epoch: 1014 loss: 16.63713761616194 loss_input: 82.1075495293313
step: 9000 epoch: 1014 loss: 16.633049820041858 loss_input: 82.18380820371087
step: 10000 epoch: 1014 loss: 16.662571926281913 loss_input: 82.310167738109
step: 11000 epoch: 1014 loss: 16.622225312468853 loss_input: 82.10333406191589
step: 12000 epoch: 1014 loss: 16.637354775176785 loss_input: 82.24440151837933
step: 13000 epoch: 1014 loss: 16.616080303502425 loss_input: 82.24065339137074
step: 14000 epoch: 1014 loss: 16.63764045819343 loss_input: 82.32668365746615
step: 15000 epoch: 1014 loss: 16.64351757982001 loss_input: 82.25298791522623
Save loss: 16.651819826141 Name: 1014_train_model.pth
step: 0 epoch: 1015 loss: 19.670921325683594 loss_input: 49.6595458984375
step: 1000 epoch: 1015 loss: 16.315644631971725 loss_input: 83.4524473329405
step: 2000 epoch: 1015 loss: 16.517147400449478 loss_input: 82.68251942754209
step: 3000 epoch: 1015 loss: 16.546134232124142 loss_input: 82.4738811224709
step: 4000 epoch: 1015 loss: 16.620218984903737 loss_input: 82.38876136068522
step: 5000 epoch: 1015 loss: 16.622290012002633 loss_input: 82.52264149220937
step: 6000 epoch: 1015 loss: 16.613474349740386 loss_input: 82.38889430090101
step: 7000 epoch: 1015 loss: 16.625758881331205 loss_input: 82.34443224942474
step: 8000 epoch: 1015 loss: 16.648325627005974 loss_input: 82.2212774321789
step: 9000 epoch: 1015 loss: 16.65234947146316 loss_input: 82.3771214141354
step: 10000 epoch: 1015 loss: 16.639231896092923 loss_input: 82.2985847838556
step: 11000 epoch: 1015 loss: 16.634618637876265 loss_input: 82.20866364776498
step: 12000 epoch: 1015 loss: 16.632050392240515 loss_input: 82.13277702627157
step: 13000 epoch: 1015 loss: 16.63083830617701 loss_input: 82.15516775621377
step: 14000 epoch: 1015 loss: 16.646289702390195 loss_input: 82.18876974572966
step: 15000 epoch: 1015 loss: 16.638698574606796 loss_input: 82.13502523904005
Save loss: 16.64634258237481 Name: 1015_train_model.pth
step: 0 epoch: 1016 loss: 22.586360931396484 loss_input: 140.07354736328125
step: 1000 epoch: 1016 loss: 16.489402043593156 loss_input: 82.38411899379916
step: 2000 epoch: 1016 loss: 16.650988094571947 loss_input: 82.260885761953
step: 3000 epoch: 1016 loss: 16.592537074754812 loss_input: 81.84936610892072
step: 4000 epoch: 1016 loss: 16.583831024658558 loss_input: 82.07646443491666
step: 5000 epoch: 1016 loss: 16.5666195382311 loss_input: 82.05368716295803
step: 6000 epoch: 1016 loss: 16.58380916451955 loss_input: 82.21626820538049
step: 7000 epoch: 1016 loss: 16.621815481997782 loss_input: 82.39986530070202
step: 8000 epoch: 1016 loss: 16.602180292659693 loss_input: 82.51229462100929
step: 9000 epoch: 1016 loss: 16.611898440703353 loss_input: 82.52157794506122
step: 10000 epoch: 1016 loss: 16.62082476146745 loss_input: 82.39674059327466
step: 11000 epoch: 1016 loss: 16.61613366931322 loss_input: 82.21890275156093
step: 12000 epoch: 1016 loss: 16.611423577083052 loss_input: 82.2428339050766
step: 13000 epoch: 1016 loss: 16.630795996974555 loss_input: 82.32698435006202
step: 14000 epoch: 1016 loss: 16.63552561384228 loss_input: 82.25033636687782
step: 15000 epoch: 1016 loss: 16.65024354858213 loss_input: 82.25034139976606
Save loss: 16.66027559246123 Name: 1016_train_model.pth
step: 0 epoch: 1017 loss: 17.024627685546875 loss_input: 108.26220703125
step: 1000 epoch: 1017 loss: 16.52034387054977 loss_input: 82.30285931109906
step: 2000 epoch: 1017 loss: 16.508952011411516 loss_input: 81.5694490706307
step: 3000 epoch: 1017 loss: 16.56307940751622 loss_input: 81.80052661482631
step: 4000 epoch: 1017 loss: 16.57260483558462 loss_input: 82.10774433466828
step: 5000 epoch: 1017 loss: 16.629739784617538 loss_input: 82.14646221146396
step: 6000 epoch: 1017 loss: 16.595496050299573 loss_input: 81.86125227094313
step: 7000 epoch: 1017 loss: 16.59452018613833 loss_input: 82.01455779580317
step: 8000 epoch: 1017 loss: 16.582309468241576 loss_input: 81.91060027103546
step: 9000 epoch: 1017 loss: 16.618091756907347 loss_input: 82.0182554179093
step: 10000 epoch: 1017 loss: 16.611309393181013 loss_input: 81.98422724866185
step: 11000 epoch: 1017 loss: 16.629703505647562 loss_input: 82.03318741264998
step: 12000 epoch: 1017 loss: 16.637750216279763 loss_input: 82.07484281298498
step: 13000 epoch: 1017 loss: 16.637773852872442 loss_input: 82.0327235241595
step: 14000 epoch: 1017 loss: 16.647419651153555 loss_input: 82.15045339517462
step: 15000 epoch: 1017 loss: 16.650786643854403 loss_input: 82.20575099273981
Save loss: 16.6482400226444 Name: 1017_train_model.pth
step: 0 epoch: 1018 loss: 21.536972045898438 loss_input: 113.11322021484375
step: 1000 epoch: 1018 loss: 16.40668887739534 loss_input: 81.87351436405392
step: 2000 epoch: 1018 loss: 16.50573913315902 loss_input: 81.93829897795243
step: 3000 epoch: 1018 loss: 16.61807998995986 loss_input: 81.96760953136064
step: 4000 epoch: 1018 loss: 16.640741010511913 loss_input: 81.97949275193409
step: 5000 epoch: 1018 loss: 16.673560933813146 loss_input: 81.95736032236913
step: 6000 epoch: 1018 loss: 16.6725798031426 loss_input: 82.28160009017051
step: 7000 epoch: 1018 loss: 16.655046549716552 loss_input: 82.21422452344977
step: 8000 epoch: 1018 loss: 16.66768326703913 loss_input: 82.08350829162713
step: 9000 epoch: 1018 loss: 16.668027454767607 loss_input: 82.11227280356437
step: 10000 epoch: 1018 loss: 16.670186255195357 loss_input: 82.11364263353938
step: 11000 epoch: 1018 loss: 16.678252471294112 loss_input: 82.23251334173811
step: 12000 epoch: 1018 loss: 16.68338869190367 loss_input: 82.35789339840983
step: 13000 epoch: 1018 loss: 16.651967079673728 loss_input: 82.2400818082353
step: 14000 epoch: 1018 loss: 16.645798526672916 loss_input: 82.11804027896585
step: 15000 epoch: 1018 loss: 16.6599529103735 loss_input: 82.21586143524677
Save loss: 16.649179106891154 Name: 1018_train_model.pth
step: 0 epoch: 1019 loss: 20.930828094482422 loss_input: 119.3809814453125
step: 1000 epoch: 1019 loss: 16.71382771314798 loss_input: 83.06441666243913
step: 2000 epoch: 1019 loss: 16.484286270994712 loss_input: 82.38880584122002
step: 3000 epoch: 1019 loss: 16.553950140135402 loss_input: 82.78735467490654
step: 4000 epoch: 1019 loss: 16.528982672325466 loss_input: 82.39046492954398
step: 5000 epoch: 1019 loss: 16.561755686134273 loss_input: 82.39197814733929
step: 6000 epoch: 1019 loss: 16.564267889576502 loss_input: 82.2988790401795
step: 7000 epoch: 1019 loss: 16.555557284350396 loss_input: 82.39686086453331
step: 8000 epoch: 1019 loss: 16.580198021147226 loss_input: 82.38943695423082
step: 9000 epoch: 1019 loss: 16.605024337079865 loss_input: 82.4270528711328
step: 10000 epoch: 1019 loss: 16.63154846157459 loss_input: 82.57511994772203
step: 11000 epoch: 1019 loss: 16.638728050413636 loss_input: 82.50664097564284
step: 12000 epoch: 1019 loss: 16.651307883118402 loss_input: 82.4904282421045
step: 13000 epoch: 1019 loss: 16.648874460756847 loss_input: 82.41080985807582
step: 14000 epoch: 1019 loss: 16.653852928230553 loss_input: 82.35785298783067
step: 15000 epoch: 1019 loss: 16.644490799580595 loss_input: 82.28736379732888
Save loss: 16.639897080302237 Name: 1019_train_model.pth
step: 0 epoch: 1020 loss: 19.124202728271484 loss_input: 68.36322021484375
step: 1000 epoch: 1020 loss: 16.437120296142915 loss_input: 81.26069920070164
step: 2000 epoch: 1020 loss: 16.424469520067465 loss_input: 81.70796086941881
step: 3000 epoch: 1020 loss: 16.48092229935933 loss_input: 81.76168417493648
step: 4000 epoch: 1020 loss: 16.49104984603325 loss_input: 81.70443962800803
step: 5000 epoch: 1020 loss: 16.54854809563295 loss_input: 81.97724701885795
step: 6000 epoch: 1020 loss: 16.56731153929001 loss_input: 82.02799218115341
step: 7000 epoch: 1020 loss: 16.54570562726105 loss_input: 81.8964778713253
step: 8000 epoch: 1020 loss: 16.576405310121242 loss_input: 81.93632052859847
step: 9000 epoch: 1020 loss: 16.59518556141374 loss_input: 82.02958104345828
step: 10000 epoch: 1020 loss: 16.598023797163854 loss_input: 82.10722307285873
step: 11000 epoch: 1020 loss: 16.581351668214204 loss_input: 82.01551977940748
step: 12000 epoch: 1020 loss: 16.600019808044415 loss_input: 82.10318766974497
step: 13000 epoch: 1020 loss: 16.600872705059963 loss_input: 82.07172011126097
step: 14000 epoch: 1020 loss: 16.62475144465509 loss_input: 82.13634082986817
step: 15000 epoch: 1020 loss: 16.62823427480806 loss_input: 82.14454960408239
Save loss: 16.653996354430912 Name: 1020_train_model.pth
step: 0 epoch: 1021 loss: 24.860149383544922 loss_input: 84.4273681640625
step: 1000 epoch: 1021 loss: 16.730493089178584 loss_input: 81.80419336522853
step: 2000 epoch: 1021 loss: 16.60508429771778 loss_input: 82.05975695623867
step: 3000 epoch: 1021 loss: 16.588626464022592 loss_input: 82.27820987567947
step: 4000 epoch: 1021 loss: 16.636597217663738 loss_input: 82.77651956944845
step: 5000 epoch: 1021 loss: 16.62606340473925 loss_input: 82.49756595311797
step: 6000 epoch: 1021 loss: 16.650581821046895 loss_input: 82.42919226190166
step: 7000 epoch: 1021 loss: 16.649968962280465 loss_input: 82.31195234002564
step: 8000 epoch: 1021 loss: 16.656899180863444 loss_input: 82.32694038702925
step: 9000 epoch: 1021 loss: 16.640684226237063 loss_input: 82.27319334581314
step: 10000 epoch: 1021 loss: 16.667005934294266 loss_input: 82.2497674060719
step: 11000 epoch: 1021 loss: 16.64267875081723 loss_input: 82.18715683606177
step: 12000 epoch: 1021 loss: 16.65307795319018 loss_input: 82.25772136176153
step: 13000 epoch: 1021 loss: 16.64138805102444 loss_input: 82.28800315672082
step: 14000 epoch: 1021 loss: 16.64126334518341 loss_input: 82.23904530762451
step: 15000 epoch: 1021 loss: 16.64851845192056 loss_input: 82.23300817217654
Save loss: 16.64270732474327 Name: 1021_train_model.pth
step: 0 epoch: 1022 loss: 20.720699310302734 loss_input: 83.72772216796875
step: 1000 epoch: 1022 loss: 16.439881778739906 loss_input: 81.33436930501139
step: 2000 epoch: 1022 loss: 16.47453538564847 loss_input: 81.73074180683096
step: 3000 epoch: 1022 loss: 16.522460324650645 loss_input: 81.71558400210998
step: 4000 epoch: 1022 loss: 16.487638814840814 loss_input: 81.79489945018628
step: 5000 epoch: 1022 loss: 16.473654310742848 loss_input: 81.9897994888327
step: 6000 epoch: 1022 loss: 16.48316286075435 loss_input: 82.09533547997535
step: 7000 epoch: 1022 loss: 16.533150066087764 loss_input: 82.24928104753171
step: 8000 epoch: 1022 loss: 16.583239249416685 loss_input: 82.34905738101097
step: 9000 epoch: 1022 loss: 16.58083417855055 loss_input: 82.38208100711249
step: 10000 epoch: 1022 loss: 16.588105817423287 loss_input: 82.36898007586936
step: 11000 epoch: 1022 loss: 16.613337717146432 loss_input: 82.46826690626669
step: 12000 epoch: 1022 loss: 16.623804039303913 loss_input: 82.49805154888622
step: 13000 epoch: 1022 loss: 16.62131413552644 loss_input: 82.4744071153556
step: 14000 epoch: 1022 loss: 16.6299126369767 loss_input: 82.3735727817363
step: 15000 epoch: 1022 loss: 16.639448636691178 loss_input: 82.27852881503863
Save loss: 16.649370550125838 Name: 1022_train_model.pth
step: 0 epoch: 1023 loss: 16.242414474487305 loss_input: 72.4688720703125
step: 1000 epoch: 1023 loss: 16.649551081967044 loss_input: 82.35943999847808
step: 2000 epoch: 1023 loss: 16.529631268912585 loss_input: 81.64672546539231
step: 3000 epoch: 1023 loss: 16.546174531140274 loss_input: 82.06313258129451
step: 4000 epoch: 1023 loss: 16.524457462666422 loss_input: 81.84601588250398
step: 5000 epoch: 1023 loss: 16.507468231199645 loss_input: 81.65520315126427
step: 6000 epoch: 1023 loss: 16.514989258308965 loss_input: 81.69373133245696
step: 7000 epoch: 1023 loss: 16.543518051830738 loss_input: 81.6808411176333
step: 8000 epoch: 1023 loss: 16.56429703982677 loss_input: 81.8096056092964
step: 9000 epoch: 1023 loss: 16.58587490141544 loss_input: 81.95379208760876
step: 10000 epoch: 1023 loss: 16.6139369260763 loss_input: 82.07873374039549
step: 11000 epoch: 1023 loss: 16.641032858573677 loss_input: 82.14876708207635
step: 12000 epoch: 1023 loss: 16.64899182440827 loss_input: 82.11960075104419
step: 13000 epoch: 1023 loss: 16.631697563251784 loss_input: 82.16412946450032
step: 14000 epoch: 1023 loss: 16.62973788680115 loss_input: 82.22427230626802
step: 15000 epoch: 1023 loss: 16.63567144010442 loss_input: 82.26333366512544
Save loss: 16.64123991280794 Name: 1023_train_model.pth
step: 0 epoch: 1024 loss: 10.165005683898926 loss_input: 49.86785888671875
step: 1000 epoch: 1024 loss: 16.60815578454977 loss_input: 84.04772918731659
step: 2000 epoch: 1024 loss: 16.608269546104633 loss_input: 83.15102052855408
step: 3000 epoch: 1024 loss: 16.59277148311911 loss_input: 82.479138769336
step: 4000 epoch: 1024 loss: 16.712655919219696 loss_input: 82.51047218969184
step: 5000 epoch: 1024 loss: 16.71052346690086 loss_input: 82.5090285409715
step: 6000 epoch: 1024 loss: 16.71350652483022 loss_input: 82.83795620651766
step: 7000 epoch: 1024 loss: 16.677705048867725 loss_input: 82.52216030382937
step: 8000 epoch: 1024 loss: 16.67755901859695 loss_input: 82.3868286071785
step: 9000 epoch: 1024 loss: 16.678034241047715 loss_input: 82.3200526208351
step: 10000 epoch: 1024 loss: 16.660065899061188 loss_input: 82.32506069461414
step: 11000 epoch: 1024 loss: 16.67136656068778 loss_input: 82.33746095436636
step: 12000 epoch: 1024 loss: 16.661006337771287 loss_input: 82.2757152032578
step: 13000 epoch: 1024 loss: 16.652289098578684 loss_input: 82.19887638319439
step: 14000 epoch: 1024 loss: 16.638595522067334 loss_input: 82.2513383051113
step: 15000 epoch: 1024 loss: 16.638835496135126 loss_input: 82.25224176542527
Save loss: 16.641836331933735 Name: 1024_train_model.pth
step: 0 epoch: 1025 loss: 23.37816047668457 loss_input: 81.07440185546875
step: 1000 epoch: 1025 loss: 16.46793341898656 loss_input: 80.55756566431616
step: 2000 epoch: 1025 loss: 16.509017485132937 loss_input: 80.75027064714713
step: 3000 epoch: 1025 loss: 16.533072830001263 loss_input: 81.35943141836836
step: 4000 epoch: 1025 loss: 16.607600539900368 loss_input: 81.88874322251957
step: 5000 epoch: 1025 loss: 16.614838545476406 loss_input: 82.00800251326687
step: 6000 epoch: 1025 loss: 16.597075998097772 loss_input: 81.97966235567183
step: 7000 epoch: 1025 loss: 16.62155652270966 loss_input: 82.01505283355168
step: 8000 epoch: 1025 loss: 16.61061142412011 loss_input: 81.88503085063586
step: 9000 epoch: 1025 loss: 16.62768932186674 loss_input: 82.00393929347477
step: 10000 epoch: 1025 loss: 16.62424097281911 loss_input: 82.02021174835642
step: 11000 epoch: 1025 loss: 16.622032443216394 loss_input: 82.00215651449122
step: 12000 epoch: 1025 loss: 16.63559280732365 loss_input: 82.23415231653058
step: 13000 epoch: 1025 loss: 16.647668740408445 loss_input: 82.29385539585147
step: 14000 epoch: 1025 loss: 16.648965801360667 loss_input: 82.29721554312806
step: 15000 epoch: 1025 loss: 16.65075702146247 loss_input: 82.3405409243876
Save loss: 16.643686113864185 Name: 1025_train_model.pth
step: 0 epoch: 1026 loss: 10.903640747070312 loss_input: 60.70538330078125
step: 1000 epoch: 1026 loss: 16.624907820374816 loss_input: 81.58509136127545
step: 2000 epoch: 1026 loss: 16.646525646793073 loss_input: 82.39010151906469
step: 3000 epoch: 1026 loss: 16.648673607325087 loss_input: 82.4956451893965
step: 4000 epoch: 1026 loss: 16.61096396609504 loss_input: 82.61056496434973
step: 5000 epoch: 1026 loss: 16.56737287600883 loss_input: 82.53775378957411
step: 6000 epoch: 1026 loss: 16.57568631548024 loss_input: 82.41827177973593
step: 7000 epoch: 1026 loss: 16.600836358876112 loss_input: 82.31006313586062
step: 8000 epoch: 1026 loss: 16.5997912024665 loss_input: 82.31936407119032
step: 9000 epoch: 1026 loss: 16.609735194133027 loss_input: 82.23855090048589
step: 10000 epoch: 1026 loss: 16.625735588233933 loss_input: 82.34456015458025
step: 11000 epoch: 1026 loss: 16.634207575551574 loss_input: 82.41731766394817
step: 12000 epoch: 1026 loss: 16.635037235911316 loss_input: 82.33409819099548
step: 13000 epoch: 1026 loss: 16.63575320214127 loss_input: 82.27381965397193
step: 14000 epoch: 1026 loss: 16.640918748718136 loss_input: 82.28093424845352
step: 15000 epoch: 1026 loss: 16.630876015253158 loss_input: 82.22934989555384
Save loss: 16.642986213117837 Name: 1026_train_model.pth
step: 0 epoch: 1027 loss: 15.332406997680664 loss_input: 94.07952880859375
step: 1000 epoch: 1027 loss: 16.539883434951125 loss_input: 81.6953196339793
step: 2000 epoch: 1027 loss: 16.575249259916323 loss_input: 81.95915286985414
step: 3000 epoch: 1027 loss: 16.599856797237074 loss_input: 82.01300591860004
step: 4000 epoch: 1027 loss: 16.61474006279085 loss_input: 82.33792488767456
step: 5000 epoch: 1027 loss: 16.627282560455683 loss_input: 82.25313766592384
step: 6000 epoch: 1027 loss: 16.65520936531775 loss_input: 82.12711170859544
step: 7000 epoch: 1027 loss: 16.65588524307188 loss_input: 82.21921709635788
step: 8000 epoch: 1027 loss: 16.64252424675172 loss_input: 82.28582257143394
step: 9000 epoch: 1027 loss: 16.64061343877822 loss_input: 82.27459716118781
step: 10000 epoch: 1027 loss: 16.65203818854374 loss_input: 82.2627092637416
step: 11000 epoch: 1027 loss: 16.65398459009296 loss_input: 82.14523113754574
step: 12000 epoch: 1027 loss: 16.63715639041668 loss_input: 82.1041827243563
step: 13000 epoch: 1027 loss: 16.63073612700865 loss_input: 82.10143694156189
step: 14000 epoch: 1027 loss: 16.625334199552288 loss_input: 82.13169285515599
step: 15000 epoch: 1027 loss: 16.646207271166766 loss_input: 82.26712920066397
Save loss: 16.638854111611842 Name: 1027_train_model.pth
step: 0 epoch: 1028 loss: 11.12359619140625 loss_input: 56.44049072265625
step: 1000 epoch: 1028 loss: 16.43988750888394 loss_input: 81.28654676622206
step: 2000 epoch: 1028 loss: 16.50890140292765 loss_input: 81.49549257963791
step: 3000 epoch: 1028 loss: 16.51821283664913 loss_input: 81.7417796362801
step: 4000 epoch: 1028 loss: 16.534222400537047 loss_input: 81.5983159208977
step: 5000 epoch: 1028 loss: 16.55072906579382 loss_input: 81.62340445729214
step: 6000 epoch: 1028 loss: 16.594530129388975 loss_input: 81.91577781063182
step: 7000 epoch: 1028 loss: 16.59487266261958 loss_input: 82.1185155660659
step: 8000 epoch: 1028 loss: 16.61403381921339 loss_input: 82.25248814570786
step: 9000 epoch: 1028 loss: 16.62436059238831 loss_input: 82.14770837365727
step: 10000 epoch: 1028 loss: 16.628453774996228 loss_input: 82.03263250945413
step: 11000 epoch: 1028 loss: 16.637884571924825 loss_input: 82.1424583238793
step: 12000 epoch: 1028 loss: 16.656190585061 loss_input: 82.1904031953636
step: 13000 epoch: 1028 loss: 16.646935780354074 loss_input: 82.2084238102102
step: 14000 epoch: 1028 loss: 16.62797251071907 loss_input: 82.15318906577124
step: 15000 epoch: 1028 loss: 16.619228941815255 loss_input: 82.08799222955449
Save loss: 16.640705339550973 Name: 1028_train_model.pth
step: 0 epoch: 1029 loss: 11.763620376586914 loss_input: 69.8028564453125
step: 1000 epoch: 1029 loss: 16.76159157214703 loss_input: 82.56339266202548
step: 2000 epoch: 1029 loss: 16.662425327396345 loss_input: 81.94635415446574
step: 3000 epoch: 1029 loss: 16.6949604822373 loss_input: 81.99281896268953
step: 4000 epoch: 1029 loss: 16.66998864072348 loss_input: 82.2167143608385
step: 5000 epoch: 1029 loss: 16.635145207734805 loss_input: 81.91489532903967
step: 6000 epoch: 1029 loss: 16.67143615768902 loss_input: 82.29390011332747
step: 7000 epoch: 1029 loss: 16.68651998293364 loss_input: 82.42967516394143
step: 8000 epoch: 1029 loss: 16.67045412032608 loss_input: 82.40632239390486
step: 9000 epoch: 1029 loss: 16.68788865894334 loss_input: 82.35629285683753
step: 10000 epoch: 1029 loss: 16.651864775775994 loss_input: 82.16785546787118
step: 11000 epoch: 1029 loss: 16.668940122079203 loss_input: 82.32166995938047
step: 12000 epoch: 1029 loss: 16.66260740775067 loss_input: 82.24675997459197
step: 13000 epoch: 1029 loss: 16.665583028948113 loss_input: 82.23433272936336
step: 14000 epoch: 1029 loss: 16.687183002226437 loss_input: 82.28184692042784
step: 15000 epoch: 1029 loss: 16.668971918080775 loss_input: 82.2487189005108
Save loss: 16.65583407497406 Name: 1029_train_model.pth
step: 0 epoch: 1030 loss: 10.119699478149414 loss_input: 74.7215576171875
step: 1000 epoch: 1030 loss: 16.49826708992759 loss_input: 81.79897257283731
step: 2000 epoch: 1030 loss: 16.55898174031385 loss_input: 82.09145988529173
step: 3000 epoch: 1030 loss: 16.590862561606915 loss_input: 82.10924743835388
step: 4000 epoch: 1030 loss: 16.566271549044654 loss_input: 81.93547191205128
step: 5000 epoch: 1030 loss: 16.562282111448805 loss_input: 82.13970136880828
step: 6000 epoch: 1030 loss: 16.58350171186272 loss_input: 82.16649812921804
step: 7000 epoch: 1030 loss: 16.61499802505369 loss_input: 82.28224834022038
step: 8000 epoch: 1030 loss: 16.61061981734209 loss_input: 82.09841028324828
step: 9000 epoch: 1030 loss: 16.581612830850208 loss_input: 81.97528080240433
step: 10000 epoch: 1030 loss: 16.604134635464714 loss_input: 82.06391503183666
step: 11000 epoch: 1030 loss: 16.606634125905885 loss_input: 82.04154585023521
step: 12000 epoch: 1030 loss: 16.63302247232819 loss_input: 82.10624570348304
step: 13000 epoch: 1030 loss: 16.640219007984562 loss_input: 82.0986948757848
step: 14000 epoch: 1030 loss: 16.640612550330463 loss_input: 82.09479741925522
step: 15000 epoch: 1030 loss: 16.648186268039435 loss_input: 82.27015547596307
Save loss: 16.64495599731803 Name: 1030_train_model.pth
step: 0 epoch: 1031 loss: 15.034719467163086 loss_input: 99.37274169921875
step: 1000 epoch: 1031 loss: 16.668572301035756 loss_input: 82.77940040916116
step: 2000 epoch: 1031 loss: 16.66408962443255 loss_input: 82.29348783860023
step: 3000 epoch: 1031 loss: 16.61807651815316 loss_input: 82.15051993573756
step: 4000 epoch: 1031 loss: 16.595616833801955 loss_input: 81.98151432600834
step: 5000 epoch: 1031 loss: 16.62128517232497 loss_input: 82.09167913731707
step: 6000 epoch: 1031 loss: 16.599151027260213 loss_input: 81.90944511567407
step: 7000 epoch: 1031 loss: 16.571137303404257 loss_input: 81.79158038981318
step: 8000 epoch: 1031 loss: 16.59299544757075 loss_input: 81.80556258134969
step: 9000 epoch: 1031 loss: 16.583696840789738 loss_input: 81.84145443122422
step: 10000 epoch: 1031 loss: 16.58074329807906 loss_input: 81.94581122942394
step: 11000 epoch: 1031 loss: 16.599299076827762 loss_input: 81.91467867156786
step: 12000 epoch: 1031 loss: 16.621635488088643 loss_input: 82.0224263232784
step: 13000 epoch: 1031 loss: 16.629339767468817 loss_input: 82.0456300818657
step: 14000 epoch: 1031 loss: 16.629305156994935 loss_input: 82.11491151336567
step: 15000 epoch: 1031 loss: 16.636133024544886 loss_input: 82.14975649165707
Save loss: 16.641594417244196 Name: 1031_train_model.pth
step: 0 epoch: 1032 loss: 14.616950035095215 loss_input: 46.6058349609375
step: 1000 epoch: 1032 loss: 16.438495077215112 loss_input: 82.30684098616227
step: 2000 epoch: 1032 loss: 16.485533187414394 loss_input: 82.60535726399496
step: 3000 epoch: 1032 loss: 16.530894864125873 loss_input: 82.66800367081416
step: 4000 epoch: 1032 loss: 16.51280085684746 loss_input: 82.49429892587888
step: 5000 epoch: 1032 loss: 16.50115269366514 loss_input: 82.3368582509084
step: 6000 epoch: 1032 loss: 16.544241494207537 loss_input: 82.36376169971021
step: 7000 epoch: 1032 loss: 16.572516872140106 loss_input: 82.32267579240911
step: 8000 epoch: 1032 loss: 16.59337194432856 loss_input: 82.24179008873891
step: 9000 epoch: 1032 loss: 16.59862526974775 loss_input: 82.03676890447503
step: 10000 epoch: 1032 loss: 16.616528752731952 loss_input: 82.11379911813506
step: 11000 epoch: 1032 loss: 16.60395119662892 loss_input: 82.01614797935628
step: 12000 epoch: 1032 loss: 16.599525981282763 loss_input: 82.02844679732092
step: 13000 epoch: 1032 loss: 16.61788203763775 loss_input: 82.0375045237656
step: 14000 epoch: 1032 loss: 16.625311443681557 loss_input: 82.0905926632682
step: 15000 epoch: 1032 loss: 16.62418501547643 loss_input: 82.13301773940982
Save loss: 16.638498026922345 Name: 1032_train_model.pth
step: 0 epoch: 1033 loss: 14.131111145019531 loss_input: 42.14117431640625
step: 1000 epoch: 1033 loss: 16.63932933721628 loss_input: 82.38268146648274
step: 2000 epoch: 1033 loss: 16.77990593950728 loss_input: 82.76759435223795
step: 3000 epoch: 1033 loss: 16.709505759331037 loss_input: 82.40505002157484
step: 4000 epoch: 1033 loss: 16.69234922092517 loss_input: 82.42957035704453
step: 5000 epoch: 1033 loss: 16.667779286607125 loss_input: 82.58080870398186
step: 6000 epoch: 1033 loss: 16.628107345058527 loss_input: 82.39333489584418
step: 7000 epoch: 1033 loss: 16.641034487502402 loss_input: 82.24524840306016
step: 8000 epoch: 1033 loss: 16.64489595977355 loss_input: 82.34151607971611
step: 9000 epoch: 1033 loss: 16.655428743961057 loss_input: 82.45625635237577
step: 10000 epoch: 1033 loss: 16.63064417420906 loss_input: 82.39038611397649
step: 11000 epoch: 1033 loss: 16.62077417816642 loss_input: 82.2846504199636
step: 12000 epoch: 1033 loss: 16.62751249531569 loss_input: 82.36597287445285
step: 13000 epoch: 1033 loss: 16.64291959501727 loss_input: 82.34098331077898
step: 14000 epoch: 1033 loss: 16.63517288561932 loss_input: 82.29614946588636
step: 15000 epoch: 1033 loss: 16.639983754103664 loss_input: 82.24625223552805
Save loss: 16.641020227894188 Name: 1033_train_model.pth
step: 0 epoch: 1034 loss: 11.773056030273438 loss_input: 68.78997802734375
step: 1000 epoch: 1034 loss: 16.41352353610478 loss_input: 82.48039204447896
step: 2000 epoch: 1034 loss: 16.56050883037695 loss_input: 82.19901166970226
step: 3000 epoch: 1034 loss: 16.498940554351577 loss_input: 81.99162459762762
step: 4000 epoch: 1034 loss: 16.445767343833847 loss_input: 82.03392968896448
step: 5000 epoch: 1034 loss: 16.464957253214504 loss_input: 82.14340778084618
step: 6000 epoch: 1034 loss: 16.483322982608506 loss_input: 82.06308405996461
step: 7000 epoch: 1034 loss: 16.495833779178234 loss_input: 82.17398871171306
step: 8000 epoch: 1034 loss: 16.536555795159405 loss_input: 82.29936745577702
step: 9000 epoch: 1034 loss: 16.555169023390466 loss_input: 82.27208808803145
step: 10000 epoch: 1034 loss: 16.557886089256865 loss_input: 82.0934690283413
step: 11000 epoch: 1034 loss: 16.564002240357556 loss_input: 82.11227588707312
step: 12000 epoch: 1034 loss: 16.5939069442099 loss_input: 82.23224567101505
step: 13000 epoch: 1034 loss: 16.604410032741512 loss_input: 82.21535570637445
step: 14000 epoch: 1034 loss: 16.629503912827296 loss_input: 82.2100824759727
step: 15000 epoch: 1034 loss: 16.632811943266155 loss_input: 82.25555413411892
Save loss: 16.643539206787946 Name: 1034_train_model.pth
step: 0 epoch: 1035 loss: 16.609554290771484 loss_input: 114.09521484375
step: 1000 epoch: 1035 loss: 16.585613920972065 loss_input: 82.32782988495879
step: 2000 epoch: 1035 loss: 16.539229014109278 loss_input: 82.02177118838041
step: 3000 epoch: 1035 loss: 16.510308564166074 loss_input: 82.12482297567477
step: 4000 epoch: 1035 loss: 16.58121988511747 loss_input: 82.22563187023069
step: 5000 epoch: 1035 loss: 16.557919598989216 loss_input: 82.46673316703847
step: 6000 epoch: 1035 loss: 16.569936003849477 loss_input: 82.499109208117
step: 7000 epoch: 1035 loss: 16.579417884698888 loss_input: 82.3902447791359
step: 8000 epoch: 1035 loss: 16.590662289255782 loss_input: 82.22848703360322
step: 9000 epoch: 1035 loss: 16.58923072206247 loss_input: 82.11760550368535
step: 10000 epoch: 1035 loss: 16.603778233684046 loss_input: 82.328057953482
step: 11000 epoch: 1035 loss: 16.631059177961472 loss_input: 82.37339304603346
step: 12000 epoch: 1035 loss: 16.622268130308232 loss_input: 82.30572198003682
step: 13000 epoch: 1035 loss: 16.619591011394768 loss_input: 82.21883334883194
step: 14000 epoch: 1035 loss: 16.63080352909624 loss_input: 82.22991278123689
step: 15000 epoch: 1035 loss: 16.630485272329654 loss_input: 82.24358064442839
Save loss: 16.642471518844367 Name: 1035_train_model.pth
step: 0 epoch: 1036 loss: 19.721866607666016 loss_input: 123.97418212890625
step: 1000 epoch: 1036 loss: 16.661246613189057 loss_input: 82.96323994608906
step: 2000 epoch: 1036 loss: 16.70737511345531 loss_input: 82.60462764833403
step: 3000 epoch: 1036 loss: 16.666235439779758 loss_input: 82.43202406245445
step: 4000 epoch: 1036 loss: 16.6046951870059 loss_input: 82.34107964708042
step: 5000 epoch: 1036 loss: 16.592496119268272 loss_input: 82.09700317016674
step: 6000 epoch: 1036 loss: 16.61750038451621 loss_input: 82.20911697212249
step: 7000 epoch: 1036 loss: 16.644020390091683 loss_input: 82.24856128428361
step: 8000 epoch: 1036 loss: 16.62193236886196 loss_input: 82.15577616701721
step: 9000 epoch: 1036 loss: 16.619265417829645 loss_input: 82.25251413173059
step: 10000 epoch: 1036 loss: 16.610871558188915 loss_input: 82.14930961737811
step: 11000 epoch: 1036 loss: 16.628765140356947 loss_input: 82.28511927715378
step: 12000 epoch: 1036 loss: 16.640668144822865 loss_input: 82.35752423259976
step: 13000 epoch: 1036 loss: 16.660586429425326 loss_input: 82.37635873055588
step: 14000 epoch: 1036 loss: 16.637306802894855 loss_input: 82.36500118940305
step: 15000 epoch: 1036 loss: 16.635939143227066 loss_input: 82.26592071596268
Save loss: 16.636904751658438 Name: 1036_train_model.pth
step: 0 epoch: 1037 loss: 13.979326248168945 loss_input: 72.01318359375
step: 1000 epoch: 1037 loss: 16.488430676998554 loss_input: 81.50770067335009
step: 2000 epoch: 1037 loss: 16.395779687723238 loss_input: 81.45315104898722
step: 3000 epoch: 1037 loss: 16.476096429494334 loss_input: 81.62709722166178
step: 4000 epoch: 1037 loss: 16.5042631051684 loss_input: 81.56195173618704
step: 5000 epoch: 1037 loss: 16.555205900224298 loss_input: 81.95799702364215
step: 6000 epoch: 1037 loss: 16.5601020163247 loss_input: 81.94982026311044
step: 7000 epoch: 1037 loss: 16.54919673200846 loss_input: 81.94746473683168
step: 8000 epoch: 1037 loss: 16.569042315201198 loss_input: 81.96793204041633
step: 9000 epoch: 1037 loss: 16.592781034499694 loss_input: 82.07814045374082
step: 10000 epoch: 1037 loss: 16.633498226186177 loss_input: 82.28515580448791
step: 11000 epoch: 1037 loss: 16.65586682241967 loss_input: 82.35964817962217
step: 12000 epoch: 1037 loss: 16.64589173924316 loss_input: 82.35527016832256
step: 13000 epoch: 1037 loss: 16.623521575028416 loss_input: 82.22091787711041
step: 14000 epoch: 1037 loss: 16.642592373681694 loss_input: 82.24493464438645
step: 15000 epoch: 1037 loss: 16.660440467244886 loss_input: 82.24144597228674
Save loss: 16.646107042625548 Name: 1037_train_model.pth
step: 0 epoch: 1038 loss: 16.466304779052734 loss_input: 78.00518798828125
step: 1000 epoch: 1038 loss: 16.49621989557912 loss_input: 82.5790821727101
step: 2000 epoch: 1038 loss: 16.61039999816967 loss_input: 83.01344682883168
step: 3000 epoch: 1038 loss: 16.617302201422643 loss_input: 82.95216415898477
step: 4000 epoch: 1038 loss: 16.619264271878446 loss_input: 82.83260064195377
step: 5000 epoch: 1038 loss: 16.623588828176672 loss_input: 82.62661006617036
step: 6000 epoch: 1038 loss: 16.62674345812824 loss_input: 82.53970234697212
step: 7000 epoch: 1038 loss: 16.65741530048423 loss_input: 82.61447141375172
step: 8000 epoch: 1038 loss: 16.664850649632836 loss_input: 82.6569148415164
step: 9000 epoch: 1038 loss: 16.646121512226443 loss_input: 82.46683294955075
step: 10000 epoch: 1038 loss: 16.637594965431358 loss_input: 82.41242470601573
step: 11000 epoch: 1038 loss: 16.65134558674639 loss_input: 82.33743921672959
step: 12000 epoch: 1038 loss: 16.638851771661813 loss_input: 82.27969292455609
step: 13000 epoch: 1038 loss: 16.639878254891837 loss_input: 82.27445897153044
step: 14000 epoch: 1038 loss: 16.634814625271353 loss_input: 82.27684329746536
step: 15000 epoch: 1038 loss: 16.637924655883538 loss_input: 82.26532235090578
Save loss: 16.637659534722566 Name: 1038_train_model.pth
step: 0 epoch: 1039 loss: 23.17557144165039 loss_input: 119.52764892578125
step: 1000 epoch: 1039 loss: 16.51750287285575 loss_input: 82.86185232004324
step: 2000 epoch: 1039 loss: 16.60771665187075 loss_input: 82.7679256685134
step: 3000 epoch: 1039 loss: 16.585252443260845 loss_input: 82.78382234404938
step: 4000 epoch: 1039 loss: 16.628010869830646 loss_input: 82.62511358854623
step: 5000 epoch: 1039 loss: 16.642103391131315 loss_input: 82.71782939554667
step: 6000 epoch: 1039 loss: 16.66319721219063 loss_input: 82.72131602944106
step: 7000 epoch: 1039 loss: 16.69704887264542 loss_input: 82.90869700324619
step: 8000 epoch: 1039 loss: 16.674042396255768 loss_input: 82.85034420746234
step: 9000 epoch: 1039 loss: 16.65479916117613 loss_input: 82.65920714348796
step: 10000 epoch: 1039 loss: 16.670756245694058 loss_input: 82.66505340432754
step: 11000 epoch: 1039 loss: 16.67614014945608 loss_input: 82.63156977628277
step: 12000 epoch: 1039 loss: 16.674611988648525 loss_input: 82.53621292463909
step: 13000 epoch: 1039 loss: 16.66718658765695 loss_input: 82.44658388188138
step: 14000 epoch: 1039 loss: 16.6522654456655 loss_input: 82.3497868002453
step: 15000 epoch: 1039 loss: 16.666203436299362 loss_input: 82.3624498926634
Save loss: 16.645902377232908 Name: 1039_train_model.pth
step: 0 epoch: 1040 loss: 15.650701522827148 loss_input: 112.16461181640625
step: 1000 epoch: 1040 loss: 16.395889556134023 loss_input: 81.90775813541927
step: 2000 epoch: 1040 loss: 16.555065431694935 loss_input: 82.22220987894724
step: 3000 epoch: 1040 loss: 16.573561746332892 loss_input: 82.14120506787451
step: 4000 epoch: 1040 loss: 16.62713821415185 loss_input: 82.14613430382639
step: 5000 epoch: 1040 loss: 16.63867794702206 loss_input: 82.19555811738043
step: 6000 epoch: 1040 loss: 16.618850576541877 loss_input: 82.14393257455137
step: 7000 epoch: 1040 loss: 16.595370885457776 loss_input: 82.01047224140972
step: 8000 epoch: 1040 loss: 16.622202401249993 loss_input: 82.13639205438676
step: 9000 epoch: 1040 loss: 16.603327987856527 loss_input: 82.26156664016499
step: 10000 epoch: 1040 loss: 16.614193513195296 loss_input: 82.28137987592842
step: 11000 epoch: 1040 loss: 16.618861021621477 loss_input: 82.21232724404315
step: 12000 epoch: 1040 loss: 16.625348486032955 loss_input: 82.13863325822295
step: 13000 epoch: 1040 loss: 16.62776161259793 loss_input: 82.13819725048944
step: 14000 epoch: 1040 loss: 16.639239212741394 loss_input: 82.25611533380288
step: 15000 epoch: 1040 loss: 16.64374072571912 loss_input: 82.21753604691328
Save loss: 16.63522032503784 Name: 1040_train_model.pth
step: 0 epoch: 1041 loss: 17.404043197631836 loss_input: 139.14141845703125
step: 1000 epoch: 1041 loss: 16.862601795158426 loss_input: 82.84766887165569
step: 2000 epoch: 1041 loss: 16.644993153171264 loss_input: 82.00324258036997
step: 3000 epoch: 1041 loss: 16.643419703019934 loss_input: 82.07959069795746
step: 4000 epoch: 1041 loss: 16.666677765713963 loss_input: 82.54477059140976
step: 5000 epoch: 1041 loss: 16.66472397587629 loss_input: 82.71514016288539
step: 6000 epoch: 1041 loss: 16.67006554112516 loss_input: 82.59873933161205
step: 7000 epoch: 1041 loss: 16.635185134868898 loss_input: 82.39445385975831
step: 8000 epoch: 1041 loss: 16.63683026895689 loss_input: 82.28470371562442
step: 9000 epoch: 1041 loss: 16.6296815455536 loss_input: 82.20630263900799
step: 10000 epoch: 1041 loss: 16.638129735395868 loss_input: 82.27115141879366
step: 11000 epoch: 1041 loss: 16.636907051372674 loss_input: 82.29390870583143
step: 12000 epoch: 1041 loss: 16.64345624236085 loss_input: 82.14712175779785
step: 13000 epoch: 1041 loss: 16.62693306847945 loss_input: 82.07386623805381
step: 14000 epoch: 1041 loss: 16.63092812108411 loss_input: 82.02241024270448
step: 15000 epoch: 1041 loss: 16.63668273062827 loss_input: 82.10719293111579
Save loss: 16.642794898599387 Name: 1041_train_model.pth
step: 0 epoch: 1042 loss: 6.717484951019287 loss_input: 60.2850341796875
step: 1000 epoch: 1042 loss: 16.474588397499566 loss_input: 82.32927381456435
step: 2000 epoch: 1042 loss: 16.551136497138202 loss_input: 82.12717014905633
step: 3000 epoch: 1042 loss: 16.519623171842245 loss_input: 81.71552906843552
step: 4000 epoch: 1042 loss: 16.527852525355904 loss_input: 81.8474681103894
step: 5000 epoch: 1042 loss: 16.528534089915873 loss_input: 81.91519022855586
step: 6000 epoch: 1042 loss: 16.580339900177133 loss_input: 82.15742517848588
step: 7000 epoch: 1042 loss: 16.639933105877954 loss_input: 82.41904275428566
step: 8000 epoch: 1042 loss: 16.646480422573617 loss_input: 82.4331876423162
step: 9000 epoch: 1042 loss: 16.612975422825606 loss_input: 82.3104181322518
step: 10000 epoch: 1042 loss: 16.62700506334674 loss_input: 82.43213604054634
step: 11000 epoch: 1042 loss: 16.62779049803567 loss_input: 82.31392101894323
step: 12000 epoch: 1042 loss: 16.64465366928848 loss_input: 82.27580908784945
step: 13000 epoch: 1042 loss: 16.644689473268865 loss_input: 82.26235842435197
step: 14000 epoch: 1042 loss: 16.636789733073975 loss_input: 82.21960580434963
step: 15000 epoch: 1042 loss: 16.632737411959045 loss_input: 82.20071276660983
Save loss: 16.639883380919695 Name: 1042_train_model.pth
step: 0 epoch: 1043 loss: 8.918928146362305 loss_input: 44.99761962890625
step: 1000 epoch: 1043 loss: 16.627363178756212 loss_input: 81.88299182506947
step: 2000 epoch: 1043 loss: 16.677761473100464 loss_input: 83.05657050015031
step: 3000 epoch: 1043 loss: 16.679940664065118 loss_input: 82.5092498870819
step: 4000 epoch: 1043 loss: 16.65123664257199 loss_input: 82.44204622672457
step: 5000 epoch: 1043 loss: 16.63981739699042 loss_input: 82.31063655432976
step: 6000 epoch: 1043 loss: 16.594283133899147 loss_input: 81.91665919449623
step: 7000 epoch: 1043 loss: 16.589551654513812 loss_input: 81.89532404445849
step: 8000 epoch: 1043 loss: 16.562791894665512 loss_input: 81.8826341699353
step: 9000 epoch: 1043 loss: 16.58394211539294 loss_input: 81.93930020823424
step: 10000 epoch: 1043 loss: 16.59653375444621 loss_input: 82.00642708556293
step: 11000 epoch: 1043 loss: 16.579514662468068 loss_input: 81.92088230903902
step: 12000 epoch: 1043 loss: 16.590162635455478 loss_input: 82.04996306561856
step: 13000 epoch: 1043 loss: 16.5947101994777 loss_input: 82.08104331705555
step: 14000 epoch: 1043 loss: 16.607571568423342 loss_input: 82.23652570348631
step: 15000 epoch: 1043 loss: 16.627334597237738 loss_input: 82.23717428847843
Save loss: 16.63622771447897 Name: 1043_train_model.pth
step: 0 epoch: 1044 loss: 19.791730880737305 loss_input: 83.41156005859375
step: 1000 epoch: 1044 loss: 16.71074014491254 loss_input: 82.48476816152598
step: 2000 epoch: 1044 loss: 16.614848296919924 loss_input: 82.90822600174522
step: 3000 epoch: 1044 loss: 16.662502068354662 loss_input: 82.41962963436016
step: 4000 epoch: 1044 loss: 16.66004589235029 loss_input: 82.51178294293614
step: 5000 epoch: 1044 loss: 16.669467612663 loss_input: 82.41926401096734
step: 6000 epoch: 1044 loss: 16.647557439336854 loss_input: 82.17053714924465
step: 7000 epoch: 1044 loss: 16.64684922493214 loss_input: 82.20409477112923
step: 8000 epoch: 1044 loss: 16.625897433069138 loss_input: 82.11307660181393
step: 9000 epoch: 1044 loss: 16.632697300121077 loss_input: 82.13873398832422
step: 10000 epoch: 1044 loss: 16.639774537374944 loss_input: 82.22168788370186
step: 11000 epoch: 1044 loss: 16.61507475221171 loss_input: 82.20279008615516
step: 12000 epoch: 1044 loss: 16.619728913059653 loss_input: 82.20811969587658
step: 13000 epoch: 1044 loss: 16.609207237915722 loss_input: 82.1357713647553
step: 14000 epoch: 1044 loss: 16.606523554918212 loss_input: 82.17216372362213
step: 15000 epoch: 1044 loss: 16.61896632533178 loss_input: 82.17748837902994
Save loss: 16.638092653006314 Name: 1044_train_model.pth
step: 0 epoch: 1045 loss: 20.861976623535156 loss_input: 141.03533935546875
step: 1000 epoch: 1045 loss: 16.504441971545454 loss_input: 82.3620197551472
step: 2000 epoch: 1045 loss: 16.543119179136095 loss_input: 82.333389183094
step: 3000 epoch: 1045 loss: 16.49968114911378 loss_input: 82.19211853698506
step: 4000 epoch: 1045 loss: 16.498526932745687 loss_input: 82.10598630554763
step: 5000 epoch: 1045 loss: 16.51466495088281 loss_input: 81.81973175677365
step: 6000 epoch: 1045 loss: 16.563185498826087 loss_input: 81.91462750999834
step: 7000 epoch: 1045 loss: 16.570721173555473 loss_input: 82.01320691456745
step: 8000 epoch: 1045 loss: 16.592111314092485 loss_input: 82.0250974106142
step: 9000 epoch: 1045 loss: 16.59185267654926 loss_input: 82.023651160342
step: 10000 epoch: 1045 loss: 16.623174720997216 loss_input: 82.22988590423184
step: 11000 epoch: 1045 loss: 16.60173411751192 loss_input: 82.13697626165732
step: 12000 epoch: 1045 loss: 16.624100256478744 loss_input: 82.17287731718972
step: 13000 epoch: 1045 loss: 16.6445778005628 loss_input: 82.26256200319327
step: 14000 epoch: 1045 loss: 16.652473044645767 loss_input: 82.31887383846187
step: 15000 epoch: 1045 loss: 16.649365026961483 loss_input: 82.30776600570378
Save loss: 16.645273375824093 Name: 1045_train_model.pth
step: 0 epoch: 1046 loss: 8.262914657592773 loss_input: 66.73822021484375
step: 1000 epoch: 1046 loss: 16.50541788595659 loss_input: 82.59293379959884
step: 2000 epoch: 1046 loss: 16.467870286081745 loss_input: 82.50684615577953
step: 3000 epoch: 1046 loss: 16.46375133267485 loss_input: 82.3553958576308
step: 4000 epoch: 1046 loss: 16.489474238231463 loss_input: 82.20472639103407
step: 5000 epoch: 1046 loss: 16.504005384359377 loss_input: 82.14450308180551
step: 6000 epoch: 1046 loss: 16.526328775291145 loss_input: 82.20796828865112
step: 7000 epoch: 1046 loss: 16.53900342865546 loss_input: 82.14789296981012
step: 8000 epoch: 1046 loss: 16.56115126577024 loss_input: 82.20056243885041
step: 9000 epoch: 1046 loss: 16.589201220537923 loss_input: 82.25988641371661
step: 10000 epoch: 1046 loss: 16.61788763083073 loss_input: 82.3332886865122
step: 11000 epoch: 1046 loss: 16.622870031936245 loss_input: 82.31259376921878
step: 12000 epoch: 1046 loss: 16.63227358041987 loss_input: 82.32508985253833
step: 13000 epoch: 1046 loss: 16.63144658976413 loss_input: 82.33622107569249
step: 14000 epoch: 1046 loss: 16.64527937805863 loss_input: 82.33714119297004
step: 15000 epoch: 1046 loss: 16.637170628765155 loss_input: 82.26156983007773
Save loss: 16.640926771059632 Name: 1046_train_model.pth
step: 0 epoch: 1047 loss: 16.815855026245117 loss_input: 67.76214599609375
step: 1000 epoch: 1047 loss: 16.673735109361616 loss_input: 82.30398391891312
step: 2000 epoch: 1047 loss: 16.65407864371876 loss_input: 82.27735509686563
step: 3000 epoch: 1047 loss: 16.61161574043063 loss_input: 82.2504850067881
step: 4000 epoch: 1047 loss: 16.598142680154087 loss_input: 82.25983707930439
step: 5000 epoch: 1047 loss: 16.6139323914964 loss_input: 82.41333909731726
step: 6000 epoch: 1047 loss: 16.60647813257943 loss_input: 82.33815418174119
step: 7000 epoch: 1047 loss: 16.570540820610656 loss_input: 82.2115044268927
step: 8000 epoch: 1047 loss: 16.59537980223042 loss_input: 82.23513731010675
step: 9000 epoch: 1047 loss: 16.587675081254535 loss_input: 82.33068956151139
step: 10000 epoch: 1047 loss: 16.61332065355133 loss_input: 82.31870679509197
step: 11000 epoch: 1047 loss: 16.62173104968876 loss_input: 82.30812766133477
step: 12000 epoch: 1047 loss: 16.623063640945723 loss_input: 82.20753925922025
step: 13000 epoch: 1047 loss: 16.604602150931356 loss_input: 82.10523572161367
step: 14000 epoch: 1047 loss: 16.6404905886269 loss_input: 82.24932814247225
step: 15000 epoch: 1047 loss: 16.647192141618913 loss_input: 82.20716793929233
Save loss: 16.64672509559989 Name: 1047_train_model.pth
step: 0 epoch: 1048 loss: 21.629108428955078 loss_input: 106.91204833984375
step: 1000 epoch: 1048 loss: 16.541590750634253 loss_input: 82.83383297610592
step: 2000 epoch: 1048 loss: 16.62696242129904 loss_input: 82.72994118175288
step: 3000 epoch: 1048 loss: 16.629990032218924 loss_input: 82.25606857447256
step: 4000 epoch: 1048 loss: 16.606337764328106 loss_input: 82.17174164100577
step: 5000 epoch: 1048 loss: 16.663457677402967 loss_input: 82.31368701845568
step: 6000 epoch: 1048 loss: 16.68408697792896 loss_input: 82.464052744139
step: 7000 epoch: 1048 loss: 16.653582280405146 loss_input: 82.37310806708138
step: 8000 epoch: 1048 loss: 16.64539693150129 loss_input: 82.36783032583574
step: 9000 epoch: 1048 loss: 16.616478017960212 loss_input: 82.25344806930089
step: 10000 epoch: 1048 loss: 16.629021055137454 loss_input: 82.19133037868089
step: 11000 epoch: 1048 loss: 16.628950208309032 loss_input: 82.08452014283326
step: 12000 epoch: 1048 loss: 16.65190826637647 loss_input: 82.10989873463507
step: 13000 epoch: 1048 loss: 16.65727050289192 loss_input: 82.15369806288939
step: 14000 epoch: 1048 loss: 16.646489604235835 loss_input: 82.13176056010647
step: 15000 epoch: 1048 loss: 16.64810931314652 loss_input: 82.21518713937951
Save loss: 16.64331894290447 Name: 1048_train_model.pth
step: 0 epoch: 1049 loss: 9.913225173950195 loss_input: 45.69854736328125
step: 1000 epoch: 1049 loss: 16.394222414576923 loss_input: 82.48381019615151
step: 2000 epoch: 1049 loss: 16.482897633257537 loss_input: 82.41945213916479
step: 3000 epoch: 1049 loss: 16.548091223223214 loss_input: 82.41299223907787
step: 4000 epoch: 1049 loss: 16.60242586003575 loss_input: 82.34902800789061
step: 5000 epoch: 1049 loss: 16.60772178445285 loss_input: 82.3443453277118
step: 6000 epoch: 1049 loss: 16.58364722645694 loss_input: 82.1890005230248
step: 7000 epoch: 1049 loss: 16.564914449285293 loss_input: 82.14728068283772
step: 8000 epoch: 1049 loss: 16.57040622627388 loss_input: 82.14974534280627
step: 9000 epoch: 1049 loss: 16.610457787234548 loss_input: 82.2522526387042
step: 10000 epoch: 1049 loss: 16.636548270727204 loss_input: 82.26315230415435
step: 11000 epoch: 1049 loss: 16.66367487456189 loss_input: 82.47041720683418
step: 12000 epoch: 1049 loss: 16.647830080105933 loss_input: 82.38708177211632
step: 13000 epoch: 1049 loss: 16.658437955692158 loss_input: 82.37427740872397
step: 14000 epoch: 1049 loss: 16.65692442708778 loss_input: 82.27573003033963
step: 15000 epoch: 1049 loss: 16.64198400672138 loss_input: 82.2849995872671
Save loss: 16.637126378044485 Name: 1049_train_model.pth
step: 0 epoch: 1050 loss: 15.072149276733398 loss_input: 83.3050537109375
step: 1000 epoch: 1050 loss: 16.582679599434226 loss_input: 83.61934775429648
step: 2000 epoch: 1050 loss: 16.574386481342763 loss_input: 83.32440274003623
step: 3000 epoch: 1050 loss: 16.535407208236446 loss_input: 82.96664509047115
step: 4000 epoch: 1050 loss: 16.607550938765247 loss_input: 82.86336510659933
step: 5000 epoch: 1050 loss: 16.5807580453018 loss_input: 82.48403285651463
step: 6000 epoch: 1050 loss: 16.58775125688522 loss_input: 82.21529173441796
step: 7000 epoch: 1050 loss: 16.57160574585144 loss_input: 82.06913282407758
step: 8000 epoch: 1050 loss: 16.59822991013214 loss_input: 82.15251568743116
step: 9000 epoch: 1050 loss: 16.598539187237762 loss_input: 82.11539108908688
step: 10000 epoch: 1050 loss: 16.627729232472642 loss_input: 82.14491791177328
step: 11000 epoch: 1050 loss: 16.631224634603807 loss_input: 82.11616835547798
step: 12000 epoch: 1050 loss: 16.629755979895084 loss_input: 82.12854384327022
step: 13000 epoch: 1050 loss: 16.621354917610308 loss_input: 82.0955946785644
step: 14000 epoch: 1050 loss: 16.61783647840342 loss_input: 82.0014589455594
step: 15000 epoch: 1050 loss: 16.627867578768775 loss_input: 82.13329644397405
Save loss: 16.64282619249821 Name: 1050_train_model.pth
step: 0 epoch: 1051 loss: 17.78807830810547 loss_input: 103.366943359375
step: 1000 epoch: 1051 loss: 16.499232300273427 loss_input: 83.23029918079968
step: 2000 epoch: 1051 loss: 16.58715493318023 loss_input: 82.43749743780454
step: 3000 epoch: 1051 loss: 16.52592868385455 loss_input: 81.98438486406224
step: 4000 epoch: 1051 loss: 16.55569748150292 loss_input: 81.89475978478525
step: 5000 epoch: 1051 loss: 16.59815814985654 loss_input: 82.14110503094693
step: 6000 epoch: 1051 loss: 16.592171609650013 loss_input: 82.1446364367332
step: 7000 epoch: 1051 loss: 16.621073763432836 loss_input: 82.21058864672513
step: 8000 epoch: 1051 loss: 16.599822036565325 loss_input: 82.12352253118019
step: 9000 epoch: 1051 loss: 16.622251386311884 loss_input: 82.19717994794198
step: 10000 epoch: 1051 loss: 16.62866550700067 loss_input: 82.24716678112462
step: 11000 epoch: 1051 loss: 16.62580428330663 loss_input: 82.28037459130135
step: 12000 epoch: 1051 loss: 16.633377041044696 loss_input: 82.28224465283004
step: 13000 epoch: 1051 loss: 16.639791562074734 loss_input: 82.21721875980244
step: 14000 epoch: 1051 loss: 16.63665598786292 loss_input: 82.23175393652127
step: 15000 epoch: 1051 loss: 16.648045548597388 loss_input: 82.2407163293606
Save loss: 16.64185678872466 Name: 1051_train_model.pth
step: 0 epoch: 1052 loss: 11.95026969909668 loss_input: 38.78900146484375
step: 1000 epoch: 1052 loss: 16.553812115104286 loss_input: 82.13950130679866
step: 2000 epoch: 1052 loss: 16.68494472475066 loss_input: 82.88046684055433
step: 3000 epoch: 1052 loss: 16.588633843637712 loss_input: 82.75329482050905
step: 4000 epoch: 1052 loss: 16.56175195786453 loss_input: 82.13505281516535
step: 5000 epoch: 1052 loss: 16.59819011873208 loss_input: 81.96315949212502
step: 6000 epoch: 1052 loss: 16.59016136152906 loss_input: 82.17353824614842
step: 7000 epoch: 1052 loss: 16.651608599711956 loss_input: 82.50352834814056
step: 8000 epoch: 1052 loss: 16.643671086483813 loss_input: 82.42563938388317
step: 9000 epoch: 1052 loss: 16.63157219707191 loss_input: 82.3384591246589
step: 10000 epoch: 1052 loss: 16.628546930529954 loss_input: 82.3718982532899
step: 11000 epoch: 1052 loss: 16.63218696641224 loss_input: 82.34227813229693
step: 12000 epoch: 1052 loss: 16.624789601732775 loss_input: 82.37207642567871
step: 13000 epoch: 1052 loss: 16.60419570903633 loss_input: 82.1388725572417
step: 14000 epoch: 1052 loss: 16.623756211771724 loss_input: 82.1413565630845
step: 15000 epoch: 1052 loss: 16.63676726949206 loss_input: 82.18439324910891
Save loss: 16.643409553915262 Name: 1052_train_model.pth
step: 0 epoch: 1053 loss: 19.334125518798828 loss_input: 77.53851318359375
step: 1000 epoch: 1053 loss: 16.911611079693316 loss_input: 82.6702322945609
step: 2000 epoch: 1053 loss: 16.73002556000633 loss_input: 82.62733571568708
step: 3000 epoch: 1053 loss: 16.731955921995525 loss_input: 82.32213771760324
step: 4000 epoch: 1053 loss: 16.737186622691137 loss_input: 82.10133895359168
step: 5000 epoch: 1053 loss: 16.673148809111467 loss_input: 81.8240115892837
step: 6000 epoch: 1053 loss: 16.680074500036408 loss_input: 81.99117630582077
step: 7000 epoch: 1053 loss: 16.6480142473238 loss_input: 82.0513922554172
step: 8000 epoch: 1053 loss: 16.656667140793346 loss_input: 82.11066755541056
step: 9000 epoch: 1053 loss: 16.636021380609385 loss_input: 82.04552634749568
step: 10000 epoch: 1053 loss: 16.64456780468651 loss_input: 82.11911188031587
step: 11000 epoch: 1053 loss: 16.645327873939102 loss_input: 82.15476802357456
step: 12000 epoch: 1053 loss: 16.643781275384057 loss_input: 82.13225018927459
step: 13000 epoch: 1053 loss: 16.622215091425698 loss_input: 82.08600606068163
step: 14000 epoch: 1053 loss: 16.637690119705542 loss_input: 82.1622687438481
step: 15000 epoch: 1053 loss: 16.633166924944465 loss_input: 82.15225575537802
Save loss: 16.637787302404643 Name: 1053_train_model.pth
step: 0 epoch: 1054 loss: 19.03383445739746 loss_input: 176.59368896484375
step: 1000 epoch: 1054 loss: 16.402372724645502 loss_input: 81.55544315303837
step: 2000 epoch: 1054 loss: 16.48611229053442 loss_input: 81.41879148926513
step: 3000 epoch: 1054 loss: 16.471593482460193 loss_input: 81.35677513826771
step: 4000 epoch: 1054 loss: 16.516995001065673 loss_input: 81.70904095570346
step: 5000 epoch: 1054 loss: 16.55951389744863 loss_input: 81.82757844183116
step: 6000 epoch: 1054 loss: 16.578125624711504 loss_input: 82.01736050990199
step: 7000 epoch: 1054 loss: 16.604983581541063 loss_input: 82.18284555714976
step: 8000 epoch: 1054 loss: 16.627483374028635 loss_input: 82.28724346773548
step: 9000 epoch: 1054 loss: 16.5812669047752 loss_input: 82.1552791131389
step: 10000 epoch: 1054 loss: 16.591367999382847 loss_input: 82.1263541065327
step: 11000 epoch: 1054 loss: 16.615622508613622 loss_input: 82.11054140896637
step: 12000 epoch: 1054 loss: 16.625358567993974 loss_input: 82.25627685628248
step: 13000 epoch: 1054 loss: 16.630390509799135 loss_input: 82.1866097533513
step: 14000 epoch: 1054 loss: 16.653164540961015 loss_input: 82.26268781879001
step: 15000 epoch: 1054 loss: 16.644483910982675 loss_input: 82.21445899374937
Save loss: 16.639147897988558 Name: 1054_train_model.pth
step: 0 epoch: 1055 loss: 22.395153045654297 loss_input: 51.71636962890625
step: 1000 epoch: 1055 loss: 16.482639683829202 loss_input: 83.27753569672515
step: 2000 epoch: 1055 loss: 16.46247206992474 loss_input: 82.37503352205734
step: 3000 epoch: 1055 loss: 16.503658976963226 loss_input: 82.23568458137652
step: 4000 epoch: 1055 loss: 16.455796962438658 loss_input: 81.82987373884067
step: 5000 epoch: 1055 loss: 16.49258957479363 loss_input: 81.74409742075022
step: 6000 epoch: 1055 loss: 16.546342157320346 loss_input: 81.94447951911987
step: 7000 epoch: 1055 loss: 16.534581644162845 loss_input: 81.76293864234518
step: 8000 epoch: 1055 loss: 16.56735127834272 loss_input: 81.92755783761893
step: 9000 epoch: 1055 loss: 16.602287799646398 loss_input: 82.1056624405014
step: 10000 epoch: 1055 loss: 16.61735689836721 loss_input: 82.1079667790057
step: 11000 epoch: 1055 loss: 16.612465488055697 loss_input: 82.07573466443569
step: 12000 epoch: 1055 loss: 16.63452510964462 loss_input: 81.99465504237766
step: 13000 epoch: 1055 loss: 16.638124660531993 loss_input: 82.04574633081108
step: 14000 epoch: 1055 loss: 16.652872122455413 loss_input: 82.08743100206848
step: 15000 epoch: 1055 loss: 16.641517208159886 loss_input: 82.13637280952103
Save loss: 16.655132879465818 Name: 1055_train_model.pth
step: 0 epoch: 1056 loss: 17.566970825195312 loss_input: 117.6209716796875
step: 1000 epoch: 1056 loss: 16.651403464756527 loss_input: 81.39127693595466
step: 2000 epoch: 1056 loss: 16.553561313815976 loss_input: 81.91299400348653
step: 3000 epoch: 1056 loss: 16.47921996051492 loss_input: 81.73803558400455
step: 4000 epoch: 1056 loss: 16.50982489784906 loss_input: 81.52330780219984
step: 5000 epoch: 1056 loss: 16.55955583127683 loss_input: 81.6264889478159
step: 6000 epoch: 1056 loss: 16.628860893854995 loss_input: 81.92310573156428
step: 7000 epoch: 1056 loss: 16.603942710933406 loss_input: 81.89964044443148
step: 8000 epoch: 1056 loss: 16.635529423755283 loss_input: 81.99229582386187
step: 9000 epoch: 1056 loss: 16.62438205975504 loss_input: 82.10519159302395
step: 10000 epoch: 1056 loss: 16.655246144806714 loss_input: 82.16901227357732
step: 11000 epoch: 1056 loss: 16.63903271526697 loss_input: 82.07604624277765
step: 12000 epoch: 1056 loss: 16.644601976878445 loss_input: 82.1611816721572
step: 13000 epoch: 1056 loss: 16.644817168011535 loss_input: 82.27979903363804
step: 14000 epoch: 1056 loss: 16.64268349352994 loss_input: 82.20525796532111
step: 15000 epoch: 1056 loss: 16.637731392807456 loss_input: 82.23458155768131
Save loss: 16.63480933883786 Name: 1056_train_model.pth
step: 0 epoch: 1057 loss: 27.487049102783203 loss_input: 110.72113037109375
step: 1000 epoch: 1057 loss: 16.713895092239152 loss_input: 82.04099696690028
step: 2000 epoch: 1057 loss: 16.685234150607727 loss_input: 82.15153382171219
step: 3000 epoch: 1057 loss: 16.631891439295497 loss_input: 82.02244625326713
step: 4000 epoch: 1057 loss: 16.648219633388447 loss_input: 82.11611603629854
step: 5000 epoch: 1057 loss: 16.673204820696245 loss_input: 82.34228791448741
step: 6000 epoch: 1057 loss: 16.643010816461263 loss_input: 82.42950984609661
step: 7000 epoch: 1057 loss: 16.657682737781734 loss_input: 82.42892174025363
step: 8000 epoch: 1057 loss: 16.63235091310846 loss_input: 82.34145126848158
step: 9000 epoch: 1057 loss: 16.645012808698983 loss_input: 82.42554213404456
step: 10000 epoch: 1057 loss: 16.64204824582277 loss_input: 82.31896688871151
step: 11000 epoch: 1057 loss: 16.649564282199098 loss_input: 82.19189142428813
step: 12000 epoch: 1057 loss: 16.6488928088406 loss_input: 82.17802590547467
step: 13000 epoch: 1057 loss: 16.63253045049083 loss_input: 82.15929604911702
step: 14000 epoch: 1057 loss: 16.646791935971734 loss_input: 82.13388370818798
step: 15000 epoch: 1057 loss: 16.63355189659986 loss_input: 82.13650555213637
Save loss: 16.64221768192947 Name: 1057_train_model.pth
step: 0 epoch: 1058 loss: 8.058273315429688 loss_input: 61.10589599609375
step: 1000 epoch: 1058 loss: 16.756417160148505 loss_input: 82.40887384100274
step: 2000 epoch: 1058 loss: 16.78083641036995 loss_input: 81.91933458319669
step: 3000 epoch: 1058 loss: 16.781719677609548 loss_input: 82.3214293559048
step: 4000 epoch: 1058 loss: 16.713545932974764 loss_input: 82.12492677611847
step: 5000 epoch: 1058 loss: 16.674348323494403 loss_input: 82.32917065268587
step: 6000 epoch: 1058 loss: 16.647891115296026 loss_input: 82.32409637456257
step: 7000 epoch: 1058 loss: 16.61569662646487 loss_input: 82.22914312499164
step: 8000 epoch: 1058 loss: 16.627661123199474 loss_input: 82.21454166373735
step: 9000 epoch: 1058 loss: 16.633472132266938 loss_input: 82.3008042576975
step: 10000 epoch: 1058 loss: 16.633976296441173 loss_input: 82.24095918044914
step: 11000 epoch: 1058 loss: 16.637466243197405 loss_input: 82.21622716360835
step: 12000 epoch: 1058 loss: 16.6400458742982 loss_input: 82.20231538017177
step: 13000 epoch: 1058 loss: 16.646492110757716 loss_input: 82.2670785812825
step: 14000 epoch: 1058 loss: 16.637653384683098 loss_input: 82.22286654904812
step: 15000 epoch: 1058 loss: 16.639367837697996 loss_input: 82.21831929547795
Save loss: 16.62968386888504 Name: 1058_train_model.pth
step: 0 epoch: 1059 loss: 19.377300262451172 loss_input: 86.256591796875
step: 1000 epoch: 1059 loss: 16.65191946020136 loss_input: 82.64753197242212
step: 2000 epoch: 1059 loss: 16.650563281991968 loss_input: 82.9493111720507
step: 3000 epoch: 1059 loss: 16.499088281951163 loss_input: 82.41422864334935
step: 4000 epoch: 1059 loss: 16.561709043175778 loss_input: 82.48433814397963
step: 5000 epoch: 1059 loss: 16.559423300724607 loss_input: 82.11090733513454
step: 6000 epoch: 1059 loss: 16.545176471914576 loss_input: 81.95363333812854
step: 7000 epoch: 1059 loss: 16.574016171001634 loss_input: 82.08859964759499
step: 8000 epoch: 1059 loss: 16.586658706129857 loss_input: 82.05858425259099
step: 9000 epoch: 1059 loss: 16.59125991072102 loss_input: 82.16368469367119
step: 10000 epoch: 1059 loss: 16.614222969797634 loss_input: 82.26381412151754
step: 11000 epoch: 1059 loss: 16.621550483862254 loss_input: 82.27583085375583
step: 12000 epoch: 1059 loss: 16.609488290226984 loss_input: 82.19782682192663
step: 13000 epoch: 1059 loss: 16.597161670178966 loss_input: 82.14207038442206
step: 14000 epoch: 1059 loss: 16.609611244459135 loss_input: 82.2204790242731
step: 15000 epoch: 1059 loss: 16.63173194205266 loss_input: 82.2798782652771
Save loss: 16.63635918587446 Name: 1059_train_model.pth
step: 0 epoch: 1060 loss: 15.021387100219727 loss_input: 59.5589599609375
step: 1000 epoch: 1060 loss: 16.474741417449433 loss_input: 82.38996733247221
step: 2000 epoch: 1060 loss: 16.53938081799478 loss_input: 82.12296403067997
step: 3000 epoch: 1060 loss: 16.712055392838923 loss_input: 82.18559619531119
step: 4000 epoch: 1060 loss: 16.6320117995966 loss_input: 81.95171484533651
step: 5000 epoch: 1060 loss: 16.582080434022295 loss_input: 81.62436487311912
step: 6000 epoch: 1060 loss: 16.593097438297356 loss_input: 81.85288315188029
step: 7000 epoch: 1060 loss: 16.60083957107352 loss_input: 82.04883379174069
step: 8000 epoch: 1060 loss: 16.586158045171693 loss_input: 81.96205429613046
step: 9000 epoch: 1060 loss: 16.591441266789143 loss_input: 81.99645312575946
step: 10000 epoch: 1060 loss: 16.613844345383807 loss_input: 81.95885334562247
step: 11000 epoch: 1060 loss: 16.633141376898294 loss_input: 82.03570148297065
step: 12000 epoch: 1060 loss: 16.61532263270657 loss_input: 81.96598860291915
step: 13000 epoch: 1060 loss: 16.634519607578277 loss_input: 82.10092360496007
step: 14000 epoch: 1060 loss: 16.638726520245438 loss_input: 82.13343433843035
step: 15000 epoch: 1060 loss: 16.638483501594788 loss_input: 82.18370570135819
Save loss: 16.634042702287434 Name: 1060_train_model.pth
step: 0 epoch: 1061 loss: 10.036620140075684 loss_input: 57.036865234375
step: 1000 epoch: 1061 loss: 16.704789141436795 loss_input: 82.69909228954639
step: 2000 epoch: 1061 loss: 16.635334028833096 loss_input: 82.48100223569855
step: 3000 epoch: 1061 loss: 16.606291070933025 loss_input: 82.17872039479678
step: 4000 epoch: 1061 loss: 16.556723842499288 loss_input: 82.0074480128747
step: 5000 epoch: 1061 loss: 16.61878568247494 loss_input: 82.21520070985802
step: 6000 epoch: 1061 loss: 16.624885813313075 loss_input: 82.18896571844145
step: 7000 epoch: 1061 loss: 16.614099322242613 loss_input: 82.45440863881889
step: 8000 epoch: 1061 loss: 16.609487789449894 loss_input: 82.20386767449968
step: 9000 epoch: 1061 loss: 16.60790426529006 loss_input: 82.10040186235817
step: 10000 epoch: 1061 loss: 16.60768326818079 loss_input: 82.23667618589704
step: 11000 epoch: 1061 loss: 16.60688760312901 loss_input: 82.26566284945332
step: 12000 epoch: 1061 loss: 16.610662895523998 loss_input: 82.24894921984854
step: 13000 epoch: 1061 loss: 16.633053262163497 loss_input: 82.36797182593527
step: 14000 epoch: 1061 loss: 16.637981823721695 loss_input: 82.24731919608979
step: 15000 epoch: 1061 loss: 16.627260365920673 loss_input: 82.15296192167918
Save loss: 16.637416368544102 Name: 1061_train_model.pth
step: 0 epoch: 1062 loss: 17.60711669921875 loss_input: 74.31231689453125
step: 1000 epoch: 1062 loss: 16.563979745744824 loss_input: 82.68884295636005
step: 2000 epoch: 1062 loss: 16.70765113496947 loss_input: 82.69074560010034
step: 3000 epoch: 1062 loss: 16.65410063084822 loss_input: 82.48274915848363
step: 4000 epoch: 1062 loss: 16.580680091093253 loss_input: 81.86836089744654
step: 5000 epoch: 1062 loss: 16.591234089874835 loss_input: 81.85670995683675
step: 6000 epoch: 1062 loss: 16.583732783367306 loss_input: 82.06079513481151
step: 7000 epoch: 1062 loss: 16.632816368912717 loss_input: 82.17840604941482
step: 8000 epoch: 1062 loss: 16.63345659907021 loss_input: 82.30663552920143
step: 9000 epoch: 1062 loss: 16.616167992171334 loss_input: 82.25738616255094
step: 10000 epoch: 1062 loss: 16.630169939939982 loss_input: 82.32898775552133
step: 11000 epoch: 1062 loss: 16.63114458118262 loss_input: 82.23451997860812
step: 12000 epoch: 1062 loss: 16.636374345472202 loss_input: 82.30871944143001
step: 13000 epoch: 1062 loss: 16.64286436872128 loss_input: 82.29089706159172
step: 14000 epoch: 1062 loss: 16.637111296100997 loss_input: 82.23160889683243
step: 15000 epoch: 1062 loss: 16.632276552707765 loss_input: 82.24747302409656
Save loss: 16.633477741599084 Name: 1062_train_model.pth
step: 0 epoch: 1063 loss: 17.61347770690918 loss_input: 104.6357421875
step: 1000 epoch: 1063 loss: 16.68419169331645 loss_input: 82.8415802947053
step: 2000 epoch: 1063 loss: 16.64670065246422 loss_input: 82.26618310107642
step: 3000 epoch: 1063 loss: 16.665587139701653 loss_input: 82.17986373113258
step: 4000 epoch: 1063 loss: 16.642326112092658 loss_input: 81.96962202015415
step: 5000 epoch: 1063 loss: 16.60501932387494 loss_input: 82.20724440317015
step: 6000 epoch: 1063 loss: 16.588032367884605 loss_input: 82.12513235302254
step: 7000 epoch: 1063 loss: 16.588936968337396 loss_input: 82.23715723890727
step: 8000 epoch: 1063 loss: 16.617775886152078 loss_input: 82.35993439357678
step: 9000 epoch: 1063 loss: 16.635196683115726 loss_input: 82.24937658158426
step: 10000 epoch: 1063 loss: 16.64168180376157 loss_input: 82.27964622361006
step: 11000 epoch: 1063 loss: 16.646880535849245 loss_input: 82.30167058454558
step: 12000 epoch: 1063 loss: 16.634907213412426 loss_input: 82.25308454621783
step: 13000 epoch: 1063 loss: 16.639831016837757 loss_input: 82.24259385959193
step: 14000 epoch: 1063 loss: 16.62698515734752 loss_input: 82.16482126974189
step: 15000 epoch: 1063 loss: 16.632932501994375 loss_input: 82.19548188161154
Save loss: 16.63236365607381 Name: 1063_train_model.pth
step: 0 epoch: 1064 loss: 22.61667823791504 loss_input: 60.7843017578125
step: 1000 epoch: 1064 loss: 16.64585430876954 loss_input: 82.39784257490557
step: 2000 epoch: 1064 loss: 16.641986684403616 loss_input: 82.03533551217555
step: 3000 epoch: 1064 loss: 16.532062962388085 loss_input: 81.99402020208758
step: 4000 epoch: 1064 loss: 16.587271694659115 loss_input: 82.00681709760548
step: 5000 epoch: 1064 loss: 16.567490288982913 loss_input: 82.20480579177134
step: 6000 epoch: 1064 loss: 16.596693238542986 loss_input: 82.24422981301619
step: 7000 epoch: 1064 loss: 16.628092080963423 loss_input: 82.43819624191522
step: 8000 epoch: 1064 loss: 16.65180885635336 loss_input: 82.51994075996848
step: 9000 epoch: 1064 loss: 16.66251194544626 loss_input: 82.41307348771143
step: 10000 epoch: 1064 loss: 16.649015243548583 loss_input: 82.48398266862768
step: 11000 epoch: 1064 loss: 16.65605350507735 loss_input: 82.4765762649507
step: 12000 epoch: 1064 loss: 16.660500992219575 loss_input: 82.41423527612933
step: 13000 epoch: 1064 loss: 16.656320054644247 loss_input: 82.31754432257465
step: 14000 epoch: 1064 loss: 16.652137841150495 loss_input: 82.29418774862202
step: 15000 epoch: 1064 loss: 16.650411556660433 loss_input: 82.28044244478754
Save loss: 16.634369425803424 Name: 1064_train_model.pth
step: 0 epoch: 1065 loss: 14.727580070495605 loss_input: 69.65582275390625
step: 1000 epoch: 1065 loss: 16.68848738875184 loss_input: 84.40815343104161
step: 2000 epoch: 1065 loss: 16.69332024814009 loss_input: 83.65380548251265
step: 3000 epoch: 1065 loss: 16.74088165339769 loss_input: 82.73278623515469
step: 4000 epoch: 1065 loss: 16.691966444633806 loss_input: 82.75840724572275
step: 5000 epoch: 1065 loss: 16.689424051186773 loss_input: 82.759565664992
step: 6000 epoch: 1065 loss: 16.63335910723063 loss_input: 82.28035079683985
step: 7000 epoch: 1065 loss: 16.620125391060686 loss_input: 82.1285186885272
step: 8000 epoch: 1065 loss: 16.62351195762462 loss_input: 82.28605636789298
step: 9000 epoch: 1065 loss: 16.62841818452133 loss_input: 82.45439293230656
step: 10000 epoch: 1065 loss: 16.631952673396448 loss_input: 82.39848867286123
step: 11000 epoch: 1065 loss: 16.640424776506386 loss_input: 82.42310023596043
step: 12000 epoch: 1065 loss: 16.653928380719364 loss_input: 82.35362724391294
step: 13000 epoch: 1065 loss: 16.656501033912942 loss_input: 82.26639415751823
step: 14000 epoch: 1065 loss: 16.647628386730247 loss_input: 82.22112280327494
step: 15000 epoch: 1065 loss: 16.646828303773532 loss_input: 82.17375741845075
Save loss: 16.650557595700025 Name: 1065_train_model.pth
step: 0 epoch: 1066 loss: 15.05076789855957 loss_input: 92.14306640625
step: 1000 epoch: 1066 loss: 16.50844602937346 loss_input: 81.46461546266234
step: 2000 epoch: 1066 loss: 16.674762661489233 loss_input: 82.60112088731024
step: 3000 epoch: 1066 loss: 16.631295461727753 loss_input: 82.52529300129123
step: 4000 epoch: 1066 loss: 16.62448014107027 loss_input: 82.75618261267202
step: 5000 epoch: 1066 loss: 16.6409657297552 loss_input: 82.90914535598739
step: 6000 epoch: 1066 loss: 16.60732567673861 loss_input: 82.68117781151058
step: 7000 epoch: 1066 loss: 16.608230878993147 loss_input: 82.54779831257365
step: 8000 epoch: 1066 loss: 16.599624757632036 loss_input: 82.31054318283456
step: 9000 epoch: 1066 loss: 16.605018249287525 loss_input: 82.40853689596555
step: 10000 epoch: 1066 loss: 16.599351688309106 loss_input: 82.45399759950763
step: 11000 epoch: 1066 loss: 16.628366450268228 loss_input: 82.43530063717574
step: 12000 epoch: 1066 loss: 16.63605927165454 loss_input: 82.44758060627923
step: 13000 epoch: 1066 loss: 16.6345480135171 loss_input: 82.38828737370262
step: 14000 epoch: 1066 loss: 16.632701789930067 loss_input: 82.33656177530288
step: 15000 epoch: 1066 loss: 16.642527729342376 loss_input: 82.2846406064791
Save loss: 16.637526875644923 Name: 1066_train_model.pth
step: 0 epoch: 1067 loss: 15.44165325164795 loss_input: 105.0235595703125
step: 1000 epoch: 1067 loss: 16.517954692497597 loss_input: 82.07183825695789
step: 2000 epoch: 1067 loss: 16.52778306595985 loss_input: 82.22309893027119
step: 3000 epoch: 1067 loss: 16.471191189520283 loss_input: 82.09871371814864
step: 4000 epoch: 1067 loss: 16.520837300183086 loss_input: 82.56060006909894
step: 5000 epoch: 1067 loss: 16.53032884796103 loss_input: 82.42343677260642
step: 6000 epoch: 1067 loss: 16.539290541153196 loss_input: 82.23252752959142
step: 7000 epoch: 1067 loss: 16.547423831395907 loss_input: 82.11403884963978
step: 8000 epoch: 1067 loss: 16.589986702037326 loss_input: 82.22840363948647
step: 9000 epoch: 1067 loss: 16.614866864242867 loss_input: 82.32916503879126
step: 10000 epoch: 1067 loss: 16.606201887869762 loss_input: 82.12738745656684
step: 11000 epoch: 1067 loss: 16.5951040857825 loss_input: 82.12924920865682
step: 12000 epoch: 1067 loss: 16.589568317736124 loss_input: 81.9928734881631
step: 13000 epoch: 1067 loss: 16.60121366589099 loss_input: 82.01677913297534
step: 14000 epoch: 1067 loss: 16.628912747242598 loss_input: 82.10407687728639
step: 15000 epoch: 1067 loss: 16.637929384696168 loss_input: 82.21151771848022
Save loss: 16.636173903629185 Name: 1067_train_model.pth
step: 0 epoch: 1068 loss: 19.934127807617188 loss_input: 76.94964599609375
step: 1000 epoch: 1068 loss: 16.398242509329354 loss_input: 81.451280713915
step: 2000 epoch: 1068 loss: 16.484074211311246 loss_input: 81.23716297368894
step: 3000 epoch: 1068 loss: 16.581593360951725 loss_input: 82.25821948917418
step: 4000 epoch: 1068 loss: 16.60103269059549 loss_input: 82.14698206856977
step: 5000 epoch: 1068 loss: 16.621400549230135 loss_input: 82.17983574915876
step: 6000 epoch: 1068 loss: 16.66090514063537 loss_input: 82.48499490197271
step: 7000 epoch: 1068 loss: 16.686114968580615 loss_input: 82.55804574130315
step: 8000 epoch: 1068 loss: 16.68536894736536 loss_input: 82.41875719514553
step: 9000 epoch: 1068 loss: 16.659284450573175 loss_input: 82.2514253448293
step: 10000 epoch: 1068 loss: 16.639997943164516 loss_input: 82.28955234359377
step: 11000 epoch: 1068 loss: 16.64193004190396 loss_input: 82.33122073796736
step: 12000 epoch: 1068 loss: 16.641944143218048 loss_input: 82.34686165882661
step: 13000 epoch: 1068 loss: 16.638230162944694 loss_input: 82.32889719026271
step: 14000 epoch: 1068 loss: 16.650720454209125 loss_input: 82.33485838832382
step: 15000 epoch: 1068 loss: 16.645591694182947 loss_input: 82.312810998147
Save loss: 16.631893612653016 Name: 1068_train_model.pth
step: 0 epoch: 1069 loss: 8.20248794555664 loss_input: 69.3900146484375
step: 1000 epoch: 1069 loss: 16.641353176547575 loss_input: 82.0984189150693
step: 2000 epoch: 1069 loss: 16.634900366408058 loss_input: 82.38934554939327
step: 3000 epoch: 1069 loss: 16.616873932138994 loss_input: 82.26116998272711
step: 4000 epoch: 1069 loss: 16.575035408776095 loss_input: 81.8607721574245
step: 5000 epoch: 1069 loss: 16.57852343773036 loss_input: 81.71364023093818
step: 6000 epoch: 1069 loss: 16.582888494191856 loss_input: 81.94870563158094
step: 7000 epoch: 1069 loss: 16.593079101220315 loss_input: 81.96617311505723
step: 8000 epoch: 1069 loss: 16.619151477083058 loss_input: 82.07340979960513
step: 9000 epoch: 1069 loss: 16.62688128006033 loss_input: 82.1020001548222
step: 10000 epoch: 1069 loss: 16.64459029916119 loss_input: 82.22075648270133
step: 11000 epoch: 1069 loss: 16.6475282906294 loss_input: 82.22400794485658
step: 12000 epoch: 1069 loss: 16.64693748759246 loss_input: 82.21102492557587
step: 13000 epoch: 1069 loss: 16.636026460898307 loss_input: 82.18156213043709
step: 14000 epoch: 1069 loss: 16.6272987729864 loss_input: 82.19621932609584
step: 15000 epoch: 1069 loss: 16.63604331494935 loss_input: 82.24604119409618
Save loss: 16.633908300876616 Name: 1069_train_model.pth
step: 0 epoch: 1070 loss: 16.69659996032715 loss_input: 142.8323974609375
step: 1000 epoch: 1070 loss: 16.406137985663932 loss_input: 81.6249020754636
step: 2000 epoch: 1070 loss: 16.48766806267429 loss_input: 81.94929765951984
step: 3000 epoch: 1070 loss: 16.51476842250398 loss_input: 82.18451997527517
step: 4000 epoch: 1070 loss: 16.52642564009619 loss_input: 82.08606805207877
step: 5000 epoch: 1070 loss: 16.527304564445313 loss_input: 81.83221817550552
step: 6000 epoch: 1070 loss: 16.567359338103085 loss_input: 82.05435359162145
step: 7000 epoch: 1070 loss: 16.614372678457574 loss_input: 82.3670014914164
step: 8000 epoch: 1070 loss: 16.629726299508306 loss_input: 82.60526775583716
step: 9000 epoch: 1070 loss: 16.61557257001949 loss_input: 82.38453685674465
step: 10000 epoch: 1070 loss: 16.625890760442733 loss_input: 82.45051558076614
step: 11000 epoch: 1070 loss: 16.622217430416946 loss_input: 82.35433253959334
step: 12000 epoch: 1070 loss: 16.636319819137203 loss_input: 82.33254206151766
step: 13000 epoch: 1070 loss: 16.63495371074147 loss_input: 82.31310076039072
step: 14000 epoch: 1070 loss: 16.643388093705603 loss_input: 82.2602896920937
step: 15000 epoch: 1070 loss: 16.638755857098158 loss_input: 82.26762295436815
Save loss: 16.627415791645646 Name: 1070_train_model.pth
step: 0 epoch: 1071 loss: 19.44625473022461 loss_input: 84.564453125
step: 1000 epoch: 1071 loss: 16.648695993852186 loss_input: 81.5502235801308
step: 2000 epoch: 1071 loss: 16.630709167005776 loss_input: 82.28963182593273
step: 3000 epoch: 1071 loss: 16.62037554012859 loss_input: 81.98027354325902
step: 4000 epoch: 1071 loss: 16.60426876634933 loss_input: 81.86968893457788
step: 5000 epoch: 1071 loss: 16.623734389417436 loss_input: 81.92089150529269
step: 6000 epoch: 1071 loss: 16.575645603471706 loss_input: 81.97741838559133
step: 7000 epoch: 1071 loss: 16.571643772201526 loss_input: 82.03333380875834
step: 8000 epoch: 1071 loss: 16.575864672318144 loss_input: 82.06549975473037
step: 9000 epoch: 1071 loss: 16.579554514412933 loss_input: 82.22633951587845
step: 10000 epoch: 1071 loss: 16.582224501691904 loss_input: 82.19777788456506
step: 11000 epoch: 1071 loss: 16.584014425320188 loss_input: 82.14147660160333
step: 12000 epoch: 1071 loss: 16.590449520413372 loss_input: 82.05768940641666
step: 13000 epoch: 1071 loss: 16.608519214216116 loss_input: 82.08111753479149
step: 14000 epoch: 1071 loss: 16.612440499890898 loss_input: 82.07078545816063
step: 15000 epoch: 1071 loss: 16.62329887760519 loss_input: 82.19686729543551
Save loss: 16.633357035934925 Name: 1071_train_model.pth
step: 0 epoch: 1072 loss: 22.292224884033203 loss_input: 77.93731689453125
step: 1000 epoch: 1072 loss: 16.52486789333713 loss_input: 81.63229385360734
step: 2000 epoch: 1072 loss: 16.416734787299 loss_input: 81.37536807157944
step: 3000 epoch: 1072 loss: 16.44251818594953 loss_input: 82.06777682585623
step: 4000 epoch: 1072 loss: 16.524810868243936 loss_input: 82.38833887414377
step: 5000 epoch: 1072 loss: 16.5368203916113 loss_input: 82.62118194036974
step: 6000 epoch: 1072 loss: 16.547876299907518 loss_input: 82.67094534537213
step: 7000 epoch: 1072 loss: 16.552367507994234 loss_input: 82.55812528490628
step: 8000 epoch: 1072 loss: 16.57118206494392 loss_input: 82.49672835237637
step: 9000 epoch: 1072 loss: 16.579621947085826 loss_input: 82.3950354257513
step: 10000 epoch: 1072 loss: 16.592740260223284 loss_input: 82.39397282195608
step: 11000 epoch: 1072 loss: 16.610782044376638 loss_input: 82.35950326204365
step: 12000 epoch: 1072 loss: 16.633926685666374 loss_input: 82.40847817226982
step: 13000 epoch: 1072 loss: 16.642275996377272 loss_input: 82.3759034386177
step: 14000 epoch: 1072 loss: 16.64274956367585 loss_input: 82.32256405251884
step: 15000 epoch: 1072 loss: 16.632205464555284 loss_input: 82.30290442388477
Save loss: 16.630610214293004 Name: 1072_train_model.pth
step: 0 epoch: 1073 loss: 19.70492935180664 loss_input: 75.80633544921875
step: 1000 epoch: 1073 loss: 16.416333506276438 loss_input: 80.72086568216939
step: 2000 epoch: 1073 loss: 16.466628237881107 loss_input: 81.23923905356892
step: 3000 epoch: 1073 loss: 16.529107261442892 loss_input: 81.2229169053024
step: 4000 epoch: 1073 loss: 16.542152232451606 loss_input: 81.46008198572915
step: 5000 epoch: 1073 loss: 16.53129657757947 loss_input: 81.63683825734853
step: 6000 epoch: 1073 loss: 16.56632234449884 loss_input: 81.99994004295182
step: 7000 epoch: 1073 loss: 16.5788569188496 loss_input: 81.93890142856266
step: 8000 epoch: 1073 loss: 16.572795134606235 loss_input: 81.95884237919967
step: 9000 epoch: 1073 loss: 16.58366885059159 loss_input: 82.00367663412833
step: 10000 epoch: 1073 loss: 16.59411573331364 loss_input: 81.9453669623272
step: 11000 epoch: 1073 loss: 16.598109580551363 loss_input: 81.9904932293087
step: 12000 epoch: 1073 loss: 16.610429239614778 loss_input: 82.10084845326203
step: 13000 epoch: 1073 loss: 16.60799952672359 loss_input: 82.17259522986663
step: 14000 epoch: 1073 loss: 16.618397653617993 loss_input: 82.13160844520453
step: 15000 epoch: 1073 loss: 16.63121680642484 loss_input: 82.1984968091971
Save loss: 16.626577444732188 Name: 1073_train_model.pth
step: 0 epoch: 1074 loss: 18.980220794677734 loss_input: 71.22772216796875
step: 1000 epoch: 1074 loss: 16.632225359117353 loss_input: 83.31511079252779
step: 2000 epoch: 1074 loss: 16.496268297183043 loss_input: 82.24000283183604
step: 3000 epoch: 1074 loss: 16.54050249832545 loss_input: 81.95536101003441
step: 4000 epoch: 1074 loss: 16.54943829266854 loss_input: 81.74210185856856
step: 5000 epoch: 1074 loss: 16.566156393860464 loss_input: 81.80077325599333
step: 6000 epoch: 1074 loss: 16.55192422743659 loss_input: 81.8170489549756
step: 7000 epoch: 1074 loss: 16.58431354592041 loss_input: 82.0980700092509
step: 8000 epoch: 1074 loss: 16.573450947207164 loss_input: 82.1469207904798
step: 9000 epoch: 1074 loss: 16.585065664099186 loss_input: 82.23017558324683
step: 10000 epoch: 1074 loss: 16.577373158680224 loss_input: 82.23123236382416
step: 11000 epoch: 1074 loss: 16.573001119074263 loss_input: 82.12883550003977
step: 12000 epoch: 1074 loss: 16.58122329530731 loss_input: 82.11954645971072
step: 13000 epoch: 1074 loss: 16.600959788468348 loss_input: 82.16347003036275
step: 14000 epoch: 1074 loss: 16.60297273033117 loss_input: 82.18626473231126
step: 15000 epoch: 1074 loss: 16.607780283573874 loss_input: 82.14821884188841
Save loss: 16.63354548870027 Name: 1074_train_model.pth
step: 0 epoch: 1075 loss: 13.772048950195312 loss_input: 72.4010009765625
step: 1000 epoch: 1075 loss: 16.289486501124 loss_input: 81.41888378955029
step: 2000 epoch: 1075 loss: 16.496945530816593 loss_input: 81.81296854624446
step: 3000 epoch: 1075 loss: 16.538037830811664 loss_input: 82.10220024721578
step: 4000 epoch: 1075 loss: 16.5486101871787 loss_input: 82.05572949108914
step: 5000 epoch: 1075 loss: 16.536303623369566 loss_input: 82.2728498733847
step: 6000 epoch: 1075 loss: 16.536312157184835 loss_input: 82.20020973473822
step: 7000 epoch: 1075 loss: 16.57043332557068 loss_input: 82.28403690307077
step: 8000 epoch: 1075 loss: 16.581753970205657 loss_input: 82.22712219871441
step: 9000 epoch: 1075 loss: 16.604580359966963 loss_input: 82.30448692685405
step: 10000 epoch: 1075 loss: 16.61940345841877 loss_input: 82.29715284086825
step: 11000 epoch: 1075 loss: 16.631738894809953 loss_input: 82.45538535359535
step: 12000 epoch: 1075 loss: 16.636252983919075 loss_input: 82.40478314225767
step: 13000 epoch: 1075 loss: 16.625983752541227 loss_input: 82.44763066227478
step: 14000 epoch: 1075 loss: 16.624409716637267 loss_input: 82.33831271835075
step: 15000 epoch: 1075 loss: 16.627900626657645 loss_input: 82.28954509315264
Save loss: 16.62878291282058 Name: 1075_train_model.pth
step: 0 epoch: 1076 loss: 16.72548484802246 loss_input: 109.1297607421875
step: 1000 epoch: 1076 loss: 16.588878380073297 loss_input: 82.93376557524506
step: 2000 epoch: 1076 loss: 16.68701114206538 loss_input: 82.85054295972131
step: 3000 epoch: 1076 loss: 16.714170900355654 loss_input: 82.65267599602215
step: 4000 epoch: 1076 loss: 16.682382417243588 loss_input: 82.59557943766012
step: 5000 epoch: 1076 loss: 16.67507684080631 loss_input: 82.3655150073501
step: 6000 epoch: 1076 loss: 16.681839920365597 loss_input: 82.37534131274344
step: 7000 epoch: 1076 loss: 16.656878814375787 loss_input: 82.35840474065925
step: 8000 epoch: 1076 loss: 16.68145291281706 loss_input: 82.37026043436465
step: 9000 epoch: 1076 loss: 16.644563864395494 loss_input: 82.21854514808685
step: 10000 epoch: 1076 loss: 16.640956943529318 loss_input: 82.17236326904418
step: 11000 epoch: 1076 loss: 16.63243380007706 loss_input: 82.07650592888217
step: 12000 epoch: 1076 loss: 16.622809824432178 loss_input: 82.13838989064551
step: 13000 epoch: 1076 loss: 16.626650220402826 loss_input: 82.27269714885965
step: 14000 epoch: 1076 loss: 16.63235036237488 loss_input: 82.31390787150109
step: 15000 epoch: 1076 loss: 16.62905326049158 loss_input: 82.22935970121523
Save loss: 16.63007423532009 Name: 1076_train_model.pth
step: 0 epoch: 1077 loss: 17.321216583251953 loss_input: 83.3995361328125
step: 1000 epoch: 1077 loss: 16.531956164867847 loss_input: 81.25546450619693
step: 2000 epoch: 1077 loss: 16.51126768278039 loss_input: 81.21724288002483
step: 3000 epoch: 1077 loss: 16.546200574775092 loss_input: 81.62071564117299
step: 4000 epoch: 1077 loss: 16.51735750099207 loss_input: 81.89393880777853
step: 5000 epoch: 1077 loss: 16.524761049586804 loss_input: 81.82166697763962
step: 6000 epoch: 1077 loss: 16.55283267794798 loss_input: 81.94653787153638
step: 7000 epoch: 1077 loss: 16.559291042612035 loss_input: 81.99093415644863
step: 8000 epoch: 1077 loss: 16.571778007275373 loss_input: 82.09352444225722
step: 9000 epoch: 1077 loss: 16.588784782744266 loss_input: 82.19527741546513
step: 10000 epoch: 1077 loss: 16.601650581254017 loss_input: 82.18007169009661
step: 11000 epoch: 1077 loss: 16.595632876410136 loss_input: 82.11397921788715
step: 12000 epoch: 1077 loss: 16.59945460093358 loss_input: 82.12285859723278
step: 13000 epoch: 1077 loss: 16.600266121064834 loss_input: 82.16732453592061
step: 14000 epoch: 1077 loss: 16.62036108027253 loss_input: 82.29741369704759
step: 15000 epoch: 1077 loss: 16.63480806771886 loss_input: 82.27131170177847
Save loss: 16.635085331961513 Name: 1077_train_model.pth
step: 0 epoch: 1078 loss: 14.773758888244629 loss_input: 56.32415771484375
step: 1000 epoch: 1078 loss: 16.42097964605966 loss_input: 81.3215920432107
step: 2000 epoch: 1078 loss: 16.448019835783327 loss_input: 81.51498280400816
step: 3000 epoch: 1078 loss: 16.40892908995011 loss_input: 81.43695259539138
step: 4000 epoch: 1078 loss: 16.448057877424507 loss_input: 81.62637625810832
step: 5000 epoch: 1078 loss: 16.605297049387197 loss_input: 82.330356853434
step: 6000 epoch: 1078 loss: 16.60772367378728 loss_input: 82.19874890846643
step: 7000 epoch: 1078 loss: 16.58930270087121 loss_input: 82.11097199214286
step: 8000 epoch: 1078 loss: 16.586766509231428 loss_input: 82.09047107484368
step: 9000 epoch: 1078 loss: 16.62033274488679 loss_input: 82.20082503040027
step: 10000 epoch: 1078 loss: 16.642704192977728 loss_input: 82.30971661134667
step: 11000 epoch: 1078 loss: 16.645693623535244 loss_input: 82.25677128495244
step: 12000 epoch: 1078 loss: 16.62878641040412 loss_input: 82.2703198602265
step: 13000 epoch: 1078 loss: 16.618562815272067 loss_input: 82.19306297821592
step: 14000 epoch: 1078 loss: 16.628919180746838 loss_input: 82.23554670847311
step: 15000 epoch: 1078 loss: 16.635508426180934 loss_input: 82.25728268506057
Save loss: 16.638216439798473 Name: 1078_train_model.pth
step: 0 epoch: 1079 loss: 13.559235572814941 loss_input: 64.673583984375
step: 1000 epoch: 1079 loss: 16.625091360760972 loss_input: 83.2654328703523
step: 2000 epoch: 1079 loss: 16.708546052272172 loss_input: 82.97351235881082
step: 3000 epoch: 1079 loss: 16.656919158565646 loss_input: 82.67512007309412
step: 4000 epoch: 1079 loss: 16.586858609234564 loss_input: 82.30928707337326
step: 5000 epoch: 1079 loss: 16.617133022808737 loss_input: 82.25147023817892
step: 6000 epoch: 1079 loss: 16.60548284065324 loss_input: 82.22640749653704
step: 7000 epoch: 1079 loss: 16.60828866350397 loss_input: 82.24469513737156
step: 8000 epoch: 1079 loss: 16.61013020618068 loss_input: 82.31886587296705
step: 9000 epoch: 1079 loss: 16.610211347900567 loss_input: 82.40039592090679
step: 10000 epoch: 1079 loss: 16.598288118880983 loss_input: 82.26403778689514
step: 11000 epoch: 1079 loss: 16.603899146413685 loss_input: 82.27876491874318
step: 12000 epoch: 1079 loss: 16.609794566675223 loss_input: 82.24042810987268
step: 13000 epoch: 1079 loss: 16.622231696259195 loss_input: 82.1984129554262
step: 14000 epoch: 1079 loss: 16.613054845428632 loss_input: 82.14507575369198
step: 15000 epoch: 1079 loss: 16.616110128288657 loss_input: 82.22213711143216
Save loss: 16.63207533839345 Name: 1079_train_model.pth
step: 0 epoch: 1080 loss: 23.138999938964844 loss_input: 83.5308837890625
step: 1000 epoch: 1080 loss: 16.61423198231212 loss_input: 82.753615890945
step: 2000 epoch: 1080 loss: 16.68224589530377 loss_input: 83.37946703528118
step: 3000 epoch: 1080 loss: 16.691384917058695 loss_input: 83.37747423222207
step: 4000 epoch: 1080 loss: 16.65288567763512 loss_input: 82.73844629357798
step: 5000 epoch: 1080 loss: 16.610686352433646 loss_input: 82.40927256443243
step: 6000 epoch: 1080 loss: 16.603698535792688 loss_input: 82.31108000961429
step: 7000 epoch: 1080 loss: 16.634010593783188 loss_input: 82.32455243384595
step: 8000 epoch: 1080 loss: 16.62302182165031 loss_input: 82.18086186797451
step: 9000 epoch: 1080 loss: 16.599389456229904 loss_input: 81.9713471031973
step: 10000 epoch: 1080 loss: 16.613905506507837 loss_input: 82.05137503827743
step: 11000 epoch: 1080 loss: 16.612676683506873 loss_input: 82.05047226819046
step: 12000 epoch: 1080 loss: 16.620042949624384 loss_input: 82.0677845327668
step: 13000 epoch: 1080 loss: 16.62419536155734 loss_input: 82.15017113129218
step: 14000 epoch: 1080 loss: 16.632305237899704 loss_input: 82.2143154194692
step: 15000 epoch: 1080 loss: 16.621543799437774 loss_input: 82.19177192793266
Save loss: 16.625860629945993 Name: 1080_train_model.pth
step: 0 epoch: 1081 loss: 17.933929443359375 loss_input: 69.91021728515625
step: 1000 epoch: 1081 loss: 16.626540365514458 loss_input: 83.16142134232955
step: 2000 epoch: 1081 loss: 16.5249639849017 loss_input: 82.71433317154899
step: 3000 epoch: 1081 loss: 16.560332166556716 loss_input: 82.53879993289998
step: 4000 epoch: 1081 loss: 16.578370913539164 loss_input: 82.44932905872653
step: 5000 epoch: 1081 loss: 16.620094118249867 loss_input: 82.67155447084411
step: 6000 epoch: 1081 loss: 16.62156432550205 loss_input: 82.47618846769353
step: 7000 epoch: 1081 loss: 16.639232386624602 loss_input: 82.48492810876348
step: 8000 epoch: 1081 loss: 16.634883840267932 loss_input: 82.4494503418396
step: 9000 epoch: 1081 loss: 16.6023489822614 loss_input: 82.36527654560734
step: 10000 epoch: 1081 loss: 16.626380639914906 loss_input: 82.37363163708044
step: 11000 epoch: 1081 loss: 16.611292175830098 loss_input: 82.23209107787964
step: 12000 epoch: 1081 loss: 16.62581397171726 loss_input: 82.22485254422972
step: 13000 epoch: 1081 loss: 16.612974031917243 loss_input: 82.18826046771943
step: 14000 epoch: 1081 loss: 16.624125782927994 loss_input: 82.16481058499309
step: 15000 epoch: 1081 loss: 16.62623370463797 loss_input: 82.20698113533967
Save loss: 16.623974709555508 Name: 1081_train_model.pth
step: 0 epoch: 1082 loss: 21.567916870117188 loss_input: 139.40972900390625
step: 1000 epoch: 1082 loss: 16.536841829816304 loss_input: 81.75783243855754
step: 2000 epoch: 1082 loss: 16.492077728201902 loss_input: 81.6996622233317
step: 3000 epoch: 1082 loss: 16.54963541944517 loss_input: 81.93774015432356
step: 4000 epoch: 1082 loss: 16.55090528290083 loss_input: 81.79172592716078
step: 5000 epoch: 1082 loss: 16.539583731212513 loss_input: 81.89611107319551
step: 6000 epoch: 1082 loss: 16.566603984143054 loss_input: 81.8069455871938
step: 7000 epoch: 1082 loss: 16.58972569258447 loss_input: 82.11401341905119
step: 8000 epoch: 1082 loss: 16.5955387249453 loss_input: 82.12430528550874
step: 9000 epoch: 1082 loss: 16.597422154317126 loss_input: 81.92416327927539
step: 10000 epoch: 1082 loss: 16.604458808422137 loss_input: 81.96981969893831
step: 11000 epoch: 1082 loss: 16.60798447496858 loss_input: 81.98278247121355
step: 12000 epoch: 1082 loss: 16.593387176231406 loss_input: 82.04735507779137
step: 13000 epoch: 1082 loss: 16.613253384826276 loss_input: 82.106145122438
step: 14000 epoch: 1082 loss: 16.62558379046245 loss_input: 82.20778565603992
step: 15000 epoch: 1082 loss: 16.627321207239646 loss_input: 82.20892407628554
Save loss: 16.631960261106492 Name: 1082_train_model.pth
step: 0 epoch: 1083 loss: 17.255285263061523 loss_input: 57.4210205078125
step: 1000 epoch: 1083 loss: 16.525247396169963 loss_input: 82.44789140303057
step: 2000 epoch: 1083 loss: 16.617970774734932 loss_input: 83.01920444806893
step: 3000 epoch: 1083 loss: 16.515207805620832 loss_input: 82.46042324677582
step: 4000 epoch: 1083 loss: 16.58080940263267 loss_input: 81.94050407075966
step: 5000 epoch: 1083 loss: 16.59822174626049 loss_input: 81.98008121764319
step: 6000 epoch: 1083 loss: 16.600828422186595 loss_input: 82.23033128104952
step: 7000 epoch: 1083 loss: 16.588093330512436 loss_input: 82.05305930591109
step: 8000 epoch: 1083 loss: 16.616033410984283 loss_input: 82.31014392549076
step: 9000 epoch: 1083 loss: 16.616889486364784 loss_input: 82.20737203922872
step: 10000 epoch: 1083 loss: 16.6170992140841 loss_input: 82.23378004606766
step: 11000 epoch: 1083 loss: 16.636027080255708 loss_input: 82.27281605769326
step: 12000 epoch: 1083 loss: 16.625649756088364 loss_input: 82.17030639495863
step: 13000 epoch: 1083 loss: 16.62805261018505 loss_input: 82.15593389507147
step: 14000 epoch: 1083 loss: 16.639656060525805 loss_input: 82.2312498613729
step: 15000 epoch: 1083 loss: 16.63454226577817 loss_input: 82.23596684944312
Save loss: 16.622397569611667 Name: 1083_train_model.pth
